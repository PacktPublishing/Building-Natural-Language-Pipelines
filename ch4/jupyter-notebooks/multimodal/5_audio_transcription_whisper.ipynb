{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "378d6f72",
   "metadata": {},
   "source": [
    "**ðŸ”§ Setup Required**: Before running this notebook, please follow the [setup instructions](../../README.md#setup-instructions) to configure your environment and API keys."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005eaea3",
   "metadata": {},
   "source": [
    "# Audio Transcription with Whisper for Multimodal RAG\n",
    "\n",
    "This notebook demonstrates how to add audio transcription capabilities to multimodal RAG pipelines using OpenAI's Whisper.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- How to transcribe audio files using Whisper (Remote and Local)\n",
    "- Building audio transcription pipelines\n",
    "- Creating a complete multimodal pipeline with text, images, and audio\n",
    "- Indexing transcribed audio alongside other content\n",
    "\n",
    "## Whisper Options\n",
    "\n",
    "Haystack supports two Whisper implementations:\n",
    "\n",
    "1. **Remote Whisper** (OpenAI API):\n",
    "   - Easier to use\n",
    "   - Requires OpenAI API key\n",
    "   - API costs apply\n",
    "   - No local setup needed\n",
    "\n",
    "2. **Local Whisper**:\n",
    "   - Free to use\n",
    "   - Requires local installation\n",
    "   - More privacy\n",
    "   - Requires more computational resources\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "- Meeting transcription and analysis\n",
    "- Podcast search and Q&A\n",
    "- Voice command processing\n",
    "- Accessibility features\n",
    "- Converting speech to searchable text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961eac6a",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1571a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(\"../../.env\")\n",
    "\n",
    "# Set up OpenAI API key\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OpenAI API key: \")\n",
    "\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\"5_audio_transcription_whisper.ipynb\")) if os.path.exists(\"5_audio_transcription_whisper.ipynb\") else os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712a8c33",
   "metadata": {},
   "source": [
    "## Option 1: Remote Whisper (OpenAI API)\n",
    "\n",
    "This is the easiest option - it uses OpenAI's hosted Whisper API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05bd7979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.audio import RemoteWhisperTranscriber \n",
    "from haystack.dataclasses import ByteStream\n",
    "from haystack.utils import Secret\n",
    "\n",
    "# Initialize remote transcriber\n",
    "remote_transcriber = RemoteWhisperTranscriber(api_key=Secret.from_env_var(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624fd1c9",
   "metadata": {},
   "source": [
    "## Option 2: Local Whisper\n",
    "\n",
    "To use Local Whisper, you need to install the `openai-whisper` package:\n",
    "\n",
    "```bash\n",
    "pip install openai-whisper\n",
    "```\n",
    "\n",
    "**Note**: Local Whisper downloads models from the internet on first use. If you encounter SSL certificate errors, you may need to fix SSL certificates on your system or use Remote Whisper instead.\n",
    "\n",
    "### Fixing SSL Certificate Issues (macOS)\n",
    "\n",
    "If you encounter SSL certificate verification errors, try one of these solutions:\n",
    "\n",
    "1. **Run the certificates install script** (recommended for macOS):\n",
    "   ```bash\n",
    "   /Applications/Python\\ 3.12/Install\\ Certificates.command\n",
    "   ```\n",
    "\n",
    "2. **Or manually bypass SSL verification** (not recommended for production):\n",
    "   ```python\n",
    "   import ssl\n",
    "   ssl._create_default_https_context = ssl._create_unverified_context\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cce18e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Whisper transcriber can be initialized with LocalWhisperTranscriber()\n",
      "Available models: tiny, base, small, medium, large\n",
      "Trade-off: Larger models are more accurate but slower\n",
      "\n",
      "Note: Commented out by default due to model download requirements.\n",
      "Uncomment the code above to use Local Whisper after resolving SSL issues.\n"
     ]
    }
   ],
   "source": [
    "# Uncomment to use Local Whisper (requires openai-whisper package and model download)\n",
    "# from haystack.components.audio import LocalWhisperTranscriber\n",
    "\n",
    "# Initialize local transcriber (requires local Whisper installation)\n",
    "# Available models: tiny, base, small, medium, large\n",
    "# local_transcriber = LocalWhisperTranscriber(model=\"base\")\n",
    "# local_transcriber.warm_up()  # This will download the model on first run\n",
    "\n",
    "# Example usage:\n",
    "# result = local_transcriber.run(sources=[\"data_for_indexing/harvard.wav\"])\n",
    "# print(result[\"documents\"][0].content)\n",
    "\n",
    "print(\"Local Whisper transcriber can be initialized with LocalWhisperTranscriber()\")\n",
    "print(\"Available models: tiny, base, small, medium, large\")\n",
    "print(\"Trade-off: Larger models are more accurate but slower\")\n",
    "print(\"\\nNote: Commented out by default due to model download requirements.\")\n",
    "print(\"Uncomment the code above to use Local Whisper after resolving SSL issues.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572762d1",
   "metadata": {},
   "source": [
    "## Building an Audio Transcription Pipeline\n",
    "\n",
    "Here's a complete pipeline for processing audio files:\n",
    "\n",
    "**Pipeline Flow**:\n",
    "1. Audio files â†’ Whisper Transcriber â†’ Text documents\n",
    "2. Text â†’ Document Splitter â†’ Smaller chunks\n",
    "3. Chunks â†’ Embedder â†’ Vector representations\n",
    "4. Vectors â†’ Document Writer â†’ Store\n",
    "\n",
    "This pipeline can be combined with text and image pipelines for truly multimodal applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90015463",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurafunderburk/Documents/GitHub/Building-Natural-Language-Pipelines/ch4/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio transcription pipeline built!\n",
      "\n",
      "To use the pipeline:\n",
      "result = audio_pipeline.run({'transcriber': {'audio_files': [audio_stream]}})\n"
     ]
    }
   ],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.components.audio import RemoteWhisperTranscriber\n",
    "from haystack.components.preprocessors import DocumentSplitter\n",
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "\n",
    "# Create document store for audio transcripts\n",
    "audio_doc_store = InMemoryDocumentStore()\n",
    "transcriber = RemoteWhisperTranscriber(api_key=Secret.from_env_var(\"OPENAI_API_KEY\"))\n",
    "doc_splitter = DocumentSplitter(split_by=\"sentence\", split_length=10)\n",
    "doc_embedder = SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "doc_writer = DocumentWriter(audio_doc_store)\n",
    "# Build audio transcription pipeline\n",
    "audio_pipeline = Pipeline()\n",
    "audio_pipeline.add_component( \"transcriber\", transcriber)\n",
    "audio_pipeline.add_component(\"splitter\", doc_splitter)\n",
    "audio_pipeline.add_component(\"embedder\", doc_embedder)\n",
    "audio_pipeline.add_component(\"writer\", doc_writer)\n",
    "\n",
    "# Connect components\n",
    "audio_pipeline.connect(\"transcriber.documents\", \"splitter.documents\")\n",
    "audio_pipeline.connect(\"splitter.documents\", \"embedder.documents\")\n",
    "audio_pipeline.connect(\"embedder.documents\", \"writer.documents\")\n",
    "\n",
    "print(\"Audio transcription pipeline built!\")\n",
    "print(\"\\nTo use the pipeline:\")\n",
    "print(\"result = audio_pipeline.run({'transcriber': {'audio_files': [audio_stream]}})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47448aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.77it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.77it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'writer': {'documents_written': 1}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_pipeline.run({'transcriber': {'sources': [\"./data_for_indexing/harvard.wav\"]}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d38a013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The stale smell of old beer lingers. It takes heat to bring out the odor. A cold dip restores health and zest. A salt pickle tastes fine with ham. Tacos al pastor are my favorite. A zestful food is the hot cross bun.']\n"
     ]
    }
   ],
   "source": [
    "documents =[ item.content for item in audio_doc_store.filter_documents()]\n",
    "\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4a09fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_pipeline.draw(path=\"./images/audio_pipeline.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f7f103",
   "metadata": {},
   "source": [
    "![](./images/audio_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb8778d",
   "metadata": {},
   "source": [
    "## Audio Format Support\n",
    "\n",
    "Whisper supports various audio formats:\n",
    "\n",
    "- **MP3**: Most common format\n",
    "- **WAV**: Uncompressed audio\n",
    "- **M4A**: Apple audio format\n",
    "- **FLAC**: Lossless compression\n",
    "- **OGG**: Open-source format\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "1. **Audio Quality**: Higher quality audio produces better transcriptions\n",
    "2. **File Size**: Consider splitting large audio files\n",
    "3. **Language**: Whisper supports multiple languages\n",
    "4. **Noise**: Clean audio produces better results\n",
    "5. **Cost**: Remote Whisper charges per minute of audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cf59ac",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we covered:\n",
    "\n",
    "1. **Audio Transcription**:\n",
    "   - Remote Whisper (OpenAI API)\n",
    "   - Local Whisper (self-hosted)\n",
    "\n",
    "2. **Audio Processing Pipeline**:\n",
    "   - Transcription â†’ Splitting â†’ Embedding â†’ Storage\n",
    "\n",
    "3. **Complete Multimodal Pipeline**:\n",
    "   - Unified pipeline for text, images, and audio\n",
    "   - File routing and parallel processing\n",
    "   - Consistent embedding and storage\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- **Whisper is powerful**: State-of-the-art transcription\n",
    "- **Two options**: Remote (easy) vs Local (free)\n",
    "- **Pipeline flexibility**: Easy to combine modalities\n",
    "- **Production ready**: Scalable architecture\n",
    "\n",
    "### Real-World Applications:\n",
    "\n",
    "1. **Meeting Assistant**: Transcribe meetings, index with slides\n",
    "2. **Podcast Search**: Make audio content searchable\n",
    "3. **Lecture Notes**: Combine audio, slides, and documents\n",
    "4. **Voice Command**: Process voice inputs in RAG\n",
    "5. **Accessibility**: Provide text alternatives for audio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426625ae",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-with-haystack-ch4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
