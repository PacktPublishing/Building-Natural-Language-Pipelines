{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f87f3b5",
   "metadata": {},
   "source": [
    "**ðŸ”§ Setup Required**: Before running this notebook, please follow the [setup instructions](../../README.md#setup-instructions) to configure your environment and API keys."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1570d911",
   "metadata": {},
   "source": [
    "# Multimodal RAG Pipeline with Vision LLM\n",
    "\n",
    "This notebook demonstrates how to build a complete multimodal RAG (Retrieval-Augmented Generation) pipeline.\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "We'll build a RAG pipeline that:\n",
    "\n",
    "1. **Retrieves** documents based on text embeddings (using Approach 2 indexing)\n",
    "2. **Routes** documents by type (images vs PDFs)\n",
    "3. **Converts** image documents back to image format\n",
    "4. **Generates** answers using both text and images with a vision LLM\n",
    "\n",
    "## Why This Approach?\n",
    "\n",
    "- **Text embeddings for efficient retrieval**: Fast and accurate\n",
    "- **Original images for accurate visual reasoning**: LLM sees actual images\n",
    "- **Best of both worlds**: Speed and accuracy\n",
    "\n",
    "## Key Components\n",
    "\n",
    "- **Query Embedder**: Converts user questions to vectors\n",
    "- **Retriever**: Finds relevant documents from our store\n",
    "- **Document Type Router**: Separates images from text\n",
    "- **Document to Image Converter**: Prepares images for the LLM\n",
    "- **Vision LLM**: Generates answers using both modalities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f497ff16",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "400ebce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\"4_multimodal_rag_vision_llm.ipynb\")) if os.path.exists(\"4_multimodal_rag_vision_llm.ipynb\") else os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be4cbc0",
   "metadata": {},
   "source": [
    "## Step 1: Build the Indexing Pipeline (Approach 2)\n",
    "\n",
    "First, we need to index our documents using Approach 2 (LLM content extraction).\n",
    "This creates text embeddings for efficient retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f93ef5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data downloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import ssl\n",
    "\n",
    "# Create SSL context\n",
    "ssl_context = ssl.create_default_context()\n",
    "ssl_context.check_hostname = False\n",
    "ssl_context.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "def download_file(url, filename):\n",
    "    \"\"\"Download file with SSL context\"\"\"\n",
    "    req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    with urllib.request.urlopen(req, context=ssl_context) as response:\n",
    "        with open(filename, 'wb') as out_file:\n",
    "            out_file.write(response.read())\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "os.makedirs(\"data_for_indexing\", exist_ok=True)\n",
    "\n",
    "# Download images\n",
    "download_file(\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/2/26/Pink_Lady_Apple_%284107712628%29.jpg\",\n",
    "    \"images/apple.jpg\"\n",
    ")\n",
    "\n",
    "# Download PDF\n",
    "download_file(\n",
    "    \"https://arxiv.org/pdf/1706.03762\",\n",
    "    \"data_for_indexing/attention_is_all_you_need.pdf\"\n",
    ")\n",
    "\n",
    "print(\"Data downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e5e3a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurafunderburk/Documents/GitHub/Building-Natural-Language-Pipelines/ch4/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x30864dbe0>\n",
       "ðŸš… Components\n",
       "  - file_type_router: FileTypeRouter\n",
       "  - pdf_converter: PyPDFToDocument\n",
       "  - pdf_splitter: DocumentSplitter\n",
       "  - image_converter: ImageFileToDocument\n",
       "  - image_path_fixer: ImagePathFixer\n",
       "  - llm_content_extractor: LLMDocumentContentExtractor\n",
       "  - doc_embedder: SentenceTransformersDocumentEmbedder\n",
       "  - final_doc_joiner: DocumentJoiner\n",
       "  - document_writer: DocumentWriter\n",
       "ðŸ›¤ï¸ Connections\n",
       "  - file_type_router.application/pdf -> pdf_converter.sources (list[Union[str, Path, ByteStream]])\n",
       "  - file_type_router.image/jpeg -> image_converter.sources (list[Union[str, Path, ByteStream]])\n",
       "  - pdf_converter.documents -> pdf_splitter.documents (list[Document])\n",
       "  - pdf_splitter.documents -> final_doc_joiner.documents (list[Document])\n",
       "  - image_converter.documents -> image_path_fixer.documents (list[Document])\n",
       "  - image_path_fixer.documents -> llm_content_extractor.documents (List[Document])\n",
       "  - llm_content_extractor.documents -> final_doc_joiner.documents (list[Document])\n",
       "  - doc_embedder.documents -> document_writer.documents (list[Document])\n",
       "  - final_doc_joiner.documents -> doc_embedder.documents (list[Document])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.components.converters import PyPDFToDocument\n",
    "from haystack.components.converters.image import ImageFileToDocument\n",
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
    "from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "from haystack.components.extractors.image import LLMDocumentContentExtractor\n",
    "from haystack.components.joiners import DocumentJoiner\n",
    "from haystack.components.preprocessors.document_splitter import DocumentSplitter\n",
    "from haystack.components.routers.file_type_router import FileTypeRouter\n",
    "from haystack.components.writers.document_writer import DocumentWriter\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack import component\n",
    "from haystack.dataclasses import Document\n",
    "from typing import List\n",
    "from haystack.utils import Secret\n",
    "\n",
    "@component\n",
    "class ImagePathFixer:\n",
    "    \"\"\"Fixes the file paths in image documents to be absolute paths.\"\"\"\n",
    "    \n",
    "    @component.output_types(documents=List[Document])\n",
    "    def run(self, documents: List[Document]) -> dict:\n",
    "        for doc in documents:\n",
    "            if \"file_path\" in doc.meta:\n",
    "                file_path = doc.meta[\"file_path\"]\n",
    "                if os.path.basename(file_path) == file_path:\n",
    "                    notebook_dir = os.path.dirname(os.path.abspath(\"4_multimodal_rag_vision_llm.ipynb\")) if os.path.exists(\"4_multimodal_rag_vision_llm.ipynb\") else os.getcwd()\n",
    "                    doc.meta[\"file_path\"] = os.path.join(notebook_dir, \"images\", file_path)\n",
    "        return {\"documents\": documents}\n",
    "\n",
    "# Create document store\n",
    "doc_store_text = InMemoryDocumentStore(embedding_similarity_function=\"cosine\")\n",
    "\n",
    "llm_generator = OpenAIChatGenerator(model=\"gpt-4o-mini\",\n",
    "                                    api_key=Secret.from_env_var(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# optional - if you want to use open source model instead of OpenAI\n",
    "# from haystack_integrations.components.generators.ollama import OllamaGenerator, OllamaChatGenerator\n",
    "# llm_generator = OllamaChatGenerator(model=\"mistral-nemo:12b\",\n",
    "#                             generation_kwargs={\n",
    "#                               \"num_predict\": 100,\n",
    "#                               \"temperature\": 0.9,\n",
    "#                             })\n",
    "\n",
    "# Build indexing pipeline\n",
    "indexing_pipe = Pipeline()\n",
    "indexing_pipe.add_component(\"file_type_router\", FileTypeRouter(mime_types=[\"application/pdf\", \"image/jpeg\"]))\n",
    "indexing_pipe.add_component(\"pdf_converter\", PyPDFToDocument())\n",
    "indexing_pipe.add_component(\"pdf_splitter\", DocumentSplitter(split_by=\"page\", split_length=1))\n",
    "indexing_pipe.add_component(\"image_converter\", ImageFileToDocument())\n",
    "indexing_pipe.add_component(\"image_path_fixer\", ImagePathFixer())\n",
    "indexing_pipe.add_component(\n",
    "    \"llm_content_extractor\",\n",
    "    LLMDocumentContentExtractor(\n",
    "        chat_generator=llm_generator,\n",
    "        max_workers=1\n",
    "    )\n",
    ")\n",
    "indexing_pipe.add_component(\"doc_embedder\", SentenceTransformersDocumentEmbedder(\n",
    "    model=\"mixedbread-ai/mxbai-embed-large-v1\",\n",
    "    progress_bar=False\n",
    "))\n",
    "indexing_pipe.add_component(\"final_doc_joiner\", DocumentJoiner(sort_by_score=False))\n",
    "indexing_pipe.add_component(\"document_writer\", DocumentWriter(doc_store_text))\n",
    "\n",
    "# Connect components\n",
    "indexing_pipe.connect(\"file_type_router.application/pdf\", \"pdf_converter.sources\")\n",
    "indexing_pipe.connect(\"pdf_converter.documents\", \"pdf_splitter.documents\")\n",
    "indexing_pipe.connect(\"pdf_splitter.documents\", \"final_doc_joiner.documents\")\n",
    "indexing_pipe.connect(\"file_type_router.image/jpeg\", \"image_converter.sources\")\n",
    "indexing_pipe.connect(\"image_converter.documents\", \"image_path_fixer.documents\")\n",
    "indexing_pipe.connect(\"image_path_fixer.documents\", \"llm_content_extractor.documents\")\n",
    "indexing_pipe.connect(\"llm_content_extractor.documents\", \"final_doc_joiner.documents\")\n",
    "indexing_pipe.connect(\"final_doc_joiner.documents\", \"doc_embedder.documents\")\n",
    "indexing_pipe.connect(\"doc_embedder.documents\", \"document_writer.documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987403a0",
   "metadata": {},
   "source": [
    "## Run the Indexing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a0e8af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 16 documents\n"
     ]
    }
   ],
   "source": [
    "# Index the documents\n",
    "indexing_result = indexing_pipe.run(\n",
    "    data={\"file_type_router\": {\"sources\": [\n",
    "        os.path.join(notebook_dir, \"data_for_indexing/attention_is_all_you_need.pdf\"),\n",
    "        os.path.join(notebook_dir, \"images/apple.jpg\")\n",
    "    ]}}\n",
    ")\n",
    "\n",
    "indexed_documents = doc_store_text.filter_documents()\n",
    "print(f\"Indexed {len(indexed_documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e873e501",
   "metadata": {},
   "source": [
    "## Step 2: Build the RAG Query Pipeline\n",
    "\n",
    "Now we'll build the query pipeline that retrieves and generates answers.\n",
    "\n",
    "### Pipeline Flow:\n",
    "\n",
    "1. **Query** â†’ Text Embedder â†’ Vector\n",
    "2. **Vector** â†’ Retriever â†’ Relevant Documents\n",
    "3. **Documents** â†’ Document Type Router â†’ Separate images and text\n",
    "4. **Image Documents** â†’ Document to Image Converter â†’ Image Content\n",
    "5. **Text + Images** â†’ Prompt Builder â†’ Formatted Prompt\n",
    "6. **Prompt** â†’ Vision LLM â†’ Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31013487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query components initialized!\n"
     ]
    }
   ],
   "source": [
    "from haystack.components.embedders.sentence_transformers_text_embedder import SentenceTransformersTextEmbedder\n",
    "from haystack.components.retrievers.in_memory.embedding_retriever import InMemoryEmbeddingRetriever\n",
    "from haystack.components.builders import ChatPromptBuilder\n",
    "from haystack.components.converters.image import DocumentToImageContent\n",
    "from haystack.components.routers import DocumentTypeRouter\n",
    "from haystack.utils import Secret\n",
    "\n",
    "# Initialize components\n",
    "query_text_embedder = SentenceTransformersTextEmbedder(\n",
    "    model=\"mixedbread-ai/mxbai-embed-large-v1\",\n",
    "    progress_bar=False\n",
    ")\n",
    "retriever = InMemoryEmbeddingRetriever(document_store=doc_store_text, top_k=3)\n",
    "doc_type_router = DocumentTypeRouter(\n",
    "    file_path_meta_field=\"file_path\",\n",
    "    mime_types=[\"image/jpeg\", \"application/pdf\"]\n",
    ")\n",
    "doc_to_image = DocumentToImageContent(detail=\"auto\")\n",
    "\n",
    "# Chat prompt builder with multimodal template\n",
    "chat_prompt_builder = ChatPromptBuilder(\n",
    "    required_variables=[\"question\"],\n",
    "    template=\"\"\"{% message role=\"system\" %}\n",
    "You are a friendly assistant that answers questions based on provided documents and images.\n",
    "{% endmessage %}\n",
    "\n",
    "{%- message role=\"user\" -%}\n",
    "Only provide an answer to the question using the images and text passages provided.\n",
    "\n",
    "These are the text-only documents:\n",
    "{%- if documents|length > 0 %}\n",
    "{%- for doc in documents %}\n",
    "Text Document [{{ loop.index }}] :\n",
    "{{ doc.content }}\n",
    "{% endfor -%}\n",
    "{%- else %}\n",
    "No relevant text documents were found.\n",
    "{% endif %}\n",
    "End of text documents.\n",
    "\n",
    "Question: {{ question }}\n",
    "Answer:\n",
    "\n",
    "Images:\n",
    "{%- if image_contents|length > 0 %}\n",
    "{%- for img in image_contents -%}\n",
    "  {{ img | templatize_part }}\n",
    "{%- endfor -%}\n",
    "{% endif %}\n",
    "{%- endmessage -%}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "llm_generator = OpenAIChatGenerator(model=\"gpt-4o-mini\", api_key=Secret.from_env_var(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# optional - if you want to use open source model instead of OpenAI\n",
    "# from haystack_integrations.components.generators.ollama import OllamaGenerator, OllamaChatGenerator\n",
    "# llm_generator = OllamaChatGenerator(model=\"mistral-nemo:12b\",\n",
    "#                             generation_kwargs={\n",
    "#                               \"num_predict\": 100,\n",
    "#                               \"temperature\": 0.9,\n",
    "#                             })\n",
    "\n",
    "# LLM content extractor for images\n",
    "llm_content_extractor = LLMDocumentContentExtractor(\n",
    "    chat_generator= llm_generator,\n",
    "    max_workers=1\n",
    ")\n",
    "\n",
    "print(\"Query components initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10229eb",
   "metadata": {},
   "source": [
    "## Build the Query Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b50ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG pipeline built!\n"
     ]
    }
   ],
   "source": [
    "# Build the query pipeline\n",
    "rag_pipe = Pipeline()\n",
    "rag_pipe.add_component(\"text_embedder\", query_text_embedder)\n",
    "rag_pipe.add_component(\"retriever\", retriever)\n",
    "rag_pipe.add_component(\"doc_type_router\", doc_type_router)\n",
    "rag_pipe.add_component(\"doc_to_image\", doc_to_image)\n",
    "rag_pipe.add_component(\"chat_prompt_builder\", chat_prompt_builder)\n",
    "rag_pipe.add_component(\"llm\", llm_generator)\n",
    "\n",
    "# Connect components\n",
    "rag_pipe.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "rag_pipe.connect(\"retriever.documents\", \"doc_type_router.documents\")\n",
    "rag_pipe.connect(\"doc_type_router.image/jpeg\", \"doc_to_image.documents\")\n",
    "rag_pipe.connect(\"doc_to_image.image_contents\", \"chat_prompt_builder.image_contents\")\n",
    "rag_pipe.connect(\"doc_type_router.application/pdf\", \"chat_prompt_builder.documents\")\n",
    "rag_pipe.connect(\"chat_prompt_builder.prompt\", \"llm.messages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6e8108",
   "metadata": {},
   "source": [
    "## Visualize the RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac97f93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the RAG pipeline\n",
    "rag_pipe.draw(path=\"images/multimodal_rag.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71285dcc",
   "metadata": {},
   "source": [
    "![](./images/multimodal_rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9cf526",
   "metadata": {},
   "source": [
    "![](./images/multimodal_rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ba60c8",
   "metadata": {},
   "source": [
    "## Query the RAG Pipeline\n",
    "\n",
    "Now let's ask questions about our indexed content!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67b38c4",
   "metadata": {},
   "source": [
    "### Query 1: Ask about the Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c282b329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the color of the background of the image with an apple in it?\n",
      "\n",
      "Answer: The background of the image with the apple is a light brown color.\n"
     ]
    }
   ],
   "source": [
    "# Query about the image\n",
    "query = \"What is the color of the background of the image with an apple in it?\"\n",
    "result = rag_pipe.run(\n",
    "    data={\n",
    "        \"text_embedder\": {\"text\": query},\n",
    "        \"chat_prompt_builder\": {\"question\": query}\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Question: {query}\")\n",
    "print(f\"\\nAnswer: {result['llm']['replies'][0].text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bb2b98",
   "metadata": {},
   "source": [
    "### Query 2: Ask about the PDF Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa2e7bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is attention in the transformers architecture?\n",
      "\n",
      "Answer: Attention in the transformers architecture is a mechanism that maps a query and a set of key-value pairs to an output, where the output is computed as a weighted sum of the values. The weights are determined by a compatibility function of the query with the corresponding keys. Specifically, the attention function is calculated using the formula:\n",
      "\n",
      "Attention(Q, K, V) = softmax(QKT / âˆšdk) * V \n",
      "\n",
      "This allows the model to focus on relevant parts of the input sequence, thereby improving performance on tasks such as sequence transduction. Multi-head attention extends this by allowing the model to jointly attend to information from different representation subspaces at different positions, enhancing the richness of the information processed.\n"
     ]
    }
   ],
   "source": [
    "# Query about the PDF document\n",
    "query = \"What is attention in the transformers architecture?\"\n",
    "result = rag_pipe.run(\n",
    "    data={\n",
    "        \"text_embedder\": {\"text\": query},\n",
    "        \"chat_prompt_builder\": {\"question\": query}\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Question: {query}\")\n",
    "print(f\"\\nAnswer: {result['llm']['replies'][0].text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9ea679",
   "metadata": {},
   "source": [
    "### Query 3: Your Custom Question\n",
    "\n",
    "Try asking your own questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f445987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Describe the apple image in detail\n",
      "\n",
      "Answer: I'm unable to describe the apple image in detail.\n"
     ]
    }
   ],
   "source": [
    "# Try your own query\n",
    "query = \"Describe the apple image in detail\"\n",
    "result = rag_pipe.run(\n",
    "    data={\n",
    "        \"text_embedder\": {\"text\": query},\n",
    "        \"chat_prompt_builder\": {\"question\": query}\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Question: {query}\")\n",
    "print(f\"\\nAnswer: {result['llm']['replies'][0].text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2239d87",
   "metadata": {},
   "source": [
    "## Inspect Retrieved Documents\n",
    "\n",
    "Let's see which documents were retrieved for a query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2cc7653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 3 documents:\n",
      "\n",
      "Document 1:\n",
      "  File: /Users/laurafunderburk/Documents/GitHub/Building-Natural-Language-Pipelines/ch4/jupyter-notebooks/multimodal/images/apple.jpg\n",
      "  Score: 0.7125\n",
      "  Content preview: [img-caption]A red and yellow apple resting on a textured brown surface.[/img-caption]...\n",
      "\n",
      "Document 2:\n",
      "  File: attention_is_all_you_need.pdf\n",
      "  Score: 0.4056\n",
      "  Content preview: Attention Visualizations\n",
      "Input-Input Layer5\n",
      "It\n",
      "is\n",
      "in\n",
      "this\n",
      "spirit\n",
      "that\n",
      "a\n",
      "majority\n",
      "of\n",
      "American\n",
      "governm...\n",
      "\n",
      "Document 3:\n",
      "  File: attention_is_all_you_need.pdf\n",
      "  Score: 0.3980\n",
      "  Content preview: Figure 1: The Transformer - model architecture.\n",
      "The Transformer follows this overall architecture us...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run pipeline to get intermediate results\n",
    "# We'll need to enable include_outputs_from to see intermediate results\n",
    "query = \"What is the apple?\"\n",
    "result = rag_pipe.run(\n",
    "    data={\n",
    "        \"text_embedder\": {\"text\": query},\n",
    "        \"chat_prompt_builder\": {\"question\": query}\n",
    "    },\n",
    "    include_outputs_from={\"retriever\", \"doc_type_router\"}\n",
    ")\n",
    "\n",
    "# Check retrieved documents\n",
    "retrieved_docs = result['retriever']['documents']\n",
    "print(f\"Retrieved {len(retrieved_docs)} documents:\\n\")\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"Document {i+1}:\")\n",
    "    print(f\"  File: {doc.meta.get('file_path', 'Unknown')}\")\n",
    "    print(f\"  Score: {doc.score:.4f}\")\n",
    "    print(f\"  Content preview: {str(doc.content)[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec64d47",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we built a complete multimodal RAG pipeline:\n",
    "\n",
    "1. **Indexed documents** using Approach 2 (LLM content extraction)\n",
    "2. **Built a query pipeline** that retrieves using text embeddings\n",
    "3. **Converted image documents** back to images for the vision LLM\n",
    "4. **Generated answers** using both text and images\n",
    "\n",
    "### Key Features:\n",
    "- **Efficient retrieval**: Text embeddings for fast search\n",
    "- **Accurate generation**: Original images for vision LLM\n",
    "- **Multimodal understanding**: Works with both text and images\n",
    "\n",
    "### Pipeline Architecture Benefits:\n",
    "1. **Separation of concerns**: Retrieval and generation are independent\n",
    "2. **Flexibility**: Can swap out components easily\n",
    "3. **Scalability**: Can handle large document collections\n",
    "\n",
    "### Best Practices:\n",
    "- Use text embeddings for retrieval (faster)\n",
    "- Use original images for generation (more accurate)\n",
    "- Adjust `top_k` in retriever based on your needs\n",
    "- Monitor LLM costs (vision LLMs are more expensive)\n",
    "\n",
    "### Next Steps:\n",
    "- Continue to notebook 5 to add audio transcription with Whisper\n",
    "- Experiment with different vision LLMs (GPT-4 Vision, Claude 3, etc.)\n",
    "- Try different embedding models\n",
    "- Add more document types to your index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-with-haystack-ch4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
