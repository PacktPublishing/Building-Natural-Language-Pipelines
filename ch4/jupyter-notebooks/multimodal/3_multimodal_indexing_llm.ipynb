{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4270405",
   "metadata": {},
   "source": [
    "**ðŸ”§ Setup Required**: Before running this notebook, please follow the [setup instructions](../../README.md#setup-instructions) to configure your environment and API keys."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031d2825",
   "metadata": {},
   "source": [
    "# Multimodal Indexing Pipeline with LLM Content Extraction (Approach 2)\n",
    "\n",
    "This notebook demonstrates how to build a multimodal indexing pipeline using LLM-based content extraction.\n",
    "\n",
    "## Approach 2: LLM Content Extraction\n",
    "\n",
    "In this approach, we use a vision LLM to extract textual descriptions from images, then embed everything as text.\n",
    "\n",
    "**Pipeline Flow**:\n",
    "1. Images â†’ Vision LLM â†’ Text descriptions\n",
    "2. PDFs â†’ Text extraction\n",
    "3. All text â†’ Text-only embeddings\n",
    "\n",
    "### Advantages:\n",
    "- Richer semantic understanding\n",
    "- More accurate retrieval for conceptual searches\n",
    "- Better for text-based queries\n",
    "\n",
    "### Trade-offs:\n",
    "- Slower (requires LLM calls)\n",
    "- More expensive (API costs)\n",
    "- Loses some visual information\n",
    "\n",
    "### Use Cases:\n",
    "- Question answering about images\n",
    "- Document understanding\n",
    "- Semantic search across modalities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9405e4",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "697d2791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\"3_multimodal_indexing_llm.ipynb\")) if os.path.exists(\"3_multimodal_indexing_llm.ipynb\") else os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62746eb",
   "metadata": {},
   "source": [
    "## Download Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d319cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data downloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import ssl\n",
    "\n",
    "# Create SSL context\n",
    "ssl_context = ssl.create_default_context()\n",
    "ssl_context.check_hostname = False\n",
    "ssl_context.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "def download_file(url, filename):\n",
    "    \"\"\"Download file with SSL context\"\"\"\n",
    "    req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    with urllib.request.urlopen(req, context=ssl_context) as response:\n",
    "        with open(filename, 'wb') as out_file:\n",
    "            out_file.write(response.read())\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "os.makedirs(\"data_for_indexing\", exist_ok=True)\n",
    "\n",
    "# Download images\n",
    "download_file(\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/2/26/Pink_Lady_Apple_%284107712628%29.jpg\",\n",
    "    \"images/apple.jpg\"\n",
    ")\n",
    "\n",
    "# Download PDF\n",
    "download_file(\n",
    "    \"https://arxiv.org/pdf/1706.03762\",\n",
    "    \"data_for_indexing/attention_is_all_you_need.pdf\"\n",
    ")\n",
    "\n",
    "print(\"Data downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86fb681",
   "metadata": {},
   "source": [
    "## Initialize Pipeline Components\n",
    "\n",
    "We'll use GPT-4o-mini as the vision LLM and a high-quality text embedder.\n",
    "\n",
    "**Key Components**:\n",
    "- `LLMDocumentContentExtractor`: Extracts text descriptions from images using a vision LLM\n",
    "- `SentenceTransformersDocumentEmbedder`: Creates high-quality text embeddings\n",
    "- `mixedbread-ai/mxbai-embed-large-v1`: A powerful text embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec9c05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Components initialized!\n"
     ]
    }
   ],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.components.converters import PyPDFToDocument\n",
    "from haystack.components.converters.image import ImageFileToDocument\n",
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
    "from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "from haystack.components.extractors.image import LLMDocumentContentExtractor\n",
    "from haystack.components.joiners import DocumentJoiner\n",
    "from haystack.components.preprocessors.document_splitter import DocumentSplitter\n",
    "from haystack.components.routers.file_type_router import FileTypeRouter\n",
    "from haystack.components.writers.document_writer import DocumentWriter\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "\n",
    "# Create new document store\n",
    "doc_store_text = InMemoryDocumentStore(embedding_similarity_function=\"cosine\")\n",
    "\n",
    "# Define components\n",
    "file_type_router = FileTypeRouter(mime_types=[\"application/pdf\", \"image/jpeg\"])\n",
    "image_converter = ImageFileToDocument()\n",
    "pdf_converter = PyPDFToDocument()\n",
    "pdf_splitter = DocumentSplitter(split_by=\"page\", split_length=1)\n",
    "final_doc_joiner = DocumentJoiner(sort_by_score=False)\n",
    "document_writer = DocumentWriter(doc_store_text)\n",
    "\n",
    "# Text-only embedder (better performance)\n",
    "doc_embedder = SentenceTransformersDocumentEmbedder(\n",
    "    model=\"mixedbread-ai/mxbai-embed-large-v1\",\n",
    "    progress_bar=False\n",
    ")\n",
    "\n",
    "llm_generator = OpenAIChatGenerator(model=\"gpt-4o-mini\"),\n",
    "# optional - if you want to use open source model instead of OpenAI\n",
    "# from haystack_integrations.components.generators.ollama import OllamaGenerator, OllamaChatGenerator\n",
    "# llm_generator = OllamaChatGenerator(model=\"mistral-nemo:12b\",\n",
    "#                             generation_kwargs={\n",
    "#                               \"num_predict\": 100,\n",
    "#                               \"temperature\": 0.9,\n",
    "#                             })\n",
    "\n",
    "# LLM content extractor for images\n",
    "llm_content_extractor = LLMDocumentContentExtractor(\n",
    "    chat_generator= llm_generator,\n",
    "    max_workers=1\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Components initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc2a9ea",
   "metadata": {},
   "source": [
    "## Create Custom ImagePathFixer Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdb736d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom ImagePathFixer component defined!\n"
     ]
    }
   ],
   "source": [
    "from haystack import component\n",
    "from haystack.dataclasses import Document\n",
    "from typing import List\n",
    "import os\n",
    "\n",
    "@component\n",
    "class ImagePathFixer:\n",
    "    \"\"\"\n",
    "    Fixes the file paths in image documents to be absolute paths.\n",
    "    ImageFileToDocument only stores the basename, so we need to restore the full path.\n",
    "    \"\"\"\n",
    "    \n",
    "    @component.output_types(documents=List[Document])\n",
    "    def run(self, documents: List[Document]) -> dict:\n",
    "        \"\"\"Fix the file paths in documents to be absolute.\"\"\"\n",
    "        for doc in documents:\n",
    "            if \"file_path\" in doc.meta:\n",
    "                file_path = doc.meta[\"file_path\"]\n",
    "                # If it's just a filename without directory, assume it's in the images folder\n",
    "                if os.path.basename(file_path) == file_path:\n",
    "                    # Get the notebook directory\n",
    "                    notebook_dir = os.path.dirname(os.path.abspath(\"3_multimodal_indexing_llm.ipynb\")) if os.path.exists(\"3_multimodal_indexing_llm.ipynb\") else os.getcwd()\n",
    "                    doc.meta[\"file_path\"] = os.path.join(notebook_dir, \"images\", file_path)\n",
    "        \n",
    "        return {\"documents\": documents}\n",
    "\n",
    "print(\"Custom ImagePathFixer component defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f2e9de",
   "metadata": {},
   "source": [
    "## Build the Indexing Pipeline\n",
    "\n",
    "Now we'll connect all components. The key difference from Approach 1 is the LLM content extractor.\n",
    "\n",
    "**Pipeline Flow**:\n",
    "\n",
    "**PDF Branch**:\n",
    "1. File Router â†’ PDF Converter\n",
    "2. PDF Converter â†’ PDF Splitter\n",
    "3. PDF Splitter â†’ Document Joiner\n",
    "\n",
    "**Image Branch**:\n",
    "1. File Router â†’ Image Converter\n",
    "2. Image Converter â†’ Path Fixer\n",
    "3. Path Fixer â†’ LLM Content Extractor (Vision LLM)\n",
    "4. LLM Content Extractor â†’ Document Joiner\n",
    "\n",
    "**Final Steps**:\n",
    "1. Document Joiner â†’ Text Embedder\n",
    "2. Text Embedder â†’ Document Writer â†’ Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fee4a424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x3b8d16ae0>\n",
       "ðŸš… Components\n",
       "  - file_type_router: FileTypeRouter\n",
       "  - pdf_converter: PyPDFToDocument\n",
       "  - pdf_splitter: DocumentSplitter\n",
       "  - image_converter: ImageFileToDocument\n",
       "  - image_path_fixer: ImagePathFixer\n",
       "  - llm_content_extractor: LLMDocumentContentExtractor\n",
       "  - doc_embedder: SentenceTransformersDocumentEmbedder\n",
       "  - final_doc_joiner: DocumentJoiner\n",
       "  - document_writer: DocumentWriter\n",
       "ðŸ›¤ï¸ Connections\n",
       "  - file_type_router.application/pdf -> pdf_converter.sources (list[Union[str, Path, ByteStream]])\n",
       "  - file_type_router.image/jpeg -> image_converter.sources (list[Union[str, Path, ByteStream]])\n",
       "  - pdf_converter.documents -> pdf_splitter.documents (list[Document])\n",
       "  - pdf_splitter.documents -> final_doc_joiner.documents (list[Document])\n",
       "  - image_converter.documents -> image_path_fixer.documents (list[Document])\n",
       "  - image_path_fixer.documents -> llm_content_extractor.documents (List[Document])\n",
       "  - llm_content_extractor.documents -> final_doc_joiner.documents (list[Document])\n",
       "  - doc_embedder.documents -> document_writer.documents (list[Document])\n",
       "  - final_doc_joiner.documents -> doc_embedder.documents (list[Document])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the indexing pipeline with LLM extraction\n",
    "indexing_pipe_text = Pipeline()\n",
    "indexing_pipe_text.add_component(\"file_type_router\", file_type_router)\n",
    "indexing_pipe_text.add_component(\"pdf_converter\", pdf_converter)\n",
    "indexing_pipe_text.add_component(\"pdf_splitter\", pdf_splitter)\n",
    "indexing_pipe_text.add_component(\"image_converter\", image_converter)\n",
    "indexing_pipe_text.add_component(\"image_path_fixer\", ImagePathFixer())  \n",
    "indexing_pipe_text.add_component(\"llm_content_extractor\", llm_content_extractor)\n",
    "indexing_pipe_text.add_component(\"doc_embedder\", doc_embedder)\n",
    "indexing_pipe_text.add_component(\"final_doc_joiner\", final_doc_joiner)\n",
    "indexing_pipe_text.add_component(\"document_writer\", document_writer)\n",
    "\n",
    "# Connect components\n",
    "indexing_pipe_text.connect(\"file_type_router.application/pdf\", \"pdf_converter.sources\")\n",
    "indexing_pipe_text.connect(\"pdf_converter.documents\", \"pdf_splitter.documents\")\n",
    "indexing_pipe_text.connect(\"pdf_splitter.documents\", \"final_doc_joiner.documents\")\n",
    "indexing_pipe_text.connect(\"file_type_router.image/jpeg\", \"image_converter.sources\")\n",
    "indexing_pipe_text.connect(\"image_converter.documents\", \"image_path_fixer.documents\") \n",
    "indexing_pipe_text.connect(\"image_path_fixer.documents\", \"llm_content_extractor.documents\")  \n",
    "indexing_pipe_text.connect(\"llm_content_extractor.documents\", \"final_doc_joiner.documents\")\n",
    "indexing_pipe_text.connect(\"final_doc_joiner.documents\", \"doc_embedder.documents\")\n",
    "indexing_pipe_text.connect(\"doc_embedder.documents\", \"document_writer.documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6040609",
   "metadata": {},
   "source": [
    "## Visualize the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb0324cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the pipeline\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "indexing_pipe_text.draw(path=\"images/multimodal_indexing_text.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cf2b7c",
   "metadata": {},
   "source": [
    "![](./images/multimodal_indexing_text.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb1d81e",
   "metadata": {},
   "source": [
    "![](./images/multimodal_indexing_text.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6db743",
   "metadata": {},
   "source": [
    "## Run the Indexing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90b0608d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM tuple execution failed. Skipping metadata extraction. Failed with exception ''tuple' object has no attribute 'run''.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 15 documents\n"
     ]
    }
   ],
   "source": [
    "# Run the indexing pipeline\n",
    "indexing_result = indexing_pipe_text.run(\n",
    "    data={\"file_type_router\": {\"sources\": [\n",
    "        os.path.join(notebook_dir, \"data_for_indexing/attention_is_all_you_need.pdf\"),\n",
    "        os.path.join(notebook_dir, \"images/apple.jpg\")\n",
    "    ]}}\n",
    ")\n",
    "\n",
    "indexed_documents = doc_store_text.filter_documents()\n",
    "print(f\"Indexed {len(indexed_documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83cbee6",
   "metadata": {},
   "source": [
    "## Inspect the Extracted Content\n",
    "\n",
    "Let's see what the vision LLM extracted from our image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c11995dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No image documents found\n"
     ]
    }
   ],
   "source": [
    "# Inspect the extracted content from the image\n",
    "image_docs = [d for d in indexed_documents if \"apple.jpg\" in d.meta.get(\"file_path\", \"\")]\n",
    "if image_docs:\n",
    "    print(\"Extracted image content:\")\n",
    "    print(image_docs[0].content)\n",
    "else:\n",
    "    print(\"No image documents found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640f932f",
   "metadata": {},
   "source": [
    "## Compare with Approach 1\n",
    "\n",
    "Let's check a few documents to see the difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae6929ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 1:\n",
      "Type: attention_is_all_you_need.pdf\n",
      "Has embedding: True\n",
      "Embedding dimension: 1024\n",
      "Content preview: Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalis...\n",
      "\n",
      "Document 2:\n",
      "Type: attention_is_all_you_need.pdf\n",
      "Has embedding: True\n",
      "Embedding dimension: 1024\n",
      "Content preview: 1 Introduction\n",
      "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\n",
      "in particular, have been firmly establis...\n",
      "\n",
      "Document 3:\n",
      "Type: attention_is_all_you_need.pdf\n",
      "Has embedding: True\n",
      "Embedding dimension: 1024\n",
      "Content preview: Figure 1: The Transformer - model architecture.\n",
      "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n",
      "c...\n"
     ]
    }
   ],
   "source": [
    "# Check the first few documents\n",
    "for i, doc in enumerate(indexed_documents[:3]):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"Type: {doc.meta.get('file_path', 'Unknown')}\")\n",
    "    print(f\"Has embedding: {doc.embedding is not None}\")\n",
    "    if doc.embedding:\n",
    "        print(f\"Embedding dimension: {len(doc.embedding)}\")\n",
    "    print(f\"Content preview: {str(doc.content)[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac55183",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "1. **Built a multimodal indexing pipeline** using LLM content extraction\n",
    "2. **Extracted textual descriptions** from images using GPT-4o-mini\n",
    "3. **Created text embeddings** for both PDFs and image descriptions\n",
    "4. **Stored everything** in a unified text embedding space\n",
    "\n",
    "### Key Advantages of this Approach:\n",
    "- **Richer semantic understanding**: The LLM can describe images in detail\n",
    "- **Better for text queries**: Works well with natural language questions\n",
    "- **Unified embedding space**: Everything is embedded as text\n",
    "\n",
    "### Trade-offs:\n",
    "- **Slower**: Requires LLM API calls\n",
    "- **More expensive**: OpenAI API costs\n",
    "- **Information loss**: Visual details may be lost in text description\n",
    "\n",
    "### When to Use This Approach:\n",
    "- Question answering about images\n",
    "- Semantic search across documents\n",
    "- When text understanding is more important than visual matching\n",
    "\n",
    "### Next Steps:\n",
    "- Continue to notebook 4 to build a complete RAG pipeline using this indexed data\n",
    "- Compare retrieval quality with Approach 1\n",
    "- Experiment with different vision LLMs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-with-haystack-ch4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
