{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a7bc245",
   "metadata": {},
   "source": [
    "ðŸ”§ **Setup Required**: Before running this notebook, please follow the [setup instructions](../README.md#setup-instructions) to configure your environment and API keys."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee060006",
   "metadata": {},
   "source": [
    "# Building Advanced RAG Systems with Haystack SuperComponents\n",
    "\n",
    "This notebook demonstrates how to build an advanced Retrieval-Augmented Generation (RAG) system using Haystack's SuperComponent feature. We'll cover:\n",
    "\n",
    "1. Setting up a hybrid RAG pipeline with both dense and sparse retrieval\n",
    "2. Creating a SuperComponent for simplified interface\n",
    "3. Building tools from components for agent-based systems\n",
    "4. Implementing a multi-tool agent for complex queries\n",
    "\n",
    "## Prerequisites\n",
    "- Basic understanding of RAG systems\n",
    "- Familiarity with Haystack components\n",
    "- OpenAI API key for LLM access\n",
    "- SerperDev API key for web search capabilities\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will be able to:\n",
    "- Build a hybrid RAG pipeline combining multiple retrieval methods\n",
    "- Create a SuperComponent to simplify pipeline interfaces\n",
    "- Transform components into tools for agent-based systems\n",
    "- Implement an agent that can use multiple tools for complex queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ac1266d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurafunderburk/Documents/GitHub/Building-Natural-Language-Pipelines/ch4/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running unified indexing pipeline for web, local files, and CSV...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing document 1384ec36dd6d99f90ab589732d5219b7371dac846d0f0bd89c6385189c4079c0. Keeping it, but skipping cleaning. Error: Error tokenizing data. C error: Expected 5 fields in line 5, saw 7\n",
      "\n",
      "Error processing document 1384ec36dd6d99f90ab589732d5219b7371dac846d0f0bd89c6385189c4079c0. Keeping it, but skipping splitting. Error: Error tokenizing data. C error: Expected 5 fields in line 5, saw 7\n",
      "\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.59it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scripts.indexing import document_store\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f22b0e",
   "metadata": {},
   "source": [
    "## 1. Setting Up Components\n",
    "\n",
    "In this section, we'll set up the core components needed for our hybrid RAG pipeline. We'll use both dense and sparse retrieval methods to achieve better search results:\n",
    "\n",
    "1. **Dense Retrieval**: Uses embeddings to find semantically similar documents\n",
    "   - SentenceTransformersTextEmbedder: Converts text into vector representations\n",
    "   - InMemoryEmbeddingRetriever: Searches for similar vectors in the document store\n",
    "\n",
    "2. **Sparse Retrieval**: Uses keyword matching (BM25 algorithm)\n",
    "   - InMemoryBM25Retriever: Performs traditional keyword-based search\n",
    "\n",
    "3. **Post-processing**:\n",
    "   - DocumentJoiner: Combines results from both retrievers\n",
    "   - SentenceTransformersSimilarityRanker: Re-ranks results for better precision\n",
    "\n",
    "4. **Generation**:\n",
    "   - PromptBuilder: Creates structured prompts with context\n",
    "   - OpenAIGenerator: Generates responses using GPT model\n",
    "   - OllamaGenerator: Generates responses using Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d7f43e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.utils import Secret\n",
    "from haystack import Pipeline\n",
    "\n",
    "from haystack.components.retrievers.in_memory import InMemoryBM25Retriever\n",
    "from haystack.components.joiners import DocumentJoiner\n",
    "from haystack.components.rankers import SentenceTransformersSimilarityRanker\n",
    "\n",
    "# --- 1. Initialize Query Pipeline Components ---\n",
    "\n",
    "# Text Embedder: To embed the user's query. Must be compatible with the document embedder.\n",
    "text_embedder = SentenceTransformersTextEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Retriever: Fetches documents from the DocumentStore based on vector similarity.\n",
    "retriever = InMemoryEmbeddingRetriever(document_store=document_store, top_k=3)\n",
    "\n",
    "# PromptBuilder: Creates a prompt using the retrieved documents and the query.\n",
    "# The Jinja2 template iterates through the documents and adds their content to the prompt.\n",
    "prompt_template_for_pipeline = \"\"\"\n",
    "Given the following information, answer the user's question.\n",
    "If the information is not available in the provided documents, \n",
    "say that you don't have enough information to answer.\n",
    "\n",
    "Context:\n",
    "{% for doc in documents %}\n",
    "    {{ doc.content }}\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{question}}\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt_builder_inst = PromptBuilder(template=prompt_template_for_pipeline,\n",
    "                                    required_variables=\"*\")\n",
    "llm_generator_inst = OpenAIGenerator(api_key=Secret.from_env_var(\"OPENAI_API_KEY\"), model=\"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "# IF YOU PREFER TO USE OLLAMA INSTEAD OF OPENAI, UNCOMMENT THE FOLLOWING LINES AND COMMENT OUT THE OPENAI GENERATOR ABOVE\n",
    "# downlod Ollama https://ollama.com/download and install it\n",
    "# run in your terminal\n",
    "# $ ollama pull mistral-nemo:12b\n",
    "\n",
    "# then uncomment below\n",
    "# from haystack_integrations.components.generators.ollama import OllamaGenerator\n",
    "\n",
    "# llm_generator_inst = OllamaGenerator(model=\"mistral-nemo:12b\",\n",
    "#                             generation_kwargs={\n",
    "#                               \"num_predict\": 100,\n",
    "#                               \"temperature\": 0.9,\n",
    "#                               })\n",
    "\n",
    "\n",
    "\n",
    "# Sparse Retriever (BM25): For keyword-based search.\n",
    "# This retriever needs to be \"warmed up\" by calculating statistics on the documents in the store.\n",
    "bm25_retriever = InMemoryBM25Retriever(document_store=document_store, top_k=3)\n",
    "\n",
    "# DocumentJoiner: To merge the results from the two retrievers.\n",
    "# The default 'concatenate' mode works well here as the ranker will handle final ordering.\n",
    "document_joiner = DocumentJoiner()\n",
    "\n",
    "# Ranker: A cross-encoder model to re-rank the combined results for higher precision.\n",
    "# This model is highly effective at identifying the most relevant documents from a candidate set.\n",
    "ranker = SentenceTransformersSimilarityRanker(model=\"BAAI/bge-reranker-base\", top_k=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a628b16",
   "metadata": {},
   "source": [
    "## 2. Building the Hybrid RAG Pipeline\n",
    "\n",
    "A hybrid RAG (Retrieval Augmented Generation) pipeline combines multiple retrieval methods to improve the quality of document search. Here's how we'll build it:\n",
    "\n",
    "1. **Component Creation**:\n",
    "   - Initialize both dense and sparse retrievers\n",
    "   - Set up the document joiner and ranker\n",
    "   - Configure the prompt builder and generator\n",
    "\n",
    "2. **Pipeline Assembly**:\n",
    "   - Chain components together in a logical sequence\n",
    "   - Define how documents flow through the pipeline\n",
    "   - Set parameters for each component\n",
    "\n",
    "3. **Benefits**:\n",
    "   - Better search accuracy by combining methods\n",
    "   - More robust to different types of queries\n",
    "   - Improved context selection for generation\n",
    "\n",
    "The resulting pipeline will provide both semantic understanding and keyword matching capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "942bd06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Build the Hybrid RAG Pipeline ---\n",
    "hybrid_rag_pipeline = Pipeline()\n",
    "\n",
    "# Add all necessary components\n",
    "hybrid_rag_pipeline.add_component(\"text_embedder\", text_embedder)\n",
    "hybrid_rag_pipeline.add_component(\"embedding_retriever\", retriever) # Dense retriever\n",
    "hybrid_rag_pipeline.add_component(\"bm25_retriever\", bm25_retriever) # Sparse retriever\n",
    "hybrid_rag_pipeline.add_component(\"document_joiner\", document_joiner)\n",
    "hybrid_rag_pipeline.add_component(\"ranker\", ranker)\n",
    "hybrid_rag_pipeline.add_component(\"prompt_builder\", prompt_builder_inst)\n",
    "hybrid_rag_pipeline.add_component(\"llm\", llm_generator_inst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c73c1dfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x16d78afc0>\n",
       "ðŸš… Components\n",
       "  - text_embedder: SentenceTransformersTextEmbedder\n",
       "  - embedding_retriever: InMemoryEmbeddingRetriever\n",
       "  - bm25_retriever: InMemoryBM25Retriever\n",
       "  - document_joiner: DocumentJoiner\n",
       "  - ranker: SentenceTransformersSimilarityRanker\n",
       "  - prompt_builder: PromptBuilder\n",
       "  - llm: OpenAIGenerator\n",
       "ðŸ›¤ï¸ Connections\n",
       "  - text_embedder.embedding -> embedding_retriever.query_embedding (list[float])\n",
       "  - embedding_retriever.documents -> document_joiner.documents (list[Document])\n",
       "  - bm25_retriever.documents -> document_joiner.documents (list[Document])\n",
       "  - document_joiner.documents -> ranker.documents (list[Document])\n",
       "  - ranker.documents -> prompt_builder.documents (list[Document])\n",
       "  - prompt_builder.prompt -> llm.prompt (str)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 3. Connect the Components in a Graph ---\n",
    "\n",
    "# The query is embedded for the dense retriever\n",
    "hybrid_rag_pipeline.connect(\"text_embedder.embedding\", \"embedding_retriever.query_embedding\")\n",
    "\n",
    "# The raw query text is sent to the BM25 retriever and the ranker\n",
    "# Note: The query input for these components is the raw text string.\n",
    "\n",
    "# The outputs of both retrievers are fed into the document joiner\n",
    "hybrid_rag_pipeline.connect(\"embedding_retriever.documents\", \"document_joiner.documents\")\n",
    "hybrid_rag_pipeline.connect(\"bm25_retriever.documents\", \"document_joiner.documents\")\n",
    "\n",
    "# The joined documents are sent to the ranker\n",
    "hybrid_rag_pipeline.connect(\"document_joiner.documents\", \"ranker.documents\")\n",
    "\n",
    "# The ranked documents are sent to the prompt builder\n",
    "hybrid_rag_pipeline.connect(\"ranker.documents\", \"prompt_builder.documents\")\n",
    "\n",
    "# The final prompt is sent to the LLM\n",
    "hybrid_rag_pipeline.connect(\"prompt_builder.prompt\", \"llm.prompt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9ca6dd",
   "metadata": {},
   "source": [
    "## 3. Creating the SuperComponent\n",
    "\n",
    "A SuperComponent is a special type of pipeline component that can contain and coordinate multiple sub-pipelines. Here's what makes it powerful:\n",
    "\n",
    "1. **Encapsulation**:\n",
    "   - Groups related components into a single unit\n",
    "   - Manages internal data flow and state\n",
    "   - Provides a clean interface to the outside\n",
    "\n",
    "2. **Flexibility**:\n",
    "   - Can switch between different sub-pipelines\n",
    "   - Adapts behavior based on input or conditions\n",
    "   - Easy to modify internal logic\n",
    "\n",
    "3. **Reusability**:\n",
    "   - Package complex behavior into a single component\n",
    "   - Share across different pipelines\n",
    "   - Maintain consistency in processing\n",
    "\n",
    "The SuperComponent pattern helps manage complexity while keeping our pipeline modular and maintainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0242f2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a super component with simplified input/output mapping\n",
    "from haystack import SuperComponent\n",
    "\n",
    "hybrid_rag_sc = SuperComponent(\n",
    "    pipeline=hybrid_rag_pipeline,\n",
    "    input_mapping={\n",
    "        \"query\": [\"text_embedder.text\", \n",
    "                  \"bm25_retriever.query\",\n",
    "                  \"ranker.query\",\n",
    "                  \"prompt_builder.question\"],\n",
    "    },\n",
    "    output_mapping={\n",
    "        \"llm.replies\": \"replies\",\n",
    "        \"ranker.documents\": \"documents\"\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ec3ab2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 11.75it/s]\n"
     ]
    }
   ],
   "source": [
    "# Run the pipeline with simplified interface\n",
    "no_answer_question = hybrid_rag_sc.run(query=\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40e94195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def pretty_print_response(response):\n",
    "    # Display as Markdown for better formatting\n",
    "    display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a140bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I don't have enough information to answer."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pretty_print_response(no_answer_question['replies'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2adb9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 21.05it/s]\n"
     ]
    }
   ],
   "source": [
    "pdf_ai_question = hybrid_rag_sc.run(query=\"Summarize how people use AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "980fb2e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "People use AI, particularly generative AI, in various ways both at work and outside of work. The primary usage includes seeking information or advice to make better decisions, acting as an advisor or research assistant, rather than just performing specific job tasks. This usage is classified into three intents: Asking (seeking information or advice), Doing (performing tasks or actions), and Expressing (communicating feelings or ideas). Generative AI improves worker output by providing decision support, especially in knowledge-intensive jobs, where the quality of decision-making enhances productivity."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pretty_print_response(pdf_ai_question['replies'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9e39d4",
   "metadata": {},
   "source": [
    "## 5. Creating Tools from Components\n",
    "\n",
    "Components can be transformed into tools that agents can use. Here's how:\n",
    "\n",
    "1. **Tool Creation**:\n",
    "   - Define tool interface and parameters\n",
    "   - Map component functionality to tool actions\n",
    "   - Add input validation and error handling\n",
    "\n",
    "2. **Tool Configuration**:\n",
    "   - Set default parameters\n",
    "   - Define input/output formats\n",
    "   - Add usage documentation\n",
    "\n",
    "3. **Integration with Agents**:\n",
    "   - Register tools with agent\n",
    "   - Define tool selection logic\n",
    "   - Handle tool responses\n",
    "\n",
    "This abstraction allows agents to use complex pipeline functionality through a simple interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84daec98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Supercomponent has been successfully wrapped into a Tool.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from haystack.tools.component_tool import ComponentTool\n",
    "\n",
    "# --- 1. Create a Tool from our Supercomponent ---\n",
    "\n",
    "# The name should be a simple, machine-readable identifier.\n",
    "tool_name = \"internal_document_search\"\n",
    "\n",
    "# The description is crucial. It tells the agent's LLM what the tool is for.\n",
    "# It should be clear, detailed, and specific.\n",
    "tool_description = (\n",
    "    \"Use this tool to search and answer questions about internal knowledge, \"\n",
    "    \"including information about Haystack, LLM models, and AI frameworks. \"\n",
    "    \"This is the primary source for any questions related to company-specific data.\"\n",
    ")\n",
    "\n",
    "# Wrap the supercomponent instance in a ComponentTool\n",
    "internal_search_tool = ComponentTool(\n",
    "    name=tool_name,\n",
    "    component=hybrid_rag_sc,\n",
    "    description=tool_description,\n",
    ")\n",
    "\n",
    "print(\"RAG Supercomponent has been successfully wrapped into a Tool.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abf3d661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the SERPER_API_KEY environment variable.\n",
    "\n",
    "from haystack.components.agents import Agent\n",
    "from haystack.components.websearch import SearchApiWebSearch\n",
    "from haystack.dataclasses import ChatMessage\n",
    "from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "\n",
    "\n",
    "# --- 1. Create a Web Search Tool ---\n",
    "\n",
    "# Instantiate the web search component\n",
    "web_search_component = SearchApiWebSearch(api_key=Secret.from_env_var(\"SEARCHAPI_API_KEY\"))\n",
    "\n",
    "# Wrap it in a ComponentTool with a clear name and description\n",
    "web_search_tool = ComponentTool(\n",
    "    name=\"web_search\",\n",
    "    component=web_search_component,\n",
    "    description=\"Use this tool to search the public internet for current events, news, and general knowledge. \"\n",
    "                \"It is best for information that is not specific to our internal documents.\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b26074c",
   "metadata": {},
   "source": [
    "## 6. Building an Agent with Multiple Tools\n",
    "\n",
    "Agents become more powerful when equipped with multiple tools. Here's our approach:\n",
    "\n",
    "1. **Agent Architecture**:\n",
    "   - Define agent's capabilities and goals\n",
    "   - Create tool selection strategy\n",
    "   - Implement decision-making logic\n",
    "\n",
    "2. **Tool Management**:\n",
    "   - Register multiple tools\n",
    "   - Handle tool dependencies\n",
    "   - Manage tool state\n",
    "\n",
    "3. **Coordination**:\n",
    "   - Select appropriate tools for tasks\n",
    "   - Chain tool operations\n",
    "   - Handle tool failures\n",
    "\n",
    "4. **Benefits**:\n",
    "   - More flexible problem-solving\n",
    "   - Better task completion rates\n",
    "   - Adaptable to different scenarios\n",
    "\n",
    "The resulting agent can handle complex tasks by combining tool capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e169876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Initialize the Agent ---\n",
    "\n",
    "# The agent needs a list of all available tools\n",
    "tools = [internal_search_tool, web_search_tool]\n",
    "\n",
    "# The agent's reasoning is powered by an LLM. It must be a model that supports tool calling.\n",
    "agent_llm = OpenAIChatGenerator(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Alternatively, you can use an open-source chat model with Ollama\n",
    "# from haystack_integrations.components.generators.ollama import OllamaGenerator, OllamaChatGenerator\n",
    "# agent_llm = OllamaChatGenerator(model=\"mistral-nemo:12b\",\n",
    "#                             generation_kwargs={\n",
    "#                               \"num_predict\": 100,\n",
    "#                               \"temperature\": 0.9,\n",
    "#                             })\n",
    "\n",
    "\n",
    "# Define a system prompt to guide the agent's behavior\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful research assistant. Your goal is to answer the user's question accurately and comprehensively.\n",
    "You have access to two tools:\n",
    "1. internal_document_search: For questions about our internal knowledge base (Haystack, AI models, etc.).\n",
    "2. web_search: For questions about current events or general public information.\n",
    "\n",
    "First, think about which tool is most appropriate for the user's question.\n",
    "Then, call that tool with the necessary query.\n",
    "If the question requires information from both sources, you can call the tools sequentially.\n",
    "Finally, synthesize the information from the tools into a final answer for the user.\n",
    "\"\"\"\n",
    "\n",
    "# Instantiate the Agent\n",
    "agent = Agent(chat_generator=agent_llm, tools=tools, system_prompt=system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a618e6",
   "metadata": {},
   "source": [
    "## 7. Running Complex Queries\n",
    "\n",
    "Let's explore how our agent handles complex queries using multiple tools:\n",
    "\n",
    "1. **Query Processing**:\n",
    "   - Parse user input\n",
    "   - Identify required tools\n",
    "   - Plan execution strategy\n",
    "\n",
    "2. **Tool Orchestration**:\n",
    "   - Execute tools in sequence\n",
    "   - Handle intermediate results\n",
    "   - Combine tool outputs\n",
    "\n",
    "3. **Result Generation**:\n",
    "   - Synthesize final response\n",
    "   - Format output\n",
    "   - Provide explanations\n",
    "\n",
    "We'll demonstrate this with increasingly complex query examples to show the system's capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cf7d965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running agent with complex query: 'Using the internal documents, explain how people use AI, then investigate the latest trends in 2025 in AI from a web search.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 15.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Run the Agent with a Complex Query ---\n",
    "\n",
    "# This query requires both internal knowledge (about Haystack) and external knowledge (current news).\n",
    "complex_query = (\n",
    "    \"Using the internal documents, explain how people use AI, then investigate the latest trends in 2025 in AI from a web search.\"\n",
    ")\n",
    "\n",
    "print(f\"\\nRunning agent with complex query: '{complex_query}'\")\n",
    "\n",
    "# The agent will now perform a multi-step reasoning process.\n",
    "# We can inspect the 'transcript' to see its thoughts and actions.\n",
    "agent_result = agent.run(messages=[ChatMessage.from_user(complex_query)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f22e7b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## System Prompt"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "\n",
       "You are a helpful research assistant. Your goal is to answer the user's question accurately and comprehensively.\n",
       "You have access to two tools:\n",
       "1. internal_document_search: For questions about our internal knowledge base (Haystack, AI models, etc.).\n",
       "2. web_search: For questions about current events or general public information.\n",
       "\n",
       "First, think about which tool is most appropriate for the user's question.\n",
       "Then, call that tool with the necessary query.\n",
       "If the question requires information from both sources, you can call the tools sequentially.\n",
       "Finally, synthesize the information from the tools into a final answer for the user.\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## User Query"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "*Using the internal documents, explain how people use AI, then investigate the latest trends in 2025 in AI from a web search.*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Agent Actions"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Tool Used:** internal_document_search"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Query:** how people use AI"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Tool Used:** web_search"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Query:** latest trends in AI 2025"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Final Response"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### How People Use AI\n",
       "\n",
       "Based on our internal documents, AI is being used in a variety of sectors and for many purposes, broadly categorized as follows:\n",
       "\n",
       "1. **Professional Use**:\n",
       "   - AI is significantly augmenting workplace tasks, leading to enhanced productivity in roles such as computer programming and IT support. Approximately 37% of AI interactions are related to 'computer and mathematical' occupations, indicating a strong usage in technical fields.\n",
       "   - The workplace applications of AI include automating routine tasks, assisting in software development, and even facilitating complex decision-making processes.\n",
       "\n",
       "2. **Generative AI**:\n",
       "   - This technology is versatile, enabling users to seek practical guidance, create content, and even engage in therapeutic interactions. For instance, many users consult AI for creating customized workout plans or seeking informational advice tailored to their specific needs.\n",
       "   - The adoption of generative AI spans across different demographics, indicating a widening user base with increasingly varied applications.\n",
       "\n",
       "3. **Social Interactions**:\n",
       "   - Beyond professional contexts, AI is being utilized for social purposes, particularly in mental health applications where AI serves as a companion or therapeutic assistant. Interestingly, this is reported to be one of the most prevalent use cases for generative AI.\n",
       "\n",
       "4. **User Intent Classification**:\n",
       "   - Users approach AI with different intentsâ€”ranging from asking specific questions, seeking help with tasks (Doing), or expressing feelings and thoughts (Expressing). Each interaction type reflects a unique way people leverage AI for their personal and professional growth.\n",
       "\n",
       "### Latest Trends in AI (2025)\n",
       "\n",
       "A web search reveals several key trends shaping the AI landscape in 2025:\n",
       "\n",
       "1. **Integration and Autonomy**:\n",
       "   - AI technologies are becoming increasingly integrated into daily life, facilitating not only work but also personal tasks. AI-powered agents are expected to display greater autonomy, simplifying complex tasks for users.\n",
       "\n",
       "2. **Generative AI Surge**:\n",
       "   - The trend towards generative AI continues, enhancing areas like entertainment, healthcare, and education. Its impact is anticipated to be profound as it transforms creative processes and interactions.\n",
       "\n",
       "3. **Multimodal AI**:\n",
       "   - Thereâ€™s a growing emphasis on multimodal AI capabilities, which combine various forms of input (text, sound, images) enhancing contextual understanding and the effectiveness of AI applications.\n",
       "\n",
       "4. **AI in Public Sector**:\n",
       "   - AI applications in the public sector are increasing, driving efficiency in government and social services. This involves the use of AI for data analysis, predictive modeling, and improving citizen engagement.\n",
       "\n",
       "5. **Open-Weight Models**:\n",
       "   - The gap between closed and open-weight AI models is narrowing, leading to more democratized AI technologies that are accessible and affordable.\n",
       "\n",
       "6. **Ethical Considerations and Regulations**:\n",
       "   - With the potent capabilities of AI growing, there is an increasing focus on ethical implications and the importance of regulatory frameworks to guide its development and use, ensuring responsible implementation.\n",
       "\n",
       "Overall, the interplay between the evolving uses of AI and emerging trends highlights a dynamic landscape where technology continues to adapt and integrate more seamlessly into various aspects of life and work. For further reading on specific trends, you can access the following resources:\n",
       "- [The 2025 AI Index Report](https://hai.stanford.edu/ai-index/2025-ai-index-report)\n",
       "- [The State of AI: Global Survey 2025](https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai)\n",
       "- [6 AI Trends You'll See More Of In 2025](https://news.microsoft.com/source/features/ai/6-ai-trends-youll-see-more-of-in-2025/)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def pretty_print_agent_steps(agent_result):\n",
    "    \"\"\"\n",
    "    Pretty prints the steps taken by the agent and its final response.\n",
    "    \n",
    "    Args:\n",
    "        agent_result (dict): The result dictionary from the agent's run\n",
    "    \"\"\"\n",
    "    # Print system message\n",
    "    system_msg = agent_result['messages'][0]._content[0].text\n",
    "    display(Markdown(\"## System Prompt\"))\n",
    "    display(Markdown(f\"```\\n{system_msg}\\n```\"))\n",
    "    \n",
    "    # Print user query\n",
    "    user_query = agent_result['messages'][1]._content[0].text\n",
    "    display(Markdown(\"## User Query\"))\n",
    "    display(Markdown(f\"*{user_query}*\"))\n",
    "    \n",
    "    # Print tool calls and results\n",
    "    display(Markdown(\"## Agent Actions\"))\n",
    "    for msg in agent_result['messages']:\n",
    "        if msg._role.value == 'assistant' and hasattr(msg._content[0], 'tool_name'):\n",
    "            for tool_call in msg._content:\n",
    "                display(Markdown(f\"**Tool Used:** {tool_call.tool_name}\"))\n",
    "                display(Markdown(f\"**Query:** {tool_call.arguments['query']}\"))\n",
    "    \n",
    "    # Print final response\n",
    "    display(Markdown(\"## Final Response\"))\n",
    "    display(Markdown(agent_result['last_message']._content[0].text))\n",
    "\n",
    "# Use the function\n",
    "pretty_print_agent_steps(agent_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7fa2d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-with-haystack-ch4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
