{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building LLM-powered pipelines to extract and process data with Haystack\n",
    "\n",
    "In this notebook, we will show how to build a pipeline to extract and process data using Haystack by deepset.\n",
    "\n",
    "For extraction, we will look into extracting and processing content from: \n",
    "\n",
    "* The internet \n",
    "\n",
    "* Files of different formats: PDF, txt, Markdown, JSON, CSV \n",
    "\n",
    "For cleaning and processing, we will focus on: \n",
    "\n",
    "* Removing certain characters and white space \n",
    "\n",
    "* Chunking and splitting text  \n",
    "\n",
    "Once the data has been extracted and cleaned, we will store it into a Haystack document store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting content from the internet\n",
    "\n",
    "We will use the following components to extract content from the internet:\n",
    "\n",
    "* `SerperDevWebSearch()` - this component will enable us to perform web searches using natural language queries.\n",
    "* `LinkContentFetcher()` - this component will enable us to fetch content from the links returned by the web search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade haystack-ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the appropriate modules and classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.websearch import SerperDevWebSearch\n",
    "from haystack.components.fetchers import LinkContentFetcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize the `SerperDevWebSearch()` and `LinkContentFetcher()` components and use them to perform a web search and fetch content from the links returned by the web search. \n",
    "\n",
    "**Note** You will need to have a Serper API key to use the `SerperDevWebSearch()`component. You can get a free or paid API key by signing up at [https://serper.dev/](https://serper.dev/).\n",
    "\n",
    "This notebook assumes you have a `.env` file in the root directory of this repository with the following content:\n",
    "\n",
    "```bash\n",
    "SERPER_API_KEY=your_api_key\n",
    "OPENAI_API_KEY=your_api_key\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"./../../.env\")\n",
    "\n",
    "open_ai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "serper_api_key = os.getenv(\"SERPERDEV_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the components.\n",
    "\n",
    "For the `SerperDevWebSearch()` component, we will use the following parameters:\n",
    "\n",
    "* `api_key` - by default this is set to `SERPERDEV_API_KEY` so as long as we have loaded it, we don't need to pass it explicitly\n",
    "* `top_k` - the number of search results to return\n",
    "* `allowed_domains` - a list of domains to restrict the search to\n",
    "* `search_params` - a dictionary of search parameters to pass to the Serper API\n",
    "\n",
    "For the `LinkContentFetcher()` component, we will use the following parameters:\n",
    "\n",
    "* `retry_attempts` - the number of times to retry fetching content from a link\n",
    "* `timeout` - the time to wait before retrying to fetch content from a link\n",
    "\n",
    "\n",
    "Let's limit our search to five results from Wikipedia and Encyclopedia Britannica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_search = SerperDevWebSearch(top_k=5, \n",
    "                                allowed_domains=[\"https://en.wikipedia.org/\",\n",
    "                                                 \"https://www.britannica.com/\"],\n",
    "                                search_params={\"type\":\"search\"})\n",
    "link_content = LinkContentFetcher(retry_attempts=3,\n",
    "                                  timeout=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting the components\n",
    "\n",
    "We will initialize the `Pipeline()` class, add the components and connect them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.pipeline import Pipeline\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = Pipeline()\n",
    "\n",
    "# Add components\n",
    "pipeline.add_component(name=\"search\", instance=web_search)\n",
    "pipeline.add_component(name=\"fetcher\", instance=link_content)\n",
    "\n",
    "# Connect components to one another\n",
    "pipeline.connect(\"search.links\", \"fetcher.urls\")\n",
    "\n",
    "# Draw pipeline\n",
    "pipeline.draw(\"./images/search_fetch_pipeline.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can execute the pipeline as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What can you tell me about the year of the dragon?\"\n",
    "output = pipeline.run(data={\"search\":{\"query\":query}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['search', 'fetcher'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a few results. Due to the length of the response, we will only display the first 50 characters of the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Dragon, also known as Loong is the fifth of th\n",
      "Title:  Dragon (zodiac) - Wikipedia URL:  https://en.wikipedia.org/wiki/Dragon_(zodiac)\n",
      "-------\n",
      "Dragon, Chenshi, 07:00 to 08:59, Dragons are hover\n",
      "Title:  Chinese zodiac - Wikipedia URL:  https://en.wikipedia.org/wiki/Chinese_zodiac\n",
      "-------\n",
      "The Chinese Dragon, also known as the loong, long \n",
      "Title:  Chinese dragon - Wikipedia URL:  https://en.wikipedia.org/wiki/Chinese_dragon\n",
      "-------\n",
      "The Dragon Years is a compilation album by the New\n",
      "Title:  The Dragon Years - Wikipedia URL:  https://en.wikipedia.org/wiki/The_Dragon_Years\n",
      "-------\n",
      "Note: according to this website, Abraham Lincoln w\n",
      "Title:  Talk:Dragon (zodiac) - Wikipedia URL:  https://en.wikipedia.org/wiki/Talk%3ADragon_(zodiac)\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "for item in output[\"search\"][\"documents\"]:\n",
    "    print(item.content[0:50])\n",
    "    print(\"Title: \", item.meta['title'], \"URL: \", item.meta['link'])\n",
    "    print(\"-------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the scraped content from the first link. We will only show the first 50 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<!DOCTYPE html>\\n<html class=\"client-nojs vector-fe'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"fetcher\"]['streams'][0].data[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding cleaning and splitting components\n",
    "\n",
    "We will reinitialize the `Pipeline()` class and add new instances of each of the following components:\n",
    "\n",
    "* `SerperDevWebSearch()` - to perform web searches using natural language queries\n",
    "* `LinkContentFetcher()` - to fetch content from the links returned by the web search\n",
    "* `HTMLToDocument()` - to convert the HTML content to a Haystack document\n",
    "* `DocumentCleaner()` - to clean the content of the document\n",
    "* `DocumentWriter()` - to write the document to a Haystack document store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.converters import HTMLToDocument\n",
    "from haystack.components.preprocessors import DocumentCleaner\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.writers import DocumentWriter\n",
    "\n",
    "# Initialize document store\n",
    "document_store = InMemoryDocumentStore()\n",
    "\n",
    "# Initialize components\n",
    "web_search = SerperDevWebSearch(top_k=5, \n",
    "                                allowed_domains=[\"https://en.wikipedia.org/\",\n",
    "                                                 \"https://www.britannica.com/\"],\n",
    "                                search_params={\"type\":\"search\"})\n",
    "link_content = LinkContentFetcher(retry_attempts=3,\n",
    "                                  timeout=10)\n",
    "html_to_document = HTMLToDocument()\n",
    "cleaner = DocumentCleaner(\n",
    "\tremove_empty_lines=True,\n",
    "\tremove_extra_whitespaces=True,\n",
    "\tremove_repeated_substrings=False)\n",
    "writer = DocumentWriter(document_store=document_store)\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = Pipeline()\n",
    "\n",
    "# Add components\n",
    "pipeline.add_component(name=\"search\", instance=web_search)\n",
    "pipeline.add_component(name=\"fetcher\", instance=link_content)\n",
    "pipeline.add_component(name=\"html_to_document\", instance=html_to_document)\n",
    "pipeline.add_component(name=\"cleaner\", instance=cleaner)\n",
    "pipeline.add_component(name=\"writer\", instance=writer)\n",
    "\n",
    "# Connect components to one another\n",
    "pipeline.connect(\"search.links\", \"fetcher.urls\")\n",
    "pipeline.connect(\"fetcher\", \"html_to_document\")\n",
    "pipeline.connect(\"html_to_document.documents\", \"cleaner.documents\")\n",
    "pipeline.connect(\"cleaner.documents\", \"writer.documents\")\n",
    "\n",
    "# Draw pipeline\n",
    "pipeline.draw(\"./images/search_fetch_clean_save_pipeline.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What can you tell me about the year of the dragon?\"\n",
    "result = pipeline.run(data={\"search\":{\"query\":query}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show documents in the document store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id=c7843dbad70fe9f99b7674017b0ec29dccaa6c668bd3fd6845aca6d97f6416cb, content: 'Dragon (zodiac)\n",
       " 39 languages\n",
       " Dragon\n",
       " Zodiac dragon, showing the lóng (龍) character for dragon\n",
       " It has ...', meta: {'content_type': 'text/html', 'url': 'https://en.wikipedia.org/wiki/Dragon_(zodiac)'}),\n",
       " Document(id=44088fd4874191fb4410c1718f1e907342da2ffc25446ce0bd9923f9049ebee6, content: 'Chinese zodiac\n",
       " 44 languages\n",
       " Lunar calendar classification in a 12-year cycle\n",
       " Unless otherwise specif...', meta: {'content_type': 'text/html', 'url': 'https://en.wikipedia.org/wiki/Chinese_zodiac'}),\n",
       " Document(id=0cda3111923c649feeb40e8a9e8a1b16ca2c371dfc67c4e19bf2f874171b144b, content: 'Chinese dragon\n",
       " 57 languages\n",
       " \"Loong\" redirects here. For the airlines, see Loong Air .\n",
       " This article h...', meta: {'content_type': 'text/html', 'url': 'https://en.wikipedia.org/wiki/Chinese_dragon'}),\n",
       " Document(id=fedf29d7ee1459fe0929c99453480b068d1ab56886221d8724f22fbd6ece8b3e, content: 'The Dragon Years\n",
       " The Dragon Years (subtitled The 40th Anniversary Collection) is a compilation album...', meta: {'content_type': 'text/html', 'url': 'https://en.wikipedia.org/wiki/The_Dragon_Years'}),\n",
       " Document(id=04ac0d0ee3f545285529d9ed4ad0a25b2ab4a75c0601203dc01092992fe75e30, content: 'Talk:Dragon (zodiac)\n",
       " Add languages\n",
       " From Wikipedia, the free encyclopedia\n",
       " This is the talk page for d...', meta: {'content_type': 'text/html', 'url': 'https://en.wikipedia.org/wiki/Talk%3ADragon_(zodiac)'})]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_store.filter_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-pipelines",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
