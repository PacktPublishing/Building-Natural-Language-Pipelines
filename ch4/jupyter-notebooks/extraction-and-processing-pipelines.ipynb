{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building LLM-powered pipelines to extract and process data with Haystack\n",
    "\n",
    "In this notebook, we will show how to build a pipeline to extract and process data using Haystack by deepset.\n",
    "\n",
    "For extraction, we will look into extracting and processing content from: \n",
    "\n",
    "* The internet \n",
    "\n",
    "* Files of different formats: PDF, txt, Markdown, JSON, CSV \n",
    "\n",
    "For cleaning and processing, we will focus on: \n",
    "\n",
    "* Removing certain characters and white space \n",
    "\n",
    "* Chunking and splitting text  \n",
    "\n",
    "Once the data has been extracted and cleaned, we will store it into a Haystack document store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting content from the internet\n",
    "\n",
    "We will use the following components to extract content from the internet:\n",
    "\n",
    "* `SerperDevWebSearch()` - this component will enable us to perform web searches using natural language queries.\n",
    "* `LinkContentFetcher()` - this component will enable us to fetch content from the links returned by the web search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade haystack-ai\n",
    "!pip install markdown-it-py mdit_plain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the appropriate modules and classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.websearch import SerperDevWebSearch\n",
    "from haystack.components.fetchers import LinkContentFetcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize the `SerperDevWebSearch()` and `LinkContentFetcher()` components and use them to perform a web search and fetch content from the links returned by the web search. \n",
    "\n",
    "**Note** You will need to have a Serper API key to use the `SerperDevWebSearch()`component. You can get a free or paid API key by signing up at [https://serper.dev/](https://serper.dev/).\n",
    "\n",
    "This notebook assumes you have a `.env` file in the root directory of this repository with the following content:\n",
    "\n",
    "```bash\n",
    "SERPER_API_KEY=your_api_key\n",
    "OPENAI_API_KEY=your_api_key\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"./../../.env\")\n",
    "\n",
    "open_ai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "serper_api_key = os.getenv(\"SERPERDEV_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the components.\n",
    "\n",
    "For the `SerperDevWebSearch()` component, we will use the following parameters:\n",
    "\n",
    "* `api_key` - by default this is set to `SERPERDEV_API_KEY` so as long as we have loaded it, we don't need to pass it explicitly\n",
    "* `top_k` - the number of search results to return\n",
    "* `allowed_domains` - a list of domains to restrict the search to\n",
    "* `search_params` - a dictionary of search parameters to pass to the Serper API\n",
    "\n",
    "For the `LinkContentFetcher()` component, we will use the following parameters:\n",
    "\n",
    "* `retry_attempts` - the number of times to retry fetching content from a link\n",
    "* `timeout` - the time to wait before retrying to fetch content from a link\n",
    "\n",
    "\n",
    "Let's limit our search to five results from Wikipedia and Encyclopedia Britannica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_search = SerperDevWebSearch(top_k=5,\n",
    "                                allowed_domains=[\"https://en.wikipedia.org/\",\n",
    "                                                 \"https://www.britannica.com/\"])\n",
    "link_content = LinkContentFetcher(retry_attempts=3,\n",
    "                                  timeout=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting the components\n",
    "\n",
    "We will initialize the `Pipeline()` class, add the components and connect them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.pipeline import Pipeline\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = Pipeline()\n",
    "\n",
    "# Add components\n",
    "pipeline.add_component(name='search', instance=web_search)\n",
    "pipeline.add_component(name ='fetcher' , instance= link_content)\n",
    "\n",
    "# Connect components to one another\n",
    "pipeline.connect(\"search.links\", \"fetcher.urls\")\n",
    "\n",
    "# Draw pipeline\n",
    "pipeline.draw(\"./images/search_fetch_pipeline.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can execute the pipeline as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What can you tell me about the year of the dragon?\"\n",
    "output = pipeline.run(data={\"search\":{\"query\":query}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['search', 'fetcher'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a few results. Due to the length of the response, we will only display the first 50 characters of the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dragon, also known as loong is the fifth of the 12-year cycle of animals that appear in the Chin\n",
      "Title:  Dragon (zodiac) - Wikipedia URL:  https://en.wikipedia.org/wiki/Dragon_(zodiac)\n",
      "-------\n",
      "Dragon, Chenshi, 07:00 to 08:59, Dragons are hovering in the sky to give rain. Snake, Sishi, 09:00 t\n",
      "Title:  Chinese zodiac - Wikipedia URL:  https://en.wikipedia.org/wiki/Chinese_zodiac\n",
      "-------\n",
      "The Chinese Dragon, also known as the loong, long or lung is a legendary creature in Chinese mytholo\n",
      "Title:  Chinese dragon - Wikipedia URL:  https://en.wikipedia.org/wiki/Chinese_dragon\n",
      "-------\n",
      "Note: according to this website, Abraham Lincoln was born in the year of the Snake, which means Darw\n",
      "Title:  Talk:Dragon (zodiac) - Wikipedia URL:  https://en.wikipedia.org/wiki/Talk%3ADragon_(zodiac)\n",
      "-------\n",
      "Feb. 12, 2024, 5:02 AM ET (Yahoo News). Year of the dragon, dance of the lion: NL Chinese Associatio\n",
      "Title:  Dragon | Description, Mythical Dragons, Types, & Facts | Britannica URL:  https://www.britannica.com/topic/dragon-mythological-creature\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "for item in output[\"search\"][\"documents\"]:\n",
    "    print(item.content[0:100])\n",
    "    print(\"Title: \", item.meta['title'], \"URL: \", item.meta['link'])\n",
    "    print(\"-------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the scraped content from the first link. We will only show the first 50 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<!DOCTYPE html>\\n<html class=\"client-nojs vector-fe'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"fetcher\"]['streams'][0].data[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding cleaning and splitting components\n",
    "\n",
    "We will reinitialize the `Pipeline()` class and add new instances of each of the following components:\n",
    "\n",
    "* `SerperDevWebSearch()` - to perform web searches using natural language queries\n",
    "* `LinkContentFetcher()` - to fetch content from the links returned by the web search\n",
    "* `HTMLToDocument()` - to convert the HTML content to a Haystack document\n",
    "* `DocumentCleaner()` - to clean the content of the document\n",
    "* `DocumentWriter()` - to write the document to a Haystack document store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.converters import HTMLToDocument\n",
    "from haystack.components.preprocessors import DocumentCleaner\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.writers import DocumentWriter\n",
    "\n",
    "# Initialize document store\n",
    "document_store = InMemoryDocumentStore()\n",
    "\n",
    "# Initialize components\n",
    "web_search = SerperDevWebSearch(top_k=5, \n",
    "                                allowed_domains=[\"https://en.wikipedia.org/\",\n",
    "                                                 \"https://www.britannica.com/\"],\n",
    "                                search_params={\"type\":\"search\"})\n",
    "link_content = LinkContentFetcher(retry_attempts=3,\n",
    "                                  timeout=10)\n",
    "html_to_document = HTMLToDocument()\n",
    "cleaner = DocumentCleaner(\n",
    "\tremove_empty_lines=True,\n",
    "\tremove_extra_whitespaces=True,\n",
    "\tremove_repeated_substrings=False)\n",
    "writer = DocumentWriter(document_store=document_store)\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = Pipeline()\n",
    "\n",
    "# Add components\n",
    "pipeline.add_component(name=\"search\", instance=web_search)\n",
    "pipeline.add_component(name=\"fetcher\", instance=link_content)\n",
    "pipeline.add_component(name=\"html_to_document\", instance=html_to_document)\n",
    "pipeline.add_component(name=\"cleaner\", instance=cleaner)\n",
    "pipeline.add_component(name=\"writer\", instance=writer)\n",
    "\n",
    "# Connect components to one another\n",
    "pipeline.connect(\"search.links\", \"fetcher.urls\")\n",
    "pipeline.connect(\"fetcher\", \"html_to_document\")\n",
    "pipeline.connect(\"html_to_document.documents\", \"cleaner.documents\")\n",
    "pipeline.connect(\"cleaner.documents\", \"writer.documents\")\n",
    "\n",
    "# Draw pipeline\n",
    "pipeline.draw(\"./images/search_fetch_clean_save_pipeline.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What can you tell me about the year of the dragon?\"\n",
    "result = pipeline.run(data={\"search\":{\"query\":query}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show documents in the document store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store.filter_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting and processing content from files\n",
    "\n",
    "We will use the following components to extract content from files:\n",
    "\n",
    "Generate a table from the information below\n",
    "\n",
    "| Component | Description |\n",
    "| --- | --- |\n",
    "| `AzureOCRDocumentConverter` | Converts PDF (both searchable and image-only), JPEG, PNG, BMP, TIFF, DOCX, XLSX, PPTX, and HTML to Documents. |\n",
    "| `HTMLToDocument` | Converts HTML files to Documents. |\n",
    "| `MarkdownToDocument` | Converts markdown files to Documents. |\n",
    "| `PyPDFToDocument` | Converts PDF files to Documents. |\n",
    "| `TikaDocumentConverter` | Converts various file types to Documents using Apache Tika. |\n",
    "| `TextFileToDocument` | Converts text files to Documents. |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow to incorporate the components into a pipeline\n",
    "\n",
    "The workflow below may be adapted to create a preprocessing pipeline:\n",
    "\n",
    "1. Choose the appropriate file converter.  \n",
    "\n",
    "2. Initialize the component to convert files into Haystack document objects. \n",
    " \n",
    "3. Initialize components to clean and store into a document store.  \n",
    "\n",
    "4. Initialize a pipeline instance. \n",
    "\n",
    "5. Add components to the pipeline.  \n",
    "\n",
    "6. Connect components in the correct order.  \n",
    "\n",
    "7. Execute the pipeline.\n",
    "\n",
    "Let's create a couple examples with the `MarkdownToDocument` and `TextFileToDocument` components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.converters import MarkdownToDocument, TextFileToDocument\n",
    "from haystack.components.preprocessors import DocumentCleaner\n",
    "from haystack.components.preprocessors import DocumentSplitter\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize document store and components\n",
    "document_store = InMemoryDocumentStore()\n",
    "markdown_converter =  MarkdownToDocument()\n",
    "document_cleaner = DocumentCleaner(\n",
    "                    remove_empty_lines=True,\n",
    "                    remove_extra_whitespaces=True,\n",
    "                    remove_repeated_substrings=False\n",
    "                )\n",
    "document_splitter = DocumentSplitter(split_by=\"word\", split_length=5)\n",
    "document_writer = DocumentWriter(document_store=document_store)\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = Pipeline()\n",
    "\n",
    "# Add components\n",
    "pipeline.add_component(\"converter\", markdown_converter)\n",
    "pipeline.add_component(\"cleaner\", document_cleaner)\n",
    "pipeline.add_component(\"splitter\", document_splitter)\n",
    "pipeline.add_component(\"writer\", document_writer)\n",
    "\n",
    "# Connect components to one another\n",
    "pipeline.connect(\"converter\", \"cleaner\")\n",
    "pipeline.connect(\"cleaner.documents\", \"splitter.documents\")\n",
    "pipeline.connect(\"splitter.documents\", \"writer.documents\")\n",
    "\n",
    "# Execute pipeline\n",
    "file_names = [str(f) for f in Path(\"./markdown_pages\").rglob(\"*.md\")]\n",
    "pipeline.run({\"converter\": {\"sources\": file_names}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id=f10a8f3a4b330064c1c0660196236d13e816f94b864a92c0749698c50fd70d03, content: 'Tables | Option | Description |\n",
       " | ------ | ----------- |\n",
       " | data | path to data files to supply the d...', meta: {'file_path': 'markdown_pages/page3.md', 'source_id': 'daafb2334a53a68782bd589997cc0b01d8e4f60aaf94abbb99448f2c848010a0'}),\n",
       " Document(id=3b2ba3eb5de58bcf3caf8ede5900c01febb44b1256c97ce406d90b432536036b, content: 'files. | Links link text link with title Autoconverted link https://github.com/nodeca/pica (enable l...', meta: {'file_path': 'markdown_pages/page3.md', 'source_id': 'daafb2334a53a68782bd589997cc0b01d8e4f60aaf94abbb99448f2c848010a0'}),\n",
       " Document(id=31e41a3879d0fd48c26cef9e076af64c9383675cd19dff14d7606be29f800596, content: 'Advertisement :) pica - high quality and fast image\n",
       " resize in browser.\n",
       " babelfish - developer friendl...', meta: {'file_path': 'markdown_pages/page2.md', 'source_id': 'd07c32e07de151200abfb29f5f22dc759300ddcc75e064078d7d36ab7930f490'}),\n",
       " Document(id=94bdc3fcb6060aa575d8500a2789f2a8697d06d1f797a1384daff815efd61223, content: 'dolor sit amet Consectetur adipiscing elit Integer molestie lorem at massa You can use sequential nu...', meta: {'file_path': 'markdown_pages/page2.md', 'source_id': 'd07c32e07de151200abfb29f5f22dc759300ddcc75e064078d7d36ab7930f490'}),\n",
       " Document(id=5564a4603b7374b377b491383413b368a36be857a4981de47f31729b85ed9699, content: 'Page 1 This is the content for page 1. Section 1 This is the content for section 1. Subsection 1.1 T...', meta: {'file_path': 'markdown_pages/page1.md', 'source_id': '5d579048753c4aaba6045cb4c340e9aac5da477ab0f79dd5c734a15066fdb223'})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_store.filter_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a pipeline for the `TextFileToDocument` component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.pipeline.Pipeline at 0x7f88f17b6d40>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize document store and components\n",
    "document_store = InMemoryDocumentStore()\n",
    "text_converter =  TextFileToDocument()\n",
    "document_cleaner = DocumentCleaner(\n",
    "                    remove_empty_lines=True,\n",
    "                    remove_extra_whitespaces=True,\n",
    "                    remove_repeated_substrings=False\n",
    "                )\n",
    "document_splitter = DocumentSplitter(split_by=\"word\", split_length=5)\n",
    "document_writer = DocumentWriter(document_store=document_store)\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = Pipeline()\n",
    "\n",
    "# Add components\n",
    "pipeline.add_component(\"converter\", text_converter)\n",
    "pipeline.add_component(\"cleaner\", document_cleaner)\n",
    "pipeline.add_component(\"splitter\", document_splitter)\n",
    "pipeline.add_component(\"writer\", document_writer)\n",
    "\n",
    "# Connect components to one another\n",
    "pipeline.connect(\"converter\", \"cleaner\")\n",
    "pipeline.connect(\"cleaner.documents\", \"splitter.documents\")\n",
    "pipeline.connect(\"splitter.documents\", \"writer.documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.draw(\"./images/text_file_to_document_pipeline.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'writer': {'documents_written': 3}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute pipeline\n",
    "file_names = [str(f) for f in Path(\"./textfile-pages\").rglob(\"*.txt\")]\n",
    "pipeline.run({\"converter\": {\"sources\": file_names}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id=d5a6e4891649cc95de12efe9c221e7956d9964fdd4e1860ecb209d9ddd088d91, content: 'This is page 1 of the text file.\n",
       " Lorem ipsum dolor sit amet, consectetur adipiscing elit.\n",
       " Sed euismo...', meta: {'file_path': 'textfile-pages/page1.txt', 'source_id': '8c31b71f452f1d2bcaafeb90b6aee523eea0002ba49cb12ff39946b2532c7c1f'}),\n",
       " Document(id=e912e8a17c8e6524f68e23a64d7e5918ab6251ae99b5ee02fb75be55e6a2885d, content: 'This is page 2 of the text file.\n",
       " Lorem ipsum dolor sit amet, consectetur adipiscing elit.\n",
       " Nullam auc...', meta: {'file_path': 'textfile-pages/page2.txt', 'source_id': '964dbbbb44b231a6409552a36be07d3d97439bce04f47540370e25148535a216'}),\n",
       " Document(id=22ec06e031cd39e68c5380be4dfd82e05aaa5e6d2b18293d05136b49f59bd6f1, content: 'This is page 3 of the text file.\n",
       " Lorem ipsum dolor sit amet, consectetur adipiscing elit.\n",
       " Sed euismo...', meta: {'file_path': 'textfile-pages/page3.txt', 'source_id': '936add0cc365ed200afc5caf76cb6ddc686ea3c1ce231bdd989065731f3ea9bf'})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_store.filter_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorporating a duplicate policy\n",
    "\n",
    "The `DuplicatePolicy` is a class that defines the different options for handling documents with the same ID in a DocumentStore. It has three possible values:\n",
    "\n",
    "* `OVERWRITE`: Indicates that if a document with the same ID already exists in the DocumentStore, it should be overwritten with the new document.\n",
    "* `SKIP`: If a document with the same ID already exists, the new document will be skipped and not added to the DocumentStore.\n",
    "* `FAIL`: Raises an error if a document with the same ID already exists in the DocumentStore. It prevents duplicate documents from being added.\n",
    "\n",
    "We will pass the duplicate policy when we initialize the   `DocumentWriter()` component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'writer': {'documents_written': 47}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from haystack.document_stores.types import DuplicatePolicy\n",
    "\n",
    "# Initialize document store and components\n",
    "document_store = InMemoryDocumentStore()\n",
    "text_converter =  TextFileToDocument()\n",
    "document_cleaner = DocumentCleaner(\n",
    "                    remove_empty_lines=True,\n",
    "                    remove_extra_whitespaces=True,\n",
    "                    remove_repeated_substrings=False\n",
    "                )\n",
    "document_splitter = DocumentSplitter(split_by=\"word\", split_length=5)\n",
    "document_writer = DocumentWriter(document_store=document_store,\n",
    "                                 policy=DuplicatePolicy.SKIP)\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = Pipeline()\n",
    "\n",
    "# Add components\n",
    "pipeline.add_component(\"converter\", text_converter)\n",
    "pipeline.add_component(\"cleaner\", document_cleaner)\n",
    "pipeline.add_component(\"splitter\", document_splitter)\n",
    "pipeline.add_component(\"writer\", document_writer)\n",
    "\n",
    "# Connect components to one another\n",
    "pipeline.connect(\"converter\", \"cleaner\")\n",
    "pipeline.connect(\"cleaner.documents\", \"splitter.documents\")\n",
    "pipeline.connect(\"splitter.documents\", \"writer.documents\")\n",
    "\n",
    "file_names = [str(f) for f in Path(\"./textfile-pages\").rglob(\"*.txt\")]\n",
    "pipeline.run({\"converter\": {\"sources\": file_names}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-pipelines",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
