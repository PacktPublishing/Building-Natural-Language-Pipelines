{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30f9638a",
   "metadata": {},
   "source": [
    "## Building a semantic search RAG pipeline (Naive RAG)\n",
    "\n",
    "Welcome! In this notebook, you'll learn how to build a semantic search pipeline using the Haystack framework. We'll walk through each step, from initializing components to running a Retrieval-Augmented Generation (RAG) pipeline that answers questions using indexed documents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb61715",
   "metadata": {},
   "source": [
    "### 1 Imports\n",
    "\n",
    "Let's start by importing the necessary Haystack components and initializing the core elements of our semantic search pipeline. We'll use a text embedder, a retriever, a prompt builder, and an LLM generator. These components will work together to process queries and generate answers based on your indexed data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82280be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue from the previous script, assuming 'document_store' is populated.\n",
    "from scripts.indexing import document_store  # Adjust the import as necessary\n",
    "\n",
    "# Import necessary components for the query pipeline\n",
    "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.utils import Secret\n",
    "from haystack import Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5481d1ad",
   "metadata": {},
   "source": [
    "## 2. Building the Naive RAG Pipeline\n",
    "\n",
    "Now, we'll assemble our pipeline. The pipeline connects the components so that a user's question is embedded, relevant documents are retrieved, a prompt is built, and an answer is generated by the LLM. This forms the backbone of a basic RAG (Retrieval-Augmented Generation) system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a9cd5b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x309f520c0>\n",
       "🚅 Components\n",
       "  - text_embedder: SentenceTransformersTextEmbedder\n",
       "  - retriever: InMemoryEmbeddingRetriever\n",
       "  - prompt_builder: PromptBuilder\n",
       "  - llm: OpenAIGenerator\n",
       "🛤️ Connections\n",
       "  - text_embedder.embedding -> retriever.query_embedding (list[float])\n",
       "  - retriever.documents -> prompt_builder.documents (list[Document])\n",
       "  - prompt_builder.prompt -> llm.prompt (str)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# --- 1. Initialize Query Pipeline Components ---\n",
    "\n",
    "# Text Embedder: To embed the user's query. Must be compatible with the document embedder.\n",
    "text_embedder = SentenceTransformersTextEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Retriever: Fetches documents from the DocumentStore based on vector similarity.\n",
    "retriever = InMemoryEmbeddingRetriever(document_store=document_store, top_k=3)\n",
    "\n",
    "# PromptBuilder: Creates a prompt using the retrieved documents and the query.\n",
    "# The Jinja2 template iterates through the documents and adds their content to the prompt.\n",
    "prompt_template_for_pipeline = \"\"\"\n",
    "Given the following information, answer the user's question.\n",
    "If the information is not available in the provided documents, say that you don't have enough information to answer.\n",
    "\n",
    "Context:\n",
    "{% for doc in documents %}\n",
    "    {{ doc.content }}\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{question}}\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt_builder_inst = PromptBuilder(template=prompt_template_for_pipeline,\n",
    "                                    required_variables=\"*\")\n",
    "llm_generator_inst = OpenAIGenerator(api_key=Secret.from_env_var(\"OPENAI_API_KEY\"), model=\"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "# --- 2. Build the Naive RAG Pipeline ---\n",
    "\n",
    "naive_rag_pipeline = Pipeline()\n",
    "\n",
    "# Add components to the pipeline\n",
    "naive_rag_pipeline.add_component(\"text_embedder\", text_embedder)\n",
    "naive_rag_pipeline.add_component(\"retriever\", retriever)\n",
    "naive_rag_pipeline.add_component(\"prompt_builder\", prompt_builder_inst)\n",
    "naive_rag_pipeline.add_component(\"llm\", llm_generator_inst)\n",
    "\n",
    "# --- 3. Connect the Components ---\n",
    "\n",
    "# The query embedding is sent to the retriever\n",
    "naive_rag_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "# The retriever's documents are sent to the prompt builder\n",
    "naive_rag_pipeline.connect(\"retriever.documents\", \"prompt_builder.documents\")\n",
    "# The final prompt is sent to the LLM\n",
    "naive_rag_pipeline.connect(\"prompt_builder.prompt\", \"llm.prompt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1684f0b",
   "metadata": {},
   "source": [
    "## 3. Visualizing the Pipeline\n",
    "\n",
    "It's helpful to visualize the pipeline to understand how data flows between components. The diagram below shows the connections between each part of our semantic search pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26885c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Visualize the Pipeline ---\n",
    "naive_rag_pipeline.draw(path=\"./images/naive_rag_pipeline.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8528c173",
   "metadata": {},
   "source": [
    "![](./images/naive_rag_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3909452",
   "metadata": {},
   "source": [
    "## 4. Running the Pipeline\n",
    "\n",
    "Let's put our pipeline to the test! We'll ask a question, and the pipeline will retrieve relevant documents and generate an answer. You can modify the question to experiment with different queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba680288",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: Which company released the Claude 3 model family?\n",
      "Answer: ['The Claude 3 model family was released by Anthropic.']\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Run the Pipeline ---\n",
    "\n",
    "question = \"Which company released the Claude 3 model family?\"\n",
    "\n",
    "# The run method requires inputs for the components that don't have an incoming connection.\n",
    "# In this case, 'text_embedder' needs the 'text' (the query) and 'prompt_builder' needs the 'question'.\n",
    "result = naive_rag_pipeline.run({\n",
    "    \"text_embedder\": {\"text\": question},\n",
    "    \"prompt_builder\": {\"question\": question}\n",
    "})\n",
    "\n",
    "print(f\"\\nQuestion: {question}\")\n",
    "print(f\"Answer: {result['llm']['replies']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28bd20c",
   "metadata": {},
   "source": [
    "Try additional questions to see how the pipeline responds using different sources of indexed data, such as web content or CSV files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23483d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 69.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is Haystack 2.0?\n",
      "Answer: ['Haystack 2.0 is an open-source Python framework for building production-ready LLM (Large Language Model) applications. It allows for the implementation of composable AI systems that are easy to use, customize, extend, optimize, evaluate, and deploy to production. This version is a major rework of the previous version, with the goal of providing flexibility and a common component interface for seamless interaction between different components. Haystack 2.0 integrates with almost all major model providers and databases and enables users to create custom components and foster an open ecosystem around its framework.']\n"
     ]
    }
   ],
   "source": [
    "# Another example question using the web data\n",
    "question_2 = \"What is Haystack 2.0?\"\n",
    "result_2 = naive_rag_pipeline.run({\n",
    "    \"text_embedder\": {\"text\": question_2},\n",
    "    \"prompt_builder\": {\"question\": question_2}\n",
    "})\n",
    "print(f\"\\nQuestion: {question_2}\")\n",
    "print(f\"Answer: {result_2['llm']['replies']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e266a19",
   "metadata": {},
   "source": [
    "You can also ask questions that target structured data, like information from CSV files. This demonstrates the flexibility of the pipeline in handling various document types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db81f019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: When was Gemini released?\n",
      "Answer: ['Gemini was released in 2023.']\n"
     ]
    }
   ],
   "source": [
    "# Another example question using the csv data\n",
    "question_2 = \"When was Gemini released?\"\n",
    "result_2 = naive_rag_pipeline.run({\n",
    "    \"text_embedder\": {\"text\": question_2},\n",
    "    \"prompt_builder\": {\"question\": question_2}\n",
    "})\n",
    "print(f\"\\nQuestion: {question_2}\")\n",
    "print(f\"Answer: {result_2['llm']['replies']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-with-haystack-ch4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
