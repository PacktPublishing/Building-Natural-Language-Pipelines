{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a9cd5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurafunderburk/Documents/GitHub/Building-Natural-Language-Pipelines/ch4/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running unified indexing pipeline for web, local files, and CSV...\n",
      "Skipping PDF file: data_for_indexing/sample.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing document 1384ec36dd6d99f90ab589732d5219b7371dac846d0f0bd89c6385189c4079c0. Keeping it, but skipping cleaning. Error: Error tokenizing data. C error: Expected 5 fields in line 5, saw 7\n",
      "\n",
      "Error processing document 1384ec36dd6d99f90ab589732d5219b7371dac846d0f0bd89c6385189c4079c0. Keeping it, but skipping splitting. Error: Error tokenizing data. C error: Expected 5 fields in line 5, saw 7\n",
      "\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.77it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x308bf4080>\n",
       "ðŸš… Components\n",
       "  - text_embedder: SentenceTransformersTextEmbedder\n",
       "  - retriever: InMemoryEmbeddingRetriever\n",
       "  - prompt_builder: PromptBuilder\n",
       "  - llm: OpenAIGenerator\n",
       "ðŸ›¤ï¸ Connections\n",
       "  - text_embedder.embedding -> retriever.query_embedding (list[float])\n",
       "  - retriever.documents -> prompt_builder.documents (list[Document])\n",
       "  - prompt_builder.prompt -> llm.prompt (str)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Continue from the previous script, assuming 'document_store' is populated.\n",
    "from scripts.indexing import document_store  # Adjust the import as necessary\n",
    "\n",
    "# Import necessary components for the query pipeline\n",
    "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.utils import Secret\n",
    "from haystack import Pipeline\n",
    "\n",
    "# --- 1. Initialize Query Pipeline Components ---\n",
    "\n",
    "# Text Embedder: To embed the user's query. Must be compatible with the document embedder.\n",
    "text_embedder = SentenceTransformersTextEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Retriever: Fetches documents from the DocumentStore based on vector similarity.\n",
    "retriever = InMemoryEmbeddingRetriever(document_store=document_store, top_k=3)\n",
    "\n",
    "# PromptBuilder: Creates a prompt using the retrieved documents and the query.\n",
    "# The Jinja2 template iterates through the documents and adds their content to the prompt.\n",
    "prompt_template_for_pipeline = \"\"\"\n",
    "Given the following information, answer the user's question.\n",
    "If the information is not available in the provided documents, say that you don't have enough information to answer.\n",
    "\n",
    "Context:\n",
    "{% for doc in documents %}\n",
    "    {{ doc.content }}\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{question}}\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt_builder_inst = PromptBuilder(template=prompt_template_for_pipeline,\n",
    "                                    required_variables=\"*\")\n",
    "llm_generator_inst = OpenAIGenerator(api_key=Secret.from_env_var(\"OPENAI_API_KEY\"), model=\"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "# --- 2. Build the Naive RAG Pipeline ---\n",
    "\n",
    "naive_rag_pipeline = Pipeline()\n",
    "\n",
    "# Add components to the pipeline\n",
    "naive_rag_pipeline.add_component(\"text_embedder\", text_embedder)\n",
    "naive_rag_pipeline.add_component(\"retriever\", retriever)\n",
    "naive_rag_pipeline.add_component(\"prompt_builder\", prompt_builder_inst)\n",
    "naive_rag_pipeline.add_component(\"llm\", llm_generator_inst)\n",
    "\n",
    "# --- 3. Connect the Components ---\n",
    "\n",
    "# The query embedding is sent to the retriever\n",
    "naive_rag_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "# The retriever's documents are sent to the prompt builder\n",
    "naive_rag_pipeline.connect(\"retriever.documents\", \"prompt_builder.documents\")\n",
    "# The final prompt is sent to the LLM\n",
    "naive_rag_pipeline.connect(\"prompt_builder.prompt\", \"llm.prompt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26885c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Visualize the Pipeline ---\n",
    "naive_rag_pipeline.draw(path=\"./images/naive_rag_pipeline.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8528c173",
   "metadata": {},
   "source": [
    "![](./images/naive_rag_pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba680288",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: Which company released the Claude 3 model family?\n",
      "Answer: ['The Claude 3 model family was released by Anthropic.']\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Run the Pipeline ---\n",
    "\n",
    "question = \"Which company released the Claude 3 model family?\"\n",
    "\n",
    "# The run method requires inputs for the components that don't have an incoming connection.\n",
    "# In this case, 'text_embedder' needs the 'text' (the query) and 'prompt_builder' needs the 'question'.\n",
    "result = naive_rag_pipeline.run({\n",
    "    \"text_embedder\": {\"text\": question},\n",
    "    \"prompt_builder\": {\"question\": question}\n",
    "})\n",
    "\n",
    "print(f\"\\nQuestion: {question}\")\n",
    "print(f\"Answer: {result['llm']['replies']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23483d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 69.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is Haystack 2.0?\n",
      "Answer: ['Haystack 2.0 is an open-source Python framework for building production-ready LLM (Large Language Model) applications. It allows for the implementation of composable AI systems that are easy to use, customize, extend, optimize, evaluate, and deploy to production. This version is a major rework of the previous version, with the goal of providing flexibility and a common component interface for seamless interaction between different components. Haystack 2.0 integrates with almost all major model providers and databases and enables users to create custom components and foster an open ecosystem around its framework.']\n"
     ]
    }
   ],
   "source": [
    "# Another example question using the web data\n",
    "question_2 = \"What is Haystack 2.0?\"\n",
    "result_2 = naive_rag_pipeline.run({\n",
    "    \"text_embedder\": {\"text\": question_2},\n",
    "    \"prompt_builder\": {\"question\": question_2}\n",
    "})\n",
    "print(f\"\\nQuestion: {question_2}\")\n",
    "print(f\"Answer: {result_2['llm']['replies']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db81f019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  7.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: When was Gemini released?\n",
      "Answer: ['Gemini was released in 2023.']\n"
     ]
    }
   ],
   "source": [
    "# Another example question using the csv data\n",
    "question_2 = \"When was Gemini released?\"\n",
    "result_2 = naive_rag_pipeline.run({\n",
    "    \"text_embedder\": {\"text\": question_2},\n",
    "    \"prompt_builder\": {\"question\": question_2}\n",
    "})\n",
    "print(f\"\\nQuestion: {question_2}\")\n",
    "print(f\"Answer: {result_2['llm']['replies']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-with-haystack-ch4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
