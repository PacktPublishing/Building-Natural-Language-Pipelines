{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06c1936d",
   "metadata": {},
   "source": [
    "ðŸ”§ **Setup Required**: Before running this notebook, please follow the [setup instructions](../README.md#setup-instructions) to configure your environment and API keys."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f9638a",
   "metadata": {},
   "source": [
    "# Building a Semantic Search RAG Pipeline (Naive RAG)\n",
    "\n",
    "In this notebook, we'll build a semantic search pipeline using the Haystack framework. This implementation represents a \"naive\" RAG (Retrieval-Augmented Generation) approach, which follows these key steps:\n",
    "\n",
    "1. **Query Processing**: Convert user questions into vector embeddings\n",
    "2. **Document Retrieval**: Find relevant documents using semantic similarity\n",
    "3. **Context Building**: Create a prompt combining the question and retrieved documents\n",
    "4. **Answer Generation**: Use an LLM to generate answers based on the context\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- How to build a basic RAG pipeline using Haystack components\n",
    "- Understanding semantic search with vector embeddings\n",
    "- Techniques for connecting pipeline components\n",
    "- Working with document stores and retrievers\n",
    "- Using LLMs for answer generation\n",
    "\n",
    "This represents the simplest form of RAG, providing a foundation for more advanced implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb61715",
   "metadata": {},
   "source": [
    "# 1. Component Imports and Setup\n",
    "\n",
    "We'll import several key components from the Haystack framework:\n",
    "\n",
    "1. **`SentenceTransformersTextEmbedder`**\n",
    "   - Converts text into vector embeddings\n",
    "   - Uses the powerful sentence-transformers library\n",
    "   - Essential for semantic understanding\n",
    "\n",
    "2. **`InMemoryEmbeddingRetriever`**\n",
    "   - Finds relevant documents using vector similarity\n",
    "   - Works with our document store\n",
    "   - Configurable for precision vs. recall trade-offs\n",
    "\n",
    "3. **`PromptBuilder`**\n",
    "   - Creates structured prompts for the LLM\n",
    "   - Uses Jinja2 templates for flexible formatting\n",
    "   - Combines context with user questions\n",
    "\n",
    "4. **`OpenAIGenerator`**\n",
    "   - Interfaces with OpenAI's LLM models\n",
    "   - Generates natural language responses\n",
    "   - Handles API communication securely\n",
    "\n",
    "We'll also import Pipeline from Haystack, which lets us connect these components into a cohesive system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b82280be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurafunderburk/Documents/GitHub/Building-Natural-Language-Pipelines/ch4/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running unified indexing pipeline for web, local files, and CSV...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing document 1384ec36dd6d99f90ab589732d5219b7371dac846d0f0bd89c6385189c4079c0. Keeping it, but skipping cleaning. Error: Error tokenizing data. C error: Expected 5 fields in line 5, saw 7\n",
      "\n",
      "Error processing document 1384ec36dd6d99f90ab589732d5219b7371dac846d0f0bd89c6385189c4079c0. Keeping it, but skipping splitting. Error: Error tokenizing data. C error: Expected 5 fields in line 5, saw 7\n",
      "\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# Continue from the previous script, assuming 'document_store' is populated.\n",
    "from scripts.indexing import document_store  # Adjust the import as necessary\n",
    "\n",
    "# Import necessary components for the query pipeline\n",
    "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.utils import Secret\n",
    "from haystack import Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5481d1ad",
   "metadata": {},
   "source": [
    "# 2. Building the Naive RAG Pipeline\n",
    "\n",
    "In this section, we'll initialize and connect the core components of our RAG pipeline. Each component plays a crucial role:\n",
    "\n",
    "**Text Embedder Configuration**\n",
    "- Uses the all-MiniLM-L6-v2 model\n",
    "- Optimized for semantic similarity tasks\n",
    "- Produces 384-dimensional embeddings\n",
    "\n",
    "**Retriever Setup**\n",
    "- Connected to our document store\n",
    "- Returns top 3 most similar documents\n",
    "- Uses cosine similarity for matching\n",
    "\n",
    "**Prompt Engineering**\n",
    "- Template includes clear instructions\n",
    "- Handles multiple documents elegantly\n",
    "- Includes fallback for missing information\n",
    "\n",
    "**LLM Integration**\n",
    "- Uses GPT-4 for high-quality responses\n",
    "- Securely manages API keys\n",
    "- Optimized for contextual understanding\n",
    "\n",
    "The code below shows how these components are initialized and assembled into a working pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a9cd5b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x3b3c23470>\n",
       "ðŸš… Components\n",
       "  - text_embedder: SentenceTransformersTextEmbedder\n",
       "  - retriever: InMemoryEmbeddingRetriever\n",
       "  - prompt_builder: PromptBuilder\n",
       "  - llm: OpenAIGenerator\n",
       "ðŸ›¤ï¸ Connections\n",
       "  - text_embedder.embedding -> retriever.query_embedding (list[float])\n",
       "  - retriever.documents -> prompt_builder.documents (list[Document])\n",
       "  - prompt_builder.prompt -> llm.prompt (str)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# --- 1. Initialize Query Pipeline Components ---\n",
    "\n",
    "# Text Embedder: To embed the user's query. Must be compatible with the document embedder.\n",
    "text_embedder = SentenceTransformersTextEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Retriever: Fetches documents from the DocumentStore based on vector similarity.\n",
    "retriever = InMemoryEmbeddingRetriever(document_store=document_store, top_k=3)\n",
    "\n",
    "# PromptBuilder: Creates a prompt using the retrieved documents and the query.\n",
    "# The Jinja2 template iterates through the documents and adds their content to the prompt.\n",
    "prompt_template_for_pipeline = \"\"\"\n",
    "Given the following information, answer the user's question.\n",
    "If the information is not available in the provided documents, say that you don't have enough information to answer.\n",
    "\n",
    "Context:\n",
    "{% for doc in documents %}\n",
    "    {{ doc.content }}\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{question}}\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt_builder_inst = PromptBuilder(template=prompt_template_for_pipeline,\n",
    "                                    required_variables=\"*\")\n",
    "llm_generator_inst = OpenAIGenerator(api_key=Secret.from_env_var(\"OPENAI_API_KEY\"), model=\"gpt-4o-mini\")\n",
    "\n",
    "# optional - if you want to use open source model instead of OpenAI\n",
    "# from haystack_integrations.components.generators.ollama import OllamaGenerator, OllamaChatGenerator\n",
    "# llm_generator_inst = OllamaGenerator(model=\"mistral-nemo:12b\",\n",
    "#                             generation_kwargs={\n",
    "#                               \"num_predict\": 100,\n",
    "#                               \"temperature\": 0.9,\n",
    "#                             })\n",
    "\n",
    "\n",
    "\n",
    "# --- 2. Build the Naive RAG Pipeline ---\n",
    "\n",
    "naive_rag_pipeline = Pipeline()\n",
    "\n",
    "# Add components to the pipeline\n",
    "naive_rag_pipeline.add_component(\"text_embedder\", text_embedder)\n",
    "naive_rag_pipeline.add_component(\"retriever\", retriever)\n",
    "naive_rag_pipeline.add_component(\"prompt_builder\", prompt_builder_inst)\n",
    "naive_rag_pipeline.add_component(\"llm\", llm_generator_inst)\n",
    "\n",
    "# --- 3. Connect the Components ---\n",
    "\n",
    "# The query embedding is sent to the retriever\n",
    "naive_rag_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "# The retriever's documents are sent to the prompt builder\n",
    "naive_rag_pipeline.connect(\"retriever.documents\", \"prompt_builder.documents\")\n",
    "# The final prompt is sent to the LLM\n",
    "naive_rag_pipeline.connect(\"prompt_builder.prompt\", \"llm.prompt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1684f0b",
   "metadata": {},
   "source": [
    "# 3. Visualizing the Pipeline\n",
    "\n",
    "Understanding the flow of information through the pipeline is crucial. The visualization below shows:\n",
    "\n",
    "**Data Flow Path:**\n",
    "1. User question â†’ Text Embedder\n",
    "2. Embeddings â†’ Retriever\n",
    "3. Retrieved Documents â†’ Prompt Builder\n",
    "4. Final Prompt â†’ LLM Generator\n",
    "\n",
    "**Key Connections:**\n",
    "- `text_embedder.embedding` â†’ `retriever.query_embedding`\n",
    "- `retriever.documents` â†’ `prompt_builder.documents`\n",
    "- `prompt_builder.prompt` â†’ `llm.prompt`\n",
    "\n",
    "Each arrow represents a data transformation step, showing how the question flows through the system to generate an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26885c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Visualize the Pipeline ---\n",
    "naive_rag_pipeline.draw(path=\"./images/naive_rag_pipeline.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8528c173",
   "metadata": {},
   "source": [
    "![](./images/naive_rag_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3909452",
   "metadata": {},
   "source": [
    "# 4. Running the Pipeline\n",
    "\n",
    "Now we'll test our pipeline with real questions. When we run the pipeline:\n",
    "\n",
    "1. The question is converted to embeddings\n",
    "2. Similar documents are retrieved from our store\n",
    "3. A prompt is constructed with the context\n",
    "4. The LLM generates a natural language answer\n",
    "\n",
    "**Input Requirements:**\n",
    "- `text_embedder`: Needs the raw question text\n",
    "- `prompt_builder`: Needs the question for template\n",
    "\n",
    "**Expected Output:**\n",
    "- Natural language answer based on retrieved documents\n",
    "- \"No information\" response if context is insufficient\n",
    "\n",
    "Try modifying the question to explore different types of queries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba680288",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  6.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: Which company released the Claude 3 model family?\n",
      "Answer: ['The Claude 3 model family was released by Anthropic.']\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Run the Pipeline ---\n",
    "\n",
    "question = \"Which company released the Claude 3 model family?\"\n",
    "\n",
    "# The run method requires inputs for the components that don't have an incoming connection.\n",
    "# In this case, 'text_embedder' needs the 'text' (the query) and 'prompt_builder' needs the 'question'.\n",
    "result = naive_rag_pipeline.run({\n",
    "    \"text_embedder\": {\"text\": question},\n",
    "    \"prompt_builder\": {\"question\": question}\n",
    "})\n",
    "\n",
    "print(f\"\\nQuestion: {question}\")\n",
    "print(f\"Answer: {result['llm']['replies']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28bd20c",
   "metadata": {},
   "source": [
    "# 5. Exploring Different Data Sources\n",
    "\n",
    "Our pipeline can handle various types of questions across different data sources. Let's explore:\n",
    "\n",
    "**Web Content Queries**\n",
    "- Questions about Haystack framework\n",
    "- Technical documentation queries\n",
    "- Current technology trends\n",
    "\n",
    "The example below demonstrates how the pipeline handles queries about technical documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23483d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 90.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is Haystack 2.0?\n",
      "Answer: ['Haystack 2.0 is an open-source Python framework designed for building production-ready LLM (Large Language Model) applications. It offers a composable AI system that is flexible, customizable, and easy to extend, optimize, evaluate, and deploy. The framework supports integrations with major model providers and databases and aims to facilitate the creation of custom components that interact seamlessly within the system.']\n"
     ]
    }
   ],
   "source": [
    "# Another example question using the web data\n",
    "question_2 = \"What is Haystack 2.0?\"\n",
    "result_2 = naive_rag_pipeline.run({\n",
    "    \"text_embedder\": {\"text\": question_2},\n",
    "    \"prompt_builder\": {\"question\": question_2}\n",
    "})\n",
    "print(f\"\\nQuestion: {question_2}\")\n",
    "print(f\"Answer: {result_2['llm']['replies']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e266a19",
   "metadata": {},
   "source": [
    "# 6. Querying Structured Data\n",
    "\n",
    "The pipeline is equally capable of handling structured data from CSV files. This demonstrates:\n",
    "\n",
    "**Advantages:**\n",
    "- Unified interface for different data types\n",
    "- Semantic understanding of tabular data\n",
    "- Natural language queries for structured information\n",
    "\n",
    "**Example Use Cases:**\n",
    "- Release dates of AI models\n",
    "- Technical specifications\n",
    "- Historical data queries\n",
    "\n",
    "The example below shows how to query specific information from our CSV dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db81f019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 20.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: When was Gemini released?\n",
      "Answer: ['Gemini was released in 2023.']\n"
     ]
    }
   ],
   "source": [
    "# Another example question using the csv data\n",
    "question_2 = \"When was Gemini released?\"\n",
    "result_2 = naive_rag_pipeline.run({\n",
    "    \"text_embedder\": {\"text\": question_2},\n",
    "    \"prompt_builder\": {\"question\": question_2}\n",
    "})\n",
    "print(f\"\\nQuestion: {question_2}\")\n",
    "print(f\"Answer: {result_2['llm']['replies']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3290baf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-with-haystack-ch4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
