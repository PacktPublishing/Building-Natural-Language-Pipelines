{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a18bcac",
   "metadata": {},
   "source": [
    "ðŸ”§ **Setup Required**: Before running this notebook, please follow the [setup instructions](../README.md#setup-instructions) to configure your environment and API keys."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf087b5",
   "metadata": {},
   "source": [
    "# Building a Hybrid RAG Pipeline with Haystack\n",
    "\n",
    "Welcome to this notebook where we'll build an advanced Retrieval-Augmented Generation (RAG) pipeline using both dense and sparse retrieval methods. This hybrid approach combines the strengths of:\n",
    "\n",
    "1. **Dense Retrieval**: Using semantic embeddings to capture meaning and context\n",
    "2. **Sparse Retrieval**: Using BM25 algorithm for keyword matching\n",
    "3. **Re-ranking**: Using a transformer model to improve result relevance\n",
    "\n",
    "This combination provides more robust and accurate document retrieval than using either method alone.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- How to combine multiple retrieval methods in a single pipeline\n",
    "- The benefits of hybrid search approaches\n",
    "- How to use re-ranking to improve search results\n",
    "- Advanced pipeline construction with multiple components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee27a95",
   "metadata": {},
   "source": [
    "## 1. Required Components\n",
    "\n",
    "Let's start by importing the specialized components needed for our hybrid pipeline:\n",
    "\n",
    "- `InMemoryBM25Retriever`: Implements the BM25 algorithm for keyword-based search\n",
    "- `DocumentJoiner`: Combines results from multiple retrievers\n",
    "- `TransformersSimilarityRanker`: Re-ranks documents using a transformer model\n",
    "\n",
    "We'll also import the basic RAG components we used in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07377961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue from the previous script, assuming 'document_store' is populated.\n",
    "from scripts.indexing import document_store #this runs our indexing pipeline\n",
    "# Import additional components for hybrid retrieval\n",
    "from haystack.components.retrievers.in_memory import InMemoryBM25Retriever\n",
    "from haystack.components.joiners import DocumentJoiner\n",
    "from haystack.components.rankers import SentenceTransformersSimilarityRanker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c3723ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary components for the query pipeline\n",
    "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.utils import Secret\n",
    "from haystack import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fbfc50",
   "metadata": {},
   "source": [
    "## 2. Component Initialization\n",
    "\n",
    "Our hybrid pipeline requires several specialized components that work together:\n",
    "\n",
    "### Dense Retrieval Components\n",
    "- **Text Embedder**: Converts text into dense vector representations\n",
    "- **Embedding Retriever**: Uses vector similarity to find relevant documents\n",
    "\n",
    "### Sparse Retrieval Components\n",
    "- **BM25 Retriever**: Uses keyword matching, great for exact matches\n",
    "- **Document Joiner**: Combines results from both retrieval methods\n",
    "\n",
    "### Reranking and Generation\n",
    "- **Ranker**: Uses BAAI/bge-reranker-base to improve result relevance\n",
    "- **Prompt Builder & LLM**: Creates context and generates answers\n",
    "\n",
    "The combination of these components allows us to leverage both semantic understanding and keyword matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc72ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TransformersSimilarityRanker is considered legacy and will no longer receive updates. It may be deprecated in a future release, with removal following after a deprecation period. Consider using SentenceTransformersSimilarityRanker instead, which provides the same functionality along with additional features.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Initialize Query Pipeline Components ---\n",
    "\n",
    "# Text Embedder: To embed the user's query. Must be compatible with the document embedder.\n",
    "text_embedder = SentenceTransformersTextEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Retriever: Fetches documents from the DocumentStore based on vector similarity.\n",
    "retriever = InMemoryEmbeddingRetriever(document_store=document_store, top_k=3)\n",
    "\n",
    "# PromptBuilder: Creates a prompt using the retrieved documents and the query.\n",
    "# The Jinja2 template iterates through the documents and adds their content to the prompt.\n",
    "prompt_template_for_pipeline = \"\"\"\n",
    "Given the following information, answer the user's question.\n",
    "If the information is not available in the provided documents, say that you don't have enough information to answer.\n",
    "\n",
    "Context:\n",
    "{% for doc in documents %}\n",
    "    {{ doc.content }}\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{question}}\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt_builder_inst = PromptBuilder(template=prompt_template_for_pipeline,\n",
    "                                    required_variables=\"*\")\n",
    "llm_generator_inst = OpenAIGenerator(api_key=Secret.from_env_var(\"OPENAI_API_KEY\"), model=\"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "\n",
    "# Sparse Retriever (BM25): For keyword-based search.\n",
    "# This retriever needs to be \"warmed up\" by calculating statistics on the documents in the store.\n",
    "bm25_retriever = InMemoryBM25Retriever(document_store=document_store, top_k=3)\n",
    "\n",
    "# DocumentJoiner: To merge the results from the two retrievers.\n",
    "# The default 'concatenate' mode works well here as the ranker will handle final ordering.\n",
    "document_joiner = DocumentJoiner()\n",
    "\n",
    "# Ranker: A cross-encoder model to re-rank the combined results for higher precision.\n",
    "# This model is highly effective at identifying the most relevant documents from a candidate set.\n",
    "ranker = SentenceTransformersSimilarityRanker(model=\"BAAI/bge-reranker-base\", top_k=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1d8f4d",
   "metadata": {},
   "source": [
    "## 3. Pipeline Assembly\n",
    "\n",
    "Now we'll assemble all components into a cohesive pipeline. The order of components is crucial:\n",
    "\n",
    "1. The query is processed by both dense and sparse retrievers in parallel\n",
    "2. Results are combined by the document joiner\n",
    "3. The ranker improves the relevance of the combined results\n",
    "4. The most relevant documents are used to build the prompt\n",
    "5. The LLM generates the final answer\n",
    "\n",
    "This architecture allows us to benefit from both retrieval methods while using the ranker to select the best documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db01788d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Build the Hybrid RAG Pipeline ---\n",
    "\n",
    "hybrid_rag_pipeline = Pipeline()\n",
    "\n",
    "# Add all necessary components\n",
    "hybrid_rag_pipeline.add_component(\"text_embedder\", text_embedder)\n",
    "hybrid_rag_pipeline.add_component(\"embedding_retriever\", retriever) # Dense retriever\n",
    "hybrid_rag_pipeline.add_component(\"bm25_retriever\", bm25_retriever) # Sparse retriever\n",
    "hybrid_rag_pipeline.add_component(\"document_joiner\", document_joiner)\n",
    "hybrid_rag_pipeline.add_component(\"ranker\", ranker)\n",
    "hybrid_rag_pipeline.add_component(\"prompt_builder\", prompt_builder_inst)\n",
    "hybrid_rag_pipeline.add_component(\"llm\", llm_generator_inst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb12ae7f",
   "metadata": {},
   "source": [
    "## 4. Component Connections\n",
    "\n",
    "The connections between components define how data flows through the pipeline. Understanding these connections is crucial:\n",
    "\n",
    "1. **Query Processing**\n",
    "   - The text embedder processes the query for dense retrieval\n",
    "   - The raw query text is sent directly to the BM25 retriever\n",
    "\n",
    "2. **Document Flow**\n",
    "   - Both retrievers send their documents to the joiner\n",
    "   - The joiner concatenates all documents\n",
    "   - The ranker processes the combined set\n",
    "   - Ranked documents flow to the prompt builder\n",
    "\n",
    "3. **Final Steps**\n",
    "   - The prompt builder creates the context\n",
    "   - The LLM receives the final prompt for answer generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e556bc3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x330ebb9b0>\n",
       "ðŸš… Components\n",
       "  - text_embedder: SentenceTransformersTextEmbedder\n",
       "  - embedding_retriever: InMemoryEmbeddingRetriever\n",
       "  - bm25_retriever: InMemoryBM25Retriever\n",
       "  - document_joiner: DocumentJoiner\n",
       "  - ranker: TransformersSimilarityRanker\n",
       "  - prompt_builder: PromptBuilder\n",
       "  - llm: OpenAIGenerator\n",
       "ðŸ›¤ï¸ Connections\n",
       "  - text_embedder.embedding -> embedding_retriever.query_embedding (list[float])\n",
       "  - embedding_retriever.documents -> document_joiner.documents (list[Document])\n",
       "  - bm25_retriever.documents -> document_joiner.documents (list[Document])\n",
       "  - document_joiner.documents -> ranker.documents (list[Document])\n",
       "  - ranker.documents -> prompt_builder.documents (list[Document])\n",
       "  - prompt_builder.prompt -> llm.prompt (str)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 3. Connect the Components in a Graph ---\n",
    "\n",
    "# The query is embedded for the dense retriever\n",
    "hybrid_rag_pipeline.connect(\"text_embedder.embedding\", \"embedding_retriever.query_embedding\")\n",
    "\n",
    "# The raw query text is sent to the BM25 retriever and the ranker\n",
    "# Note: The query input for these components is the raw text string.\n",
    "\n",
    "# The outputs of both retrievers are fed into the document joiner\n",
    "hybrid_rag_pipeline.connect(\"embedding_retriever.documents\", \"document_joiner.documents\")\n",
    "hybrid_rag_pipeline.connect(\"bm25_retriever.documents\", \"document_joiner.documents\")\n",
    "\n",
    "# The joined documents are sent to the ranker\n",
    "hybrid_rag_pipeline.connect(\"document_joiner.documents\", \"ranker.documents\")\n",
    "\n",
    "# The ranked documents are sent to the prompt builder\n",
    "hybrid_rag_pipeline.connect(\"ranker.documents\", \"prompt_builder.documents\")\n",
    "\n",
    "# The final prompt is sent to the LLM\n",
    "hybrid_rag_pipeline.connect(\"prompt_builder.prompt\", \"llm.prompt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a557517",
   "metadata": {},
   "source": [
    "## 5. Pipeline Visualization\n",
    "\n",
    "Below, we'll generate a visual representation of our hybrid pipeline. This visualization helps us understand:\n",
    "\n",
    "- The overall flow of information\n",
    "- How components are connected\n",
    "- The parallel nature of dense and sparse retrieval\n",
    "- The convergence point at the document joiner\n",
    "- The final processing stages through ranking and generation\n",
    "\n",
    "Study this diagram to understand how the different retrieval methods work together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24e1e4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid pipeline visualization saved to 'hybrid_rag_pipeline.png'\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Visualize the Pipeline (Optional) ---\n",
    "try:\n",
    "    hybrid_rag_pipeline.draw(path=\"./images/hybrid_rag_pipeline.png\")\n",
    "    print(\"Hybrid pipeline visualization saved to 'hybrid_rag_pipeline.png'\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not draw hybrid pipeline: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da4975a",
   "metadata": {},
   "source": [
    "![](./images/hybrid_rag_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5785c9",
   "metadata": {},
   "source": [
    "## 6. Running the Hybrid Pipeline\n",
    "\n",
    "Now let's test our hybrid pipeline with a question. Notice how we need to provide the query to multiple components:\n",
    "\n",
    "- `text_embedder`: For creating query embeddings\n",
    "- `bm25_retriever`: For keyword matching\n",
    "- `ranker`: For comparing query with documents\n",
    "- `prompt_builder`: For including the question in the prompt\n",
    "\n",
    "This example demonstrates how the hybrid approach can leverage both semantic understanding and keyword matching to find the most relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc4b9b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  8.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Run the Pipeline ---\n",
    "\n",
    "# A query that benefits from both semantic and keyword matching\n",
    "hybrid_question = \"What is the Haystack 2.0 framework?\"\n",
    "\n",
    "# The run dictionary must now provide inputs for all components at the start of the graph.\n",
    "# The query text needs to be passed to the text_embedder, bm25_retriever, ranker, and prompt_builder.\n",
    "hybrid_result = hybrid_rag_pipeline.run({\n",
    "    \"text_embedder\": {\"text\": hybrid_question},\n",
    "    \"bm25_retriever\": {\"query\": hybrid_question},\n",
    "    \"ranker\": {\"query\": hybrid_question},\n",
    "    \"prompt_builder\": {\"question\": hybrid_question}\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9963e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is the Haystack 2.0 framework?\n",
      "Answer: ['Haystack 2.0 is an open-source Python framework developed by deepset for building production-ready large language model (LLM) applications. It allows developers to create retrieval-augmented generative pipelines and state-of-the-art search systems. The framework is designed to be flexible and customizable, making it easier to implement composable AI systems that can be extended, optimized, evaluated, and deployed to production. Haystack 2.0 includes integrations with major model providers and databases and comes with documentation, tutorials, and a guide for getting started.']\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nQuestion: {hybrid_question}\")\n",
    "print(f\"Answer: {hybrid_result['llm']['replies']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44e32dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.28s/it]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A query that benefits from both semantic and keyword matching\n",
    "hybrid_question = \"What can I build with Haystack\"\n",
    "\n",
    "# The run dictionary must now provide inputs for all components at the start of the graph.\n",
    "# The query text needs to be passed to the text_embedder, bm25_retriever, ranker, and prompt_builder.\n",
    "hybrid_result = hybrid_rag_pipeline.run({\n",
    "    \"text_embedder\": {\"text\": hybrid_question},\n",
    "    \"bm25_retriever\": {\"query\": hybrid_question},\n",
    "    \"ranker\": {\"query\": hybrid_question},\n",
    "    \"prompt_builder\": {\"question\": hybrid_question}\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f7c0e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llm': {'replies': ['With Haystack 2.0, you can build production-ready LLM (Large Language Model) applications. The framework allows for the creation of composable AI systems that are easy to use, customize, and extend. You can build various components that implement specific logic, such as embedders that convert text into vector representations or retrievers that take embeddings as input and return documents. Additionally, you have the flexibility to create custom components that incorporate unique behaviors and functionalities, fostering an open ecosystem around Haystack. The community and third parties also contribute components, expanding the possibilities of what you can develop.'],\n",
       "  'meta': [{'model': 'gpt-4o-mini-2024-07-18',\n",
       "    'index': 0,\n",
       "    'finish_reason': 'stop',\n",
       "    'usage': {'completion_tokens': 120,\n",
       "     'prompt_tokens': 688,\n",
       "     'total_tokens': 808,\n",
       "     'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "      'audio_tokens': 0,\n",
       "      'reasoning_tokens': 0,\n",
       "      'rejected_prediction_tokens': 0},\n",
       "     'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}}]}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535d0a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-with-haystack-ch4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
