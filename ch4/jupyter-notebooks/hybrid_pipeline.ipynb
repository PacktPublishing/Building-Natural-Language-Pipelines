{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a18bcac",
   "metadata": {},
   "source": [
    "ðŸ”§ **Setup Required**: Before running this notebook, please follow the [setup instructions](../README.md#setup-instructions) to configure your environment and API keys."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf087b5",
   "metadata": {},
   "source": [
    "# Building a Hybrid RAG Pipeline with Haystack\n",
    "\n",
    "Welcome to this notebook where we'll build an advanced Retrieval-Augmented Generation (RAG) pipeline using both dense and sparse retrieval methods. This hybrid approach combines the strengths of:\n",
    "\n",
    "1. **Dense Retrieval**: Using semantic embeddings to capture meaning and context\n",
    "2. **Sparse Retrieval**: Using BM25 algorithm for keyword matching\n",
    "3. **Re-ranking**: Using a transformer model to improve result relevance\n",
    "\n",
    "This combination provides more robust and accurate document retrieval than using either method alone.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- How to combine multiple retrieval methods in a single pipeline\n",
    "- The benefits of hybrid search approaches\n",
    "- How to use re-ranking to improve search results\n",
    "- Advanced pipeline construction with multiple components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee27a95",
   "metadata": {},
   "source": [
    "## 1. Required Components\n",
    "\n",
    "Let's start by importing the specialized components needed for our hybrid pipeline:\n",
    "\n",
    "- `InMemoryBM25Retriever`: Implements the BM25 algorithm for keyword-based search\n",
    "- `DocumentJoiner`: Combines results from multiple retrievers\n",
    "- `TransformersSimilarityRanker`: Re-ranks documents using a transformer model\n",
    "\n",
    "We'll also import the basic RAG components we used in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07377961",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurafunderburk/Documents/GitHub/Building-Natural-Language-Pipelines/ch4/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running unified indexing pipeline for web, local files, and CSV...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing document 1384ec36dd6d99f90ab589732d5219b7371dac846d0f0bd89c6385189c4079c0. Keeping it, but skipping cleaning. Error: Error tokenizing data. C error: Expected 5 fields in line 5, saw 7\n",
      "\n",
      "Error processing document 1384ec36dd6d99f90ab589732d5219b7371dac846d0f0bd89c6385189c4079c0. Keeping it, but skipping splitting. Error: Error tokenizing data. C error: Expected 5 fields in line 5, saw 7\n",
      "\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# Continue from the previous script, assuming 'document_store' is populated.\n",
    "from scripts.indexing import document_store #this runs our indexing pipeline\n",
    "# Import additional components for hybrid retrieval\n",
    "from haystack.components.retrievers.in_memory import InMemoryBM25Retriever\n",
    "from haystack.components.joiners import DocumentJoiner\n",
    "from haystack.components.rankers import SentenceTransformersSimilarityRanker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c3723ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary components for the query pipeline\n",
    "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.utils import Secret\n",
    "from haystack import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fbfc50",
   "metadata": {},
   "source": [
    "## 2. Component Initialization\n",
    "\n",
    "Our hybrid pipeline requires several specialized components that work together:\n",
    "\n",
    "### Dense Retrieval Components\n",
    "- **Text Embedder**: Converts text into dense vector representations\n",
    "- **Embedding Retriever**: Uses vector similarity to find relevant documents\n",
    "\n",
    "### Sparse Retrieval Components\n",
    "- **BM25 Retriever**: Uses keyword matching, great for exact matches\n",
    "- **Document Joiner**: Combines results from both retrieval methods\n",
    "\n",
    "### Reranking and Generation\n",
    "- **Ranker**: Uses BAAI/bge-reranker-base to improve result relevance\n",
    "- **Prompt Builder & LLM**: Creates context and generates answers\n",
    "\n",
    "The combination of these components allows us to leverage both semantic understanding and keyword matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cc72ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Initialize Query Pipeline Components ---\n",
    "\n",
    "# Text Embedder: To embed the user's query. Must be compatible with the document embedder.\n",
    "text_embedder = SentenceTransformersTextEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Retriever: Fetches documents from the DocumentStore based on vector similarity.\n",
    "retriever = InMemoryEmbeddingRetriever(document_store=document_store, top_k=3)\n",
    "\n",
    "# PromptBuilder: Creates a prompt using the retrieved documents and the query.\n",
    "# The Jinja2 template iterates through the documents and adds their content to the prompt.\n",
    "prompt_template_for_pipeline = \"\"\"\n",
    "Given the following information, answer the user's question.\n",
    "If the information is not available in the provided documents, say that you don't have enough information to answer.\n",
    "\n",
    "Context:\n",
    "{% for doc in documents %}\n",
    "    {{ doc.content }}\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{question}}\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt_builder_inst = PromptBuilder(template=prompt_template_for_pipeline,\n",
    "                                    required_variables=\"*\")\n",
    "llm_generator_inst = OpenAIGenerator(api_key=Secret.from_env_var(\"OPENAI_API_KEY\"), model=\"gpt-4o-mini\")\n",
    "\n",
    "# optional - if you want to use open source model instead of OpenAI\n",
    "# from haystack_integrations.components.generators.ollama import OllamaGenerator, OllamaChatGenerator\n",
    "# llm_generator_inst = OllamaGenerator(model=\"mistral-nemo:12b\",\n",
    "#                             generation_kwargs={\n",
    "#                               \"num_predict\": 100,\n",
    "#                               \"temperature\": 0.9,\n",
    "#                             })\n",
    "\n",
    "# Sparse Retriever (BM25): For keyword-based search.\n",
    "# This retriever needs to be \"warmed up\" by calculating statistics on the documents in the store.\n",
    "bm25_retriever = InMemoryBM25Retriever(document_store=document_store, top_k=3)\n",
    "\n",
    "# DocumentJoiner: To merge the results from the two retrievers.\n",
    "# The default 'concatenate' mode works well here as the ranker will handle final ordering.\n",
    "document_joiner = DocumentJoiner()\n",
    "\n",
    "# Ranker: A cross-encoder model to re-rank the combined results for higher precision.\n",
    "# This model is highly effective at identifying the most relevant documents from a candidate set.\n",
    "ranker = SentenceTransformersSimilarityRanker(model=\"BAAI/bge-reranker-base\", top_k=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1d8f4d",
   "metadata": {},
   "source": [
    "## 3. Pipeline Assembly\n",
    "\n",
    "Now we'll assemble all components into a cohesive pipeline. The order of components is crucial:\n",
    "\n",
    "1. The query is processed by both dense and sparse retrievers in parallel\n",
    "2. Results are combined by the document joiner\n",
    "3. The ranker improves the relevance of the combined results\n",
    "4. The most relevant documents are used to build the prompt\n",
    "5. The LLM generates the final answer\n",
    "\n",
    "This architecture allows us to benefit from both retrieval methods while using the ranker to select the best documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db01788d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Build the Hybrid RAG Pipeline ---\n",
    "\n",
    "hybrid_rag_pipeline = Pipeline()\n",
    "\n",
    "# Add all necessary components\n",
    "hybrid_rag_pipeline.add_component(\"text_embedder\", text_embedder)\n",
    "hybrid_rag_pipeline.add_component(\"embedding_retriever\", retriever) # Dense retriever\n",
    "hybrid_rag_pipeline.add_component(\"bm25_retriever\", bm25_retriever) # Sparse retriever\n",
    "hybrid_rag_pipeline.add_component(\"document_joiner\", document_joiner)\n",
    "hybrid_rag_pipeline.add_component(\"ranker\", ranker)\n",
    "hybrid_rag_pipeline.add_component(\"prompt_builder\", prompt_builder_inst)\n",
    "hybrid_rag_pipeline.add_component(\"llm\", llm_generator_inst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb12ae7f",
   "metadata": {},
   "source": [
    "## 4. Component Connections\n",
    "\n",
    "The connections between components define how data flows through the pipeline. Understanding these connections is crucial:\n",
    "\n",
    "1. **Query Processing**\n",
    "   - The text embedder processes the query for dense retrieval\n",
    "   - The raw query text is sent directly to the BM25 retriever\n",
    "\n",
    "2. **Document Flow**\n",
    "   - Both retrievers send their documents to the joiner\n",
    "   - The joiner concatenates all documents\n",
    "   - The ranker processes the combined set\n",
    "   - Ranked documents flow to the prompt builder\n",
    "\n",
    "3. **Final Steps**\n",
    "   - The prompt builder creates the context\n",
    "   - The LLM receives the final prompt for answer generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e556bc3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x177f8bcb0>\n",
       "ðŸš… Components\n",
       "  - text_embedder: SentenceTransformersTextEmbedder\n",
       "  - embedding_retriever: InMemoryEmbeddingRetriever\n",
       "  - bm25_retriever: InMemoryBM25Retriever\n",
       "  - document_joiner: DocumentJoiner\n",
       "  - ranker: SentenceTransformersSimilarityRanker\n",
       "  - prompt_builder: PromptBuilder\n",
       "  - llm: OpenAIGenerator\n",
       "ðŸ›¤ï¸ Connections\n",
       "  - text_embedder.embedding -> embedding_retriever.query_embedding (list[float])\n",
       "  - embedding_retriever.documents -> document_joiner.documents (list[Document])\n",
       "  - bm25_retriever.documents -> document_joiner.documents (list[Document])\n",
       "  - document_joiner.documents -> ranker.documents (list[Document])\n",
       "  - ranker.documents -> prompt_builder.documents (list[Document])\n",
       "  - prompt_builder.prompt -> llm.prompt (str)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 3. Connect the Components in a Graph ---\n",
    "\n",
    "# The query is embedded for the dense retriever\n",
    "hybrid_rag_pipeline.connect(\"text_embedder.embedding\", \"embedding_retriever.query_embedding\")\n",
    "\n",
    "# The raw query text is sent to the BM25 retriever and the ranker\n",
    "# Note: The query input for these components is the raw text string.\n",
    "\n",
    "# The outputs of both retrievers are fed into the document joiner\n",
    "hybrid_rag_pipeline.connect(\"embedding_retriever.documents\", \"document_joiner.documents\")\n",
    "hybrid_rag_pipeline.connect(\"bm25_retriever.documents\", \"document_joiner.documents\")\n",
    "\n",
    "# The joined documents are sent to the ranker\n",
    "hybrid_rag_pipeline.connect(\"document_joiner.documents\", \"ranker.documents\")\n",
    "\n",
    "# The ranked documents are sent to the prompt builder\n",
    "hybrid_rag_pipeline.connect(\"ranker.documents\", \"prompt_builder.documents\")\n",
    "\n",
    "# The final prompt is sent to the LLM\n",
    "hybrid_rag_pipeline.connect(\"prompt_builder.prompt\", \"llm.prompt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a557517",
   "metadata": {},
   "source": [
    "## 5. Pipeline Visualization\n",
    "\n",
    "Below, we'll generate a visual representation of our hybrid pipeline. This visualization helps us understand:\n",
    "\n",
    "- The overall flow of information\n",
    "- How components are connected\n",
    "- The parallel nature of dense and sparse retrieval\n",
    "- The convergence point at the document joiner\n",
    "- The final processing stages through ranking and generation\n",
    "\n",
    "Study this diagram to understand how the different retrieval methods work together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24e1e4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid pipeline visualization saved to 'hybrid_rag_pipeline.png'\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Visualize the Pipeline (Optional) ---\n",
    "try:\n",
    "    hybrid_rag_pipeline.draw(path=\"./images/hybrid_rag_pipeline.png\")\n",
    "    print(\"Hybrid pipeline visualization saved to 'hybrid_rag_pipeline.png'\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not draw hybrid pipeline: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da4975a",
   "metadata": {},
   "source": [
    "![](./images/hybrid_rag_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5785c9",
   "metadata": {},
   "source": [
    "## 6. Running the Hybrid Pipeline\n",
    "\n",
    "Now let's test our hybrid pipeline with a question. Notice how we need to provide the query to multiple components:\n",
    "\n",
    "- `text_embedder`: For creating query embeddings\n",
    "- `bm25_retriever`: For keyword matching\n",
    "- `ranker`: For comparing query with documents\n",
    "- `prompt_builder`: For including the question in the prompt\n",
    "\n",
    "This example demonstrates how the hybrid approach can leverage both semantic understanding and keyword matching to find the most relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc4b9b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  8.27it/s]\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Run the Pipeline ---\n",
    "\n",
    "# A query that benefits from both semantic and keyword matching\n",
    "hybrid_question = \"What is the Haystack 2.0 framework?\"\n",
    "\n",
    "# The run dictionary must now provide inputs for all components at the start of the graph.\n",
    "# The query text needs to be passed to the text_embedder, bm25_retriever, ranker, and prompt_builder.\n",
    "hybrid_result = hybrid_rag_pipeline.run({\n",
    "    \"text_embedder\": {\"text\": hybrid_question},\n",
    "    \"bm25_retriever\": {\"query\": hybrid_question},\n",
    "    \"ranker\": {\"query\": hybrid_question},\n",
    "    \"prompt_builder\": {\"question\": hybrid_question}\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9963e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is the Haystack 2.0 framework?\n",
      "Answer: ['Haystack 2.0 is an open-source Python framework developed by deepset for building production-ready language model (LLM) applications. It offers a flexible and customizable platform that allows developers to create retrieval-augmented generative pipelines and advanced search systems. The framework is designed to be composable, making it easier to use, extend, optimize, evaluate, and deploy LLM applications in production environments. Haystack 2.0 includes integrations with major model providers and databases, and it introduces new features and optimizations to enhance its capabilities.']\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nQuestion: {hybrid_question}\")\n",
    "print(f\"Answer: {hybrid_result['llm']['replies']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44e32dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  9.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# A query that benefits from both semantic and keyword matching\n",
    "hybrid_question = \"What can I build with Haystack\"\n",
    "\n",
    "# The run dictionary must now provide inputs for all components at the start of the graph.\n",
    "# The query text needs to be passed to the text_embedder, bm25_retriever, ranker, and prompt_builder.\n",
    "hybrid_result = hybrid_rag_pipeline.run({\n",
    "    \"text_embedder\": {\"text\": hybrid_question},\n",
    "    \"bm25_retriever\": {\"query\": hybrid_question},\n",
    "    \"ranker\": {\"query\": hybrid_question},\n",
    "    \"prompt_builder\": {\"question\": hybrid_question}\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f7c0e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llm': {'replies': ['With Haystack, you can build composable end-to-end applications that incorporate various language models such as embedding, extractive question answering (QA), and ranking, along with your choice of databases. Haystack 2.0 allows for creating custom components, fostering an open ecosystem, and implementing flexible AI systems that are easy to use, customize, extend, optimize, evaluate, and deploy to production.'],\n",
       "  'meta': [{'model': 'gpt-4o-mini-2024-07-18',\n",
       "    'index': 0,\n",
       "    'finish_reason': 'stop',\n",
       "    'usage': {'completion_tokens': 80,\n",
       "     'prompt_tokens': 691,\n",
       "     'total_tokens': 771,\n",
       "     'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "      'audio_tokens': 0,\n",
       "      'reasoning_tokens': 0,\n",
       "      'rejected_prediction_tokens': 0},\n",
       "     'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}}]}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535d0a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-with-haystack-ch4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
