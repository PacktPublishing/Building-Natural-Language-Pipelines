{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building retriever pipelines with an LLM\n",
    "\n",
    "The provided code implements a Retrieval-Augmented Generation (RAG) pipeline. It retrieves relevant documents from an in-memory document store and uses a language model (OpenAI's GPT) to generate an answer based on those documents. Here's a step-by-step breakdown of what this code does:\n",
    "\n",
    "### Imports and Setup:\n",
    "Imports various necessary components from the haystack package.\n",
    "The OpenAI API Key is set using `os.environ[\"OPENAI_API_KEY\"]`, which is required to interact with the OpenAI API for generating answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from haystack import Document\n",
    "from haystack import Pipeline\n",
    "from haystack.components.builders.answer_builder import AnswerBuilder\n",
    "from haystack.components.builders.prompt_builder import PromptBuilder\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.components.retrievers.in_memory import InMemoryBM25Retriever\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from dotenv import load_dotenv\n",
    "from haystack.utils import Secret\n",
    "\n",
    "load_dotenv('.env')\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Template Definition:\n",
    "The `prompt_template` is a string that defines how the input documents and query should be formatted for the language model (OpenAI).\n",
    "The template uses Jinja2 syntax to iterate over the documents and inject their content into the prompt, followed by the question that is asked. The question and the documents are combined in a way that OpenAI can use to generate an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "    Given these documents, answer the question.\\nDocuments:\n",
    "    {% for doc in documents %}\n",
    "        {{ doc.content }}\n",
    "    {% endfor %}\n",
    "\n",
    "    \\nQuestion: {{question}}\n",
    "    \\nAnswer:\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the RAG Pipeline:\n",
    "\n",
    "* Pipeline creation: An instance of the Haystack Pipeline class is created to represent the full RAG flow.\n",
    "\n",
    "Adding components:\n",
    "* Retriever (`InMemoryBM25Retriever`): This component is responsible for retrieving relevant documents from the InMemoryDocumentStore based on a query.\n",
    "* Prompt Builder (`PromptBuilder`): This component uses the prompt template to build a prompt from the documents retrieved and the query.\n",
    "* Generator (`OpenAIGenerator`): This component sends the generated prompt to OpenAI's GPT model and retrieves the generated answer.\n",
    "* Answer Builder (`AnswerBuilder`): This component processes the output from the generator to format and extract the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_pipeline = Pipeline()\n",
    "rag_pipeline.add_component(instance=InMemoryBM25Retriever(document_store=InMemoryDocumentStore()), name=\"retriever\")\n",
    "rag_pipeline.add_component(instance=PromptBuilder(template=prompt_template), name=\"prompt_builder\")\n",
    "rag_pipeline.add_component(instance=OpenAIGenerator(api_key= Secret.from_env_var(\"OPENAI_API_KEY\")), name=\"llm\")\n",
    "rag_pipeline.add_component(instance=AnswerBuilder(), name=\"answer_builder\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting Components:\n",
    "The components are connected sequentially using `rag_pipeline.connect()`.\n",
    "\n",
    "* The retriever component sends retrieved documents to the prompt builder.\n",
    "* The prompt builder generates the formatted prompt and sends it to the language model (OpenAI).\n",
    "* The language model's replies are sent to the answer builder for extracting and returning the final answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x14141b200>\n",
       "ðŸš… Components\n",
       "  - retriever: InMemoryBM25Retriever\n",
       "  - prompt_builder: PromptBuilder\n",
       "  - llm: OpenAIGenerator\n",
       "  - answer_builder: AnswerBuilder\n",
       "ðŸ›¤ï¸ Connections\n",
       "  - retriever.documents -> prompt_builder.documents (List[Document])\n",
       "  - retriever.documents -> answer_builder.documents (List[Document])\n",
       "  - prompt_builder.prompt -> llm.prompt (str)\n",
       "  - llm.replies -> answer_builder.replies (List[str])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_pipeline.connect(\"retriever\", \"prompt_builder.documents\")\n",
    "rag_pipeline.connect(\"prompt_builder\", \"llm\")\n",
    "rag_pipeline.connect(\"llm.replies\", \"answer_builder.replies\")\n",
    "rag_pipeline.connect(\"retriever\", \"answer_builder.documents\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawing the Pipeline:\n",
    "The `rag_pipeline.draw(\"./rag_pipeline.png\")` command generates a visualization of the pipeline and saves it as an image (`rag_pipeline.png`). This helps visualize the structure and flow of the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_pipeline.draw(\"./rag_pipeline.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./rag_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Documents:\n",
    "Three example documents are created with some content:\n",
    "* Document 1: \"There are over 7,000 languages spoken around the world today.\"\n",
    "* Document 2: \"Elephants have been observed to behave in a way that indicates a high level of self-awareness, such as recognizing themselves in mirrors.\"\n",
    "* Document 3: \"In certain parts of the world, like the Maldives, Puerto Rico, and San Diego, you can witness the phenomenon of bioluminescent waves.\"\n",
    "\n",
    "\n",
    "These documents are then added to the `InMemoryDocumentStore` using the `write_documents()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add Documents\n",
    "documents = [Document(content=\"There are over 7,000 languages spoken around the world today.\"),\n",
    "\t\t\t       Document(content=\"Elephants have been observed to behave in a way that indicates a high level of self-awareness, such as recognizing themselves in mirrors.\"),\n",
    "\t\t\t       Document(content=\"In certain parts of the world, like the Maldives, Puerto Rico, and San Diego, you can witness the phenomenon of bioluminescent waves.\")]\n",
    "rag_pipeline.get_component(\"retriever\").document_store.write_documents(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Pipeline:\n",
    "* The pipeline is executed with a query \"How many languages are there?\".\n",
    "* The retriever component fetches documents relevant to the query.\n",
    "* The prompt builder then formats the documents and query into a prompt that is sent to the OpenAI model for generating an answer.\n",
    "* The answer builder extracts the answer from the model's response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How many languages are there?\"\n",
    "result = rag_pipeline.run(\n",
    "    {\n",
    "        \"retriever\": {\"query\": question},\n",
    "        \"prompt_builder\": {\"question\": question},\n",
    "        \"answer_builder\": {\"query\": question},\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeneratedAnswer(data='There are over 7,000 languages spoken around the world today.', query='How many languages are there?', documents=[Document(id=cfe93bc1c274908801e6670440bf2bbba54fad792770d57421f85ffa2a4fcc94, content: 'There are over 7,000 languages spoken around the world today.', score: 3.9351818820430142), Document(id=6f20658aeac3c102495b198401c1c0c2bd71d77b915820304d4fbc324b2f3cdb, content: 'Elephants have been observed to behave in a way that indicates a high level of self-awareness, such ...', score: 1.8390548493969865), Document(id=7f225626ad1019b273326fbaf11308edfca6d663308a4a3533ec7787367d59a2, content: 'In certain parts of the world, like the Maldives, Puerto Rico, and San Diego, you can witness the ph...', score: 1.8390548493969865)], meta={})\n"
     ]
    }
   ],
   "source": [
    "print(result['answer_builder']['answers'][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of What Happens:\n",
    "* Document Retrieval: The `InMemoryBM25Retriever` fetches relevant documents based on the question \"How many languages are there?\".\n",
    "* Prompt Creation: The PromptBuilder formats the documents and the question into a prompt for the OpenAI model.\n",
    "* Answer Generation: The OpenAIGenerator sends the prompt to OpenAI's GPT model, and the model generates an answer.\n",
    "* Answer Extraction: The AnswerBuilder processes the model's response and returns the final answer.\n",
    "Output: The answer (e.g., \"Over 7,000 languages are spoken around the world today\") is printed as the final result.\n",
    "\n",
    "This pipeline demonstrates how to implement a Retrieval-Augmented Generation (RAG) pipeline, which combines document retrieval and language model generation to answer questions using external information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP pipelines",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
