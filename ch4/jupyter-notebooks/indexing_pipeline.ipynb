{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9191a51",
   "metadata": {},
   "source": [
    "# Building a Document Indexing Pipeline with Haystack\n",
    "\n",
    "Welcome to this tutorial notebook on building a document indexing pipeline using Haystack! In this notebook, you'll learn how to create a robust pipeline for processing and indexing documents that can be later used for question answering and information retrieval.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will:\n",
    "- Understand how to set up a Haystack document processing pipeline\n",
    "- Learn to work with different document formats (TXT, CSV, PDF)\n",
    "- Create and configure an in-memory document store\n",
    "- Implement document preprocessing and indexing\n",
    "- Test your indexed documents with basic queries\n",
    "\n",
    "## Prerequisites\n",
    "- Basic Python knowledge\n",
    "- Understanding of basic NLP concepts\n",
    "- Familiarity with Jupyter notebooks\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ea268f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291a4a1f",
   "metadata": {},
   "source": [
    "## 1. Setup and Package Installation\n",
    "\n",
    "In this section, we'll import the necessary packages for our indexing pipeline. Here's what each package does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33f8302a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurafunderburk/Documents/GitHub/Building-Natural-Language-Pipelines/ch4/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import core Haystack classes\n",
    "from haystack import Pipeline, Document\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.components.joiners import DocumentJoiner\n",
    "\n",
    "# Import components for data fetching and conversion\n",
    "from haystack.components.fetchers import LinkContentFetcher\n",
    "from haystack.components.converters import (\n",
    "    PyPDFToDocument,\n",
    "    TextFileToDocument,\n",
    "    HTMLToDocument,\n",
    ")\n",
    "from haystack.components.routers import FileTypeRouter\n",
    "\n",
    "# Import components for preprocessing\n",
    "from haystack.components.preprocessors import (\n",
    "    DocumentCleaner,\n",
    "    DocumentSplitter,\n",
    "    CSVDocumentCleaner,\n",
    "    CSVDocumentSplitter\n",
    ")\n",
    "\n",
    "# Import components for embedding\n",
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4036223e",
   "metadata": {},
   "source": [
    "## 2. Document Loading and Preprocessing\n",
    "\n",
    "In this section, we'll set up our document loading pipeline. We'll work with multiple document formats:\n",
    "\n",
    "1. Text files (.txt)\n",
    "2. CSV files\n",
    "3. PDF documents\n",
    "\n",
    "The data we'll be working with includes:\n",
    "- A text file about Haystack's introduction\n",
    "- A CSV file containing information about LLM models\n",
    "- A sample PDF document\n",
    "\n",
    "We'll create a preprocessing pipeline that will:\n",
    "1. Load these documents from different sources\n",
    "2. Convert them into a unified format\n",
    "3. Clean and prepare them for indexing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8eb5248a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 1. Create Sample Data Files ---\n",
    "# Create a directory to hold our source files\n",
    "data_dir = Path(\"data_for_indexing\")\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create a sample text file\n",
    "text_file_path = data_dir / \"haystack_intro.txt\"\n",
    "text_file_path.write_text(\n",
    "    \"Haystack is an open-source framework by deepset for building production-ready LLM applications. \"\n",
    "    \"It enables developers to create retrieval-augmented generative pipelines and state-of-the-art search systems.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4fe1d3",
   "metadata": {},
   "source": [
    "Create mock CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9b74efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a sample CSV file with some empty rows/columns for cleaning\n",
    "csv_content = \"\"\"Company,Model,Release Year,,Notes\n",
    "OpenAI,GPT-4,2023,,Generative Pre-trained Transformer 4\n",
    ",,,\n",
    "Google,Gemini,2023,,A family of multimodal models\n",
    "Anthropic,Claude 3,2024,,Includes Opus, Sonnet, and Haiku models\n",
    "\"\"\"\n",
    "csv_file_path = data_dir / \"llm_models.csv\"\n",
    "csv_file_path.write_text(csv_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd38bf3f",
   "metadata": {},
   "source": [
    "Scrape a webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481f6d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sample URL to fetch\n",
    "web_url = \"https://haystack.deepset.ai/blog/haystack-2-release\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9d86ed",
   "metadata": {},
   "source": [
    "PDF for file that does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb151a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this example, we'll skip the actual PDF creation and assume one exists.\n",
    "# You can place any PDF file in the 'data_for_indexing' directory and name it 'sample.pdf'.\n",
    "# For a runnable example, we will simulate its path.\n",
    "pdf_file_path = data_dir / \"sample.pdf\"\n",
    "# In a real scenario, you would have this file. For this script to run, we'll check for it.\n",
    "if not pdf_file_path.exists():\n",
    "    print(f\"Warning: PDF file not found at {pdf_file_path}. The PDF processing branch will not run.\")\n",
    "    # Create a dummy file to avoid path errors, but it won't be processed as PDF\n",
    "    pdf_file_path.touch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a5aae7",
   "metadata": {},
   "source": [
    "## 3. Initialize components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d455fad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DocumentStore:\n",
    "document_store = InMemoryDocumentStore()\n",
    "\n",
    "# FileTypeRouter: Directs files to the correct converter based on their MIME type.\n",
    "file_type_router = FileTypeRouter(mime_types=[\"text/plain\", \"application/pdf\", \"text/html\", \"text/csv\"])\n",
    "\n",
    "# Converters: One for each file type we want to handle.\n",
    "text_converter = TextFileToDocument()\n",
    "pdf_converter = PyPDFToDocument()\n",
    "html_converter = HTMLToDocument()\n",
    "# Added a dedicated converter instance for CSV files.\n",
    "# It's also a TextFileToDocument, as it just needs to read the file's content.\n",
    "csv_converter = TextFileToDocument()\n",
    "\n",
    "\n",
    "# LinkContentFetcher: Fetches content from URLs.\n",
    "link_fetcher = LinkContentFetcher()\n",
    "\n",
    "# DocumentJoiners:\n",
    "unstructured_doc_joiner = DocumentJoiner()\n",
    "# This joiner will gather documents from *all* processing branches\n",
    "# (the split text docs and the split csv docs) before embedding.\n",
    "final_doc_joiner = DocumentJoiner()\n",
    "\n",
    "# Preprocessors for Text Data:\n",
    "text_cleaner = DocumentCleaner()\n",
    "text_splitter = DocumentSplitter(split_by=\"word\", split_length=150, split_overlap=20)\n",
    "\n",
    "# Preprocessors for Tabular Data (CSV):\n",
    "# These will now be part of the main pipeline.\n",
    "csv_cleaner = CSVDocumentCleaner()\n",
    "csv_splitter = CSVDocumentSplitter(split_mode=\"row-wise\")\n",
    "\n",
    "# Embedder:\n",
    "doc_embedder = SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# DocumentWriter:\n",
    "writer = DocumentWriter(document_store)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574c036a",
   "metadata": {},
   "source": [
    "## 4. Building the Indexing Pipeline\n",
    "\n",
    "Now we'll construct our document processing pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1842b299",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "indexing_pipeline = Pipeline()\n",
    "\n",
    "# Add all components to the pipeline with unique names\n",
    "indexing_pipeline.add_component(\"link_fetcher\", link_fetcher)\n",
    "indexing_pipeline.add_component(\"html_converter\", html_converter)\n",
    "indexing_pipeline.add_component(\"file_type_router\", file_type_router)\n",
    "indexing_pipeline.add_component(\"text_converter\", text_converter)\n",
    "indexing_pipeline.add_component(\"pdf_converter\", pdf_converter)\n",
    "indexing_pipeline.add_component(\"unstructured_doc_joiner\", unstructured_doc_joiner)\n",
    "indexing_pipeline.add_component(\"text_cleaner\", text_cleaner)\n",
    "indexing_pipeline.add_component(\"text_splitter\", text_splitter)\n",
    "indexing_pipeline.add_component(\"doc_embedder\", doc_embedder)\n",
    "indexing_pipeline.add_component(\"writer\", writer)\n",
    "\n",
    "# **NEW**: Add the CSV components to the pipeline\n",
    "indexing_pipeline.add_component(\"csv_converter\", csv_converter)\n",
    "indexing_pipeline.add_component(\"csv_cleaner\", csv_cleaner)\n",
    "indexing_pipeline.add_component(\"csv_splitter\", csv_splitter)\n",
    "indexing_pipeline.add_component(\"final_doc_joiner\", final_doc_joiner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2211c63",
   "metadata": {},
   "source": [
    "## 5. Connect components\n",
    "\n",
    "This is where the routing logic will come in. We will first handle web-based data, then local files (TXT and PDF)\n",
    "in an unstructured data branch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea3cb37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x328391c10>\n",
       "üöÖ Components\n",
       "  - link_fetcher: LinkContentFetcher\n",
       "  - html_converter: HTMLToDocument\n",
       "  - file_type_router: FileTypeRouter\n",
       "  - text_converter: TextFileToDocument\n",
       "  - pdf_converter: PyPDFToDocument\n",
       "  - unstructured_doc_joiner: DocumentJoiner\n",
       "  - text_cleaner: DocumentCleaner\n",
       "  - text_splitter: DocumentSplitter\n",
       "  - doc_embedder: SentenceTransformersDocumentEmbedder\n",
       "  - writer: DocumentWriter\n",
       "  - csv_converter: TextFileToDocument\n",
       "  - csv_cleaner: CSVDocumentCleaner\n",
       "  - csv_splitter: CSVDocumentSplitter\n",
       "  - final_doc_joiner: DocumentJoiner\n",
       "üõ§Ô∏è Connections\n",
       "  - link_fetcher.streams -> html_converter.sources (list[ByteStream])\n",
       "  - html_converter.documents -> unstructured_doc_joiner.documents (list[Document])\n",
       "  - file_type_router.text/plain -> text_converter.sources (list[Union[str, Path, ByteStream]])\n",
       "  - file_type_router.application/pdf -> pdf_converter.sources (list[Union[str, Path, ByteStream]])\n",
       "  - file_type_router.text/csv -> csv_converter.sources (list[Union[str, Path, ByteStream]])\n",
       "  - text_converter.documents -> unstructured_doc_joiner.documents (list[Document])\n",
       "  - pdf_converter.documents -> unstructured_doc_joiner.documents (list[Document])\n",
       "  - unstructured_doc_joiner.documents -> text_cleaner.documents (list[Document])\n",
       "  - text_cleaner.documents -> text_splitter.documents (list[Document])\n",
       "  - text_splitter.documents -> final_doc_joiner.documents (list[Document])\n",
       "  - doc_embedder.documents -> writer.documents (list[Document])\n",
       "  - csv_converter.documents -> csv_cleaner.documents (list[Document])\n",
       "  - csv_cleaner.documents -> csv_splitter.documents (list[Document])\n",
       "  - csv_splitter.documents -> final_doc_joiner.documents (list[Document])\n",
       "  - final_doc_joiner.documents -> doc_embedder.documents (list[Document])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# --- Unstructured Data Branch (Web, TXT, PDF) ---\n",
    "# Web data\n",
    "indexing_pipeline.connect(\"link_fetcher.streams\", \"html_converter.sources\")\n",
    "indexing_pipeline.connect(\"html_converter.documents\", \"unstructured_doc_joiner.documents\")\n",
    "\n",
    "# Local file data (TXT, PDF)\n",
    "indexing_pipeline.connect(\"file_type_router.text/plain\", \"text_converter.sources\")\n",
    "indexing_pipeline.connect(\"file_type_router.application/pdf\", \"pdf_converter.sources\")\n",
    "indexing_pipeline.connect(\"text_converter.documents\", \"unstructured_doc_joiner.documents\")\n",
    "indexing_pipeline.connect(\"pdf_converter.documents\", \"unstructured_doc_joiner.documents\")\n",
    "\n",
    "# Processing for unstructured data\n",
    "indexing_pipeline.connect(\"unstructured_doc_joiner\", \"text_cleaner\")\n",
    "indexing_pipeline.connect(\"text_cleaner\", \"text_splitter\")\n",
    "# Connect the split *text* docs to the *final* joiner\n",
    "indexing_pipeline.connect(\"text_splitter.documents\", \"final_doc_joiner.documents\")\n",
    "\n",
    "\n",
    "#  Structured Data Branch (CSV) ---\n",
    "# Route CSV files to the csv_converter\n",
    "indexing_pipeline.connect(\"file_type_router.text/csv\", \"csv_converter.sources\")\n",
    "# Process the CSV documents\n",
    "indexing_pipeline.connect(\"csv_converter.documents\", \"csv_cleaner.documents\")\n",
    "indexing_pipeline.connect(\"csv_cleaner.documents\", \"csv_splitter.documents\")\n",
    "# Connect the split *csv* docs to the *final* joiner\n",
    "indexing_pipeline.connect(\"csv_splitter.documents\", \"final_doc_joiner.documents\")\n",
    "\n",
    "\n",
    "# --- Main Processing Path (Embedding and Writing) ---\n",
    "# The final_doc_joiner now receives documents from *both* branches\n",
    "indexing_pipeline.connect(\"final_doc_joiner\", \"doc_embedder\")\n",
    "indexing_pipeline.connect(\"doc_embedder\", \"writer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b99287e",
   "metadata": {},
   "source": [
    "Let's visualize the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e5b936d",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexing_pipeline.draw(path=\"./images/indexing_pipeline.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40df5ac5",
   "metadata": {},
   "source": [
    "![](./images/indexing_pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3dbedc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing document 1384ec36dd6d99f90ab589732d5219b7371dac846d0f0bd89c6385189c4079c0. Keeping it, but skipping cleaning. Error: Error tokenizing data. C error: Expected 5 fields in line 5, saw 7\n",
      "\n",
      "Error processing document 1384ec36dd6d99f90ab589732d5219b7371dac846d0f0bd89c6385189c4079c0. Keeping it, but skipping splitting. Error: Error tokenizing data. C error: Expected 5 fields in line 5, saw 7\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running unified indexing pipeline for web, local files, and CSV...\n",
      "Skipping PDF file: data_for_indexing/sample.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.14it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'writer': {'documents_written': 15}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 5. Run the Pipeline ---\n",
    "\n",
    "print(\"Running unified indexing pipeline for web, local files, and CSV...\")\n",
    "# Note: The PDF path will be ignored if the file doesn't exist.\n",
    "file_paths_to_process = [text_file_path]\n",
    "\n",
    "if pdf_file_path.exists() and pdf_file_path.stat().st_size > 0:\n",
    "    file_paths_to_process.append(pdf_file_path)\n",
    "else:\n",
    "    print(f\"Skipping PDF file: {pdf_file_path}\")\n",
    "\n",
    "file_paths_to_process.append(csv_file_path)\n",
    "\n",
    "\n",
    "indexing_pipeline.run({\n",
    "    \"link_fetcher\": {\"urls\": [web_url]},\n",
    "    \"file_type_router\": {\"sources\": file_paths_to_process}\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8391fb4",
   "metadata": {},
   "source": [
    "## Understanding the Results\n",
    "\n",
    "Let's examine what our pipeline has processed. We'll look at:\n",
    "1. The number of documents processed\n",
    "2. How the documents were split\n",
    "3. The metadata that was extracted\n",
    "\n",
    "This will help us verify that our pipeline is working as expected and give us insights into our indexed documents.\n",
    "\n",
    "First, let's query our document store to see what we've indexed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efc7bc7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total documents in DocumentStore: 15\n",
      "Sample document from the store:\n",
      "[Document(id=1384ec36dd6d99f90ab589732d5219b7371dac846d0f0bd89c6385189c4079c0, content: 'Company,Model,Release Year,,Notes\n",
      "OpenAI,GPT-4,2023,,Generative Pre-trained Transformer 4\n",
      ",,,\n",
      "Google...', meta: {'file_path': 'llm_models.csv'}, embedding: vector of size 384), Document(id=be2fb4afe8f3e531ae2e97314778b92789c44d794c981d67bdea8658cf3fe51e, content: 'Haystack 2.0: The Composable Open-Source LLM Framework\n",
      "Meet Haystack 2.0, a more flexible, customiza...', meta: {'content_type': 'text/html', 'url': 'https://haystack.deepset.ai/blog/haystack-2-release', 'source_id': '0b188f4690ab3496d2270baf378be9fde19707e9b1ece003123c0442af918bb7', 'page_number': 1, 'split_id': 0, 'split_idx_start': 0, '_split_overlap': [{'doc_id': 'ff9dcb8208955755eb458bdf693e12ba8c2dd7f27de0bcb51e47e474255ad47b', 'range': (0, 124)}]}, embedding: vector of size 384), Document(id=ff9dcb8208955755eb458bdf693e12ba8c2dd7f27de0bcb51e47e474255ad47b, content: 'user before or not. You can get started by installing haystack-ai\n",
      ", our new package for Haystack 2.0...', meta: {'content_type': 'text/html', 'url': 'https://haystack.deepset.ai/blog/haystack-2-release', 'source_id': '0b188f4690ab3496d2270baf378be9fde19707e9b1ece003123c0442af918bb7', 'page_number': 1, 'split_id': 1, 'split_idx_start': 834, '_split_overlap': [{'doc_id': 'be2fb4afe8f3e531ae2e97314778b92789c44d794c981d67bdea8658cf3fe51e', 'range': (834, 958)}, {'doc_id': 'c59e96866da9b70a659ed9d87e43202d20e38c1d8022fae86f73c9f83268afea', 'range': (0, 144)}]}, embedding: vector of size 384), Document(id=c59e96866da9b70a659ed9d87e43202d20e38c1d8022fae86f73c9f83268afea, content: 'question-answering. During this time, we established the core of what makes Haystack Haystack: Compo...', meta: {'content_type': 'text/html', 'url': 'https://haystack.deepset.ai/blog/haystack-2-release', 'source_id': '0b188f4690ab3496d2270baf378be9fde19707e9b1ece003123c0442af918bb7', 'page_number': 1, 'split_id': 2, 'split_idx_start': 1651, '_split_overlap': [{'doc_id': 'ff9dcb8208955755eb458bdf693e12ba8c2dd7f27de0bcb51e47e474255ad47b', 'range': (817, 961)}, {'doc_id': '026a1a2d6fd15933c794c8352dfb015d939dc76fd2b8a6b1f68b166c2bd5e4e5', 'range': (0, 138)}]}, embedding: vector of size 384), Document(id=026a1a2d6fd15933c794c8352dfb015d939dc76fd2b8a6b1f68b166c2bd5e4e5, content: 'is a complete rewrite, the underlying principle of composing components into flexible pipelines rema...', meta: {'content_type': 'text/html', 'url': 'https://haystack.deepset.ai/blog/haystack-2-release', 'source_id': '0b188f4690ab3496d2270baf378be9fde19707e9b1ece003123c0442af918bb7', 'page_number': 1, 'split_id': 3, 'split_idx_start': 2517, '_split_overlap': [{'doc_id': 'c59e96866da9b70a659ed9d87e43202d20e38c1d8022fae86f73c9f83268afea', 'range': (866, 1004)}, {'doc_id': '828f39441b56d74639d7cf80eedae532cb31da7543ccff20c838d8e440edd6a3', 'range': (0, 120)}]}, embedding: vector of size 384), Document(id=828f39441b56d74639d7cf80eedae532cb31da7543ccff20c838d8e440edd6a3, content: 'not allowed, and the pipeline graph has to be acyclic. This makes it difficult to implement, for exa...', meta: {'content_type': 'text/html', 'url': 'https://haystack.deepset.ai/blog/haystack-2-release', 'source_id': '0b188f4690ab3496d2270baf378be9fde19707e9b1ece003123c0442af918bb7', 'page_number': 1, 'split_id': 4, 'split_idx_start': 3405, '_split_overlap': [{'doc_id': '026a1a2d6fd15933c794c8352dfb015d939dc76fd2b8a6b1f68b166c2bd5e4e5', 'range': (888, 1008)}, {'doc_id': '234aad190cf53f7615c79bca110f626647098e63c6cbaf46997a1f4278a895c2', 'range': (0, 117)}]}, embedding: vector of size 384), Document(id=234aad190cf53f7615c79bca110f626647098e63c6cbaf46997a1f4278a895c2, content: 'it transparent as to how these components can ‚Äútalk‚Äù to each other.\n",
      "- Be flexible: Make it possible ...', meta: {'content_type': 'text/html', 'url': 'https://haystack.deepset.ai/blog/haystack-2-release', 'source_id': '0b188f4690ab3496d2270baf378be9fde19707e9b1ece003123c0442af918bb7', 'page_number': 1, 'split_id': 5, 'split_idx_start': 4245, '_split_overlap': [{'doc_id': '828f39441b56d74639d7cf80eedae532cb31da7543ccff20c838d8e440edd6a3', 'range': (840, 957)}, {'doc_id': 'bff241b723ea7ce505553329657bdf3f45ea0738c106869593433e9146f855f0', 'range': (0, 120)}]}, embedding: vector of size 384), Document(id=bff241b723ea7ce505553329657bdf3f45ea0738c106869593433e9146f855f0, content: 'creating a new component, to decide what inputs and outputs it should have is part of the ideation p...', meta: {'content_type': 'text/html', 'url': 'https://haystack.deepset.ai/blog/haystack-2-release', 'source_id': '0b188f4690ab3496d2270baf378be9fde19707e9b1ece003123c0442af918bb7', 'page_number': 1, 'split_id': 6, 'split_idx_start': 5109, '_split_overlap': [{'doc_id': '234aad190cf53f7615c79bca110f626647098e63c6cbaf46997a1f4278a895c2', 'range': (864, 984)}, {'doc_id': '590f38cb1024dafb259a01aa51ae51282c13a842bf2d145de16cb8d4155e25fc', 'range': (0, 120)}]}, embedding: vector of size 384), Document(id=590f38cb1024dafb259a01aa51ae51282c13a842bf2d145de16cb8d4155e25fc, content: 'few months (with contributions from Assembly AI, Jina AI, mixedbread ai and more). We will continue ...', meta: {'content_type': 'text/html', 'url': 'https://haystack.deepset.ai/blog/haystack-2-release', 'source_id': '0b188f4690ab3496d2270baf378be9fde19707e9b1ece003123c0442af918bb7', 'page_number': 1, 'split_id': 7, 'split_idx_start': 5969, '_split_overlap': [{'doc_id': 'bff241b723ea7ce505553329657bdf3f45ea0738c106869593433e9146f855f0', 'range': (860, 980)}, {'doc_id': 'fd0c89246c79336218a1fa79ad8fba3b16b358f0b1cdd1a2762790985bc4946f', 'range': (0, 121)}]}, embedding: vector of size 384), Document(id=fd0c89246c79336218a1fa79ad8fba3b16b358f0b1cdd1a2762790985bc4946f, content: 'has to come from somewhere, and Haystack 2.0 provides a common interface to access it in a consisten...', meta: {'content_type': 'text/html', 'url': 'https://haystack.deepset.ai/blog/haystack-2-release', 'source_id': '0b188f4690ab3496d2270baf378be9fde19707e9b1ece003123c0442af918bb7', 'page_number': 1, 'split_id': 8, 'split_idx_start': 6801, '_split_overlap': [{'doc_id': '590f38cb1024dafb259a01aa51ae51282c13a842bf2d145de16cb8d4155e25fc', 'range': (832, 953)}, {'doc_id': 'c6e861b73ec4721c095bb5605151489ddc4f718eb27ae2b7f62ce4a6738ab811', 'range': (0, 125)}]}, embedding: vector of size 384), Document(id=c6e861b73ec4721c095bb5605151489ddc4f718eb27ae2b7f62ce4a6738ab811, content: 'storage service.\n",
      "A clear path to production\n",
      "The experience we got over the last couple of years, wor...', meta: {'content_type': 'text/html', 'url': 'https://haystack.deepset.ai/blog/haystack-2-release', 'source_id': '0b188f4690ab3496d2270baf378be9fde19707e9b1ece003123c0442af918bb7', 'page_number': 1, 'split_id': 9, 'split_idx_start': 7631, '_split_overlap': [{'doc_id': 'fd0c89246c79336218a1fa79ad8fba3b16b358f0b1cdd1a2762790985bc4946f', 'range': (830, 955)}, {'doc_id': 'a64d4c9cf677d5e88db2266ef191b327df244cb801258d8d0ee234d8c58f8ab2', 'range': (0, 149)}]}, embedding: vector of size 384), Document(id=a64d4c9cf677d5e88db2266ef191b327df244cb801258d8d0ee234d8c58f8ab2, content: 'pipelines behind a RESTful API: Hayhooks.\n",
      "Hayhooks is a client-server application that allows you to...', meta: {'content_type': 'text/html', 'url': 'https://haystack.deepset.ai/blog/haystack-2-release', 'source_id': '0b188f4690ab3496d2270baf378be9fde19707e9b1ece003123c0442af918bb7', 'page_number': 1, 'split_id': 10, 'split_idx_start': 8549, '_split_overlap': [{'doc_id': 'c6e861b73ec4721c095bb5605151489ddc4f718eb27ae2b7f62ce4a6738ab811', 'range': (918, 1067)}, {'doc_id': '7f6f7d895d1c405a537f7b39d5eeda04f053fd9ebb147b45fee110e81422d63f', 'range': (0, 129)}]}, embedding: vector of size 384), Document(id=7f6f7d895d1c405a537f7b39d5eeda04f053fd9ebb147b45fee110e81422d63f, content: 'being good examples of how we‚Äôve been leveraging Haystack 2.0. How?:\n",
      "- It‚Äôs easier to extend the cap...', meta: {'content_type': 'text/html', 'url': 'https://haystack.deepset.ai/blog/haystack-2-release', 'source_id': '0b188f4690ab3496d2270baf378be9fde19707e9b1ece003123c0442af918bb7', 'page_number': 1, 'split_id': 11, 'split_idx_start': 9489, '_split_overlap': [{'doc_id': 'a64d4c9cf677d5e88db2266ef191b327df244cb801258d8d0ee234d8c58f8ab2', 'range': (940, 1069)}, {'doc_id': 'cb016e7e3e342381e63aa790089eaf797bfda2564f19308da2d2408e003cf59b', 'range': (0, 157)}]}, embedding: vector of size 384), Document(id=cb016e7e3e342381e63aa790089eaf797bfda2564f19308da2d2408e003cf59b, content: 'resources and more to help you get started:\n",
      "- Documentation: full technical documentation on all Hay...', meta: {'content_type': 'text/html', 'url': 'https://haystack.deepset.ai/blog/haystack-2-release', 'source_id': '0b188f4690ab3496d2270baf378be9fde19707e9b1ece003123c0442af918bb7', 'page_number': 1, 'split_id': 12, 'split_idx_start': 10387, '_split_overlap': [{'doc_id': '7f6f7d895d1c405a537f7b39d5eeda04f053fd9ebb147b45fee110e81422d63f', 'range': (898, 1055)}]}, embedding: vector of size 384), Document(id=abe67d1945474acc782933988f972e8fb8eead145c1e9a124bc45529c0913020, content: 'Haystack is an open-source framework by deepset for building production-ready LLM applications. It e...', meta: {'file_path': 'haystack_intro.txt', 'source_id': '160d6f24e29805bf0759170ff471675c29a1236687381c1140733597dbaaa14d', 'page_number': 1, 'split_id': 0, 'split_idx_start': 0, '_split_overlap': []}, embedding: vector of size 384)]\n"
     ]
    }
   ],
   "source": [
    "# --- 7. Verify the DocumentStore ---\n",
    "doc_count = document_store.count_documents()\n",
    "print(f\"\\nTotal documents in DocumentStore: {doc_count}\")\n",
    "print(\"Sample document from the store:\")\n",
    "print(document_store.filter_documents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6305e3c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-with-haystack-ch4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
