{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2913f790",
   "metadata": {},
   "source": [
    "ðŸ”§ **Setup Required**: Before running this notebook, please follow the [setup instructions](../README.md#setup-instructions) to configure your environment and API keys."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9191a51",
   "metadata": {},
   "source": [
    "# Building a Document Indexing Pipeline with Haystack\n",
    "\n",
    "Welcome to this comprehensive tutorial on building a document indexing pipeline using Haystack! This notebook demonstrates how to create a robust system for processing and indexing various types of documents for retrieval-augmented generation (RAG) applications.\n",
    "\n",
    "## What you'll learn\n",
    "After completing this notebook, you will be able to:\n",
    "- Build a multi-format document processing pipeline using Haystack\n",
    "- Handle different document types (TXT, CSV, PDF, HTML) in a unified way\n",
    "- Implement proper document cleaning and splitting strategies\n",
    "- Create and configure an efficient document store\n",
    "- Generate and store document embeddings for semantic search\n",
    "- Test and validate your indexed documents\n",
    "\n",
    "## Key Concepts\n",
    "Before we begin, let's understand some core concepts:\n",
    "- **Document Store**: A database that holds our processed documents and their embeddings\n",
    "- **Document Processing**: Converting raw files into a standardized format\n",
    "- **Embeddings**: Dense vector representations of text for semantic search\n",
    "- **Pipeline Components**: Modular pieces that handle specific processing tasks\n",
    "\n",
    "## Prerequisites\n",
    "- Basic Python knowledge\n",
    "- Understanding of basic NLP concepts\n",
    "- Familiarity with Jupyter notebooks\n",
    "- Haystack library installed\n",
    "\n",
    "Let's dive in and build our indexing pipeline step by step!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2807bc4a",
   "metadata": {},
   "source": [
    "## 1. Setting Up Our Environment\n",
    "\n",
    "Before we build our pipeline, we need to import the necessary components. Let's understand what each package brings to our indexing system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ea268f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291a4a1f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Core Components\n",
    "- **`Pipeline`**: The main orchestrator that connects all components\n",
    "- **`Document`**: The fundamental data structure for text in Haystack\n",
    "- **`InMemoryDocumentStore`**: Our database for storing processed documents\n",
    "\n",
    "## Document Processing Components\n",
    "- **`DocumentWriter`**: Handles saving documents to our store\n",
    "- **`DocumentJoiner`**: Combines documents from different sources\n",
    "- **`FileTypeRouter`**: Routes files to appropriate processors based on type\n",
    "\n",
    "## Format-Specific Components\n",
    "- **`LinkContentFetcher`**: Retrieves content from web URLs\n",
    "- **`PyPDFToDocument`**: Processes PDF files\n",
    "- **`TextFileToDocument`**: Handles plain text files\n",
    "- **`HTMLToDocument`**: Processes HTML content\n",
    "\n",
    "## Text Processing Components\n",
    "- **`DocumentCleaner`**: Removes unwanted content and normalizes text\n",
    "- **`DocumentSplitter`**: Breaks documents into manageable chunks\n",
    "- **`CSVDocumentCleaner`/`Splitter`**: Specialized handlers for tabular data\n",
    "\n",
    "## Embedding Component\n",
    "- **`SentenceTransformersDocumentEmbedder`**: Converts text to vector representations\n",
    "\n",
    "In the next cell, we'll import these components and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f8302a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurafunderburk/Documents/GitHub/Building-Natural-Language-Pipelines/ch4/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import core Haystack classes\n",
    "from haystack import Pipeline, Document\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.components.joiners import DocumentJoiner\n",
    "\n",
    "# Import components for data fetching and conversion\n",
    "from haystack.components.fetchers import LinkContentFetcher\n",
    "from haystack.components.converters import (\n",
    "    PyPDFToDocument,\n",
    "    TextFileToDocument,\n",
    "    HTMLToDocument,\n",
    ")\n",
    "from haystack.components.routers import FileTypeRouter\n",
    "\n",
    "# Import components for preprocessing\n",
    "from haystack.components.preprocessors import (\n",
    "    DocumentCleaner,\n",
    "    DocumentSplitter,\n",
    "    CSVDocumentCleaner,\n",
    "    CSVDocumentSplitter\n",
    ")\n",
    "\n",
    "# Import components for embedding\n",
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4036223e",
   "metadata": {},
   "source": [
    "## 2. Document Loading and Preprocessing\n",
    "\n",
    "In this section, we'll set up our document loading pipeline. We'll work with multiple document formats:\n",
    "\n",
    "1. Text files (.txt)\n",
    "2. CSV files\n",
    "3. PDF documents\n",
    "\n",
    "The data we'll be working with includes:\n",
    "- A text file about Haystack's introduction\n",
    "- A CSV file containing information about LLM models\n",
    "- A sample PDF document\n",
    "\n",
    "We'll create a preprocessing pipeline that will:\n",
    "1. Load these documents from different sources\n",
    "2. Convert them into a unified format\n",
    "3. Clean and prepare them for indexing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb5248a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 1. Create Sample Data Files ---\n",
    "# Create a directory to hold our source files\n",
    "data_dir = Path(\"data_for_indexing\")\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create a sample text file\n",
    "text_file_path = data_dir / \"haystack_intro.txt\"\n",
    "text_file_path.write_text(\n",
    "    \"Haystack is an open-source framework by deepset for building production-ready LLM applications. \"\n",
    "    \"It enables developers to create retrieval-augmented generative pipelines and state-of-the-art search systems.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4fe1d3",
   "metadata": {},
   "source": [
    "Create mock CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b74efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a sample CSV file with some empty rows/columns for cleaning\n",
    "csv_content = \"\"\"Company,Model,Release Year,,Notes\n",
    "OpenAI,GPT-4,2023,,Generative Pre-trained Transformer 4\n",
    ",,,\n",
    "Google,Gemini,2023,,A family of multimodal models\n",
    "Anthropic,Claude 3,2024,,Includes Opus, Sonnet, and Haiku models\n",
    "\"\"\"\n",
    "csv_file_path = data_dir / \"llm_models.csv\"\n",
    "csv_file_path.write_text(csv_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd38bf3f",
   "metadata": {},
   "source": [
    "Scrape a webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481f6d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sample URL to fetch\n",
    "web_url = \"https://haystack.deepset.ai/blog/haystack-2-release\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9d86ed",
   "metadata": {},
   "source": [
    "PDF for file that does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb151a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: PDF file not found at data_for_indexing/sample.pdf. The PDF processing branch will not run.\n"
     ]
    }
   ],
   "source": [
    "# For this example, we'll skip the actual PDF creation and assume one exists.\n",
    "# You can place any PDF file in the 'data_for_indexing' directory and name it 'sample.pdf'.\n",
    "# For a runnable example, we will simulate its path.\n",
    "pdf_file_path = data_dir / \"sample.pdf\"\n",
    "# In a real scenario, you would have this file. For this script to run, we'll check for it.\n",
    "if not pdf_file_path.exists():\n",
    "    print(f\"Warning: PDF file not found at {pdf_file_path}. The PDF processing branch will not run.\")\n",
    "    # Create a dummy file to avoid path errors, but it won't be processed as PDF\n",
    "    pdf_file_path.touch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a5aae7",
   "metadata": {},
   "source": [
    "## 3. Initializing Pipeline Components\n",
    "\n",
    "Now we'll create instances of each component needed for our pipeline. Let's understand each component's role and configuration:\n",
    "\n",
    "### Storage Components\n",
    "- **`DocumentStore`**: In-memory database for documents and embeddings\n",
    "  - Efficient for testing and development\n",
    "  - Supports vector similarity search\n",
    "  - Non-persistent (clears on restart)\n",
    "\n",
    "### Routing Components\n",
    "- **`FileTypeRouter`**: Smart file dispatcher\n",
    "  - Routes files based on MIME types\n",
    "  - Supports: text/plain, PDF, HTML, CSV\n",
    "  - Ensures proper format-specific processing\n",
    "\n",
    "### Document Conversion\n",
    "- **`TextFileToDocument`**: Converts plain text\n",
    "- **`PyPDFToDocument`**: Extracts text from PDFs\n",
    "- **`HTMLToDocument`**: Processes web content\n",
    "- **CSV `Converter`**: Handles tabular data\n",
    "\n",
    "### Document Processing\n",
    "- **`DocumentCleaner`**: Text normalization\n",
    "  - Removes extra whitespace\n",
    "  - Handles special characters\n",
    "  - Normalizes formatting\n",
    "\n",
    "- **`DocumentSplitter`**: Chunking strategy\n",
    "  - Splits by words (150 words per chunk)\n",
    "  - 20-word overlap for context\n",
    "  - Preserves semantic meaning\n",
    "\n",
    "### Embedding Generation\n",
    "- **`SentenceTransformersEmbedder`**:\n",
    "  - Model: all-MiniLM-L6-v2\n",
    "  - Creates 384-dimensional embeddings\n",
    "  - Optimized for semantic search\n",
    "\n",
    "The following cell initializes all these components with our chosen configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d455fad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DocumentStore:\n",
    "document_store = InMemoryDocumentStore()\n",
    "\n",
    "# FileTypeRouter: Directs files to the correct converter based on their MIME type.\n",
    "file_type_router = FileTypeRouter(mime_types=[\"text/plain\", \"application/pdf\", \"text/html\", \"text/csv\"])\n",
    "\n",
    "# Converters: One for each file type we want to handle.\n",
    "text_converter = TextFileToDocument()\n",
    "pdf_converter = PyPDFToDocument()\n",
    "html_converter = HTMLToDocument()\n",
    "csv_converter = TextFileToDocument()\n",
    "\n",
    "# LinkContentFetcher: Fetches content from URLs.\n",
    "link_fetcher = LinkContentFetcher()\n",
    "\n",
    "# DocumentJoiners:\n",
    "unstructured_doc_joiner = DocumentJoiner()\n",
    "# This joiner will gather documents from *all* processing branches\n",
    "# (the split text docs and the split csv docs) before embedding.\n",
    "final_doc_joiner = DocumentJoiner()\n",
    "\n",
    "# Preprocessors for Text Data:\n",
    "text_cleaner = DocumentCleaner()\n",
    "text_splitter = DocumentSplitter(split_by=\"word\", split_length=150, split_overlap=20)\n",
    "\n",
    "# Preprocessors for Tabular Data (CSV):\n",
    "# These will now be part of the main pipeline.\n",
    "csv_cleaner = CSVDocumentCleaner()\n",
    "csv_splitter = CSVDocumentSplitter(split_mode=\"row-wise\")\n",
    "\n",
    "# Embedder:\n",
    "doc_embedder = SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# DocumentWriter:\n",
    "writer = DocumentWriter(document_store)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574c036a",
   "metadata": {},
   "source": [
    "# 4. Constructing the Pipeline\n",
    "\n",
    "We'll now assemble our components into a complete indexing pipeline. Our pipeline architecture follows these principles:\n",
    "\n",
    "## Pipeline Architecture\n",
    "1. **Parallel Processing Branches**\n",
    "   - Unstructured data branch (Web, TXT, PDF)\n",
    "   - Structured data branch (CSV)\n",
    "   - Each branch optimized for its data type\n",
    "\n",
    "2. **Document Flow**\n",
    "   - Input â†’ Conversion â†’ Cleaning â†’ Splitting â†’ Embedding â†’ Storage\n",
    "   - Specialized handling for each format\n",
    "   - Joins documents at strategic points\n",
    "\n",
    "3. **Component Naming**\n",
    "   - Each component gets a unique identifier\n",
    "   - Names reflect component functionality\n",
    "   - Helps in debugging and monitoring\n",
    "\n",
    "## Key Design Decisions\n",
    "- **Split Processing**: Separate paths for structured/unstructured data\n",
    "- **Document Joining**: Strategic combination points for efficiency\n",
    "- **Embedding Strategy**: Single embedder for consistency\n",
    "- **Error Handling**: Built-in format validation\n",
    "\n",
    "The following cell implements this architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1842b299",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "indexing_pipeline = Pipeline()\n",
    "\n",
    "# Add all components to the pipeline with unique names\n",
    "indexing_pipeline.add_component(\"link_fetcher\", link_fetcher)\n",
    "indexing_pipeline.add_component(\"html_converter\", html_converter)\n",
    "indexing_pipeline.add_component(\"file_type_router\", file_type_router)\n",
    "indexing_pipeline.add_component(\"text_converter\", text_converter)\n",
    "indexing_pipeline.add_component(\"pdf_converter\", pdf_converter)\n",
    "indexing_pipeline.add_component(\"unstructured_doc_joiner\", unstructured_doc_joiner)\n",
    "indexing_pipeline.add_component(\"text_cleaner\", text_cleaner)\n",
    "indexing_pipeline.add_component(\"text_splitter\", text_splitter)\n",
    "indexing_pipeline.add_component(\"doc_embedder\", doc_embedder)\n",
    "indexing_pipeline.add_component(\"writer\", writer)\n",
    "\n",
    "# **NEW**: Add the CSV components to the pipeline\n",
    "indexing_pipeline.add_component(\"csv_converter\", csv_converter)\n",
    "indexing_pipeline.add_component(\"csv_cleaner\", csv_cleaner)\n",
    "indexing_pipeline.add_component(\"csv_splitter\", csv_splitter)\n",
    "indexing_pipeline.add_component(\"final_doc_joiner\", final_doc_joiner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2211c63",
   "metadata": {},
   "source": [
    "# 5. Connecting Pipeline Components\n",
    "\n",
    "This is where we define the flow of data through our pipeline. The connections create two main processing branches that eventually merge:\n",
    "\n",
    "## Unstructured Data Branch\n",
    "1. **Web Content Processing**\n",
    "   - URL â†’ LinkFetcher â†’ HTMLConverter\n",
    "   - Handles dynamic web content\n",
    "   - Preserves important metadata\n",
    "\n",
    "2. **Local File Processing**\n",
    "   - Files â†’ FileTypeRouter â†’ Appropriate Converter\n",
    "   - Automatic format detection\n",
    "   - Specialized handling per type\n",
    "\n",
    "3. **Text Processing**\n",
    "   - Cleaning â†’ Splitting â†’ Joining\n",
    "   - Maintains document coherence\n",
    "   - Optimizes chunk size\n",
    "\n",
    "## Structured Data Branch (CSV)\n",
    "1. **CSV Processing**\n",
    "   - CSV files â†’ CSVConverter\n",
    "   - Row-wise splitting\n",
    "   - Metadata preservation\n",
    "\n",
    "## Final Processing\n",
    "1. **Document Joining**\n",
    "   - Combines all processed documents\n",
    "   - Maintains source information\n",
    "   - Prepares for embedding\n",
    "\n",
    "2. **Embedding and Storage**\n",
    "   - Vector generation\n",
    "   - Document store writing\n",
    "   - Index updating\n",
    "\n",
    "The following cell implements these connections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea3cb37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x30673b620>\n",
       "ðŸš… Components\n",
       "  - link_fetcher: LinkContentFetcher\n",
       "  - html_converter: HTMLToDocument\n",
       "  - file_type_router: FileTypeRouter\n",
       "  - text_converter: TextFileToDocument\n",
       "  - pdf_converter: PyPDFToDocument\n",
       "  - unstructured_doc_joiner: DocumentJoiner\n",
       "  - text_cleaner: DocumentCleaner\n",
       "  - text_splitter: DocumentSplitter\n",
       "  - doc_embedder: SentenceTransformersDocumentEmbedder\n",
       "  - writer: DocumentWriter\n",
       "  - csv_converter: TextFileToDocument\n",
       "  - csv_cleaner: CSVDocumentCleaner\n",
       "  - csv_splitter: CSVDocumentSplitter\n",
       "  - final_doc_joiner: DocumentJoiner\n",
       "ðŸ›¤ï¸ Connections\n",
       "  - link_fetcher.streams -> html_converter.sources (list[ByteStream])\n",
       "  - html_converter.documents -> unstructured_doc_joiner.documents (list[Document])\n",
       "  - file_type_router.text/plain -> text_converter.sources (list[Union[str, Path, ByteStream]])\n",
       "  - file_type_router.application/pdf -> pdf_converter.sources (list[Union[str, Path, ByteStream]])\n",
       "  - file_type_router.text/csv -> csv_converter.sources (list[Union[str, Path, ByteStream]])\n",
       "  - text_converter.documents -> unstructured_doc_joiner.documents (list[Document])\n",
       "  - pdf_converter.documents -> unstructured_doc_joiner.documents (list[Document])\n",
       "  - unstructured_doc_joiner.documents -> text_cleaner.documents (list[Document])\n",
       "  - text_cleaner.documents -> text_splitter.documents (list[Document])\n",
       "  - text_splitter.documents -> final_doc_joiner.documents (list[Document])\n",
       "  - doc_embedder.documents -> writer.documents (list[Document])\n",
       "  - csv_converter.documents -> csv_cleaner.documents (list[Document])\n",
       "  - csv_cleaner.documents -> csv_splitter.documents (list[Document])\n",
       "  - csv_splitter.documents -> final_doc_joiner.documents (list[Document])\n",
       "  - final_doc_joiner.documents -> doc_embedder.documents (list[Document])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# --- Unstructured Data Branch (Web, TXT, PDF) ---\n",
    "# Web data\n",
    "indexing_pipeline.connect(\"link_fetcher.streams\", \"html_converter.sources\")\n",
    "indexing_pipeline.connect(\"html_converter.documents\", \"unstructured_doc_joiner.documents\")\n",
    "\n",
    "# Local file data (TXT, PDF)\n",
    "indexing_pipeline.connect(\"file_type_router.text/plain\", \"text_converter.sources\")\n",
    "indexing_pipeline.connect(\"file_type_router.application/pdf\", \"pdf_converter.sources\")\n",
    "indexing_pipeline.connect(\"text_converter.documents\", \"unstructured_doc_joiner.documents\")\n",
    "indexing_pipeline.connect(\"pdf_converter.documents\", \"unstructured_doc_joiner.documents\")\n",
    "\n",
    "# Processing for unstructured data\n",
    "indexing_pipeline.connect(\"unstructured_doc_joiner\", \"text_cleaner\")\n",
    "indexing_pipeline.connect(\"text_cleaner\", \"text_splitter\")\n",
    "# Connect the split *text* docs to the *final* joiner\n",
    "indexing_pipeline.connect(\"text_splitter.documents\", \"final_doc_joiner.documents\")\n",
    "\n",
    "\n",
    "#  Structured Data Branch (CSV) ---\n",
    "# Route CSV files to the csv_converter\n",
    "indexing_pipeline.connect(\"file_type_router.text/csv\", \"csv_converter.sources\")\n",
    "# Process the CSV documents\n",
    "indexing_pipeline.connect(\"csv_converter.documents\", \"csv_cleaner.documents\")\n",
    "indexing_pipeline.connect(\"csv_cleaner.documents\", \"csv_splitter.documents\")\n",
    "# Connect the split *csv* docs to the *final* joiner\n",
    "indexing_pipeline.connect(\"csv_splitter.documents\", \"final_doc_joiner.documents\")\n",
    "\n",
    "\n",
    "# --- Main Processing Path (Embedding and Writing) ---\n",
    "# The final_doc_joiner now receives documents from *both* branches\n",
    "indexing_pipeline.connect(\"final_doc_joiner\", \"doc_embedder\")\n",
    "indexing_pipeline.connect(\"doc_embedder\", \"writer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b99287e",
   "metadata": {},
   "source": [
    "Let's visualize the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5b936d",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexing_pipeline.draw(path=\"./images/indexing_pipeline.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40df5ac5",
   "metadata": {},
   "source": [
    "![](./images/indexing_pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbedc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running unified indexing pipeline for web, local files, and CSV...\n",
      "Skipping PDF file: data_for_indexing/sample.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing document 1384ec36dd6d99f90ab589732d5219b7371dac846d0f0bd89c6385189c4079c0. Keeping it, but skipping cleaning. Error: Error tokenizing data. C error: Expected 5 fields in line 5, saw 7\n",
      "\n",
      "Error processing document 1384ec36dd6d99f90ab589732d5219b7371dac846d0f0bd89c6385189c4079c0. Keeping it, but skipping splitting. Error: Error tokenizing data. C error: Expected 5 fields in line 5, saw 7\n",
      "\n",
      "Error processing document 1384ec36dd6d99f90ab589732d5219b7371dac846d0f0bd89c6385189c4079c0. Keeping it, but skipping splitting. Error: Error tokenizing data. C error: Expected 5 fields in line 5, saw 7\n",
      "\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.04it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.04it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'writer': {'documents_written': 15}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 5. Run the Pipeline ---\n",
    "\n",
    "print(\"Running unified indexing pipeline for web, local files, and CSV...\")\n",
    "# Note: The PDF path will be ignored if the file doesn't exist.\n",
    "file_paths_to_process = [text_file_path]\n",
    "\n",
    "if pdf_file_path.exists() and pdf_file_path.stat().st_size > 0:\n",
    "    file_paths_to_process.append(pdf_file_path)\n",
    "else:\n",
    "    print(f\"Skipping PDF file: {pdf_file_path}\")\n",
    "\n",
    "file_paths_to_process.append(csv_file_path)\n",
    "\n",
    "\n",
    "indexing_pipeline.run({\n",
    "    \"link_fetcher\": {\"urls\": [web_url]},\n",
    "    \"file_type_router\": {\"sources\": file_paths_to_process}\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc7bc7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'document_store' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- 7. Verify the DocumentStore ---\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m doc_count = \u001b[43mdocument_store\u001b[49m.count_documents()\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTotal documents in DocumentStore: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSample document from the store:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'document_store' is not defined"
     ]
    }
   ],
   "source": [
    "# --- 7. Verify the DocumentStore ---\n",
    "doc_count = document_store.count_documents()\n",
    "print(f\"\\nTotal documents in DocumentStore: {doc_count}\")\n",
    "print(\"Sample document from the store:\")\n",
    "print(document_store.filter_documents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6305e3c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-with-haystack-ch4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
