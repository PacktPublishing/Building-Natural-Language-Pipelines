{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9191a51",
   "metadata": {},
   "source": [
    "# Building a Document Indexing Pipeline with Haystack\n",
    "\n",
    "Welcome to this tutorial notebook on building a document indexing pipeline using Haystack! In this notebook, you'll learn how to create a robust pipeline for processing and indexing documents that can be later used for question answering and information retrieval.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will:\n",
    "- Understand how to set up a Haystack document processing pipeline\n",
    "- Learn to work with different document formats (TXT, CSV, PDF)\n",
    "- Create and configure an in-memory document store\n",
    "- Implement document preprocessing and indexing\n",
    "- Test your indexed documents with basic queries\n",
    "\n",
    "## Prerequisites\n",
    "- Basic Python knowledge\n",
    "- Understanding of basic NLP concepts\n",
    "- Familiarity with Jupyter notebooks\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ea268f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291a4a1f",
   "metadata": {},
   "source": [
    "## Setup and Package Installation\n",
    "\n",
    "In this section, we'll import the necessary packages for our indexing pipeline. Here's what each package does:\n",
    "\n",
    "- `haystack`: The main framework we'll use for building our document processing pipeline\n",
    "- `haystack.document_stores`: Contains different backends for storing our processed documents\n",
    "- `haystack.nodes`: Pipeline components for processing and transforming documents\n",
    "- `haystack.pipelines`: Tools for connecting different processing nodes together\n",
    "- `os`: For handling file paths and environment variables\n",
    "- `logging`: To get helpful feedback about what's happening in our pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33f8302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core Haystack classes\n",
    "from haystack import Pipeline, Document\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.components.joiners import DocumentJoiner\n",
    "\n",
    "# Import components for data fetching and conversion\n",
    "from haystack.components.fetchers import LinkContentFetcher\n",
    "from haystack.components.converters import (\n",
    "    PyPDFToDocument,\n",
    "    TextFileToDocument,\n",
    "    HTMLToDocument,\n",
    ")\n",
    "from haystack.components.routers import FileTypeRouter\n",
    "\n",
    "# Import components for preprocessing\n",
    "from haystack.components.preprocessors import (\n",
    "    DocumentCleaner,\n",
    "    DocumentSplitter,\n",
    "    CSVDocumentCleaner,\n",
    "    CSVDocumentSplitter\n",
    ")\n",
    "\n",
    "# Import components for embedding\n",
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4036223e",
   "metadata": {},
   "source": [
    "## Document Loading and Preprocessing\n",
    "\n",
    "In this section, we'll set up our document loading pipeline. We'll work with multiple document formats:\n",
    "\n",
    "1. Text files (.txt)\n",
    "2. CSV files\n",
    "3. PDF documents\n",
    "\n",
    "The data we'll be working with includes:\n",
    "- A text file about Haystack's introduction\n",
    "- A CSV file containing information about LLM models\n",
    "- A sample PDF document\n",
    "\n",
    "We'll create a preprocessing pipeline that will:\n",
    "1. Load these documents from different sources\n",
    "2. Convert them into a unified format\n",
    "3. Clean and prepare them for indexing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8eb5248a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 1. Create Sample Data Files ---\n",
    "# Create a directory to hold our source files\n",
    "data_dir = Path(\"data_for_indexing\")\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create a sample text file\n",
    "text_file_path = data_dir / \"haystack_intro.txt\"\n",
    "text_file_path.write_text(\n",
    "    \"Haystack is an open-source framework by deepset for building production-ready LLM applications. \"\n",
    "    \"It enables developers to create retrieval-augmented generative pipelines and state-of-the-art search systems.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9b74efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample CSV file with some empty rows/columns for cleaning\n",
    "csv_content = \"\"\"Company,Model,Release Year,,Notes\n",
    "OpenAI,GPT-4,2023,,Generative Pre-trained Transformer 4\n",
    ",,,\n",
    "Google,Gemini,2023,,A family of multimodal models\n",
    "Anthropic,Claude 3,2024,,Includes Opus, Sonnet, and Haiku models\n",
    "\"\"\"\n",
    "csv_file_path = data_dir / \"llm_models.csv\"\n",
    "csv_file_path.write_text(csv_content)\n",
    "\n",
    "# Define a sample URL to fetch\n",
    "web_url = \"https://haystack.deepset.ai/blog/haystack-2-release\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb151a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this example, we'll skip the actual PDF creation and assume one exists.\n",
    "# You can place any PDF file in the 'data_for_indexing' directory and name it 'sample.pdf'.\n",
    "# For a runnable example, we will simulate its path.\n",
    "pdf_file_path = data_dir / \"sample.pdf\"\n",
    "# In a real scenario, you would have this file. For this script to run, we'll check for it.\n",
    "if not pdf_file_path.exists():\n",
    "    print(f\"Warning: PDF file not found at {pdf_file_path}. The PDF processing branch will not run.\")\n",
    "    # Create a dummy file to avoid path errors, but it won't be processed as PDF\n",
    "    pdf_file_path.touch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d455fad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DocumentStore: For this example, we use an in-memory store.\n",
    "# For production, you would use a persistent vector database like Qdrant, Pinecone, or Weaviate. [11, 12]\n",
    "document_store = InMemoryDocumentStore()\n",
    "\n",
    "# FileTypeRouter: Directs files to the correct converter based on their MIME type. \n",
    "file_type_router = FileTypeRouter(mime_types=[\"text/plain\", \"application/pdf\", \"text/html\"])\n",
    "\n",
    "# Converters: One for each file type we want to handle.\n",
    "text_file_converter = TextFileToDocument()\n",
    "pdf_converter = PyPDFToDocument()\n",
    "html_converter = HTMLToDocument()\n",
    "\n",
    "# LinkContentFetcher: Fetches content from URLs and returns it as ByteStream objects. \n",
    "link_fetcher = LinkContentFetcher()\n",
    "\n",
    "# DocumentJoiner: Merges lists of Documents from different paths into one. \n",
    "document_joiner = DocumentJoiner()\n",
    "\n",
    "# Preprocessors for Text Data:\n",
    "# DocumentCleaner: Removes extra whitespace, etc. \n",
    "cleaner = DocumentCleaner()\n",
    "# DocumentSplitter: Chunks documents into smaller pieces. \n",
    "text_splitter = DocumentSplitter(split_by=\"word\", split_length=150, split_overlap=20)\n",
    "\n",
    "# Preprocessors for Tabular Data (CSV):\n",
    "# CSVDocumentCleaner: Removes empty rows and columns from CSV data. [16, 17]\n",
    "csv_cleaner = CSVDocumentCleaner()\n",
    "# CSVDocumentSplitter: Splits a large CSV into smaller tables or row-wise documents. \n",
    "# Here, we split each row into a separate Document.\n",
    "csv_splitter = CSVDocumentSplitter(split_mode=\"row-wise\")\n",
    "\n",
    "# Embedder: Creates vector representations of the documents.\n",
    "# It's crucial to use a model that aligns with the one you'll use for querying.\n",
    "doc_embedder = SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# DocumentWriter: Writes the final documents to the DocumentStore.\n",
    "writer = DocumentWriter(document_store)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574c036a",
   "metadata": {},
   "source": [
    "## Building the Indexing Pipeline\n",
    "\n",
    "Now we'll construct our document processing pipeline. This pipeline will consist of several key components:\n",
    "\n",
    "1. **File Converters**\n",
    "   - TextConverter: Processes plain text files\n",
    "   - PDFToTextConverter: Extracts text from PDF documents\n",
    "   - TableReader: Handles CSV and other tabular data\n",
    "\n",
    "2. **PreProcessor**\n",
    "   - Splits documents into smaller chunks\n",
    "   - Cleans and normalizes text\n",
    "   - Creates clean metadata\n",
    "\n",
    "3. **DocumentStore**\n",
    "   - InMemory backend for storing processed documents\n",
    "   - Enables efficient searching and retrieval\n",
    "   - Maintains document metadata\n",
    "\n",
    "Let's see how these components work together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1842b299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Build the Indexing Pipeline ---\n",
    "\n",
    "indexing_pipeline = Pipeline()\n",
    "\n",
    "# Add all components to the pipeline with unique names\n",
    "indexing_pipeline.add_component(\"link_fetcher\", link_fetcher)\n",
    "indexing_pipeline.add_component(\"html_converter\", html_converter)\n",
    "indexing_pipeline.add_component(\"file_type_router\", file_type_router)\n",
    "indexing_pipeline.add_component(\"text_file_converter\", text_file_converter)\n",
    "indexing_pipeline.add_component(\"pdf_converter\", pdf_converter)\n",
    "indexing_pipeline.add_component(\"document_joiner\", document_joiner)\n",
    "indexing_pipeline.add_component(\"cleaner\", cleaner)\n",
    "indexing_pipeline.add_component(\"text_splitter\", text_splitter)\n",
    "indexing_pipeline.add_component(\"doc_embedder\", doc_embedder)\n",
    "indexing_pipeline.add_component(\"writer\", writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cea3cb37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x32350fe30>\n",
       "üöÖ Components\n",
       "  - link_fetcher: LinkContentFetcher\n",
       "  - html_converter: HTMLToDocument\n",
       "  - file_type_router: FileTypeRouter\n",
       "  - text_file_converter: TextFileToDocument\n",
       "  - pdf_converter: PyPDFToDocument\n",
       "  - document_joiner: DocumentJoiner\n",
       "  - cleaner: DocumentCleaner\n",
       "  - text_splitter: DocumentSplitter\n",
       "  - doc_embedder: SentenceTransformersDocumentEmbedder\n",
       "  - writer: DocumentWriter\n",
       "üõ§Ô∏è Connections\n",
       "  - link_fetcher.streams -> html_converter.sources (list[ByteStream])\n",
       "  - html_converter.documents -> document_joiner.documents (list[Document])\n",
       "  - file_type_router.text/plain -> text_file_converter.sources (list[Union[str, Path, ByteStream]])\n",
       "  - file_type_router.application/pdf -> pdf_converter.sources (list[Union[str, Path, ByteStream]])\n",
       "  - text_file_converter.documents -> document_joiner.documents (list[Document])\n",
       "  - pdf_converter.documents -> document_joiner.documents (list[Document])\n",
       "  - document_joiner.documents -> cleaner.documents (list[Document])\n",
       "  - cleaner.documents -> text_splitter.documents (list[Document])\n",
       "  - text_splitter.documents -> doc_embedder.documents (list[Document])\n",
       "  - doc_embedder.documents -> writer.documents (list[Document])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add CSV-specific components\n",
    "# We'll create a separate pipeline for CSV processing for clarity, then integrate the concept.\n",
    "# In a single large pipeline, you would route CSV files similarly.\n",
    "# For this example, we'll process the CSV separately and add it to the store.\n",
    "\n",
    "# --- 4. Connect the Pipeline Components ---\n",
    "\n",
    "# Web data branch\n",
    "indexing_pipeline.connect(\"link_fetcher.streams\", \"html_converter.sources\")\n",
    "indexing_pipeline.connect(\"html_converter.documents\", \"document_joiner.documents\")\n",
    "\n",
    "# Local file data branch\n",
    "indexing_pipeline.connect(\"file_type_router.text/plain\", \"text_file_converter.sources\")\n",
    "indexing_pipeline.connect(\"file_type_router.application/pdf\", \"pdf_converter.sources\")\n",
    "indexing_pipeline.connect(\"text_file_converter.documents\", \"document_joiner.documents\")\n",
    "indexing_pipeline.connect(\"pdf_converter.documents\", \"document_joiner.documents\")\n",
    "\n",
    "# Main processing path after joining\n",
    "indexing_pipeline.connect(\"document_joiner\", \"cleaner\")\n",
    "indexing_pipeline.connect(\"cleaner\", \"text_splitter\")\n",
    "indexing_pipeline.connect(\"text_splitter\", \"doc_embedder\")\n",
    "indexing_pipeline.connect(\"doc_embedder\", \"writer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b99287e",
   "metadata": {},
   "source": [
    "Let's visualize the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e5b936d",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexing_pipeline.draw(path=\"./images/indexing_pipeline.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40df5ac5",
   "metadata": {},
   "source": [
    "![](./images/indexing_pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3dbedc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running indexing pipeline for web and local files...\n",
      "Skipping PDF file: data_for_indexing/sample.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.96it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'writer': {'documents_written': 14}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 5. Run the Pipeline ---\n",
    "\n",
    "print(\"Running indexing pipeline for web and local files...\")\n",
    "# Note: The PDF path will be ignored if the file doesn't exist.\n",
    "file_paths_to_process = [text_file_path]\n",
    "if pdf_file_path.exists() and pdf_file_path.stat().st_size > 0:\n",
    "    file_paths_to_process.append(pdf_file_path)\n",
    "else:\n",
    "    print(f\"Skipping PDF file: {pdf_file_path}\")\n",
    "\n",
    "indexing_pipeline.run({\n",
    "    \"link_fetcher\": {\"urls\": [web_url]},\n",
    "    \"file_type_router\": {\"sources\": file_paths_to_process}\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8391fb4",
   "metadata": {},
   "source": [
    "## Understanding the Results\n",
    "\n",
    "Let's examine what our pipeline has processed. We'll look at:\n",
    "1. The number of documents processed\n",
    "2. How the documents were split\n",
    "3. The metadata that was extracted\n",
    "\n",
    "This will help us verify that our pipeline is working as expected and give us insights into our indexed documents.\n",
    "\n",
    "First, let's query our document store to see what we've indexed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "efc7bc7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total documents in DocumentStore: 14\n",
      "Sample document from the store:\n",
      "[Document(id=be2fb4afe8f3e531ae2e97314778b92789c44d794c981d67bdea8658cf3fe51e, content: 'Haystack 2.0: The Composable Open-Source LLM Framework\n",
      "Meet Haystack 2.0, a more flexible, customiza...', meta: {'content_type': 'text/html', 'url': 'https://haystack.deepset.ai/blog/haystack-2-release', 'source_id': '0b188f4690ab3496d2270baf378be9fde19707e9b1ece003123c0442af918bb7', 'page_number': 1, 'split_id': 0, 'split_idx_start': 0, '_split_overlap': [{'doc_id': 'ff9dcb8208955755eb458bdf693e12ba8c2dd7f27de0bcb51e47e474255ad47b', 'range': (0, 124)}]}, embedding: vector of size 384), Document(id=ff9dcb8208955755eb458bdf693e12ba8c2dd7f27de0bcb51e47e474255ad47b, content: 'user before or not. You can get started by installing haystack-ai\n",
      ", our new package for Haystack 2.0...', meta: {'content_type': 'text/html', 'url': 'https://haystack.deepset.ai/blog/haystack-2-release', 'source_id': '0b188f4690ab3496d2270baf378be9fde19707e9b1ece003123c0442af918bb7', 'page_number': 1, 'split_id': 1, 'split_idx_start': 834, '_split_overlap': [{'doc_id': 'be2fb4afe8f3e531ae2e97314778b92789c44d794c981d67bdea8658cf3fe51e', 'range': (834, 958)}, {'doc_id': 'c59e96866da9b70a659ed9d87e43202d20e38c1d8022fae86f73c9f83268afea', 'range': (0, 144)}]}, embedding: vector of size 384), Document(id=c59e96866da9b70a659ed9d87e43202d20e38c1d8022fae86f73c9f83268afea, content: 'question-answering. During this time, we established the core of what makes Haystack Haystack: Compo...', meta: {'content_type': 'text/html', 'url': 'https://haystack.deepset.ai/blog/haystack-2-release', 'source_id': '0b188f4690ab3496d2270baf378be9fde19707e9b1ece003123c0442af918bb7', 'page_number': 1, 'split_id': 2, 'split_idx_start': 1651, '_split_overlap': [{'doc_id': 'ff9dcb8208955755eb458bdf693e12ba8c2dd7f27de0bcb51e47e474255ad47b', 'range': (817, 961)}, {'doc_id': '026a1a2d6fd15933c794c8352dfb015d939dc76fd2b8a6b1f68b166c2bd5e4e5', 'range': (0, 138)}]}, embedding: vector of size 384), Document(id=026a1a2d6fd15933c794c8352dfb015d939dc76fd2b8a6b1f68b166c2bd5e4e5, content: 'is a complete rewrite, the underlying principle of composing components into flexible pipelines rema...', meta: {'content_type': 'text/html', 'url': 'https://haystack.deepset.ai/blog/haystack-2-release', 'source_id': '0b188f4690ab3496d2270baf378be9fde19707e9b1ece003123c0442af918bb7', 'page_number': 1, 'split_id': 3, 'split_idx_start': 2517, '_split_overlap': [{'doc_id': 'c59e96866da9b70a659ed9d87e43202d20e38c1d8022fae86f73c9f83268afea', 'range': (866, 1004)}, {'doc_id': '828f39441b56d74639d7cf80eedae532cb31da7543ccff20c838d8e440edd6a3', 'range': (0, 120)}]}, embedding: vector of size 384), Document(id=828f39441b56d74639d7cf80eedae532cb31da7543ccff20c838d8e440edd6a3, content: 'not allowed, and the pipeline graph has to be acyclic. This makes it difficult to implement, for exa...', meta: {'content_type': 'text/html', 'url': 'https://haystack.deepset.ai/blog/haystack-2-release', 'source_id': '0b188f4690ab3496d2270baf378be9fde19707e9b1ece003123c0442af918bb7', 'page_number': 1, 'split_id': 4, 'split_idx_start': 3405, '_split_overlap': [{'doc_id': '026a1a2d6fd15933c794c8352dfb015d939dc76fd2b8a6b1f68b166c2bd5e4e5', 'range': (888, 1008)}, {'doc_id': '234aad190cf53f7615c79bca110f626647098e63c6cbaf46997a1f4278a895c2', 'range': (0, 117)}]}, embedding: vector of size 384), Document(id=234aad190cf53f7615c79bca110f626647098e63c6cbaf46997a1f4278a895c2, content: 'it transparent as to how these components can ‚Äútalk‚Äù to each other.\n",
      "- Be flexible: Make it possible ...', meta: {'content_type': 'text/html', 'url': 'https://haystack.deepset.ai/blog/haystack-2-release', 'source_id': '0b188f4690ab3496d2270baf378be9fde19707e9b1ece003123c0442af918bb7', 'page_number': 1, 'split_id': 5, 'split_idx_start': 4245, '_split_overlap': [{'doc_id': '828f39441b56d74639d7cf80eedae532cb31da7543ccff20c838d8e440edd6a3', 'range': (840, 957)}, {'doc_id': 'bff241b723ea7ce505553329657bdf3f45ea0738c106869593433e9146f855f0', 'range': (0, 120)}]}, embedding: vector of size 384), Document(id=bff241b723ea7ce505553329657bdf3f45ea0738c106869593433e9146f855f0, content: 'creating a new component, to decide what inputs and outputs it should have is part of the ideation p...', meta: {'content_type': 'text/html', 'url': 'https://haystack.deepset.ai/blog/haystack-2-release', 'source_id': '0b188f4690ab3496d2270baf378be9fde19707e9b1ece003123c0442af918bb7', 'page_number': 1, 'split_id': 6, 'split_idx_start': 5109, '_split_overlap': [{'doc_id': '234aad190cf53f7615c79bca110f626647098e63c6cbaf46997a1f4278a895c2', 'range': (864, 984)}, {'doc_id': '590f38cb1024dafb259a01aa51ae51282c13a842bf2d145de16cb8d4155e25fc', 'range': (0, 120)}]}, embedding: vector of size 384), Document(id=590f38cb1024dafb259a01aa51ae51282c13a842bf2d145de16cb8d4155e25fc, content: 'few months (with contributions from Assembly AI, Jina AI, mixedbread ai and more). We will continue ...', meta: {'content_type': 'text/html', 'url': 'https://haystack.deepset.ai/blog/haystack-2-release', 'source_id': '0b188f4690ab3496d2270baf378be9fde19707e9b1ece003123c0442af918bb7', 'page_number': 1, 'split_id': 7, 'split_idx_start': 5969, '_split_overlap': [{'doc_id': 'bff241b723ea7ce505553329657bdf3f45ea0738c106869593433e9146f855f0', 'range': (860, 980)}, {'doc_id': 'fd0c89246c79336218a1fa79ad8fba3b16b358f0b1cdd1a2762790985bc4946f', 'range': (0, 121)}]}, embedding: vector of size 384), Document(id=fd0c89246c79336218a1fa79ad8fba3b16b358f0b1cdd1a2762790985bc4946f, content: 'has to come from somewhere, and Haystack 2.0 provides a common interface to access it in a consisten...', meta: {'content_type': 'text/html', 'url': 'https://haystack.deepset.ai/blog/haystack-2-release', 'source_id': '0b188f4690ab3496d2270baf378be9fde19707e9b1ece003123c0442af918bb7', 'page_number': 1, 'split_id': 8, 'split_idx_start': 6801, '_split_overlap': [{'doc_id': '590f38cb1024dafb259a01aa51ae51282c13a842bf2d145de16cb8d4155e25fc', 'range': (832, 953)}, {'doc_id': 'c6e861b73ec4721c095bb5605151489ddc4f718eb27ae2b7f62ce4a6738ab811', 'range': (0, 125)}]}, embedding: vector of size 384), Document(id=c6e861b73ec4721c095bb5605151489ddc4f718eb27ae2b7f62ce4a6738ab811, content: 'storage service.\n",
      "A clear path to production\n",
      "The experience we got over the last couple of years, wor...', meta: {'content_type': 'text/html', 'url': 'https://haystack.deepset.ai/blog/haystack-2-release', 'source_id': '0b188f4690ab3496d2270baf378be9fde19707e9b1ece003123c0442af918bb7', 'page_number': 1, 'split_id': 9, 'split_idx_start': 7631, '_split_overlap': [{'doc_id': 'fd0c89246c79336218a1fa79ad8fba3b16b358f0b1cdd1a2762790985bc4946f', 'range': (830, 955)}, {'doc_id': 'a64d4c9cf677d5e88db2266ef191b327df244cb801258d8d0ee234d8c58f8ab2', 'range': (0, 149)}]}, embedding: vector of size 384), Document(id=a64d4c9cf677d5e88db2266ef191b327df244cb801258d8d0ee234d8c58f8ab2, content: 'pipelines behind a RESTful API: Hayhooks.\n",
      "Hayhooks is a client-server application that allows you to...', meta: {'content_type': 'text/html', 'url': 'https://haystack.deepset.ai/blog/haystack-2-release', 'source_id': '0b188f4690ab3496d2270baf378be9fde19707e9b1ece003123c0442af918bb7', 'page_number': 1, 'split_id': 10, 'split_idx_start': 8549, '_split_overlap': [{'doc_id': 'c6e861b73ec4721c095bb5605151489ddc4f718eb27ae2b7f62ce4a6738ab811', 'range': (918, 1067)}, {'doc_id': '7f6f7d895d1c405a537f7b39d5eeda04f053fd9ebb147b45fee110e81422d63f', 'range': (0, 129)}]}, embedding: vector of size 384), Document(id=7f6f7d895d1c405a537f7b39d5eeda04f053fd9ebb147b45fee110e81422d63f, content: 'being good examples of how we‚Äôve been leveraging Haystack 2.0. How?:\n",
      "- It‚Äôs easier to extend the cap...', meta: {'content_type': 'text/html', 'url': 'https://haystack.deepset.ai/blog/haystack-2-release', 'source_id': '0b188f4690ab3496d2270baf378be9fde19707e9b1ece003123c0442af918bb7', 'page_number': 1, 'split_id': 11, 'split_idx_start': 9489, '_split_overlap': [{'doc_id': 'a64d4c9cf677d5e88db2266ef191b327df244cb801258d8d0ee234d8c58f8ab2', 'range': (940, 1069)}, {'doc_id': 'cb016e7e3e342381e63aa790089eaf797bfda2564f19308da2d2408e003cf59b', 'range': (0, 157)}]}, embedding: vector of size 384), Document(id=cb016e7e3e342381e63aa790089eaf797bfda2564f19308da2d2408e003cf59b, content: 'resources and more to help you get started:\n",
      "- Documentation: full technical documentation on all Hay...', meta: {'content_type': 'text/html', 'url': 'https://haystack.deepset.ai/blog/haystack-2-release', 'source_id': '0b188f4690ab3496d2270baf378be9fde19707e9b1ece003123c0442af918bb7', 'page_number': 1, 'split_id': 12, 'split_idx_start': 10387, '_split_overlap': [{'doc_id': '7f6f7d895d1c405a537f7b39d5eeda04f053fd9ebb147b45fee110e81422d63f', 'range': (898, 1055)}]}, embedding: vector of size 384), Document(id=abe67d1945474acc782933988f972e8fb8eead145c1e9a124bc45529c0913020, content: 'Haystack is an open-source framework by deepset for building production-ready LLM applications. It e...', meta: {'file_path': 'haystack_intro.txt', 'source_id': '160d6f24e29805bf0759170ff471675c29a1236687381c1140733597dbaaa14d', 'page_number': 1, 'split_id': 0, 'split_idx_start': 0, '_split_overlap': []}, embedding: vector of size 384)]\n"
     ]
    }
   ],
   "source": [
    "# --- 7. Verify the DocumentStore ---\n",
    "doc_count = document_store.count_documents()\n",
    "print(f\"\\nTotal documents in DocumentStore: {doc_count}\")\n",
    "print(\"Sample document from the store:\")\n",
    "print(document_store.filter_documents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6305e3c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-with-haystack-ch4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
