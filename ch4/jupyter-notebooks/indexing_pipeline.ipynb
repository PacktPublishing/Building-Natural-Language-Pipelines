{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9191a51",
   "metadata": {},
   "source": [
    "# Building a Document Indexing Pipeline with Haystack\n",
    "\n",
    "Welcome to this comprehensive tutorial on building a document indexing pipeline using Haystack! This notebook demonstrates how to create a robust system for processing and indexing various types of documents for retrieval-augmented generation (RAG) applications.\n",
    "\n",
    "## What you'll learn\n",
    "After completing this notebook, you will be able to:\n",
    "- Build a multi-format document processing pipeline using Haystack\n",
    "- Handle different document types (TXT, CSV, PDF, HTML) in a unified way\n",
    "- Implement proper document cleaning and splitting strategies\n",
    "- Create and configure an efficient document store\n",
    "- Generate and store document embeddings for semantic search\n",
    "- Test and validate your indexed documents\n",
    "\n",
    "## Key Concepts\n",
    "Before we begin, let's understand some core concepts:\n",
    "- **Document Store**: A database that holds our processed documents and their embeddings\n",
    "- **Document Processing**: Converting raw files into a standardized format\n",
    "- **Embeddings**: Dense vector representations of text for semantic search\n",
    "- **Pipeline Components**: Modular pieces that handle specific processing tasks\n",
    "\n",
    "## Prerequisites\n",
    "- Basic Python knowledge\n",
    "- Understanding of basic NLP concepts\n",
    "- Familiarity with Jupyter notebooks\n",
    "- Haystack library installed\n",
    "\n",
    "Let's dive in and build our indexing pipeline step by step!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2807bc4a",
   "metadata": {},
   "source": [
    "## 1. Setting Up Our Environment\n",
    "\n",
    "Before we build our pipeline, we need to import the necessary components. Let's understand what each package brings to our indexing system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ea268f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291a4a1f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Core Components\n",
    "- **`Pipeline`**: The main orchestrator that connects all components\n",
    "- **`Document`**: The fundamental data structure for text in Haystack\n",
    "- **`InMemoryDocumentStore`**: Our database for storing processed documents\n",
    "\n",
    "## Document Processing Components\n",
    "- **`DocumentWriter`**: Handles saving documents to our store\n",
    "- **`DocumentJoiner`**: Combines documents from different sources\n",
    "- **`FileTypeRouter`**: Routes files to appropriate processors based on type\n",
    "\n",
    "## Format-Specific Components\n",
    "- **`LinkContentFetcher`**: Retrieves content from web URLs\n",
    "- **`PyPDFToDocument`**: Processes PDF files\n",
    "- **`TextFileToDocument`**: Handles plain text files\n",
    "- **`HTMLToDocument`**: Processes HTML content\n",
    "\n",
    "## Text Processing Components\n",
    "- **`DocumentCleaner`**: Removes unwanted content and normalizes text\n",
    "- **`DocumentSplitter`**: Breaks documents into manageable chunks\n",
    "- **`CSVDocumentCleaner`/`Splitter`**: Specialized handlers for tabular data\n",
    "\n",
    "## Embedding Component\n",
    "- **`SentenceTransformersDocumentEmbedder`**: Converts text to vector representations\n",
    "\n",
    "In the next cell, we'll import these components and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33f8302a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurafunderburk/Documents/GitHub/Building-Natural-Language-Pipelines/ch4/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import core Haystack classes\n",
    "from haystack import Pipeline, Document\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.components.joiners import DocumentJoiner\n",
    "\n",
    "# Import components for data fetching and conversion\n",
    "from haystack.components.fetchers import LinkContentFetcher\n",
    "from haystack.components.converters import (\n",
    "    PyPDFToDocument,\n",
    "    TextFileToDocument,\n",
    "    HTMLToDocument,\n",
    ")\n",
    "from haystack.components.routers import FileTypeRouter\n",
    "\n",
    "# Import components for preprocessing\n",
    "from haystack.components.preprocessors import (\n",
    "    DocumentCleaner,\n",
    "    DocumentSplitter,\n",
    "    CSVDocumentCleaner,\n",
    "    CSVDocumentSplitter\n",
    ")\n",
    "\n",
    "# Import components for embedding\n",
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4036223e",
   "metadata": {},
   "source": [
    "## 2. Document Loading and Preprocessing\n",
    "\n",
    "In this section, we'll set up our document loading pipeline. We'll work with multiple document formats:\n",
    "\n",
    "1. Text files (.txt)\n",
    "2. CSV files\n",
    "3. PDF documents\n",
    "\n",
    "The data we'll be working with includes:\n",
    "- A text file about Haystack's introduction\n",
    "- A CSV file containing information about LLM models\n",
    "- A sample PDF document\n",
    "\n",
    "We'll create a preprocessing pipeline that will:\n",
    "1. Load these documents from different sources\n",
    "2. Convert them into a unified format\n",
    "3. Clean and prepare them for indexing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8eb5248a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 1. Create Sample Data Files ---\n",
    "# Create a directory to hold our source files\n",
    "data_dir = Path(\"data_for_indexing\")\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create a sample text file\n",
    "text_file_path = data_dir / \"haystack_intro.txt\"\n",
    "text_file_path.write_text(\n",
    "    \"Haystack is an open-source framework by deepset for building production-ready LLM applications. \"\n",
    "    \"It enables developers to create retrieval-augmented generative pipelines and state-of-the-art search systems.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4fe1d3",
   "metadata": {},
   "source": [
    "Create mock CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9b74efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a sample CSV file with some empty rows/columns for cleaning\n",
    "csv_content = \"\"\"Company,Model,Release Year,,Notes\n",
    "OpenAI,GPT-4,2023,,Generative Pre-trained Transformer 4\n",
    ",,,\n",
    "Google,Gemini,2023,,A family of multimodal models\n",
    "Anthropic,Claude 3,2024,,Includes Opus, Sonnet, and Haiku models\n",
    "\"\"\"\n",
    "csv_file_path = data_dir / \"llm_models.csv\"\n",
    "csv_file_path.write_text(csv_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd38bf3f",
   "metadata": {},
   "source": [
    "Scrape a webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481f6d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sample URL to fetch\n",
    "web_url = \"https://haystack.deepset.ai/blog/haystack-2-release\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9d86ed",
   "metadata": {},
   "source": [
    "PDF for file that does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb151a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this example, we'll skip the actual PDF creation and assume one exists.\n",
    "# You can place any PDF file in the 'data_for_indexing' directory and name it 'sample.pdf'.\n",
    "# For a runnable example, we will simulate its path.\n",
    "pdf_file_path = data_dir / \"sample.pdf\"\n",
    "# In a real scenario, you would have this file. For this script to run, we'll check for it.\n",
    "if not pdf_file_path.exists():\n",
    "    print(f\"Warning: PDF file not found at {pdf_file_path}. The PDF processing branch will not run.\")\n",
    "    # Create a dummy file to avoid path errors, but it won't be processed as PDF\n",
    "    pdf_file_path.touch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a5aae7",
   "metadata": {},
   "source": [
    "## 3. Initializing Pipeline Components\n",
    "\n",
    "Now we'll create instances of each component needed for our pipeline. Let's understand each component's role and configuration:\n",
    "\n",
    "### Storage Components\n",
    "- **`DocumentStore`**: In-memory database for documents and embeddings\n",
    "  - Efficient for testing and development\n",
    "  - Supports vector similarity search\n",
    "  - Non-persistent (clears on restart)\n",
    "\n",
    "### Routing Components\n",
    "- **`FileTypeRouter`**: Smart file dispatcher\n",
    "  - Routes files based on MIME types\n",
    "  - Supports: text/plain, PDF, HTML, CSV\n",
    "  - Ensures proper format-specific processing\n",
    "\n",
    "### Document Conversion\n",
    "- **`TextFileToDocument`**: Converts plain text\n",
    "- **`PyPDFToDocument`**: Extracts text from PDFs\n",
    "- **`HTMLToDocument`**: Processes web content\n",
    "- **CSV `Converter`**: Handles tabular data\n",
    "\n",
    "### Document Processing\n",
    "- **`DocumentCleaner`**: Text normalization\n",
    "  - Removes extra whitespace\n",
    "  - Handles special characters\n",
    "  - Normalizes formatting\n",
    "\n",
    "- **`DocumentSplitter`**: Chunking strategy\n",
    "  - Splits by words (150 words per chunk)\n",
    "  - 20-word overlap for context\n",
    "  - Preserves semantic meaning\n",
    "\n",
    "### Embedding Generation\n",
    "- **`SentenceTransformersEmbedder`**:\n",
    "  - Model: all-MiniLM-L6-v2\n",
    "  - Creates 384-dimensional embeddings\n",
    "  - Optimized for semantic search\n",
    "\n",
    "The following cell initializes all these components with our chosen configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d455fad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DocumentStore:\n",
    "document_store = InMemoryDocumentStore()\n",
    "\n",
    "# FileTypeRouter: Directs files to the correct converter based on their MIME type.\n",
    "file_type_router = FileTypeRouter(mime_types=[\"text/plain\", \"application/pdf\", \"text/html\", \"text/csv\"])\n",
    "\n",
    "# Converters: One for each file type we want to handle.\n",
    "text_converter = TextFileToDocument()\n",
    "pdf_converter = PyPDFToDocument()\n",
    "html_converter = HTMLToDocument()\n",
    "csv_converter = TextFileToDocument()\n",
    "\n",
    "# LinkContentFetcher: Fetches content from URLs.\n",
    "link_fetcher = LinkContentFetcher()\n",
    "\n",
    "# DocumentJoiners:\n",
    "unstructured_doc_joiner = DocumentJoiner()\n",
    "# This joiner will gather documents from *all* processing branches\n",
    "# (the split text docs and the split csv docs) before embedding.\n",
    "final_doc_joiner = DocumentJoiner()\n",
    "\n",
    "# Preprocessors for Text Data:\n",
    "text_cleaner = DocumentCleaner()\n",
    "text_splitter = DocumentSplitter(split_by=\"word\", split_length=150, split_overlap=20)\n",
    "\n",
    "# Preprocessors for Tabular Data (CSV):\n",
    "# These will now be part of the main pipeline.\n",
    "csv_cleaner = CSVDocumentCleaner()\n",
    "csv_splitter = CSVDocumentSplitter(split_mode=\"row-wise\")\n",
    "\n",
    "# Embedder:\n",
    "doc_embedder = SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# DocumentWriter:\n",
    "writer = DocumentWriter(document_store)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574c036a",
   "metadata": {},
   "source": [
    "# 4. Constructing the Pipeline\n",
    "\n",
    "We'll now assemble our components into a complete indexing pipeline. Our pipeline architecture follows these principles:\n",
    "\n",
    "## Pipeline Architecture\n",
    "1. **Parallel Processing Branches**\n",
    "   - Unstructured data branch (Web, TXT, PDF)\n",
    "   - Structured data branch (CSV)\n",
    "   - Each branch optimized for its data type\n",
    "\n",
    "2. **Document Flow**\n",
    "   - Input → Conversion → Cleaning → Splitting → Embedding → Storage\n",
    "   - Specialized handling for each format\n",
    "   - Joins documents at strategic points\n",
    "\n",
    "3. **Component Naming**\n",
    "   - Each component gets a unique identifier\n",
    "   - Names reflect component functionality\n",
    "   - Helps in debugging and monitoring\n",
    "\n",
    "## Key Design Decisions\n",
    "- **Split Processing**: Separate paths for structured/unstructured data\n",
    "- **Document Joining**: Strategic combination points for efficiency\n",
    "- **Embedding Strategy**: Single embedder for consistency\n",
    "- **Error Handling**: Built-in format validation\n",
    "\n",
    "The following cell implements this architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1842b299",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "indexing_pipeline = Pipeline()\n",
    "\n",
    "# Add all components to the pipeline with unique names\n",
    "indexing_pipeline.add_component(\"link_fetcher\", link_fetcher)\n",
    "indexing_pipeline.add_component(\"html_converter\", html_converter)\n",
    "indexing_pipeline.add_component(\"file_type_router\", file_type_router)\n",
    "indexing_pipeline.add_component(\"text_converter\", text_converter)\n",
    "indexing_pipeline.add_component(\"pdf_converter\", pdf_converter)\n",
    "indexing_pipeline.add_component(\"unstructured_doc_joiner\", unstructured_doc_joiner)\n",
    "indexing_pipeline.add_component(\"text_cleaner\", text_cleaner)\n",
    "indexing_pipeline.add_component(\"text_splitter\", text_splitter)\n",
    "indexing_pipeline.add_component(\"doc_embedder\", doc_embedder)\n",
    "indexing_pipeline.add_component(\"writer\", writer)\n",
    "\n",
    "# **NEW**: Add the CSV components to the pipeline\n",
    "indexing_pipeline.add_component(\"csv_converter\", csv_converter)\n",
    "indexing_pipeline.add_component(\"csv_cleaner\", csv_cleaner)\n",
    "indexing_pipeline.add_component(\"csv_splitter\", csv_splitter)\n",
    "indexing_pipeline.add_component(\"final_doc_joiner\", final_doc_joiner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2211c63",
   "metadata": {},
   "source": [
    "# 5. Connecting Pipeline Components\n",
    "\n",
    "This is where we define the flow of data through our pipeline. The connections create two main processing branches that eventually merge:\n",
    "\n",
    "## Unstructured Data Branch\n",
    "1. **Web Content Processing**\n",
    "   - URL → LinkFetcher → HTMLConverter\n",
    "   - Handles dynamic web content\n",
    "   - Preserves important metadata\n",
    "\n",
    "2. **Local File Processing**\n",
    "   - Files → FileTypeRouter → Appropriate Converter\n",
    "   - Automatic format detection\n",
    "   - Specialized handling per type\n",
    "\n",
    "3. **Text Processing**\n",
    "   - Cleaning → Splitting → Joining\n",
    "   - Maintains document coherence\n",
    "   - Optimizes chunk size\n",
    "\n",
    "## Structured Data Branch (CSV)\n",
    "1. **CSV Processing**\n",
    "   - CSV files → CSVConverter\n",
    "   - Row-wise splitting\n",
    "   - Metadata preservation\n",
    "\n",
    "## Final Processing\n",
    "1. **Document Joining**\n",
    "   - Combines all processed documents\n",
    "   - Maintains source information\n",
    "   - Prepares for embedding\n",
    "\n",
    "2. **Embedding and Storage**\n",
    "   - Vector generation\n",
    "   - Document store writing\n",
    "   - Index updating\n",
    "\n",
    "The following cell implements these connections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cea3cb37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x328391c10>\n",
       "🚅 Components\n",
       "  - link_fetcher: LinkContentFetcher\n",
       "  - html_converter: HTMLToDocument\n",
       "  - file_type_router: FileTypeRouter\n",
       "  - text_converter: TextFileToDocument\n",
       "  - pdf_converter: PyPDFToDocument\n",
       "  - unstructured_doc_joiner: DocumentJoiner\n",
       "  - text_cleaner: DocumentCleaner\n",
       "  - text_splitter: DocumentSplitter\n",
       "  - doc_embedder: SentenceTransformersDocumentEmbedder\n",
       "  - writer: DocumentWriter\n",
       "  - csv_converter: TextFileToDocument\n",
       "  - csv_cleaner: CSVDocumentCleaner\n",
       "  - csv_splitter: CSVDocumentSplitter\n",
       "  - final_doc_joiner: DocumentJoiner\n",
       "🛤️ Connections\n",
       "  - link_fetcher.streams -> html_converter.sources (list[ByteStream])\n",
       "  - html_converter.documents -> unstructured_doc_joiner.documents (list[Document])\n",
       "  - file_type_router.text/plain -> text_converter.sources (list[Union[str, Path, ByteStream]])\n",
       "  - file_type_router.application/pdf -> pdf_converter.sources (list[Union[str, Path, ByteStream]])\n",
       "  - file_type_router.text/csv -> csv_converter.sources (list[Union[str, Path, ByteStream]])\n",
       "  - text_converter.documents -> unstructured_doc_joiner.documents (list[Document])\n",
       "  - pdf_converter.documents -> unstructured_doc_joiner.documents (list[Document])\n",
       "  - unstructured_doc_joiner.documents -> text_cleaner.documents (list[Document])\n",
       "  - text_cleaner.documents -> text_splitter.documents (list[Document])\n",
       "  - text_splitter.documents -> final_doc_joiner.documents (list[Document])\n",
       "  - doc_embedder.documents -> writer.documents (list[Document])\n",
       "  - csv_converter.documents -> csv_cleaner.documents (list[Document])\n",
       "  - csv_cleaner.documents -> csv_splitter.documents (list[Document])\n",
       "  - csv_splitter.documents -> final_doc_joiner.documents (list[Document])\n",
       "  - final_doc_joiner.documents -> doc_embedder.documents (list[Document])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# --- Unstructured Data Branch (Web, TXT, PDF) ---\n",
    "# Web data\n",
    "indexing_pipeline.connect(\"link_fetcher.streams\", \"html_converter.sources\")\n",
    "indexing_pipeline.connect(\"html_converter.documents\", \"unstructured_doc_joiner.documents\")\n",
    "\n",
    "# Local file data (TXT, PDF)\n",
    "indexing_pipeline.connect(\"file_type_router.text/plain\", \"text_converter.sources\")\n",
    "indexing_pipeline.connect(\"file_type_router.application/pdf\", \"pdf_converter.sources\")\n",
    "indexing_pipeline.connect(\"text_converter.documents\", \"unstructured_doc_joiner.documents\")\n",
    "indexing_pipeline.connect(\"pdf_converter.documents\", \"unstructured_doc_joiner.documents\")\n",
    "\n",
    "# Processing for unstructured data\n",
    "indexing_pipeline.connect(\"unstructured_doc_joiner\", \"text_cleaner\")\n",
    "indexing_pipeline.connect(\"text_cleaner\", \"text_splitter\")\n",
    "# Connect the split *text* docs to the *final* joiner\n",
    "indexing_pipeline.connect(\"text_splitter.documents\", \"final_doc_joiner.documents\")\n",
    "\n",
    "\n",
    "#  Structured Data Branch (CSV) ---\n",
    "# Route CSV files to the csv_converter\n",
    "indexing_pipeline.connect(\"file_type_router.text/csv\", \"csv_converter.sources\")\n",
    "# Process the CSV documents\n",
    "indexing_pipeline.connect(\"csv_converter.documents\", \"csv_cleaner.documents\")\n",
    "indexing_pipeline.connect(\"csv_cleaner.documents\", \"csv_splitter.documents\")\n",
    "# Connect the split *csv* docs to the *final* joiner\n",
    "indexing_pipeline.connect(\"csv_splitter.documents\", \"final_doc_joiner.documents\")\n",
    "\n",
    "\n",
    "# --- Main Processing Path (Embedding and Writing) ---\n",
    "# The final_doc_joiner now receives documents from *both* branches\n",
    "indexing_pipeline.connect(\"final_doc_joiner\", \"doc_embedder\")\n",
    "indexing_pipeline.connect(\"doc_embedder\", \"writer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b99287e",
   "metadata": {},
   "source": [
    "Let's visualize the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e5b936d",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexing_pipeline.draw(path=\"./images/indexing_pipeline.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40df5ac5",
   "metadata": {},
   "source": [
    "![](./images/indexing_pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3dbedc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing document 1384ec36dd6d99f90ab589732d5219b7371dac846d0f0bd89c6385189c4079c0. Keeping it, but skipping cleaning. Error: Error tokenizing data. C error: Expected 5 fields in line 5, saw 7\n",
      "\n",
      "Error processing document 1384ec36dd6d99f90ab589732d5219b7371dac846d0f0bd89c6385189c4079c0. Keeping it, but skipping splitting. Error: Error tokenizing data. C error: Expected 5 fields in line 5, saw 7\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running unified indexing pipeline for web, local files, and CSV...\n",
      "Skipping PDF file: data_for_indexing/sample.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.14it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'writer': {'documents_written': 15}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 5. Run the Pipeline ---\n",
    "\n",
    "print(\"Running unified indexing pipeline for web, local files, and CSV...\")\n",
    "# Note: The PDF path will be ignored if the file doesn't exist.\n",
    "file_paths_to_process = [text_file_path]\n",
    "\n",
    "if pdf_file_path.exists() and pdf_file_path.stat().st_size > 0:\n",
    "    file_paths_to_process.append(pdf_file_path)\n",
    "else:\n",
    "    print(f\"Skipping PDF file: {pdf_file_path}\")\n",
    "\n",
    "file_paths_to_process.append(csv_file_path)\n",
    "\n",
    "\n",
    "indexing_pipeline.run({\n",
    "    \"link_fetcher\": {\"urls\": [web_url]},\n",
    "    \"file_type_router\": {\"sources\": file_paths_to_process}\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efc7bc7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total documents in DocumentStore: 15\n",
      "Sample document from the store:\n",
      "[Document(id=1384ec36dd6d99f90ab589732d5219b7371dac846d0f0bd89c6385189c4079c0, content: 'Company,Model,Release Year,,Notes\n",
      "OpenAI,GPT-4,2023,,Generative Pre-trained Transformer 4\n",
      ",,,\n",
      "Google...', meta: {'file_path': 'llm_models.csv'}, embedding: vector of size 384), Document(id=be2fb4afe8f3e531ae2e97314778b92789c44d794c981d67bdea8658cf3fe51e, content: 'Haystack 2.0: The Composable Open-Source LLM Framework\n",
      "Meet Haystack 2.0, a more flexible, customiza...', meta: {'content_type': 'text/html', 'url': 'https://haystack.deepset.ai/blog/haystack-2-release', 'source_id': '0b188f4690ab3496d2270baf378be9fde19707e9b1ece003123c0442af918bb7', 'page_number': 1, 'split_id': 0, 'split_idx_start': 0, '_split_overlap': [{'doc_id': 'ff9dcb8208955755eb458bdf693e12ba8c2dd7f27de0bcb51e47e474255ad47b', 'range': (0, 124)}]}, embedding: vector of size 384), Document(id=ff9dcb8208955755eb458bdf693e12ba8c2dd7f27de0bcb51e47e474255ad47b, content: 'user before or not. You can get started by installing haystack-ai\n",
      ", our new package for Haystack 2.0...', meta: {'content_type': 'text/html', 'url': 'https://haystack.deepset.ai/blog/haystack-2-release', 'source_id': '0b188f4690ab3496d2270baf378be9fde19707e9b1ece003123c0442af918bb7', 'page_number': 1, 'split_id': 1, 'split_idx_start': 834, '_split_overlap': [{'doc_id': 'be2fb4afe8f3e531ae2e97314778b92789c44d794c981d67bdea8658cf3fe51e', 'range': (834, 958)}, {'doc_id': 'c59e96866da9b70a659ed9d87e43202d20e38c1d8022fae86f73c9f83268afea', 'range': (0, 144)}]}, embedding: vector of size 384), Document(id=c59e96866da9b70a659ed9d87e43202d20e38c1d8022fae86f73c9f83268afea, content: 'question-answering. During this time, we established the core of what makes Haystack Haystack: Compo...', meta: {'content_type': 'text/html', 'url': 'https://haystack.deepset.ai/blog/haystack-2-release', 'source_id': '0b188f4690ab3496d2270baf378be9fde19707e9b1ece003123c0442af918bb7', 'page_number': 1, 'split_id': 2, 'split_idx_start': 1651, '_split_overlap': [{'doc_id': 'ff9dcb8208955755eb458bdf693e12ba8c2dd7f27de0bcb51e47e474255ad47b', 'range': (817, 961)}, {'doc_id': '026a1a2d6fd15933c794c8352dfb015d939dc76fd2b8a6b1f68b166c2bd5e4e5', 'range': (0, 138)}]}, embedding: vector of size 384), Document(id=026a1a2d6fd15933c794c8352dfb015d939dc76fd2b8a6b1f68b166c2bd5e4e5, content: 'is a complete rewrite, the underlying principle of composing components into flexible pipelines rema...', meta: {'content_type': 'text/html', 'url': 'https://haystack.deepset.ai/blog/haystack-2-release', 'source_id': '0b188f4690ab3496d2270baf378be9fde19707e9b1ece003123c0442af918bb7', 'page_number': 1, 'split_id': 3, 'split_idx_start': 2517, '_split_overlap': [{'doc_id': 'c59e96866da9b70a659ed9d87e43202d20e38c1d8022fae86f73c9f83268afea', 'range': (866, 1004)}, {'doc_id': '828f39441b56d74639d7cf80eedae532cb31da7543ccff20c838d8e440edd6a3', 'range': (0, 120)}]}, embedding: vector of size 384), Document(id=828f39441b56d74639d7cf80eedae532cb31da7543ccff20c838d8e440edd6a3, content: 'not allowed, and the pipeline graph has to be acyclic. This makes it difficult to implement, for exa...', meta: {'content_type': 'text/html', 'url': 'https://haystack.deepset.ai/blog/haystack-2-release', 'source_id': '0b188f4690ab3496d2270baf378be9fde19707e9b1ece003123c0442af918bb7', 'page_number': 1, 'split_id': 4, 'split_idx_start': 3405, '_split_overlap': [{'doc_id': '026a1a2d6fd15933c794c8352dfb015d939dc76fd2b8a6b1f68b166c2bd5e4e5', 'range': (888, 1008)}, {'doc_id': '234aad190cf53f7615c79bca110f626647098e63c6cbaf46997a1f4278a895c2', 'range': (0, 117)}]}, embedding: vector of size 384), Document(id=234aad190cf53f7615c79bca110f626647098e63c6cbaf46997a1f4278a895c2, content: 'it transparent as to how these components can “talk” to each other.\n",
      "- Be flexible: Make it possible ...', meta: {'content_type': 'text/html', 'url': 'https://haystack.deepset.ai/blog/haystack-2-release', 'source_id': '0b188f4690ab3496d2270baf378be9fde19707e9b1ece003123c0442af918bb7', 'page_number': 1, 'split_id': 5, 'split_idx_start': 4245, '_split_overlap': [{'doc_id': '828f39441b56d74639d7cf80eedae532cb31da7543ccff20c838d8e440edd6a3', 'range': (840, 957)}, {'doc_id': 'bff241b723ea7ce505553329657bdf3f45ea0738c106869593433e9146f855f0', 'range': (0, 120)}]}, embedding: vector of size 384), Document(id=bff241b723ea7ce505553329657bdf3f45ea0738c106869593433e9146f855f0, content: 'creating a new component, to decide what inputs and outputs it should have is part of the ideation p...', meta: {'content_type': 'text/html', 'url': 'https://haystack.deepset.ai/blog/haystack-2-release', 'source_id': '0b188f4690ab3496d2270baf378be9fde19707e9b1ece003123c0442af918bb7', 'page_number': 1, 'split_id': 6, 'split_idx_start': 5109, '_split_overlap': [{'doc_id': '234aad190cf53f7615c79bca110f626647098e63c6cbaf46997a1f4278a895c2', 'range': (864, 984)}, {'doc_id': '590f38cb1024dafb259a01aa51ae51282c13a842bf2d145de16cb8d4155e25fc', 'range': (0, 120)}]}, embedding: vector of size 384), Document(id=590f38cb1024dafb259a01aa51ae51282c13a842bf2d145de16cb8d4155e25fc, content: 'few months (with contributions from Assembly AI, Jina AI, mixedbread ai and more). We will continue ...', meta: {'content_type': 'text/html', 'url': 'https://haystack.deepset.ai/blog/haystack-2-release', 'source_id': '0b188f4690ab3496d2270baf378be9fde19707e9b1ece003123c0442af918bb7', 'page_number': 1, 'split_id': 7, 'split_idx_start': 5969, '_split_overlap': [{'doc_id': 'bff241b723ea7ce505553329657bdf3f45ea0738c106869593433e9146f855f0', 'range': (860, 980)}, {'doc_id': 'fd0c89246c79336218a1fa79ad8fba3b16b358f0b1cdd1a2762790985bc4946f', 'range': (0, 121)}]}, embedding: vector of size 384), Document(id=fd0c89246c79336218a1fa79ad8fba3b16b358f0b1cdd1a2762790985bc4946f, content: 'has to come from somewhere, and Haystack 2.0 provides a common interface to access it in a consisten...', meta: {'content_type': 'text/html', 'url': 'https://haystack.deepset.ai/blog/haystack-2-release', 'source_id': '0b188f4690ab3496d2270baf378be9fde19707e9b1ece003123c0442af918bb7', 'page_number': 1, 'split_id': 8, 'split_idx_start': 6801, '_split_overlap': [{'doc_id': '590f38cb1024dafb259a01aa51ae51282c13a842bf2d145de16cb8d4155e25fc', 'range': (832, 953)}, {'doc_id': 'c6e861b73ec4721c095bb5605151489ddc4f718eb27ae2b7f62ce4a6738ab811', 'range': (0, 125)}]}, embedding: vector of size 384), Document(id=c6e861b73ec4721c095bb5605151489ddc4f718eb27ae2b7f62ce4a6738ab811, content: 'storage service.\n",
      "A clear path to production\n",
      "The experience we got over the last couple of years, wor...', meta: {'content_type': 'text/html', 'url': 'https://haystack.deepset.ai/blog/haystack-2-release', 'source_id': '0b188f4690ab3496d2270baf378be9fde19707e9b1ece003123c0442af918bb7', 'page_number': 1, 'split_id': 9, 'split_idx_start': 7631, '_split_overlap': [{'doc_id': 'fd0c89246c79336218a1fa79ad8fba3b16b358f0b1cdd1a2762790985bc4946f', 'range': (830, 955)}, {'doc_id': 'a64d4c9cf677d5e88db2266ef191b327df244cb801258d8d0ee234d8c58f8ab2', 'range': (0, 149)}]}, embedding: vector of size 384), Document(id=a64d4c9cf677d5e88db2266ef191b327df244cb801258d8d0ee234d8c58f8ab2, content: 'pipelines behind a RESTful API: Hayhooks.\n",
      "Hayhooks is a client-server application that allows you to...', meta: {'content_type': 'text/html', 'url': 'https://haystack.deepset.ai/blog/haystack-2-release', 'source_id': '0b188f4690ab3496d2270baf378be9fde19707e9b1ece003123c0442af918bb7', 'page_number': 1, 'split_id': 10, 'split_idx_start': 8549, '_split_overlap': [{'doc_id': 'c6e861b73ec4721c095bb5605151489ddc4f718eb27ae2b7f62ce4a6738ab811', 'range': (918, 1067)}, {'doc_id': '7f6f7d895d1c405a537f7b39d5eeda04f053fd9ebb147b45fee110e81422d63f', 'range': (0, 129)}]}, embedding: vector of size 384), Document(id=7f6f7d895d1c405a537f7b39d5eeda04f053fd9ebb147b45fee110e81422d63f, content: 'being good examples of how we’ve been leveraging Haystack 2.0. How?:\n",
      "- It’s easier to extend the cap...', meta: {'content_type': 'text/html', 'url': 'https://haystack.deepset.ai/blog/haystack-2-release', 'source_id': '0b188f4690ab3496d2270baf378be9fde19707e9b1ece003123c0442af918bb7', 'page_number': 1, 'split_id': 11, 'split_idx_start': 9489, '_split_overlap': [{'doc_id': 'a64d4c9cf677d5e88db2266ef191b327df244cb801258d8d0ee234d8c58f8ab2', 'range': (940, 1069)}, {'doc_id': 'cb016e7e3e342381e63aa790089eaf797bfda2564f19308da2d2408e003cf59b', 'range': (0, 157)}]}, embedding: vector of size 384), Document(id=cb016e7e3e342381e63aa790089eaf797bfda2564f19308da2d2408e003cf59b, content: 'resources and more to help you get started:\n",
      "- Documentation: full technical documentation on all Hay...', meta: {'content_type': 'text/html', 'url': 'https://haystack.deepset.ai/blog/haystack-2-release', 'source_id': '0b188f4690ab3496d2270baf378be9fde19707e9b1ece003123c0442af918bb7', 'page_number': 1, 'split_id': 12, 'split_idx_start': 10387, '_split_overlap': [{'doc_id': '7f6f7d895d1c405a537f7b39d5eeda04f053fd9ebb147b45fee110e81422d63f', 'range': (898, 1055)}]}, embedding: vector of size 384), Document(id=abe67d1945474acc782933988f972e8fb8eead145c1e9a124bc45529c0913020, content: 'Haystack is an open-source framework by deepset for building production-ready LLM applications. It e...', meta: {'file_path': 'haystack_intro.txt', 'source_id': '160d6f24e29805bf0759170ff471675c29a1236687381c1140733597dbaaa14d', 'page_number': 1, 'split_id': 0, 'split_idx_start': 0, '_split_overlap': []}, embedding: vector of size 384)]\n"
     ]
    }
   ],
   "source": [
    "# --- 7. Verify the DocumentStore ---\n",
    "doc_count = document_store.count_documents()\n",
    "print(f\"\\nTotal documents in DocumentStore: {doc_count}\")\n",
    "print(\"Sample document from the store:\")\n",
    "print(document_store.filter_documents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6305e3c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-with-haystack-ch4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
