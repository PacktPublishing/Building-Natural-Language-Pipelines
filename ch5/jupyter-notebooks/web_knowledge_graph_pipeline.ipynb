{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7fd32a5",
   "metadata": {},
   "source": [
    "üîß **Setup Required**: Before running this notebook, please follow the [setup instructions](../README.md#setup-instructions) to configure your environment and API keys."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f335dcf",
   "metadata": {},
   "source": [
    "# Web Content Knowledge Graph and Synthetic Data Generation Pipeline\n",
    "\n",
    "This notebook demonstrates how to build a comprehensive pipeline for web content processing that:\n",
    "1. **Retrieves content** from web URLs using Haystack's LinkContentFetcher\n",
    "2. **Converts HTML** to structured documents using HTMLToDocument\n",
    "3. **Preprocesses the text** with cleaning and splitting components\n",
    "4. **Creates a knowledge graph** from the processed web content\n",
    "5. **Generates synthetic test data** using the knowledge graph\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "- How to build end-to-end Haystack pipelines for web content processing\n",
    "- The differences between PDF and web content processing\n",
    "- Best practices for web scraping and content extraction\n",
    "- How web content characteristics affect synthetic test generation\n",
    "\n",
    "## Key Components for Web Processing\n",
    "- **LinkContentFetcher**: Retrieves content directly from URLs\n",
    "- **HTMLToDocument**: Converts HTML content to Haystack Documents\n",
    "- **DocumentCleaner**: Removes extra whitespaces and HTML artifacts\n",
    "- **DocumentSplitter**: Breaks web content into manageable chunks\n",
    "- **KnowledgeGraphGenerator**: Creates structured knowledge representations\n",
    "- **SyntheticTestGenerator**: Produces question-answer pairs for evaluation\n",
    "\n",
    "## Real-World Applications\n",
    "This approach is particularly useful for:\n",
    "- **Documentation Analysis**: Processing online documentation and creating test datasets\n",
    "- **Content Monitoring**: Regularly generating tests from updated web content  \n",
    "- **Multi-Source Knowledge**: Combining web content with other document types\n",
    "- **Research Applications**: Creating datasets from academic papers, blog posts, etc.\n",
    "\n",
    "## Technical Considerations\n",
    "- **Rate Limiting**: Be mindful of website rate limits when fetching content\n",
    "- **Content Quality**: Web content may require more aggressive cleaning\n",
    "- **Dynamic Content**: Some websites use JavaScript; static HTML fetching may miss content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04c41c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurafunderburk/Documents/GitHub/Building-Natural-Language-Pipelines/ch5/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Web Content Processing Pipeline created successfully!\n",
      "üåê Ready to process web content and generate knowledge graphs + synthetic tests\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from haystack import Pipeline\n",
    "from haystack.components.fetchers import LinkContentFetcher\n",
    "from haystack.components.converters import HTMLToDocument\n",
    "from haystack.components.preprocessors import (\n",
    "    DocumentCleaner,\n",
    "    DocumentSplitter)\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.components.embedders.openai_text_embedder import OpenAITextEmbedder\n",
    "from haystack.utils import Secret\n",
    "from pathlib import Path\n",
    "from scripts.knowledge_graph_component import KnowledgeGraphGenerator\n",
    "from scripts.langchaindocument_component import DocumentToLangChainConverter\n",
    "from scripts.synthetic_test_components import SyntheticTestGenerator, TestDatasetSaver\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(\"./.env\")\n",
    "\n",
    "# Helper function to create fresh generator and embedder instances\n",
    "def create_llm_components():\n",
    "    \"\"\"Create fresh instances of generator and embedder.\"\"\"\n",
    "    # You can use OpenAI models:\n",
    "    generator = OpenAIGenerator(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        api_key=Secret.from_token(os.getenv(\"OPENAI_API_KEY\"))\n",
    "    )\n",
    "    embedder = OpenAITextEmbedder(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        api_key=Secret.from_token(os.getenv(\"OPENAI_API_KEY\"))\n",
    "    )\n",
    "    \n",
    "    # Or use Ollama models (uncomment to use):\n",
    "    # from haystack_integrations.components.generators.ollama import OllamaGenerator\n",
    "    # from haystack_integrations.components.embedders.ollama import OllamaTextEmbedder\n",
    "    # \n",
    "    # generator = OllamaGenerator(\n",
    "    #     model=\"mistral-nemo:12b\",\n",
    "    #     generation_kwargs={\n",
    "    #         \"num_predict\": 100,\n",
    "    #         \"temperature\": 0.9,\n",
    "    #     }\n",
    "    # )\n",
    "    # embedder = OllamaTextEmbedder(model=\"nomic-embed-text\")\n",
    "    \n",
    "    return generator, embedder\n",
    "\n",
    "# Create web content processing components\n",
    "fetcher = LinkContentFetcher()\n",
    "converter = HTMLToDocument()\n",
    "doc_cleaner = DocumentCleaner(\n",
    "    remove_empty_lines=True,\n",
    "    remove_extra_whitespaces=True,\n",
    "    remove_substrings=['<1-hop>\\n\\n', '<multi-hop>\\n\\n', '<single-hop>\\n\\n', '\\n\\n\\n', '\\f', '\\r']  # Remove synthetic data generation artifacts and weird characters\n",
    ")\n",
    "doc_splitter = DocumentSplitter(split_by=\"sentence\",\n",
    "                                split_length=5,  # Reduced from 50 to create more chunks\n",
    "                                split_overlap=1)\n",
    "doc_converter = DocumentToLangChainConverter()\n",
    "\n",
    "# Create knowledge graph component with its own generator and embedder instances\n",
    "kg_gen, kg_embed = create_llm_components()\n",
    "kg_generator = KnowledgeGraphGenerator(\n",
    "    generator=kg_gen,\n",
    "    embedder=kg_embed,\n",
    "    apply_transforms=True\n",
    ")\n",
    "\n",
    "# Create test generator component with its own generator and embedder instances\n",
    "test_gen, test_embed = create_llm_components()\n",
    "test_generator = SyntheticTestGenerator(\n",
    "    generator=test_gen,\n",
    "    embedder=test_embed,\n",
    "    test_size=10,\n",
    "    query_distribution=[\n",
    "        (\"single_hop\", 0.3),\n",
    "        (\"multi_hop_specific\", 0.3),\n",
    "        (\"multi_hop_abstract\", 0.4)\n",
    "    ]\n",
    ")\n",
    "test_saver = TestDatasetSaver(\"data_for_eval/synthetic_tests_10_from_web.csv\")\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline()\n",
    "pipeline.add_component(\"fetcher\", fetcher)\n",
    "pipeline.add_component(\"converter\", converter)\n",
    "pipeline.add_component(\"doc_cleaner\", doc_cleaner)\n",
    "pipeline.add_component(\"doc_splitter\", doc_splitter)\n",
    "pipeline.add_component(\"doc_converter\", doc_converter)\n",
    "pipeline.add_component(\"kg_generator\", kg_generator)\n",
    "pipeline.add_component(\"test_generator\", test_generator)\n",
    "pipeline.add_component(\"test_saver\", test_saver)\n",
    "\n",
    "# Connect components in sequence\n",
    "pipeline.connect(\"fetcher.streams\", \"converter.sources\")\n",
    "pipeline.connect(\"converter.documents\", \"doc_cleaner.documents\")\n",
    "pipeline.connect(\"doc_cleaner.documents\", \"doc_splitter.documents\")\n",
    "pipeline.connect(\"doc_splitter.documents\", \"doc_converter.documents\")\n",
    "pipeline.connect(\"doc_converter.langchain_documents\", \"kg_generator.documents\")\n",
    "pipeline.connect(\"kg_generator.knowledge_graph\", \"test_generator.knowledge_graph\")\n",
    "pipeline.connect(\"doc_converter.langchain_documents\", \"test_generator.documents\")\n",
    "pipeline.connect(\"test_generator.testset\", \"test_saver.testset\")\n",
    "\n",
    "print(\"‚úÖ Web Content Processing Pipeline created successfully!\")\n",
    "print(\"üåê Ready to process web content and generate knowledge graphs + synthetic tests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af764802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê Processing web content from 3 URLs\n",
      "This may take a moment to fetch and process the content...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying HeadlinesExtractor: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.47it/s]\n",
      "Applying HeadlineSplitter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:00<00:00, 4153.80it/s]\n",
      "Applying HeadlinesExtractor: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.47it/s]\n",
      "Applying HeadlineSplitter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:00<00:00, 4153.80it/s]\n",
      "Applying SummaryExtractor:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:03<00:03,  1.53s/it]Property 'summary' already exists in node 'cefa86'. Skipping!\n",
      "Applying SummaryExtractor:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:03<00:00,  1.06it/s]Property 'summary' already exists in node 'cefa86'. Skipping!\n",
      "Applying SummaryExtractor: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:04<00:00,  1.08s/it]\n",
      "Applying SummaryExtractor: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:04<00:00,  1.08s/it]\n",
      "Applying CustomNodeFilter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.67it/s]\n",
      "Applying CustomNodeFilter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.67it/s]\n",
      "Applying EmbeddingExtractor:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:00,  4.21it/s]Property 'summary_embedding' already exists in node 'cefa86'. Skipping!\n",
      "Applying EmbeddingExtractor: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00,  9.04it/s]\n",
      "Applying ThemesExtractor:   0%|          | 0/4 [00:00<?, ?it/s]Property 'summary_embedding' already exists in node 'cefa86'. Skipping!\n",
      "Applying EmbeddingExtractor: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00,  9.04it/s]\n",
      "Applying ThemesExtractor: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.52it/s]\n",
      "Applying ThemesExtractor: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.52it/s]\n",
      "Applying NERExtractor: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.96it/s]\n",
      "Applying CosineSimilarityBuilder: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 1011.16it/s]\n",
      "Applying OverlapScoreBuilder: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 483.83it/s]\n",
      "Applying NERExtractor: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.96it/s]\n",
      "Applying CosineSimilarityBuilder: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 1011.16it/s]\n",
      "Applying OverlapScoreBuilder: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 483.83it/s]\n",
      "Generating personas: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  1.57it/s]\n",
      "Generating personas: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  1.57it/s]\n",
      "Generating Scenarios: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:07<00:00,  2.54s/it]\n",
      "Generating Scenarios: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:07<00:00,  2.54s/it]\n",
      "Generating Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:06<00:00,  1.74it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Pipeline Results:\n",
      "  üìÑ Documents Processed: 12\n",
      "  üß† Knowledge Graph Nodes: 12\n",
      "  üß™ Test Cases Generated: 11\n",
      "  üîß Generation Method: knowledge_graph\n"
     ]
    }
   ],
   "source": [
    "# Sample web URLs to process - using multiple Haystack docs pages for more content\n",
    "# Note: Some websites (like Wikipedia) block automated requests, so we use documentation sites\n",
    "web_urls = [\n",
    "    \"https://docs.haystack.deepset.ai/docs/intro\",\n",
    "    \"https://docs.haystack.deepset.ai/docs/creating-pipelines\", \n",
    "    \"https://docs.haystack.deepset.ai/docs/components\"\n",
    "]\n",
    "\n",
    "print(f\"üåê Processing web content from {len(web_urls)} URLs\")\n",
    "print(\"This may take a moment to fetch and process the content...\")\n",
    "\n",
    "try:\n",
    "    result = pipeline.run({\n",
    "        \"fetcher\": {\"urls\": web_urls}\n",
    "    })\n",
    "\n",
    "    print(\"\\nüìä Pipeline Results:\")\n",
    "    print(f\"  üìÑ Documents Processed: {result['doc_converter']['document_count']}\")\n",
    "    print(f\"  üß† Knowledge Graph Nodes: {result['kg_generator']['node_count']}\")\n",
    "    print(f\"  üß™ Test Cases Generated: {result['test_generator']['testset_size']}\")\n",
    "    print(f\"  üîß Generation Method: {result['test_generator']['generation_method']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error processing web content: {str(e)}\")\n",
    "    print(\"This might be due to network issues or website access restrictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1975367e",
   "metadata": {},
   "source": [
    "### üîß Troubleshooting Note\n",
    "\n",
    "**If you encountered \"No clusters found in the knowledge graph\" error above:**\n",
    "\n",
    "This happened because the original configuration had:\n",
    "- `split_length=50` sentences (too large for most web pages)\n",
    "- This resulted in only 1 document chunk ‚Üí 1 knowledge graph node\n",
    "- Cannot create clusters with just 1 node!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed64d39",
   "metadata": {},
   "source": [
    "### Understanding the Web Content Processing Pipeline Architecture\n",
    "\n",
    "The web processing pipeline follows a similar structure to PDF processing but with adapted input components:\n",
    "\n",
    "```\n",
    "Web URL ‚Üí Link Fetcher ‚Üí HTML Converter ‚Üí Document Cleaner ‚Üí Document Splitter\n",
    "    ‚Üì\n",
    "Document Converter ‚Üí Knowledge Graph Generator  \n",
    "    ‚Üì                         ‚Üì\n",
    "Test Generator ‚Üê ‚Üê ‚Üê ‚Üê ‚Üê ‚Üê ‚Üê ‚Üê\n",
    "    ‚Üì\n",
    "Test Dataset Saver\n",
    "```\n",
    "\n",
    "**Why This Works:**\n",
    "- The knowledge graph generation is **content-agnostic** - it works the same whether input comes from PDFs, web pages, or other sources\n",
    "- Document preprocessing steps ensure consistent quality regardless of input format\n",
    "- The same test generation logic produces comparable quality across all sources\n",
    "\n",
    "**Pipeline Reusability:**\n",
    "Notice how we can reuse the same components (`doc_cleaner`, `doc_splitter`, `kg_generator`, etc.) with different input sources. This demonstrates the modularity and flexibility of Haystack's component architecture.\n",
    "\n",
    "**Web-Specific Considerations:**\n",
    "- **Content Structure**: Web pages may have navigation, ads, and other non-content elements\n",
    "- **HTML Artifacts**: May require more aggressive cleaning than PDF content\n",
    "- **Dynamic Loading**: Static HTML fetching may miss JavaScript-rendered content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87b75dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì∏ Pipeline diagram saved to: ./images/web_knowledge_graph_pipeline.png\n"
     ]
    }
   ],
   "source": [
    "pipeline.draw(path=\"./images/web_knowledge_graph_pipeline.png\")\n",
    "print(\"üì∏ Pipeline diagram saved to: ./images/web_knowledge_graph_pipeline.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa2db0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Synthetic Tests Sample:\n",
      "First 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wut is a Document Store in data pipelines?</td>\n",
       "      <td>['You can check them on the documentation page...</td>\n",
       "      <td>A Document Store is a component used in data p...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How is the llm component integrated into the d...</td>\n",
       "      <td>['3. Create the pipeline\\nquery_pipeline = Pip...</td>\n",
       "      <td>The llm component is integrated into the data ...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How do I use HTMLToDocument in my pipeline?</td>\n",
       "      <td>['Pipeline.run()\\ncan be called in two ways, e...</td>\n",
       "      <td>To use HTMLToDocument in your pipeline, you fi...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are the steps to run a pipeline using Doc...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nPipeline.run()\\ncan be called in ...</td>\n",
       "      <td>To run a pipeline using DocumentWriter, you fi...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What steps should be followed to create a pipe...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nYou can check them on the documen...</td>\n",
       "      <td>To create a pipeline that utilizes documents, ...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0         Wut is a Document Store in data pipelines?   \n",
       "1  How is the llm component integrated into the d...   \n",
       "2        How do I use HTMLToDocument in my pipeline?   \n",
       "3  What are the steps to run a pipeline using Doc...   \n",
       "4  What steps should be followed to create a pipe...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  ['You can check them on the documentation page...   \n",
       "1  ['3. Create the pipeline\\nquery_pipeline = Pip...   \n",
       "2  ['Pipeline.run()\\ncan be called in two ways, e...   \n",
       "3  ['<1-hop>\\n\\nPipeline.run()\\ncan be called in ...   \n",
       "4  ['<1-hop>\\n\\nYou can check them on the documen...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  A Document Store is a component used in data p...   \n",
       "1  The llm component is integrated into the data ...   \n",
       "2  To use HTMLToDocument in your pipeline, you fi...   \n",
       "3  To run a pipeline using DocumentWriter, you fi...   \n",
       "4  To create a pipeline that utilizes documents, ...   \n",
       "\n",
       "                        synthesizer_name  \n",
       "0  single_hop_specific_query_synthesizer  \n",
       "1  single_hop_specific_query_synthesizer  \n",
       "2  single_hop_specific_query_synthesizer  \n",
       "3   multi_hop_specific_query_synthesizer  \n",
       "4   multi_hop_specific_query_synthesizer  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How do you connect components in a query pipel...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\n3. Create the pipeline\\nquery_pip...</td>\n",
       "      <td>To connect components in a query pipeline that...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How do you create a pipeline and what are the ...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nPipeline.run()\\ncan be called in ...</td>\n",
       "      <td>To create a pipeline, you first need to import...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How does the InMemoryDocumentStore relate to t...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nPipeline.run()\\ncan be called in ...</td>\n",
       "      <td>The InMemoryDocumentStore is a component used ...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How do you create a pipeline and what componen...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nPipeline.run()\\ncan be called in ...</td>\n",
       "      <td>To create a pipeline, you first need to import...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What steps are involved in validating the comp...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nPipeline.run()\\ncan be called in ...</td>\n",
       "      <td>To validate the components of a pipeline that ...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           user_input  \\\n",
       "6   How do you connect components in a query pipel...   \n",
       "7   How do you create a pipeline and what are the ...   \n",
       "8   How does the InMemoryDocumentStore relate to t...   \n",
       "9   How do you create a pipeline and what componen...   \n",
       "10  What steps are involved in validating the comp...   \n",
       "\n",
       "                                   reference_contexts  \\\n",
       "6   ['<1-hop>\\n\\n3. Create the pipeline\\nquery_pip...   \n",
       "7   ['<1-hop>\\n\\nPipeline.run()\\ncan be called in ...   \n",
       "8   ['<1-hop>\\n\\nPipeline.run()\\ncan be called in ...   \n",
       "9   ['<1-hop>\\n\\nPipeline.run()\\ncan be called in ...   \n",
       "10  ['<1-hop>\\n\\nPipeline.run()\\ncan be called in ...   \n",
       "\n",
       "                                            reference  \\\n",
       "6   To connect components in a query pipeline that...   \n",
       "7   To create a pipeline, you first need to import...   \n",
       "8   The InMemoryDocumentStore is a component used ...   \n",
       "9   To create a pipeline, you first need to import...   \n",
       "10  To validate the components of a pipeline that ...   \n",
       "\n",
       "                        synthesizer_name  \n",
       "6   multi_hop_specific_query_synthesizer  \n",
       "7   multi_hop_abstract_query_synthesizer  \n",
       "8   multi_hop_abstract_query_synthesizer  \n",
       "9   multi_hop_abstract_query_synthesizer  \n",
       "10  multi_hop_abstract_query_synthesizer  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load and display the generated synthetic tests\n",
    "test_file_path = \"data_for_eval/synthetic_tests_10_from_web.csv\"\n",
    "\n",
    "if os.path.exists(test_file_path):\n",
    "    synthetic_tests_df = pd.read_csv(test_file_path)\n",
    "    print(\"\\nüß™ Synthetic Tests Sample:\")\n",
    "    print(\"First 5 rows:\")\n",
    "    display(synthetic_tests_df.head())\n",
    "    print(\"Last 5 rows:\")\n",
    "    display(synthetic_tests_df.tail())\n",
    "else:\n",
    "    print(\"‚ùå Synthetic test file not found\")\n",
    "    print(\"Please run the previous cells to generate the test data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5e81d0",
   "metadata": {},
   "source": [
    "### Analyzing Web Content vs PDF Results\n",
    "\n",
    "Let's examine how the synthetic test generation performs when using web content versus PDF content.\n",
    "\n",
    "**Expected Differences:**\n",
    "- **Content Structure**: Web content may have different formatting and structure\n",
    "- **Question Complexity**: Depending on the source material's complexity\n",
    "- **Context Quality**: Web content might include navigation elements or ads that need filtering\n",
    "\n",
    "**Web Content Specific Benefits:**\n",
    "1. **Real-time Content**: Access to the most current information available online\n",
    "2. **Rich Media Context**: Web pages often have supplementary context that enhances understanding\n",
    "3. **Diverse Sources**: Easy to process content from multiple websites\n",
    "4. **Hyperlinked Knowledge**: Web content often contains references that enrich the knowledge graph\n",
    "\n",
    "**Potential Challenges:**\n",
    "1. **Content Quality Variability**: Web content quality can vary significantly\n",
    "2. **Noise Filtering**: Need to filter out navigation, ads, and irrelevant content\n",
    "3. **Rate Limiting**: Must respect website rate limits and robots.txt\n",
    "4. **Dynamic Content**: Some content may require JavaScript rendering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e345a4",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We've Accomplished\n",
    "\n",
    "In this notebook, we successfully:\n",
    "\n",
    "1. **Built a Web Content Processing Pipeline**: Created an end-to-end pipeline specifically optimized for web content\n",
    "2. **Demonstrated Source Flexibility**: Processed content from multiple different websites\n",
    "3. **Generated Knowledge Graphs from Web Content**: Converted unstructured web content into structured knowledge representations\n",
    "4. **Produced Comparative Synthetic Test Data**: Created question-answer pairs from different web sources\n",
    "5. **Analyzed Web-Specific Characteristics**: Examined how web content affects synthetic test generation\n",
    "\n",
    "### Key Advantages of Web Content Processing\n",
    "\n",
    "- **Real-Time Content**: Access to the most current information available\n",
    "- **Diverse Sources**: Easy to process content from multiple websites in sequence\n",
    "- **Rich Context**: Web content often includes hyperlinks and references that enhance knowledge graphs\n",
    "- **Scalable Collection**: Can systematically process large numbers of web resources\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f74d2ae",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-with-haystack-ch5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
