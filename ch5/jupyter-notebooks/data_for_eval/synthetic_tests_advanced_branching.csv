user_input,reference_contexts,reference,synthesizer_name
Wut is the role of Apple in AI technolgy?,"[""What is AI, how does it work and why are some people concerned about it?\nArtificial intelligence (AI) has increasingly become part of everyday life over the past decade.\nIt is being used to personalise social media feeds, spot friends and family in smartphone photos and pave the way for medical breakthroughs.\nBut the rise of chatbots like OpenAI's ChatGPT and Meta AI has been accompanied by concern about the technology's environmental impact, ethical implications and data use.\n What is AI and what is it used for?\nAI allows computers to process large amounts of data, identify patterns and follow detailed instructions about what to do with that information.\nComputers cannot think, empathise or reason.\nHowever, scientists have developed systems that can perform tasks which usually require human intelligence, trying to replicate how people acquire and use knowledge.\nThis could be trying to anticipate what product an online shopper might buy, based on previous purchases, in order to recommend items.\nThe technology is also behind voice-controlled virtual assistants like Apple's Siri and Amazon's Alexa, and is being used to develop systems for self-driving cars.\nAI also helps social platforms like Facebook, TikTok and X decide what posts to show users. Streaming services Spotify and Deezer use AI to suggest music.\nThere are also a number of applications in medicine, as scientists use AI to help spot cancers, review X-ray results, speed up diagnoses and identify new treatments.\n What is generative AI, and how do apps like ChatGPT and Meta AI work?\nGenerative AI is used to create new content which can seem like it has been made by a human.\nIt does this by learning from vast quantities of existing data such as online text and images.\nChatGPT and Chinese rival DeepSeek's chatbot are popular generative AI tools that can be used to produce text, images, code and more material.\nGoogle's Gemini or Meta AI can similarly hold text conversations with users.\nApps like Midjourney or Veo 3 are dedicated to creating images or video from simple text prompts.\nGenerative AI can also be used to make high-quality music.\nSongs mimicking the style or sound of famous musicians have gone viral, sometimes leaving fans confused about their authenticity.\n""]","Apple is involved in AI technology through its voice-controlled virtual assistant, Siri, which utilizes AI to enhance user interaction and provide personalized experiences.",single_hop_specific_query_synthesizer
What happened with UnitedHealthcare CEO?,"['Why is AI controversial?\nWhile acknowledging AI\'s potential, some experts are worried about the implications of its rapid growth.\nThe International Monetary Fund (IMF) has warned AI could affect nearly 40% of jobs, and worsen global financial inequality.\nProf Geoffrey Hinton, a computer scientist regarded as one of the ""godfathers"" of AI development, has expressed concern that powerful AI systems could even make humans extinct - although his fear was dismissed by his fellow ""AI godfather"", Yann LeCun.\nCritics also highlight the tech\'s potential to reproduce biased information, or discriminate against some social groups.\nThis is because much of the data used to train AI comes from public material, including social media posts or comments, which can reflect existing societal biases such as sexism or racism.\nAnd while AI programmes are growing more adept, they are still prone to errors - such as creating images of people with the wrong number of fingers or limbs.\nGenerative AI systems are known for their ability to ""hallucinate"" and assert falsehoods as fact, even sometimes inventing sources for the inaccurate information.\nApple halted a new AI feature in January after it incorrectly summarised news app notifications.\nThe BBC complained about the feature after Apple\'s AI falsely told readers that Luigi Mangione - the man accused of killing UnitedHealthcare CEO Brian Thompson - had shot himself.\nGoogle has also faced criticism over inaccurate answers produced by its AI search overviews.\nThis has added to concerns about the use of AI in schools and workplaces, where it is increasingly used to help summarise texts, write emails or essays and solve bugs in code.\nThere are worries about students using AI technology to ""cheat"" on assignments, or employees ""smuggling"" it into work.\nWriters, musicians and artists have also pushed back against the technology on ethical grounds, accusing AI developers of using their work to train systems without consent or compensation.\nThousands of creators - including Abba singer-songwriter Björn Ulvaeus, writers Ian Rankin and Joanne Harris and actress Julianne Moore - signed a statement in October 2024 calling AI a ""major, unjust threat"" to their livelihoods.\nHow does AI effect the environment?\nIt is not clear how much energy AI systems use, but some researchers estimate the industry as a whole could soon consume as much as the Netherlands.\nCreating the powerful computer chips needed to run AI programmes requires lots of power and water.\nDemand for generative AI services has also meant an increase in the number of data centres which power them.\nThese huge halls - housing thousands of racks of computer servers - use substantial amounts of energy and require large volumes of water to keep them cool.\nSome large tech companies have invested in ways to reduce or reuse the water needed, or have opted for alternative methods such as air-cooling.\nHowever, some experts and activists fear that AI will worsen water supply problems.\nThe BBC was told in February that government plans to make the UK a ""world leader"" in AI could put already stretched supplies of drinking water under strain.\nIn September 2024, Google said it would reconsider proposals for a data centre in Chile, which has struggled with drought.\n']",The BBC complained about Apple's AI falsely telling readers that Luigi Mangione - the man accused of killing UnitedHealthcare CEO Brian Thompson - had shot himself.,single_hop_specific_query_synthesizer
What laws is US having about AI and how it compare with other countries like EU and UK?,"['Are there laws governing AI?\nSome governments have already introduced rules governing how AI operates.\nThe EU\'s Artificial Intelligence Act places controls on high risk systems used in areas such as education, healthcare, law enforcement or elections. It bans some AI use altogether.\nGenerative AI developers in China are required to safeguard citizens\' data, and promote transparency and accuracy of information. But they are also bound by the country\'s strict censorship laws.\nIn the UK, Prime Minister Sir Keir Starmer has said the government ""will test and understand AI before we regulate it"".\nBoth the UK and US have AI Safety Institutes that aim to identify risks and evaluate advanced AI models.\nIn 2024 the two countries signed an agreement to collaborate on developing ""robust"" AI testing methods.\nHowever, in February 2025, neither country signed an international AI declaration which pledged an open, inclusive and sustainable approach to the technology.\nSeveral countries including the UK are also clamping down on use of AI systems to create deepfake nude imagery and child sexual abuse material.\nSign up for our Tech Decoded newsletter to follow the world\'s top tech stories and trends. Outside the UK? Sign up here.']","In the US, there are AI Safety Institutes that aim to identify risks and evaluate advanced AI models, and in 2024, the US signed an agreement with the UK to collaborate on developing robust AI testing methods. However, in February 2025, the US did not sign an international AI declaration that pledged an open, inclusive, and sustainable approach to the technology. Compared to the EU, which has the Artificial Intelligence Act placing controls on high-risk systems and banning some AI uses, and the UK, where the government is testing AI before regulation, the US appears to be more cautious and less committed to immediate regulatory frameworks.",single_hop_specific_query_synthesizer
"How does the gpt-5 classifier improve user satisfaction compared to previous models, and what role does feedback play in this assessment?","['<1-hop>\n\nPrivacy via Automated Classifiers.No one looked at the content of messages while conducting\nanalysis for this paper. All analysis of message content was performed via automated LLM-based\nclassifiers run on de-identified and PII-scrubbed message data (see Figure 1). The messages are first\nscrubbed of PII using an internal LLM-based tool,17 and then classified according to classifiers defined\nover a controlled label space—the most precise classifier we use on the message-level data set is the\nO*NET Intermediate Work Activities taxonomy, which we augment to end up with 333 categories.\nWe introduce technical and procedural frictions that prevent accidental access to the underlying text\n(for example, interfaces that do not render message text to researchers).\nOur classifications aim to discern the intent of a given message, and thus we include the prior 10\nmessages in a conversation as context. 18 For an example, see Table 2.\n Stand-Alone Message Message with Prior Context\n[user]: “10 more” [user]: “give me 3 cultural activities to do with teens”\n[assistant]: “1. Visit a museum . . . ” (truncated)\n[user]: “10 more”\nTable 2:Illustration of Context-Augmented Message Classifications (Synthetic Example). The left column\nshows a standalone message to be classified, and the right column shows the prior context included in the\nclassification of the message on the left.\nWe truncate each message to a maximum of 5,000 characters, because long context windows could\ninduce variability in the quality of the classification (Liu et al., 2023). We classify each message\nwith the “gpt-5-mini” model, with the exception ofInteraction Quality,which uses “gpt-5,” using the\nprompts listed in Appendix A.\n17Internal analyses show that the tool,Privacy Filter, has substantial alignment with human judgment.\n', '<2-hop>\n\nWe retain this classifier because theseκ\nstatistics primarily highlight the inherent difficulty of inferring the user’s latent satisfaction from text\nalone.\nWhile this latent “prior” is unobserved in our validation data, it is partially observable when users\nprovide explicit thumbs-up/down feedback. To assess whether the classifier captures a signal aligned\n56\x0cFigure 30:Agreement Between Model and Plurality for Interaction Quality\nFigure 31:Bias Between Model and Plurality for Interaction Quality\n57\x0cwith user experience, we link model predictions to voluntary feedback on assistant messages. We\ndraw a 1-in-10,000 sample of conversations from June 2024 to June 2025 and retain cases where (i)\nthe assistant message received explicit feedback and (ii) the user sent a subsequent message that our\nclassifier can score, yielding roughly 60,000 eligible items. This is a restricted sample that may not\nbe fully representative of all interactions, but it offers a unique lens on the classifier’s ability to proxy\nuser satisfaction.\nFigure 32 shows thatUnknownclassifications are split roughly evenly between thumbs-down and\nthumbs-up feedback. Thumbs-up comprises 86% of all feedback. Conversations with thumbs-down\nfeedback are about equally likely to be classified asGoodorBad, whereas thumbs-up feedback is 9.5\ntimes more likely to be followed by a message classified asGood.\nFigure 32:Correlation of User Rating and Interaction Quality Annotation\n58\x0cC  Appendix: ChatGPT Timeline\ndate event\n2022-11-30 Public launch of ChatGPT as a “research preview” (using GPT-3.5)\n2023-02-01 Launch of ChatGPT Plus subscription\n2023-03-14 Launch of GPT-4 in ChatGPT Plus\n2024-04-01 Launch of logged-out ChatGPT\n2024-05-13 Launch of GPT-4o in ChatGPT Free and Plus\n2024-09-12 Launch of o1-preview and o1-mini in ChatGPT Plus\n2024-12-01 Launch of o1-pro in ChatGPT\n2024-12-05 Launch of ChatGPT Pro subscription\n2025-01-03 Launch of o3-mini in ChatGPT\n2025-03-25 Launch of GPT-4o image generation\n2025-04-16 Launch of o3 and o4-mini\n2025-06-10 Launch of o3-pro\n2025-08-07 Launch of GPT-5 in ChatGPT\n59\x0cD ']","The gpt-5 classifier improves user satisfaction by utilizing a model that captures user intent more effectively, as indicated by the substantial alignment with human judgment. Feedback plays a crucial role in this assessment; explicit thumbs-up/down feedback from users helps to validate the classifier's predictions. A sample of conversations shows that 86% of feedback was thumbs-up, indicating a positive reception of the assistant's messages. Furthermore, conversations with thumbs-down feedback were equally likely to be classified as Good or Bad, while thumbs-up feedback was significantly more likely to be followed by a message classified as Good, demonstrating the classifier's ability to proxy user satisfaction.",multi_hop_specific_query_synthesizer
"What are the key aspects of the EU's Artificial Intelligence Act and how does it compare to the AI governance approaches in the UK and US, particularly regarding high-risk systems and ethical considerations?","['<1-hop>\n\nAre there laws governing AI?\nSome governments have already introduced rules governing how AI operates.\nThe EU\'s Artificial Intelligence Act places controls on high risk systems used in areas such as education, healthcare, law enforcement or elections. It bans some AI use altogether.\nGenerative AI developers in China are required to safeguard citizens\' data, and promote transparency and accuracy of information. But they are also bound by the country\'s strict censorship laws.\nIn the UK, Prime Minister Sir Keir Starmer has said the government ""will test and understand AI before we regulate it"".\nBoth the UK and US have AI Safety Institutes that aim to identify risks and evaluate advanced AI models.\nIn 2024 the two countries signed an agreement to collaborate on developing ""robust"" AI testing methods.\nHowever, in February 2025, neither country signed an international AI declaration which pledged an open, inclusive and sustainable approach to the technology.\nSeveral countries including the UK are also clamping down on use of AI systems to create deepfake nude imagery and child sexual abuse material.\nSign up for our Tech Decoded newsletter to follow the world\'s top tech stories and trends. Outside the UK? Sign up here.']","The EU's Artificial Intelligence Act places strict controls on high-risk AI systems used in critical areas such as education, healthcare, law enforcement, and elections, even banning certain uses of AI altogether. In contrast, the UK government, led by Prime Minister Sir Keir Starmer, has indicated a more cautious approach, stating that they will test and understand AI before implementing regulations. Both the UK and US have established AI Safety Institutes to identify risks and evaluate advanced AI models, and in 2024, they signed an agreement to collaborate on developing robust AI testing methods. However, in February 2025, neither country signed an international AI declaration that aimed for an open, inclusive, and sustainable approach to AI technology. Additionally, several countries, including the UK, are taking measures to restrict the use of AI systems for creating harmful content, such as deepfake nude imagery and child sexual abuse material.",multi_hop_specific_query_synthesizer
How Google help researchers with data access and what is the impact on AI innovation?,"['<1-hop>\n\nImproving data access\nThe United States should develop a data strategy that promotes innovation and consumer protection. Right now, there are no uniform standards in terms of data access, data sharing, or data protection. Almost all the data are proprietary in nature and not shared very broadly with the research community, and this limits innovation and system design. AI requires data to test and improve its learning capacity.50 Without structured and unstructured data sets, it will be nearly impossible to gain the full benefits of artificial intelligence.\nIn general, the research community needs better access to government and business data, although with appropriate safeguards to make sure researchers do not misuse data in the way Cambridge Analytica did with Facebook information. There is a variety of ways researchers could gain data access. One is through voluntary agreements with companies holding proprietary data. Facebook, for example, recently announced a partnership with Stanford economist Raj Chetty to use its social media data to explore inequality.51 As part of the arrangement, researchers were required to undergo background checks and could only access data from secured sites in order to protect user privacy and security.\nIn the U.S., there are no uniform standards in terms of data access, data sharing, or data protection. Almost all the data are proprietary in nature and not shared very broadly with the research community, and this limits innovation and system design.\nGoogle long has made available search results in aggregated form for researchers and the general public. Through its “Trends” site, scholars can analyze topics such as interest in Trump, views about democracy, and perspectives on the overall economy.52 That helps people track movements in public interest and identify topics that galvanize the general public.\nTwitter makes much of its tweets available to researchers through application programming interfaces, commonly referred to as APIs. These tools help people outside the company build application software and make use of data from its social media platform. They can study patterns of social media communications and see how people are commenting on or reacting to current events.\nIn some sectors where there is a discernible public benefit, governments can facilitate collaboration by building infrastructure that shares data. For example, the National Cancer Institute has pioneered a data-sharing protocol where certified researchers can query health data it has using de-identified information drawn from clinical data, claims information, and drug therapies. That enables researchers to evaluate efficacy and effectiveness, and make recommendations regarding the best medical approaches, without compromising the privacy of individual patients.\nThere could be public-private data partnerships that combine government and business data sets to improve system performance. For example, cities could integrate information from ride-sharing services with its own material on social service locations, bus lines, mass transit, and highway congestion to improve transportation. That would help metropolitan areas deal with traffic tie-ups and assist in highway and mass transit planning.\nSome combination of these approaches would improve data access for researchers, the government, and the business community, without impinging on personal privacy. As noted by Ian Buck, the vice president of NVIDIA, “Data is the fuel that drives the AI engine. The federal government has access to vast sources of information. Opening access to that data will help us get insights that will transform the U.S. economy.”53 Through its Data.gov portal, the federal government already has put over 230,000 data sets into the public domain, and this has propelled innovation and aided improvements in AI and data analytic technologies.54 The private sector also needs to facilitate research data access so that society can achieve the full benefits of artificial intelligence.\n']","Google long has made available search results in aggregated form for researchers and the general public. Through its 'Trends' site, scholars can analyze topics such as interest in Trump, views about democracy, and perspectives on the overall economy. This helps people track movements in public interest and identify topics that galvanize the general public. Improved data access from companies like Google is crucial for AI innovation, as AI requires data to test and improve its learning capacity. Without structured and unstructured data sets, it will be nearly impossible to gain the full benefits of artificial intelligence.",multi_hop_specific_query_synthesizer
"What privacy protections are implemented in the construction and analysis of datasets used for understanding ChatGPT user interactions, and how do these protections ensure user privacy?","['<1-hop>\n\nWe describe the contents of each dataset, the sampling procedures that produced them, and the\nprivacy protections we implemented in constructing and employing them in analysis.\n 3.1 Growth Dataset\nWe compiled a dataset covering all usage on consumer ChatGPT Plans (Free, Plus, Pro) since Chat-\nGPT’s launch in November 2022. We exclude users on non-consumer plans (Business f.k.a. Teams,\n14The exact beginning and end dates of this sample are May 15, 2024 and June 26, 2025.\n15The exact beginning and end dates of this sample are May 15, 2024 and July 31, 2025.\n5\x0cEnterprise, Education).\nFor each user and day, this dataset reports the total number of messages sent by the user on that\nday. It also reports, for each message, de-identified user metadata, including the timestamp of their\nfirst interaction with ChatGPT, the country from which their account is registered, their subscription\nplan on each day, and their self-reported age (reported in coarse 5–7-year buckets to protect user\nprivacy).\n', '<2-hop>\n\n3.2 Classified Messages\nTo understand usage while preserving user privacy, we construct message-level datasets without any\nhuman ever reading the contents of a message. See Figure 1 for an overview of the privacy-preserving\nclassification pipeline. Messages are categorized according to 5 different LLM-based classifiers. The\nclassifiers are introduced in more detail in Section 5, their exact text is reproduced in Appendix A,\nand our validation procedure is described in Appendix B.\nSampled From All ChatGPT Users.We uniformly sampled approximately 1.1 million conver-\nsations, and then sampled one message within each conversation, with the following restrictions:\n1. We only include messages from May 2024 to July 2025.\n2. We exclude conversations from users who opted out of sharing their messages for model training.\n3. We exclude users who self-report their age as under 18.\n4. We exclude conversations that users have deleted and from users whose accounts have been\ndeactivated or banned.\n5. We exclude logged-out users, 16 which represented a minority share of ChatGPT users over the\nsample period.\nOur sample is drawn from a table that is itself sampled, where the sampling rate varied over time.\nWe thus adjust our sampling weights to maintain a fixed ratio with aggregate messages sent.\nSampled From a Subset of ChatGPT Users.We construct two samples of classified messages\nfrom a subset of ChatGPT users (approximately 130,000 users). This sample of users does not include\nany users who opted out of sharing their messages for training, nor does it include users whose self-\nreported age is below 18, nor does it include users who have been banned or deleted their accounts.\nThe first sample contains classifications of 1.58 million messages from this subset of users, sampled\nat the conversation level (a conversation is a series of messages between the user and chatbot). This\nsample is constructed such that the user’s representation in the data is proportional to overall message\nvolume. The second sample contains messages sent from this subset of users, sampled at the user level\nwith up to six messages from each user in the group.\n16ChatGPT became available to logged-out users in April 2024, i.e., users could use ChatGPT without signing up\nfor an account with an email address. However, messages from logged-out users are only available in our dataset from\nMarch 2025, thus for consistency we drop all messages from logged-out users.\n6\x0cFigure 1:Illustration of Privacy-Preserving Automated Classification Pipeline (Synthetic Example). Mes-\nsages are first stripped of PII via an internal LLM-based tool calledPrivacy Filter. Then they are classified by\nLLM-based automated classifiers, described in detail in Appendices A and B. Humans do not see raw messages\nor PII-scrubbed messages, only the final classifications of messages.\n']","The privacy protections implemented in the construction and analysis of datasets for understanding ChatGPT user interactions include de-identifying user metadata, such as the timestamp of their first interaction, the country of account registration, subscription plans, and self-reported age in coarse 5–7-year buckets. Additionally, message-level datasets are constructed without any human ever reading the contents of a message, ensuring that user privacy is preserved. This is achieved through a privacy-preserving classification pipeline where messages are stripped of personally identifiable information (PII) using an internal tool called Privacy Filter, and then classified by automated classifiers. Only the final classifications are visible to humans, further safeguarding user privacy.",multi_hop_abstract_query_synthesizer
"How does the establishment of a federal AI advisory committee relate to the need for AI transparency and accountability in local government actions, such as those taken by the New York City Council?","['<1-hop>\n\nAI will reconfigure how society and the economy operate, and there needs to be “big picture” thinking on what this will mean for ethics, governance, and societal impact. People will need the ability to think broadly about many questions and integrate knowledge from a number of different areas.\nOne example of new ways to prepare students for a digital future is IBM’s Teacher Advisor program, utilizing Watson’s free online tools to help teachers bring the latest knowledge into the classroom. They enable instructors to develop new lesson plans in STEM and non-STEM fields, find relevant instructional videos, and help students get the most out of the classroom.58 As such, they are precursors of new educational environments that need to be created.\n Create a federal AI advisory committee\nFederal officials need to think about how they deal with artificial intelligence. As noted previously, there are many issues ranging from the need for improved data access to addressing issues of bias and discrimination. It is vital that these and other concerns be considered so we gain the full benefits of this emerging technology.\nIn order to move forward in this area, several members of Congress have introduced the “Future of Artificial Intelligence Act,” a bill designed to establish broad policy and legal principles for AI. It proposes the secretary of commerce create a federal advisory committee on the development and implementation of artificial intelligence. The legislation provides a mechanism for the federal government to get advice on ways to promote a “climate of investment and innovation to ensure the global competitiveness of the United States,” “optimize the development of artificial intelligence to address the potential growth, restructuring, or other changes in the United States workforce,” “support the unbiased development and application of artificial intelligence,” and “protect the privacy rights of individuals.”59\nAmong the specific questions the committee is asked to address include the following: competitiveness, workforce impact, education, ethics training, data sharing, international cooperation, accountability, machine learning bias, rural impact, government efficiency, investment climate, job impact, bias, and consumer impact. The committee is directed to submit a report to Congress and the administration 540 days after enactment regarding any legislative or administrative action needed on AI.\nThis legislation is a step in the right direction, although the field is moving so rapidly that we would recommend shortening the reporting timeline from 540 days to 180 days. Waiting nearly two years for a committee report will certainly result in missed opportunities and a lack of action on important issues. Given rapid advances in the field, having a much quicker turnaround time on the committee analysis would be quite beneficial.\n', '<2-hop>\n\nEngage with state and local officials\nStates and localities also are taking action on AI. For example, the New York City Council unanimously passed a bill that directed the mayor to form a taskforce that would “monitor the fairness and validity of algorithms used by municipal agencies.”60 The city employs algorithms to “determine if a lower bail will be assigned to an indigent defendant, where firehouses are established, student placement for public schools, assessing teacher performance, identifying Medicaid fraud and determine where crime will happen next.”61\nAccording to the legislation’s developers, city officials want to know how these algorithms work and make sure there is sufficient AI transparency and accountability. In addition, there is concern regarding the fairness and biases of AI algorithms, so the taskforce has been directed to analyze these issues and make recommendations regarding future usage. It is scheduled to report back to the mayor on a range of AI policy, legal, and regulatory issues by late 2019.\nSome observers already are worrying that the taskforce won’t go far enough in holding algorithms accountable. For example, Julia Powles of Cornell Tech and New York University argues that the bill originally required companies to make the AI source code available to the public for inspection, and that there be simulations of its decisionmaking using actual data. After criticism of those provisions, however, former Councilman James Vacca dropped the requirements in favor of a task force studying these issues. He and other city officials were concerned that publication of proprietary information on algorithms would slow innovation and make it difficult to find AI vendors who would work with the city.62 It remains to be seen how this local task force will balance issues of innovation, privacy, and transparency.\n Regulate broad objectives more than specific algorithms\nThe European Union has taken a restrictive stance on these issues of data collection and analysis.63 It has rules limiting the ability of companies from collecting data on road conditions and mapping street views. Because many of these countries worry that people’s personal information in unencrypted Wi-Fi networks are swept up in overall data collection, the EU has fined technology firms, demanded copies of data, and placed limits on the material collected.64 This has made it more difficult for technology companies operating there to develop the high-definition maps required for autonomous vehicles.\nThe GDPR being implemented in Europe place severe restrictions on the use of artificial intelligence and machine learning. According to published guidelines, “Regulations prohibit any automated decision that ‘significantly affects’ EU citizens. This includes techniques that evaluates a person’s ‘performance at work, economic situation, health, personal preferences, interests, reliability, behavior, location, or movements.’”65 In addition, these new rules give citizens the right to review how digital services made specific algorithmic choices affecting people.\nBy taking a restrictive stance on issues of data collection and analysis, the European Union is putting its manufacturers and software designers at a significant disadvantage to the rest of the world.\nIf interpreted stringently, these rules will make it difficult for European software designers (and American designers who work with European counterparts) to incorporate artificial intelligence and high-definition mapping in autonomous vehicles. Central to navigation in these cars and trucks is tracking location and movements. Without high-definition maps containing geo-coded data and the deep learning that makes use of this information, fully autonomous driving will stagnate in Europe. Through this and other data protection actions, the European Union is putting its manufacturers and software designers at a significant disadvantage to the rest of the world.\nIt makes more sense to think about the broad objectives desired in AI and enact policies that advance them, as opposed to governments trying to crack open the “black boxes” and see exactly how specific algorithms operate. Regulating individual algorithms will limit innovation and make it difficult for companies to make use of artificial intelligence.\n', '<3-hop>\n\nConclusion\nTo summarize, the world is on the cusp of revolutionizing many sectors through artificial intelligence and data analytics. There already are significant deployments in finance, national security, health care, criminal justice, transportation, and smart cities that have altered decisionmaking, business models, risk mitigation, and system performance. These developments are generating substantial economic and social benefits.\nThe world is on the cusp of revolutionizing many sectors through artificial intelligence, but the way AI systems are developed need to be better understood due to the major implications these technologies will have for society as a whole.\nYet the manner in which AI systems unfold has major implications for society as a whole. It matters how policy issues are addressed, ethical conflicts are reconciled, legal realities are resolved, and how much transparency is required in AI and data analytic solutions.74 Human choices about software development affect the way in which decisions are made and the manner in which they are integrated into organizational routines. Exactly how these processes are executed need to be better understood because they will have substantial impact on the general public soon, and for the foreseeable future. AI may well be a revolution in human affairs, and become the single most influential human innovation in history.\nNote: We appreciate the research assistance of Grace Gilberg, Jack Karsten, Hillary Schaub, and Kristjan Tomasson on this project.\nThe Brookings Institution is a nonprofit organization devoted to independent research and policy solutions. Its mission is to conduct high-quality, independent research and, based on that research, to provide innovative, practical recommendations for policymakers and the public. The conclusions and recommendations of any Brookings publication are solely those of its author(s), and do not reflect the views of the Institution, its management, or its other scholars.\nSupport for this publication was generously provided by Amazon. Brookings recognizes that the value it provides is in its absolute commitment to quality, independence, and impact. Activities supported by its donors reflect this commitment.\nJohn R. Allen is a member of the Board of Advisors of Amida Technology and on the Board of Directors of Spark Cognition. Both companies work in fields discussed in this piece.\n-\nFootnotes\n- Thomas Davenport, Jeff Loucks, and David Schatsky, “Bullish on the Business Value of Cognitive” (Deloitte, 2017), p. 3 (www2.deloitte.com/us/en/pages/deloitte-analytics/articles/cognitive-technology-adoption-survey.html).\n- Luke Dormehl, Thinking Machines: The Quest for Artificial Intelligence—and Where It’s Taking Us Next (New York: Penguin–TarcherPerigee, 2017).\n- Shubhendu and Vijay, “Applicability of Artificial Intelligence in Different Fields of Life.”\n- Ibid.\n- Andrew McAfee and Erik Brynjolfsson, Machine Platform Crowd: Harnessing Our Digital Future (New York: Norton, 2017).\n- Portions of this paper draw on Darrell M. West, The Future of Work: Robots, AI, and Automation, Brookings Institution Press, 2018.\n- PriceWaterhouseCoopers, “Sizing the Prize: What’s the Real Value of AI for Your Business and How Can You Capitalise?” 2017.\n- Dominic Barton, Jonathan Woetzel, Jeongmin Seong, and Qinzheng Tian, “Artificial Intelligence: Implications for China” (New York: McKinsey Global Institute, April 2017), p. 1.\n- Nathaniel Popper, “Stocks and Bots,” New York Times Magazine, February 28, 2016.\n- Ibid.\n- Ibid.\n- Michael Lewis, Flash Boys: A Wall Street Revolt (New York: Norton, 2015).\n- Cade Metz, “In Quantum Computing Race, Yale Professors Battle Tech Giants,” New York Times, November 14, 2017, p. B3.\n- Executive Office of the President, “Artificial Intelligence, Automation, and the Economy,” December 2016, pp. 27-28.\n- Christian Davenport, “Future Wars May Depend as Much on Algorithms as on Ammunition, Report Says,” Washington Post, December 3, 2017.\n- Ibid.\n- John R. Allen and Amir Husain, “On Hyperwar,” Naval Institute Proceedings, July 17, 2017, pp. 30-36.\n', '<4-hop>\n\nThis article was published in 2018. To read more recent content from Brookings on Artificial Intelligence, please visit the AI topic page.\nMost people are not very familiar with the concept of artificial intelligence (AI). As an illustration, when 1,500 senior business leaders in the United States in 2017 were asked about AI, only 17 percent said they were familiar with it.1 A number of them were not sure what it was or how it would affect their particular companies. They understood there was considerable potential for altering business processes, but were not clear how AI could be deployed within their own organizations.\nDespite its widespread lack of familiarity, AI is a technology that is transforming every walk of life. It is a wide-ranging tool that enables people to rethink how we integrate information, analyze data, and use the resulting insights to improve decisionmaking. Our hope through this comprehensive overview is to explain AI to an audience of policymakers, opinion leaders, and interested observers, and demonstrate how AI already is altering the world and raising important questions for society, the economy, and governance.\nIn this paper, we discuss novel applications in finance, national security, health care, criminal justice, transportation, and smart cities, and address issues such as data access problems, algorithmic bias, AI ethics and transparency, and legal liability for AI decisions. We contrast the regulatory approaches of the U.S. and European Union, and close by making a number of recommendations for getting the most out of AI while still protecting important human values.2\nIn order to maximize AI benefits, we recommend nine steps for going forward:\n- Encourage greater data access for researchers without compromising users’ personal privacy,\n- invest more government funding in unclassified AI research,\n- promote new models of digital education and AI workforce development so employees have the skills needed in the 21st-century economy,\n- create a federal AI advisory committee to make policy recommendations,\n- engage with state and local officials so they enact effective policies,\n- regulate broad AI principles rather than specific algorithms,\n- take bias complaints seriously so AI does not replicate historic injustice, unfairness, or discrimination in data or algorithms,\n- maintain mechanisms for human oversight and control, and\n- penalize malicious AI behavior and promote cybersecurity.\n Qualities of artificial intelligence\nAlthough there is no uniformly agreed upon definition, AI generally is thought to refer to “machines that respond to stimulation consistent with traditional responses from humans, given the human capacity for contemplation, judgment and intention.”3 According to researchers Shubhendu and Vijay, these software systems “make decisions which normally require [a] human level of expertise” and help people anticipate problems or deal with issues as they come up.4 As such, they operate in an intentional, intelligent, and adaptive manner.\n', '<5-hop>\n\nIntelligence, please visit the AI topic page.\nMost people are not very familiar with the concept of artificial intelligence (AI). As an illustration, when 1,500 senior business leaders in the United States in 2017 were asked about AI, only 17 percent said they were familiar with it.1 A number of them were not sure what it was or how it would affect their particular companies. They understood there was considerable potential for altering business processes, but were not clear how AI could be deployed within their own organizations.\nDespite its widespread lack of familiarity, AI is a technology that is transforming every walk of life. It is a wide-ranging tool that enables people to rethink how we integrate information, analyze data, and use the resulting insights to improve decisionmaking. Our hope through this comprehensive overview is to explain AI to an audience of policymakers, opinion leaders, and interested observers, and demonstrate how AI already is altering the world and raising important questions for society, the economy, and governance.\nIn this paper, we discuss novel applications in finance, national security, health care, criminal justice, transportation, and smart cities, and address issues such as data access problems, algorithmic bias, AI ethics and transparency, and legal liability for AI decisions. We contrast the regulatory approaches of the U.S. and European Union, and close by making a number of recommendations for getting the most out of AI while still protecting important human values.2\nIn order to maximize AI benefits, we recommend nine steps for going forward:\n- Encourage greater data access for researchers without compromising users’ personal privacy,\n- invest more government funding in unclassified AI research,\n- promote new models of digital education and AI workforce development so employees have the skills needed in the 21st-century economy,\n- create a federal AI advisory committee to make policy recommendations,\n- engage with state and local officials so they enact effective policies,\n- regulate broad AI principles rather than specific algorithms,\n- take bias complaints seriously so AI does not replicate historic injustice, unfairness, or discrimination in data or algorithms,\n- maintain mechanisms for human oversight and control, and\n- penalize malicious AI behavior and promote cybersecurity.\nQualities of artificial intelligence\nAlthough there is no uniformly agreed upon definition, AI generally is thought to refer to “machines that respond to stimulation consistent with traditional responses from humans, given the human capacity for contemplation, judgment and intention.”3 According to researchers Shubhendu and Vijay, these software systems “make decisions which normally require [a] human level of expertise” and help people anticipate problems or deal with issues as they come up.4 As such, they operate in an intentional, intelligent, and adaptive manner.\nIntentionality\nArtificial intelligence algorithms are designed to make decisions, often using real-time data. They are unlike passive machines that are capable only of mechanical or predetermined responses. Using sensors, digital data, or remote inputs, they combine information from a variety of different sources, analyze the material instantly, and act on the insights derived from those data. With massive improvements in storage systems, processing speeds, and analytic techniques, they are capable of tremendous sophistication in analysis and decisionmaking.\nArtificial intelligence is already altering the world and raising important questions for society, the economy, and governance.\nIntelligence\nAI generally is undertaken in conjunction with machine learning and data analytics.5 Machine learning takes data and looks for underlying trends. If it spots something that is relevant for a practical problem, software designers can take that knowledge and use it to analyze specific issues. All that is required are data that are sufficiently robust that algorithms can discern useful patterns. Data can come in the form of digital information, satellite imagery, visual information, text, or unstructured data.\n']","The establishment of a federal AI advisory committee is crucial for addressing the broader implications of artificial intelligence, including the need for AI transparency and accountability. This committee is tasked with examining various issues related to AI, such as bias, discrimination, and the ethical development of AI technologies. Similarly, local government actions, like those taken by the New York City Council, which formed a taskforce to monitor the fairness and validity of algorithms used by municipal agencies, reflect a growing recognition of the importance of transparency in AI systems. Both initiatives aim to ensure that AI technologies are developed and implemented in a manner that is fair and accountable, thereby promoting public trust and safeguarding individual rights.",multi_hop_abstract_query_synthesizer
How does the issue of discrimination claims relate to the implications of artificial intelligence in modern society?,"['<1-hop>\n\n27-28.\n- Christian Davenport, “ Future Wars May Depend as Much on Algorithms as on Ammunition, Report Says,” Washington Post, December 3, 2017.\n- Ibid.\n- John R. Allen and Amir Husain, “On Hyperwar,” Naval Institute Proceedings, July 17, 2017, pp. 30-36.\n- Paul Mozur, “ China Sets Goal to Lead in Artificial Intelligence,” New York Times, July 21, 2017, p. B1.\n- Paul Mozur and John Markoff, “Is China Outsmarting American Artificial Intelligence?” New York Times, May 28, 2017.\n- Economist, “ America v China: The Battle for Digital Supremacy,” March 15, 2018.\n- Rasmus Rothe, “Applying Deep Learning to Real-World Problems,” Medium, May 23, 2017.\n- Eric Horvitz, “Reflections on the Status and Future of Artificial Intelligence,” Testimony before the U.S. Senate Subcommittee on Space, Science, and Competitiveness, November 30, 2016, p. 5.\n- Jeff Asher and Rob Arthur, “Inside the Algorithm That Tries to Predict Gun Violence in Chicago,” New York Times Upshot, June 13, 2017.\n- Caleb Watney, “It’s Time for our Justice System to Embrace Artificial Intelligence,” TechTank (blog), Brookings Institution, July 20, 2017.\n- Asher and Arthur, “Inside the Algorithm That Tries to Predict Gun Violence in Chicago.”\n- Paul Mozur and Keith Bradsher, “China’s A.I. Advances Help Its Tech Industry, and State Security,” New York Times, December 3, 2017.\n- Simon Denyer, “China’s Watchful Eye,” Washington Post, January 7, 2018.\n- Cameron Kerry and Jack Karsten, “Gauging Investment in Self-Driving Cars,” Brookings Institution, October 16, 2017.\n- Portions of this section are drawn from Darrell M. West, “Driverless Cars in China, Europe, Japan, Korea, and the United States,” Brookings Institution, September 2016.\n- Ibid.\n- Yuming Ge, Xiaoman Liu, Libo Tang, and Darrell M. West, “ Smart Transportation in China and the United States,” Center for Technology Innovation, Brookings Institution, December 2017.\n- Peter Holley, “Uber Signs Deal to Buy 24,000 Autonomous Vehicles from Volvo,” Washington Post, November 20, 2017.\n- Daisuke Wakabayashi, “Self-Driving Uber Car Kills Pedestrian in Arizona, Where Robots Roam,” New York Times, March 19, 2018.\n- Kevin Desouza, Rashmi Krishnamurthy, and Gregory Dawson, “Learning from Public Sector Experimentation with Artificial Intelligence,” TechTank (blog), Brookings Institution, June 23, 2017.\n- Boyd Cohen, “The 10 Smartest Cities in North America,” Fast Company, November 14, 2013.\n- Teena Maddox, “66% of US Cities Are Investing in Smart City Technology,” TechRepublic, November 6, 2017.\n- Osonde Osoba and William Welser IV, “The Risks of Artificial Intelligence to Security and the Future of Work” (Santa Monica, Calif.: RAND Corp., December 2017) (www.rand.org/pubs/perspectives/PE237.html).\n- Ibid., p. 7.\n- Dominic Barton, Jonathan Woetzel, Jeongmin Seong, and Qinzheng Tian, “', '<2-hop>\n\nArtificial Intelligence: Implications for China” (New York: McKinsey Global Institute, April 2017), p. 7.\n- Executive Office of the President, “Preparing for the Future of Artificial Intelligence,” October 2016, pp. 30-31.\n- Elaine Glusac, “As Airbnb Grows, So Do Claims of Discrimination,” New York Times, June 21, 2016.\n- “Joy Buolamwini,” Bloomberg Businessweek, July 3, 2017, p. 80.\n- Ibid.\n- Mark Purdy and Paul Daugherty, “Why Artificial Intelligence is the Future of Growth,” Accenture, 2016.\n- Jon Valant, “Integrating Charter Schools and Choice-Based Education Systems,” Brown Center Chalkboard blog, Brookings Institution, June 23, 2017.\n- Tucker, “‘A White Mask Worked Better.’”\n- Cliff Kuang, “Can A.I. Be Taught to Explain Itself?” New York Times Magazine, November 21, 2017.\n- Yale Law School Information Society Project, “Governing Machine Learning,” September 2017.\n- Katie Benner, “Airbnb Vows to Fight Racism, But Its Users Can’t Sue to Prompt Fairness,” New York Times, June 19, 2016.\n- Executive Office of the President, “Artificial Intelligence, Automation, and the Economy” and “Preparing for the Future of Artificial Intelligence.”\n- Nancy Scolar, “Facebook’s Next Project: American Inequality,” Politico, February 19, 2018.\n- Darrell M. West, “What Internet Search Data Reveals about Donald Trump’s First Year in Office,” Brookings Institution policy report, January 17, 2018.\n- Ian Buck, “Testimony before the House Committee on Oversight and Government Reform Subcommittee on Information Technology,” February 14, 2018.\n- Keith Nakasone, “Testimony before the House Committee on Oversight and Government Reform Subcommittee on Information Technology,” March 7, 2018.\n- Greg Brockman, “The Dawn of Artificial Intelligence,” Testimony before U.S. Senate Subcommittee on Space, Science, and Competitiveness, November 30, 2016.\n- Amir Khosrowshahi, “Testimony before the House Committee on Oversight and Government Reform Subcommittee on Information Technology,” February 14, 2018.\n- James Kurose, “Testimony before the House Committee on Oversight and Government Reform Subcommittee on Information Technology,” March 7, 2018.\n- Stephen Noonoo, “Teachers Can Now Use IBM’s Watson to Search for Free Lesson Plans,” EdSurge, September 13, 2017.\n- Congress.gov, “H.R. 4625 FUTURE of Artificial Intelligence Act of 2017,” December 12, 2017.\n- Elizabeth Zima, “Could New York City’s AI Transparency Bill Be a Model for the Country?” Government Technology, January 4, 2018.\n', '<3-hop>\n\nTransportation\nTransportation represents an area where AI and machine learning are producing major innovations. Research by Cameron Kerry and Jack Karsten of the Brookings Institution has found that over $80 billion was invested in autonomous vehicle technology between August 2014 and June 2017. Those investments include applications both for autonomous driving and the core technologies vital to that sector.28\nAutonomous vehicles—cars, trucks, buses, and drone delivery systems—use advanced technological capabilities. Those features include automated vehicle guidance and braking, lane-changing systems, the use of cameras and sensors for collision avoidance, the use of AI to analyze information in real time, and the use of high-performance computing and deep learning systems to adapt to new circumstances through detailed maps.29\nLight detection and ranging systems (LIDARs) and AI are key to navigation and collision avoidance. LIDAR systems combine light and radar instruments. They are mounted on the top of vehicles that use imaging in a 360-degree environment from a radar and light beams to measure the speed and distance of surrounding objects. Along with sensors placed on the front, sides, and back of the vehicle, these instruments provide information that keeps fast-moving cars and trucks in their own lane, helps them avoid other vehicles, applies brakes and steering when needed, and does so instantly so as to avoid accidents.\nAdvanced software enables cars to learn from the experiences of other vehicles on the road and adjust their guidance systems as weather, driving, or road conditions change. This means that software is the key—not the physical car or truck itself.\nSince these cameras and sensors compile a huge amount of information and need to process it instantly to avoid the car in the next lane, autonomous vehicles require high-performance computing, advanced algorithms, and deep learning systems to adapt to new scenarios. This means that software is the key, not the physical car or truck itself.30 Advanced software enables cars to learn from the experiences of other vehicles on the road and adjust their guidance systems as weather, driving, or road conditions change.31\nRide-sharing companies are very interested in autonomous vehicles. They see advantages in terms of customer service and labor productivity. All of the major ride-sharing companies are exploring driverless cars. The surge of car-sharing and taxi services—such as Uber and Lyft in the United States, Daimler’s Mytaxi and Hailo service in Great Britain, and Didi Chuxing in China—demonstrate the opportunities of this transportation option. Uber recently signed an agreement to purchase 24,000 autonomous cars from Volvo for its ride-sharing service.32\nHowever, the ride-sharing firm suffered a setback in March 2018 when one of its autonomous vehicles in Arizona hit and killed a pedestrian. Uber and several auto manufacturers immediately suspended testing and launched investigations into what went wrong and how the fatality could have occurred.33 Both industry and consumers want reassurance that the technology is safe and able to deliver on its stated promises. Unless there are persuasive answers, this accident could slow AI advancements in the transportation sector.\n', '<4-hop>\n\nThis article was published in 2018. To read more recent content from Brookings on Artificial Intelligence, please visit the AI topic page.\nMost people are not very familiar with the concept of artificial intelligence (AI). As an illustration, when 1,500 senior business leaders in the United States in 2017 were asked about AI, only 17 percent said they were familiar with it.1 A number of them were not sure what it was or how it would affect their particular companies. They understood there was considerable potential for altering business processes, but were not clear how AI could be deployed within their own organizations.\nDespite its widespread lack of familiarity, AI is a technology that is transforming every walk of life. It is a wide-ranging tool that enables people to rethink how we integrate information, analyze data, and use the resulting insights to improve decisionmaking. Our hope through this comprehensive overview is to explain AI to an audience of policymakers, opinion leaders, and interested observers, and demonstrate how AI already is altering the world and raising important questions for society, the economy, and governance.\nIn this paper, we discuss novel applications in finance, national security, health care, criminal justice, transportation, and smart cities, and address issues such as data access problems, algorithmic bias, AI ethics and transparency, and legal liability for AI decisions. We contrast the regulatory approaches of the U.S. and European Union, and close by making a number of recommendations for getting the most out of AI while still protecting important human values.2\nIn order to maximize AI benefits, we recommend nine steps for going forward:\n- Encourage greater data access for researchers without compromising users’ personal privacy,\n- invest more government funding in unclassified AI research,\n- promote new models of digital education and AI workforce development so employees have the skills needed in the 21st-century economy,\n- create a federal AI advisory committee to make policy recommendations,\n- engage with state and local officials so they enact effective policies,\n- regulate broad AI principles rather than specific algorithms,\n- take bias complaints seriously so AI does not replicate historic injustice, unfairness, or discrimination in data or algorithms,\n- maintain mechanisms for human oversight and control, and\n- penalize malicious AI behavior and promote cybersecurity.\n Qualities of artificial intelligence\nAlthough there is no uniformly agreed upon definition, AI generally is thought to refer to “machines that respond to stimulation consistent with traditional responses from humans, given the human capacity for contemplation, judgment and intention.”3 According to researchers Shubhendu and Vijay, these software systems “make decisions which normally require [a] human level of expertise” and help people anticipate problems or deal with issues as they come up.4 As such, they operate in an intentional, intelligent, and adaptive manner.\n', '<5-hop>\n\nIntelligence, please visit the AI topic page.\nMost people are not very familiar with the concept of artificial intelligence (AI). As an illustration, when 1,500 senior business leaders in the United States in 2017 were asked about AI, only 17 percent said they were familiar with it.1 A number of them were not sure what it was or how it would affect their particular companies. They understood there was considerable potential for altering business processes, but were not clear how AI could be deployed within their own organizations.\nDespite its widespread lack of familiarity, AI is a technology that is transforming every walk of life. It is a wide-ranging tool that enables people to rethink how we integrate information, analyze data, and use the resulting insights to improve decisionmaking. Our hope through this comprehensive overview is to explain AI to an audience of policymakers, opinion leaders, and interested observers, and demonstrate how AI already is altering the world and raising important questions for society, the economy, and governance.\nIn this paper, we discuss novel applications in finance, national security, health care, criminal justice, transportation, and smart cities, and address issues such as data access problems, algorithmic bias, AI ethics and transparency, and legal liability for AI decisions. We contrast the regulatory approaches of the U.S. and European Union, and close by making a number of recommendations for getting the most out of AI while still protecting important human values.2\nIn order to maximize AI benefits, we recommend nine steps for going forward:\n- Encourage greater data access for researchers without compromising users’ personal privacy,\n- invest more government funding in unclassified AI research,\n- promote new models of digital education and AI workforce development so employees have the skills needed in the 21st-century economy,\n- create a federal AI advisory committee to make policy recommendations,\n- engage with state and local officials so they enact effective policies,\n- regulate broad AI principles rather than specific algorithms,\n- take bias complaints seriously so AI does not replicate historic injustice, unfairness, or discrimination in data or algorithms,\n- maintain mechanisms for human oversight and control, and\n- penalize malicious AI behavior and promote cybersecurity.\nQualities of artificial intelligence\nAlthough there is no uniformly agreed upon definition, AI generally is thought to refer to “machines that respond to stimulation consistent with traditional responses from humans, given the human capacity for contemplation, judgment and intention.”3 According to researchers Shubhendu and Vijay, these software systems “make decisions which normally require [a] human level of expertise” and help people anticipate problems or deal with issues as they come up.4 As such, they operate in an intentional, intelligent, and adaptive manner.\nIntentionality\nArtificial intelligence algorithms are designed to make decisions, often using real-time data. They are unlike passive machines that are capable only of mechanical or predetermined responses. Using sensors, digital data, or remote inputs, they combine information from a variety of different sources, analyze the material instantly, and act on the insights derived from those data. With massive improvements in storage systems, processing speeds, and analytic techniques, they are capable of tremendous sophistication in analysis and decisionmaking.\nArtificial intelligence is already altering the world and raising important questions for society, the economy, and governance.\nIntelligence\nAI generally is undertaken in conjunction with machine learning and data analytics.5 Machine learning takes data and looks for underlying trends. If it spots something that is relevant for a practical problem, software designers can take that knowledge and use it to analyze specific issues. All that is required are data that are sufficiently robust that algorithms can discern useful patterns. Data can come in the form of digital information, satellite imagery, visual information, text, or unstructured data.\n']","The issue of discrimination claims is closely related to the implications of artificial intelligence in modern society, as AI systems can inadvertently replicate historic injustices and unfairness if not properly managed. This concern is highlighted in discussions about algorithmic bias, where AI may reflect biases present in the data it is trained on. Addressing these discrimination claims is essential to ensure that AI technologies do not perpetuate existing inequalities and that they operate fairly and transparently.",multi_hop_abstract_query_synthesizer
"What trends can be observed in the usage of ChatGPT user cohorts and how do these relate to the nature of ChatGPT queries, particularly regarding work-related versus non-work-related messages?","['<1-hop>\n\nThe yellow line represents the first cohort of ChatGPT users: their usage declined somewhat over\n2023, but started growing again in late 2024 and is now higher than it has ever been. The pink line\nrepresents messages from users who signed up in Q3 of 2023 or earlier, and so thedifferencebetween\n20Note that we expect our counts of distinct accounts to somewhat exceed distinct people when one person has two\naccounts (or, for logged-out users, one person using two devices). For logged-in users, the count is based on distinct\nlogin credentials (email addresses), and one person may have multiple accounts. For logged-out users, the count is based\non distinct browser cookies; this would double-count people if someone returns to ChatGPT after clearing their cookies,\nor if they access ChatGPT with two different devices in the same week.\n10\x0cFigure 4:Daily message volumes from ChatGPT consumer plans (Free, Plus, Pro), split by sign-up date of\nthe requesting user. Reported values are moving averages of the past 90 days. Y-axis is an index normalized\nto the reported value for ”All Cohorts” at the end of Q1 2024 (April 1, 2024).\nthe yellow and pink lines represents the messages sent by users who signed up in Q2 and Q3 of 2023.\nThere has been dramatic growth in message volume both by new cohorts of users, and from growth\nin existing cohorts.\nFigure 5 normalizes each cohort, plotting daily messages per weekly active user. Each line rep-\nresents an individual cohort (instead of a cumulative cohort, as in Figure 4). The figure shows that\nearlier sign-ups have consistently had higher usage, but that usage has also consistently grown within\nevery cohort, which we interpret as due to both (1) improvements in the capabilities of the models,\nand (2) users slowly discovering new uses for existing capabilities.\n5  How ChatGPT is Used\nWe next report on thecontentof ChatGPT conversations using a variety of different taxonomies. For\neach taxonomy we describe a “prompt” which defines a set of categories, and then apply an LLM\nto map each message to a category. Our categories often apply to the user’sintention, rather than\nthe text of the conversation, and as such we never directly observe the ground truth. Nevertheless\nthe classifier results can be interpreted as the best-guess inferences that a human would make: the\nguesses from the LLM correlate highly with human guesses from the same prompt, and we get similar\nqualitative results when the prompt includes a third category for “uncertain.”\n11\x0cFigure 5:Daily messages sent per weekly active user, split by sign-up cohort. Sample only considers users of\nChatGPT consumer plans (Free, Plus, Pro). Reported values are moving averages of the past 90 days and are\nreported starting 90 days after the cohort is fully formed. Y-axis is an index normalized to the first reported\nvalue for the Q1 2023 cohort.\n5.1 ', '<2-hop>\n\nWhat share of ChatGPT queries are related to paid work?\nWe label each user message in our dataset based on whether it appears to be related to work, using\nan LLM classifier. The critical part of the prompt is as follows: 21\nDoes the last user message of this conversation transcript seem likely to be related to doing\nsome work/employment? Answer with one of the following:\n(1) likely part of work (e.g., “rewrite this HR complaint”)\n(0) likely not part of work (e.g., “does ice reduce pimples?”)\nTable 1 shows that both types of queries grew rapidly between June 2024 and June 2025, however\nnon-work-related messages grew faster: 53% of messages were not related to work in June 2024, which\nclimbed to 73% by June 2025.\nFigure 6 plots the share of non-work messages decomposed by cumulative sign-up cohorts. Succes-\nsive cohorts have had a higher share of non-work messages, but also within each cohort their non-work\nuse has increased. Comparing the share among all users (black line) to the share among the earliest\ncohort of users (yellow line), we can see that they track very closely.\n21See Appendix A for the full prompt, see Appendix B for validation.\n12\x0cFigure 6:The solid black line represents the probability that a messages on a given day is not related to\nwork, as determined by an automated classifier. Values are averaged over a 28-day lagging window. The\ndotted orange line shows the same calculation, but conditioned on messages being from users who first used\nChatGPT during or before Q2 of 2024. The remaining lines are defined similarly for successive quarters, with\ncoloring cooling for more recent cohorts. Counts are calculated from a sample of approximately 1.1 million\nsampled conversations from May 15, 2024 through June 26, 2025. Observations are reweighted to reflect total\nmessage volumes on a given day. Sampling details available in Section 3.\n5.2 ']","The trends observed in the usage of ChatGPT user cohorts indicate that the first cohort of users experienced a decline in usage throughout 2023, but began to grow again in late 2024, reaching higher levels than ever before. In contrast, the analysis of ChatGPT queries reveals that between June 2024 and June 2025, both work-related and non-work-related messages grew rapidly, with non-work-related messages increasing at a faster rate. Specifically, the share of non-work-related messages rose from 53% in June 2024 to 73% by June 2025. This suggests that while user engagement is increasing, the nature of the queries is shifting, with a significant portion of interactions moving away from work-related tasks.",multi_hop_abstract_query_synthesizer
