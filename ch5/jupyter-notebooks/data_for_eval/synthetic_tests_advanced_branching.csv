user_input,reference_contexts,reference,synthesizer_name
Why people worried about OpenAI and its impact on society and environment?,"[""What is AI, how does it work and why are some people concerned about it?\nArtificial intelligence (AI) has increasingly become part of everyday life over the past decade.\nIt is being used to personalise social media feeds, spot friends and family in smartphone photos and pave the way for medical breakthroughs.\nBut the rise of chatbots like OpenAI's ChatGPT and Meta AI has been accompanied by concern about the technology's environmental impact, ethical implications and data use.\n What is AI and what is it used for?\nAI allows computers to process large amounts of data, identify patterns and follow detailed instructions about what to do with that information.\nComputers cannot think, empathise or reason.\nHowever, scientists have developed systems that can perform tasks which usually require human intelligence, trying to replicate how people acquire and use knowledge.\nThis could be trying to anticipate what product an online shopper might buy, based on previous purchases, in order to recommend items.\nThe technology is also behind voice-controlled virtual assistants like Apple's Siri and Amazon's Alexa, and is being used to develop systems for self-driving cars.\nAI also helps social platforms like Facebook, TikTok and X decide what posts to show users. Streaming services Spotify and Deezer use AI to suggest music.\nThere are also a number of applications in medicine, as scientists use AI to help spot cancers, review X-ray results, speed up diagnoses and identify new treatments.\n What is generative AI, and how do apps like ChatGPT and Meta AI work?\nGenerative AI is used to create new content which can seem like it has been made by a human.\nIt does this by learning from vast quantities of existing data such as online text and images.\nChatGPT and Chinese rival DeepSeek's chatbot are popular generative AI tools that can be used to produce text, images, code and more material.\nGoogle's Gemini or Meta AI can similarly hold text conversations with users.\nApps like Midjourney or Veo 3 are dedicated to creating images or video from simple text prompts.\nGenerative AI can also be used to make high-quality music.\nSongs mimicking the style or sound of famous musicians have gone viral, sometimes leaving fans confused about their authenticity.\n""]","People are concerned about OpenAI, particularly with the rise of chatbots like ChatGPT, due to the technology's environmental impact, ethical implications, and data use.",single_hop_specific_query_synthesizer
What concerns has the International Monetary Fund raised regarding AI?,"['Why is AI controversial?\nWhile acknowledging AI\'s potential, some experts are worried about the implications of its rapid growth.\nThe International Monetary Fund (IMF) has warned AI could affect nearly 40% of jobs, and worsen global financial inequality.\nProf Geoffrey Hinton, a computer scientist regarded as one of the ""godfathers"" of AI development, has expressed concern that powerful AI systems could even make humans extinct - although his fear was dismissed by his fellow ""AI godfather"", Yann LeCun.\nCritics also highlight the tech\'s potential to reproduce biased information, or discriminate against some social groups.\nThis is because much of the data used to train AI comes from public material, including social media posts or comments, which can reflect existing societal biases such as sexism or racism.\nAnd while AI programmes are growing more adept, they are still prone to errors - such as creating images of people with the wrong number of fingers or limbs.\nGenerative AI systems are known for their ability to ""hallucinate"" and assert falsehoods as fact, even sometimes inventing sources for the inaccurate information.\nApple halted a new AI feature in January after it incorrectly summarised news app notifications.\nThe BBC complained about the feature after Apple\'s AI falsely told readers that Luigi Mangione - the man accused of killing UnitedHealthcare CEO Brian Thompson - had shot himself.\nGoogle has also faced criticism over inaccurate answers produced by its AI search overviews.\nThis has added to concerns about the use of AI in schools and workplaces, where it is increasingly used to help summarise texts, write emails or essays and solve bugs in code.\nThere are worries about students using AI technology to ""cheat"" on assignments, or employees ""smuggling"" it into work.\nWriters, musicians and artists have also pushed back against the technology on ethical grounds, accusing AI developers of using their work to train systems without consent or compensation.\nThousands of creators - including Abba singer-songwriter Björn Ulvaeus, writers Ian Rankin and Joanne Harris and actress Julianne Moore - signed a statement in October 2024 calling AI a ""major, unjust threat"" to their livelihoods.\nHow does AI effect the environment?\nIt is not clear how much energy AI systems use, but some researchers estimate the industry as a whole could soon consume as much as the Netherlands.\nCreating the powerful computer chips needed to run AI programmes requires lots of power and water.\nDemand for generative AI services has also meant an increase in the number of data centres which power them.\nThese huge halls - housing thousands of racks of computer servers - use substantial amounts of energy and require large volumes of water to keep them cool.\nSome large tech companies have invested in ways to reduce or reuse the water needed, or have opted for alternative methods such as air-cooling.\nHowever, some experts and activists fear that AI will worsen water supply problems.\nThe BBC was told in February that government plans to make the UK a ""world leader"" in AI could put already stretched supplies of drinking water under strain.\nIn September 2024, Google said it would reconsider proposals for a data centre in Chile, which has struggled with drought.\n']",The International Monetary Fund (IMF) has warned that AI could affect nearly 40% of jobs and worsen global financial inequality.,single_hop_specific_query_synthesizer
What is Sir Keir Starmer's stance on regulating AI?,"['Are there laws governing AI?\nSome governments have already introduced rules governing how AI operates.\nThe EU\'s Artificial Intelligence Act places controls on high risk systems used in areas such as education, healthcare, law enforcement or elections. It bans some AI use altogether.\nGenerative AI developers in China are required to safeguard citizens\' data, and promote transparency and accuracy of information. But they are also bound by the country\'s strict censorship laws.\nIn the UK, Prime Minister Sir Keir Starmer has said the government ""will test and understand AI before we regulate it"".\nBoth the UK and US have AI Safety Institutes that aim to identify risks and evaluate advanced AI models.\nIn 2024 the two countries signed an agreement to collaborate on developing ""robust"" AI testing methods.\nHowever, in February 2025, neither country signed an international AI declaration which pledged an open, inclusive and sustainable approach to the technology.\nSeveral countries including the UK are also clamping down on use of AI systems to create deepfake nude imagery and child sexual abuse material.\nSign up for our Tech Decoded newsletter to follow the world\'s top tech stories and trends. Outside the UK? Sign up here.']","Prime Minister Sir Keir Starmer has stated that the government ""will test and understand AI before we regulate it.""",single_hop_specific_query_synthesizer
How does the mapping of message content to work activities relate to the findings of Tomlinson et al in the context of ChatGPT usage?,"['<1-hop>\n\n5.4 O*NET Work Activities\nWe map message content to work activities using the Occupational Information Network (O*NET)\nDatabase Version 29.0, similar to Tomlinson et al (2025). O*NET was developed in partnership with\nthe U.S. Department of Labor and systematically classifies jobs according to the skills, tasks, and\nwork activities required to perform them. O*NET associates each occupation with a set of tasks that\nare performed at different levels of intensity. Each task is then aggregated up to three levels of detail\n- 2,087 detailed work activities (DWAs), 332 intermediate work activities (IWAs), and 41 generalized\nwork activities (GWAs).\nTo understand the work activities associated with ChatGPT usage, we mapped messages to one\nof the 332 O*NET Intermediate Work Activities (IWA), with an additional option ofAmbiguousto\naccount for situations where the user message lacked sufficient context. 22 We then used the official\n22We drew a sample of approximately 1.1 million conversations from May 2024 to June 2025, selected a random\nmessage within each, and classified it according to the prompt in A.\n19\x0cFigure 13:Shares of Asking, Doing, and Expressing messages split by work vs. non-work. See A to review\nthe prompts used by the automated classifiers. The annotations on the right show the shares of work and\nnon-work for the full sample. Each bin reports a percentage of the total population. Shares are calculated\nfrom a sample of approximately 1.1 million sampled conversations from May 15, 2024 through June 26, 2025.\nObservations are reweighted to reflect total message volumes on a given day. Sampling details available in\nSection 3.\nO*NET taxonomy to map these classified IWAs to one of the Generalized Work Activities (GWA). We\ndo not show the shares for the following GWAs as there were fewer than 100 users sending messages\nfor each category and group them intoSuppressed.\nFigure 14 presents the share of messages that belong to each GWA, in descending order. Nearly\nhalf of all messages (45.2%) fall under just three GWAs related to information use and manipula-\ntion:Getting Information(19.3%),Interpreting the Meaning of Information for Others(13.1%), and\nDocumenting/Recording Information(12.8%). The next most common work activities areProviding\nConsultation and Advice(9.2%),Thinking Creatively(9.1%),Making Decisions and Solving Problems\n(8.5%), andWorking with Computers(4.9%). These seven GWAs collectively account for 76.9% of\nall messages.\nFigure 15 presents the distribution of GWAs for the subsample of messages we classify as work-\nrelated. Among work-related messages, the most common GWAs areDocumenting/Recording In-\nformation(18.4%),Making Decisions and Solving Problems(14.9%),Thinking Creatively(13.0%),\nWorking with Computers(10.8%),Interpreting the Meaning of Information for Others(10.1%),Get-\nting Information(9.3%), andProviding Consultation and Advice to Others(4.4%). These seven GWAs\ncollectively account for nearly 81% of work-related messages. Overall, the majority of ChatGPT usage\nat work appears to be focused on two broad functions: 1) obtaining, documenting, and interpreting\ninformation; and 2) making decisions, giving advice, solving problems, and thinking creatively.\n20\x0cFigure 14:GWA Shares of 1.1M ChatGPT Messages. Messages are classified as pertaining to one of 332\nO*NET IWAs, orAmbiguoususing the prompt provided in the Appendix. IWAs were then aggregated to\nGWAs using the O*NET Work Activities taxonomy. Message sample from May 15, 2024 through June 26,\n2025. ']","The mapping of message content to work activities is based on the Occupational Information Network (O*NET) and is similar to the methodology used by Tomlinson et al (2025). This mapping involves classifying messages into one of the 332 O*NET Intermediate Work Activities (IWAs) and then aggregating these into Generalized Work Activities (GWAs). The findings indicate that nearly half of all messages (45.2%) fall under three GWAs related to information use and manipulation, which aligns with the focus on obtaining, documenting, and interpreting information as highlighted in Tomlinson et al's research.",multi_hop_specific_query_synthesizer
"How did the user growth of ChatGPT in 2023 compare to the overall message volume growth, and what implications does this have for AI technologies in that year?","['<1-hop>\n\n4 The Growth of ChatGPT\nChatGPT was released to the public on November 30, 2022 as a “research preview,” and by December\n5 it had more than one million registered users. Figure 3 reports the growth of overall weekly active\nusers (WAU) on consumer plans over time. ChatGPT had more than 100 million logged-in WAU after\none year, and almost 350 million after two years. By the end of July 2025, ChatGPT had more than\n700 million total WAU, nearly 10% of the world’s adult population. 20\nFigure 3:Weekly active ChatGPT users on consumer plans (Free, Plus, Pro), shown as point-in-time\nsnapshots every six months, November 2022–September 2025.\nFigure 4 presents growth in the total messages sent by users over time. The solid line shows that\nbetween July 2024 and July 2025, the number of messages sent grew by a factor of more than 5.\nFigure 4 also shows the contribution of individual cohorts of users to aggregate message volume.\nThe yellow line represents the first cohort of ChatGPT users: their usage declined somewhat over\n2023, but started growing again in late 2024 and is now higher than it has ever been. The pink line\nrepresents messages from users who signed up in Q3 of 2023 or earlier, and so thedifferencebetween\n20Note that we expect our counts of distinct accounts to somewhat exceed distinct people when one person has two\naccounts (or, for logged-out users, one person using two devices). For logged-in users, the count is based on distinct\nlogin credentials (email addresses), and one person may have multiple accounts. For logged-out users, the count is based\non distinct browser cookies; this would double-count people if someone returns to ChatGPT after clearing their cookies,\nor if they access ChatGPT with two different devices in the same week.\n10\x0cFigure 4:Daily message volumes from ChatGPT consumer plans (Free, Plus, Pro), split by sign-up date of\nthe requesting user. ', '<2-hop>\n\n” (truncated)\n[user]: “10 more”\nTable 2:Illustration of Context-Augmented Message Classifications (Synthetic Example). The left column\nshows a standalone message to be classified, and the right column shows the prior context included in the\nclassification of the message on the left.\nWe truncate each message to a maximum of 5,000 characters, because long context windows could\ninduce variability in the quality of the classification (Liu et al., 2023). We classify each message\nwith the “gpt-5-mini” model, with the exception ofInteraction Quality,which uses “gpt-5,” using the\nprompts listed in Appendix A.\n17Internal analyses show that the tool,Privacy Filter, has substantial alignment with human judgment.\n18In the case ofInteraction Quality,we additionally include the next two messages in the conversation as context.\n7\x0cWe validated each of the classification prompts by comparing model classification decisions against\nhuman-judged classifications of a sample of conversations from the publicly available WildChat dataset\n(Zhao et al., 2024), a set of conversations with a third-party chatbot which users affirmatively gave\ntheir assent to share publicly for research purposes. 19 Appendix B provides detail on our validation\napproach and performance relative to human judgment. For additional transparency, we classify\na sample of 100,000 public WildChat messages and provide those data in this paper’s replication\npackage.\n 3.3 Employment Dataset\nWe conduct limited analyses of aggregated employment categories based on publicly available data\nfor a sample of consumer ChatGPT users. This sample included approximately 130,000 Free, Plus,\nand Pro users, and the employment categories were aggregated by a vendor working through a secure\nData Clean Room (DCR). For this analysis, we use the same exclusion criteria as for the message-level\ndatasets: we exclude deactivated users, banned users, users who have opted out of training, and users\nwhose self-reported age is under 18. Because the data was only available for a subset of users the\nresults may not be representative of the full pool of users.\nDescription.The employment data, which is aggregated from publicly available sources, includes\nindustry, occupations coarsened to O*NET categories, seniority level, company size, and education\ninformation that is limited to the degree attained. A vendor working within a DCR procured this\ndataset, restricted us to running only aggregated queries against it through the DCR, and deleted it\nupon the study’s completion.\n']","In 2023, ChatGPT experienced a notable decline in usage among its first cohort of users, but this trend reversed towards the end of the year, leading to higher usage than ever before. Meanwhile, the total messages sent by users grew significantly, with a factor of more than 5 increase noted between July 2024 and July 2025. This growth in user engagement and message volume suggests a rising reliance on AI technologies, indicating that despite fluctuations in user numbers, the overall interaction with AI systems like ChatGPT was increasing, reflecting their growing importance in society.",multi_hop_specific_query_synthesizer
What trends in user demographics and interaction quality are expected for ChatGPT by mid-2025?,"['<1-hop>\n\n6 Who Uses ChatGPT\nIn this section we report basic descriptive facts about who uses consumer ChatGPT. Existing work\ndocuments variation in generative AI use by demographic groups within representative samples in\nthe U.S. (Bick et al. (2024), Hartley et al. (2025)) and within a subset of occupations in Denmark\n(Humlum and Vestergaard, 2025a). All of these papers find that generative AI is used more frequently\nby men, young people, and those with tertiary and/or graduate education.\nWe make three contributions relative to this prior literature. First, we confirm these broad demo-\ngraphic patterns in a global sample rather than a single country. Second, we provide more detail for\nselected demographics such as age, gender, and country of origin and study how gaps in each have\nchanged over time. Third, we use a secure data clean room to analyze how ChatGPT usage varies by\neducation and occupation.\n 6.1 Name Analysis\nWe investigate potential variation by gender by classifying a global random sample of over 1.1 million\nChatGPT users’ first names using public aggregated datasets of name-gender associations. We used\nthe World Gender Name Dictionary, and Social Security popular names, as well as datasets of popular\nBrazilian and Latin American names. This methodology is similar to that in (Hofstra et al., 2020)\nand (West et al., 2013). Names that were not in these datasets, or were flagged as ambiguous in the\ndatasets, or had significant disagreement amongst these datasets were classified asUnknown.\nExcludingUnknown, a significant share (around 80%) of the weekly active users (WAU) in the\nfirst few months after ChatGPT was released were by users with typically masculine first names.\nHowever, in the first half of 2025, we see the share of active users with typically feminine and typically\nmasculine names reach near-parity. By June 2025 we observe active users are more likely to have\ntypically feminine names. This suggests that gender gaps in ChatGPT usage have closed substantially\nover time.\nWe also study differences in usage topics. Users with typically female first names are relatively more\nlikely to send messages related toWritingandPractical Guidance. ', '<2-hop>\n\n5.5 Quality of Interactions\nWe additionally used automated classifiers to study the user’s apparent satisfaction with the chatbot’s\nresponse to their request. OurInteraction Qualityclassifier looks for an expression of satisfaction or\ndissatisfaction in the user’s subsequent message in the same conversation (if one exists), with three\npossible categories:Good,Bad, andUnknown. 23\nFigure 16 plots the overall growth of messages in these three buckets. In late 2024Goodinteractions\nwere about three times as common asBadinteractions, butGoodinteractions grew much more rapidly\nover the next nine months, and by July 2025 they were more than four times more common.\nFigure 16:Interaction quality shares, based on automated sentiment analysis of thenext responseprovided\nby the user. See Appendix B to understand how this classifier was validated. Values are averaged over a 28\nday lagging window. Shares are calculated from a sample of approximately 1.1 million sampled conversations\nfrom May 15, 2024 through June 26, 2025. Observations are reweighted to reflect total message volumes on a\ngiven day. Sampling details available in Section 3.\nDetails on the validation of this classifier, along with measurements of how it correlates with\nexplicit thumbs up/thumbs down annotations from users, are included in Appendix B.\nFigure 17 shows the ratio of good-to-bad messages by conversation topic and interaction type, as\nrated by Interaction Quality. Panel A shows thatSelf-Expressionis the highest rated topic, with a\ngood-to-bad ratio of more than seven, consistent with the growth in this category.Multimediaand\nTechnical Helphave the lowest good-to-bad ratios (1.7 and 2.7 respectively). Panel B shows that\nAskingmessages are substantially more likely to receive a good rating thanDoingorExpressing\nmessages.\n23For this classifier we do not disclose the prompt.\n23\x0cFigure 17:AverageGoodtoBadratio for user interactions by Conversation Topic (Panel A) and Ask-\ning/Doing/Expressing classification (Panel B). The prompts for each of these automated classifiers (with the\nexception of interaction quality) are available in Appendix A. Values represent the average ratio from May 15,\n2024 through June 26, 2025, where observations are reweighted to reflect total message volumes on a given\nday. Sampling details available in Section 3.\n24\x0c']","By mid-2025, it is expected that the demographic trends in ChatGPT usage will show a significant closing of the gender gap, with active users more likely to have typically feminine names, as the share of users with typically feminine names reaches near-parity with those having typically masculine names. Additionally, the quality of interactions is projected to improve, with 'Good' interactions growing rapidly; by July 2025, they are anticipated to be more than four times as common as 'Bad' interactions, indicating an overall increase in user satisfaction with the chatbot's responses.",multi_hop_specific_query_synthesizer
How AI ethical guidelines help in economic benefits and decision making?,"['<1-hop>\n\nIn the same vein, the IEEE Global Initiative has ethical guidelines for AI and autonomous systems. Its experts suggest that these models be programmed with consideration for widely accepted human norms and rules for behavior. AI algorithms need to take into effect the importance of these norms, how norm conflict can be resolved, and ways these systems can be transparent about norm resolution. Software designs should be programmed for “nondeception” and “honesty,” according to ethics experts. When failures occur, there must be mitigation mechanisms to deal with the consequences. In particular, AI must be sensitive to problems such as bias, discrimination, and fairness.68\nA group of machine learning experts claim it is possible to automate ethical decisionmaking. Using the trolley problem as a moral dilemma, they ask the following question: If an autonomous car goes out of control, should it be programmed to kill its own passengers or the pedestrians who are crossing the street? They devised a “voting-based system” that asked 1.3 million people to assess alternative scenarios, summarized the overall choices, and applied the overall perspective of these individuals to a range of vehicular possibilities. That allowed them to automate ethical decisionmaking in AI algorithms, taking public preferences into account.69 This procedure, of course, does not reduce the tragedy involved in any kind of fatality, such as seen in the Uber case, but it provides a mechanism to help AI developers incorporate ethical considerations in their planning.\n Penalize malicious behavior and promote cybersecurity\nAs with any emerging technology, it is important to discourage malicious treatment designed to trick software or use it for undesirable ends.70 This is especially important given the dual-use aspects of AI, where the same tool can be used for beneficial or malicious purposes. The malevolent use of AI exposes individuals and organizations to unnecessary risks and undermines the virtues of the emerging technology. This includes behaviors such as hacking, manipulating algorithms, compromising privacy and confidentiality, or stealing identities. Efforts to hijack AI in order to solicit confidential information should be seriously penalized as a way to deter such actions.71\nIn a rapidly changing world with many entities having advanced computing capabilities, there needs to be serious attention devoted to cybersecurity. Countries have to be careful to safeguard their own systems and keep other nations from damaging their security.72 According to the U.S. Department of Homeland Security, a major American bank receives around 11 million calls a week at its service center. In order to protect its telephony from denial of service attacks, it uses a “machine learning-based policy engine [that] blocks more than 120,000 calls per month based on voice firewall policies including harassing callers, robocalls and potential fraudulent calls.”73 This represents a way in which machine learning can help defend technology systems from malevolent attacks.\n', '<2-hop>\n\nConclusion\nTo summarize, the world is on the cusp of revolutionizing many sectors through artificial intelligence and data analytics. There already are significant deployments in finance, national security, health care, criminal justice, transportation, and smart cities that have altered decisionmaking, business models, risk mitigation, and system performance. These developments are generating substantial economic and social benefits.\nThe world is on the cusp of revolutionizing many sectors through artificial intelligence, but the way AI systems are developed need to be better understood due to the major implications these technologies will have for society as a whole.\nYet the manner in which AI systems unfold has major implications for society as a whole. It matters how policy issues are addressed, ethical conflicts are reconciled, legal realities are resolved, and how much transparency is required in AI and data analytic solutions.74 Human choices about software development affect the way in which decisions are made and the manner in which they are integrated into organizational routines. Exactly how these processes are executed need to be better understood because they will have substantial impact on the general public soon, and for the foreseeable future. AI may well be a revolution in human affairs, and become the single most influential human innovation in history.\nNote: We appreciate the research assistance of Grace Gilberg, Jack Karsten, Hillary Schaub, and Kristjan Tomasson on this project.\nThe Brookings Institution is a nonprofit organization devoted to independent research and policy solutions. Its mission is to conduct high-quality, independent research and, based on that research, to provide innovative, practical recommendations for policymakers and the public. The conclusions and recommendations of any Brookings publication are solely those of its author(s), and do not reflect the views of the Institution, its management, or its other scholars.\nSupport for this publication was generously provided by Amazon. Brookings recognizes that the value it provides is in its absolute commitment to quality, independence, and impact. Activities supported by its donors reflect this commitment.\nJohn R. Allen is a member of the Board of Advisors of Amida Technology and on the Board of Directors of Spark Cognition. Both companies work in fields discussed in this piece.\n-\nFootnotes\n- Thomas Davenport, Jeff Loucks, and David Schatsky, “Bullish on the Business Value of Cognitive” (Deloitte, 2017), p. 3 (www2.deloitte.com/us/en/pages/deloitte-analytics/articles/cognitive-technology-adoption-survey.html).\n- Luke Dormehl, Thinking Machines: The Quest for Artificial Intelligence—and Where It’s Taking Us Next (New York: Penguin–TarcherPerigee, 2017).\n- Shubhendu and Vijay, “Applicability of Artificial Intelligence in Different Fields of Life.”\n- Ibid.\n- Andrew McAfee and Erik Brynjolfsson, Machine Platform Crowd: Harnessing Our Digital Future (New York: Norton, 2017).\n- Portions of this paper draw on Darrell M. West, The Future of Work: Robots, AI, and Automation, Brookings Institution Press, 2018.\n- PriceWaterhouseCoopers, “Sizing the Prize: What’s the Real Value of AI for Your Business and How Can You Capitalise?” 2017.\n- Dominic Barton, Jonathan Woetzel, Jeongmin Seong, and Qinzheng Tian, “Artificial Intelligence: Implications for China” (New York: McKinsey Global Institute, April 2017), p. 1.\n- Nathaniel Popper, “Stocks and Bots,” New York Times Magazine, February 28, 2016.\n- Ibid.\n- Ibid.\n- Michael Lewis, Flash Boys: A Wall Street Revolt (New York: Norton, 2015).\n- Cade Metz, “In Quantum Computing Race, Yale Professors Battle Tech Giants,” New York Times, November 14, 2017, p. B3.\n- Executive Office of the President, “Artificial Intelligence, Automation, and the Economy,” December 2016, pp. 27-28.\n- Christian Davenport, “Future Wars May Depend as Much on Algorithms as on Ammunition, Report Says,” Washington Post, December 3, 2017.\n- Ibid.\n- John R. Allen and Amir Husain, “On Hyperwar,” Naval Institute Proceedings, July 17, 2017, pp. 30-36.\n', '<3-hop>\n\nTake biases seriously\nBias and discrimination are serious issues for AI. There already have been a number of cases of unfair treatment linked to historic data, and steps need to be undertaken to make sure that does not become prevalent in artificial intelligence. Existing statutes governing discrimination in the physical economy need to be extended to digital platforms. That will help protect consumers and build confidence in these systems as a whole.\nFor these advances to be widely adopted, more transparency is needed in how AI systems operate. Andrew Burt of Immuta argues, “The key problem confronting predictive analytics is really transparency. We’re in a world where data science operations are taking on increasingly important tasks, and the only thing holding them back is going to be how well the data scientists who train the models can explain what it is their models are doing.”66\n Maintaining mechanisms for human oversight and control\nSome individuals have argued that there needs to be avenues for humans to exercise oversight and control of AI systems. For example, Allen Institute for Artificial Intelligence CEO Oren Etzioni argues there should be rules for regulating these systems. First, he says, AI must be governed by all the laws that already have been developed for human behavior, including regulations concerning “cyberbullying, stock manipulation or terrorist threats,” as well as “entrap[ping] people into committing crimes.” Second, he believes that these systems should disclose they are automated systems and not human beings. Third, he states, “An A.I. system cannot retain or disclose confidential information without explicit approval from the source of that information.”67 His rationale is that these tools store so much data that people have to be cognizant of the privacy risks posed by AI.\nIn the same vein, the IEEE Global Initiative has ethical guidelines for AI and autonomous systems. Its experts suggest that these models be programmed with consideration for widely accepted human norms and rules for behavior. AI algorithms need to take into effect the importance of these norms, how norm conflict can be resolved, and ways these systems can be transparent about norm resolution. Software designs should be programmed for “nondeception” and “honesty,” according to ethics experts. When failures occur, there must be mitigation mechanisms to deal with the consequences. ']","The IEEE Global Initiative has established ethical guidelines for AI that emphasize the importance of programming AI systems with widely accepted human norms and rules for behavior. These guidelines aim to ensure transparency and fairness in AI algorithms, which can help mitigate issues like bias and discrimination. By addressing these ethical considerations, organizations can enhance decision-making processes and build public trust in AI technologies. This trust is crucial for the widespread adoption of AI, which is already generating substantial economic and social benefits across various sectors, including finance, healthcare, and transportation.",multi_hop_abstract_query_synthesizer
How does ChatGPT usage relate to the implementation of Large Language Models (LLMs) in decision support for knowledge-intensive jobs?,"['<1-hop>\n\nThis is consistent with the fact that almost half of all ChatGPT usage is\neitherPractical GuidanceorSeeking Information. We also show thatAskingis growing faster than\n9Zao-Sanders (2025) is based on a manual collection and labeling of online resources (Reddit, Quora, online articles),\nand so we believe it likely resulted in an unrepresentative distribution of use cases.\n10Among those with names commonly associated with a particular gender.\n11Appendix A gives the full prompt text and Appendix B gives detail about how the prompts were validated against\npublic conversation data.\n3\x0cDoing, and thatAskingmessages are consistently rated as having higher quality both by a classifier\nthat measures user satisfaction and from direct user feedback.\nHow does ChatGPT provide economic value, and for whom is its value the greatest? We argue that\nChatGPT likely improves worker output by providingdecision support, which is especially important in\nknowledge-intensive jobs where better decision-making increases productivity (Deming, 2021; Caplin et\nal., 2023). This explains whyAskingis relatively more common for educated users who are employed\nin highly-paid, professional occupations. Our findings are most consistent with Ide and Talamas\n(2025), who develop a model where AI agents can serve either asco-workersthat produce output or\nasco-pilotsthat give advice and improve the productivity of human problem-solving.\n2 ', '<2-hop>\n\nWhat is ChatGPT?\nHere we give a simplified overview of LLMs and chatbots. For more precise details, refer to the papers\nand system cards that OpenAI has released with each model e.g., (OpenAI, 2023, 2024a, 2025b). A\nchatbot is a statistical model trained to generate a text response given some text input, so as to\nmaximize the “quality” of that response, where the quality is measured with a variety of metrics.\nIn a prototypical interaction, a user submits a plain-text message (“prompt”) and ChatGPT\nreturns the message (“response”) generated from an underlying LLM. A large set of additional features\nhave been added to ChatGPT—including the possibility for the LLM to search the web or external\ndatabases, and generate images based on text—but the exchange of text-based messages remains the\nmost typical interaction.\nSince its launch ChatGPT has used a variety of different underlying LLMs e.g., GPT-3.5, GPT-4,\nGPT-4o, o1, o3, and GPT-5. 12 In addition there are occasional updates to the model’s weights and\nto the model’s system prompt (text instructions sent to the model along with all the queries).\nAn LLM can be thought of as a function from a string of words to a probability distribution over\nthe set of all possible words (more precisely, “tokens,” which very roughly correspond to words13). The\nfunctions are implemented with deep neural nets, typically with a transformer architecture (Vaswani\net al., 2017), parameterized with billions of model “weights”. We will refer to all of ChatGPT’s models\nas language models, though most can additionally process tokens representing images, audio, or other\nmedia.\nThe weights in an LLM-based chatbot are often trained in two stages, commonly called “pre-\ntraining” and “post-training”. In the first stage (“pre-training”), the LLMs are trained to predict the\nnext word in a string, given the preceding words, over an enormous corpus of text. At that point the\nmodels are purely predictors of the likelihood of the next word given a prior context, and as such they\nhave a relatively narrow application. In the second stage (“post-training”), the models are trained to\nproduce words that comprise “good” responses to some prompt. This stage often consists of a variety\nof different strategies: fine-tuning on a dataset of queries and ideal responses, reinforcement learning\nagainst another model that is trained to grade the quality of a response (Ouyang et al., 2022), or\nreinforcement learning against a function that knows the true response to queries (OpenAI (2024b),\n12For a timeline of model launches, see Appendix C.\n13Tokenization is a way of cutting a string of text into discrete chunks, chosen to be statistically efficient. In many\ntokenization schemes, one token corresponds to roughly three-quarters of an English word.\n4\x0cLambert et al. (2024)). This second stage also typically includes a number of “safety” constraints to\navoid certain classes of response, especially those which are deemed harmful or dangerous (OpenAI,\n2025a).\nThis two-stage process has a common statistical interpretation: the first stage teaches the model a\nlatent representation of the world; the second stage fits a function using that representation (Bengio\net al., 2014). Pre-training the model to predict the next word effectively teaches the model a low-\ndimensional representation of text, representing only the key semantic features, and therefore rendering\nthe prompt-response problem tractable with a reasonable set of training examples.\nTwo common ways of evaluating chatbots are with benchmarks (batteries of questions with known\nanswers, e.g. Measuring Massive Multitask Language Understanding (Hendrycks et al., 2021)) and\ncomparisons of human preferences over two alternative responses to the same message (e.g. Chatbot\nArena (Chiang et al., 2024)).\n3 ']","ChatGPT usage is primarily focused on providing practical guidance and seeking information, which aligns with its role in decision support. This is particularly significant in knowledge-intensive jobs where better decision-making can enhance productivity. The findings suggest that ChatGPT improves worker output by serving as a co-worker that produces output or as a co-pilot that advises and enhances human problem-solving, thereby demonstrating the economic value of LLMs in professional settings.",multi_hop_abstract_query_synthesizer
"How has the global diffusion of ChatGPT contributed to its usage patterns, particularly in work-related versus non-work-related messages?","['<1-hop>\n\nNBER WORKING PAPER SERIES\nHOW PEOPLE USE CHATGPT\nAaron Chatterji\nThomas Cunningham\nDavid J. Deming\nZoe Hitzig\nChristopher Ong\nCarl Yan Shan\nKevin Wadman\nWorking Paper 34255\nhttp://www.nber.org/papers/w34255\nNATIONAL BUREAU OF ECONOMIC RESEARCH\n1050 Massachusetts Avenue\nCambridge, MA 02138\nSeptember 2025\nWe acknowledge help and comments from Joshua Achiam, Hemanth Asirvatham, Ryan Beiermeister, Rachel Brown, Cassandra Duchan Solis, Jason Kwon, Elliott Mokski, Kevin Rao, Harrison Satcher, Gawesha Weeratunga, Hannah Wong, and Analytics & Insights team. We especially thank Tyna Eloundou and Pamela Mishkin who in several ways laid the foundation for this work. This study was approved by Harvard IRB (IRB25-0983). A repository containing all code run to produce the analyses in this paper is available on request. The views expressed herein are those of the authors and do not necessarily reflect the views of the National Bureau of Economic Research.\nAt least one co-author has disclosed additional relationships of potential relevance for this research. Further information is available online at http://www.nber.org/papers/w34255\nNBER working papers are circulated for discussion and comment purposes. They have not been peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies official NBER publications.\n© 2025 by Aaron Chatterji, Thomas Cunningham, David J. Deming, Zoe Hitzig, Christopher Ong, Carl Yan Shan, and Kevin Wadman. All rights reserved. Short sections of text, not to exceed two paragraphs, may be quoted without explicit permission provided that full credit, including © notice, is given to the source.\x0cHow People Use ChatGPT\nAaron Chatterji, Thomas Cunningham, David J. Deming, Zoe Hitzig, Christopher Ong, Carl\nYan Shan, and Kevin Wadman\nNBER Working Paper No. 34255\nSeptember 2025\nJEL No. J01, O3, O4\nABSTRACT\nDespite the rapid adoption of LLM chatbots, little is known about how they are used. We document the growth of ChatGPT’s consumer product from its launch in November 2022 through July 2025, when it had been adopted by around 10% of the world’s adult population. Early adopters were disproportionately male but the gender gap has narrowed dramatically, and we find higher growth rates in lower-income countries. Using a privacy-preserving automated pipeline, we classify usage patterns within a representative sample of ChatGPT conversations. We find steady growth in work-related messages bu t even faster growth in non-work-related messages, which have grown from 53% to more than 70% of all usage. Work usage is more common for educated users in highly-paid professional occupations. We classify messages by conversation top\nic and find that “Practical Guidance,” “Seeking Information,” and “Writing” are the three most common topics and collectively account for nearly 80% of all conversations. Writing dominates work-related tasks, highlighting chatbots’ unique ability to generate digital outputs compared to traditional sear ch engines. Computer programming and self-expression both represent relatively small shares of use. Overall, we find that ChatGPT provides economic value through decision support, which is especially important in knowledge-intensive jobs.\nAaron Chatterji\nDuke University\nFuqua School of Business and OpenAI\nronnie@duke.edu\nThomas Cunningham OpenAI\ntom.cunningham@gmail.com\nDavid J. Deming\nHarvard University\nHarvard Kennedy School and NBER\ndavid_deming@harvard.edu\nZoe Hitzig\nOpenAI\nand Harvard Society of Fellows\nzhitzig@g.harvard.edu\nChristopher Ong\nHarvard University\nand OpenAI\nchristopherong@hks.harvard.edu\nCarl Yan Shan\nOpenAI\ncshan@openai.com\nKevin Wadman\nOpenAI\nkevin.wadman@c-openai.com\x0c', '<2-hop>\n\n1 Introduction\nChatGPT launched in November 2022. By July 2025, 18 billion messages were being sent each week\nby 700 million users, representing around 10% of the global adult population. 1 For a new technology,\nthis speed of global diffusion has no precedent (Bick et al., 2024).\nThis paper studies consumer usage of ChatGPT, the first mass-market chatbot and likely the\nlargest.2 ChatGPT is based on a Large Language Model (LLM), a type of Artificial Intelligence (AI)\ndeveloped over the last decade and generally considered to represent an acceleration in AI capabilities.3\nThe sudden growth in LLM abilities and adoption has intensified interest in the effects of artificial\nintelligence on economic growth (Acemoglu, 2024; Korinek and Suh, 2024); employment (Eloundou\net al., 2025); and society (Kulveit et al., 2025). However, despite the rapid adoption of LLMs, there\nis limited public information on how they are used. A number of surveys have measured self-reported\nadoption of LLMs (Bick et al., 2024; Pew Research Center, 2025); however there are reasons to expect\nbias in self-reports (Ling and Imas, 2025), and none of these papers have been able to directly track\nthe quantity or nature of chatbot conversations.\nTwo recent papers do report statistics on chatbot conversations, classified in a variety of ways\n(Handa et al., 2025; Tomlinson et al., 2025). We build on this work in several respects. First, the pool\nof users on ChatGPT is far larger, meaning we expect our data to be a closer approximation to the\naverage chatbot user.4 Second, we use automated classifiers to report on the types of messages that\nusers send using new classification taxonomies relative to the existing literature. Third, we report the\ndiffusion of chatbot use across populations and the growth of different types of usage within cohorts.\nFourth, we use a secure data clean room protocol to analyze aggregated employment and education\ncategories for a sample of our users, lending new insights about differences in the types of messages\nsent by different groups while protecting user privacy.\nOur primary sample is a random selection of messages sent to ChatGPT on consumer plans (Free,\nPlus, Pro) between May 2024 and June 2025. 5 Messages from the user to chatbot are classified\nautomatically using a number of different taxonomies: whether the message is used for paid work,\nthe topic of conversation, and the type of interaction (asking, doing, or expressing), and the O*NET\ntask the user is performing. Each taxonomy is defined in a prompt passed to an LLM, allowing us to\nclassify messages without any human seeing them. We give the text of most prompts in Appendix A\nalong with details about how the prompts were validated in Appendix B.6 The classification pipeline is\nprotected by a series of privacy measures, detailed below, to ensure no leakage of sensitive information\nduring the automated analysis. In a secure data clean room, we relate taxonomies of messages to\naggregated employment and education categories.\nTable 1 shows the growth in total message volume for work and non-work usage. Both types of\n1Reuters (2025), Roth (2025)\n2Bick et al. (2024) report that 28% of US adults used ChatGPT in late 2024, higher than any other chatbot.\n3We use the term LLM loosely here and give more details in the following section.\n4Wiggers (2025) reports estimates that in April 2025 ChatGPT was receiving more than 10 times as many visitors\nas either Claude or Copilot.\n5Our sample includes the three consumer plans (Free, Plus, or Pro). OpenAI also offers a variety of other ChatGPT\nplans (Business fka. Teams, Enterprise, Education), which we do not include in our sample.\n6Our classifiers take into account not just the randomly-selected user message, but also a portion of the preceding\nmessages in that conversation.\n1\x0cMonth Non-Work (M)(%)Work (M)(%)Total Messages (M)\nJun 2024 238 53% 213 47% 451\nJun 2025 1,911 73% 716 27% 2,627\nTable 1:ChatGPT daily message counts (millions), broken down by likely work-related or non-work-related.\nTotal daily counts are exact measurements of message volume from all consumer plans. Daily counts of work\nand non-work related messages are estimated by classifying a random sample of conversations from that day.\n']","The global diffusion of ChatGPT has been unprecedented, with around 10% of the world's adult population adopting it by July 2025. This rapid adoption has led to significant changes in usage patterns, particularly in the context of work-related and non-work-related messages. By June 2025, non-work-related messages accounted for 73% of total messages sent, up from 53% in June 2024, while work-related messages decreased to 27%. This shift indicates that while ChatGPT is used for decision support in knowledge-intensive jobs, its primary usage has increasingly leaned towards non-work-related interactions.",multi_hop_abstract_query_synthesizer
"How does the establishment of a federal AI advisory committee relate to the need for AI transparency and accountability in local government actions, such as those taken by the New York City Council?","['<1-hop>\n\nAI will reconfigure how society and the economy operate, and there needs to be “big picture” thinking on what this will mean for ethics, governance, and societal impact. People will need the ability to think broadly about many questions and integrate knowledge from a number of different areas.\nOne example of new ways to prepare students for a digital future is IBM’s Teacher Advisor program, utilizing Watson’s free online tools to help teachers bring the latest knowledge into the classroom. They enable instructors to develop new lesson plans in STEM and non-STEM fields, find relevant instructional videos, and help students get the most out of the classroom.58 As such, they are precursors of new educational environments that need to be created.\n Create a federal AI advisory committee\nFederal officials need to think about how they deal with artificial intelligence. As noted previously, there are many issues ranging from the need for improved data access to addressing issues of bias and discrimination. It is vital that these and other concerns be considered so we gain the full benefits of this emerging technology.\nIn order to move forward in this area, several members of Congress have introduced the “Future of Artificial Intelligence Act,” a bill designed to establish broad policy and legal principles for AI. It proposes the secretary of commerce create a federal advisory committee on the development and implementation of artificial intelligence. The legislation provides a mechanism for the federal government to get advice on ways to promote a “climate of investment and innovation to ensure the global competitiveness of the United States,” “optimize the development of artificial intelligence to address the potential growth, restructuring, or other changes in the United States workforce,” “support the unbiased development and application of artificial intelligence,” and “protect the privacy rights of individuals.”59\nAmong the specific questions the committee is asked to address include the following: competitiveness, workforce impact, education, ethics training, data sharing, international cooperation, accountability, machine learning bias, rural impact, government efficiency, investment climate, job impact, bias, and consumer impact. The committee is directed to submit a report to Congress and the administration 540 days after enactment regarding any legislative or administrative action needed on AI.\nThis legislation is a step in the right direction, although the field is moving so rapidly that we would recommend shortening the reporting timeline from 540 days to 180 days. Waiting nearly two years for a committee report will certainly result in missed opportunities and a lack of action on important issues. Given rapid advances in the field, having a much quicker turnaround time on the committee analysis would be quite beneficial.\n', '<2-hop>\n\nEngage with state and local officials\nStates and localities also are taking action on AI. For example, the New York City Council unanimously passed a bill that directed the mayor to form a taskforce that would “monitor the fairness and validity of algorithms used by municipal agencies.”60 The city employs algorithms to “determine if a lower bail will be assigned to an indigent defendant, where firehouses are established, student placement for public schools, assessing teacher performance, identifying Medicaid fraud and determine where crime will happen next.”61\nAccording to the legislation’s developers, city officials want to know how these algorithms work and make sure there is sufficient AI transparency and accountability. In addition, there is concern regarding the fairness and biases of AI algorithms, so the taskforce has been directed to analyze these issues and make recommendations regarding future usage. It is scheduled to report back to the mayor on a range of AI policy, legal, and regulatory issues by late 2019.\nSome observers already are worrying that the taskforce won’t go far enough in holding algorithms accountable. For example, Julia Powles of Cornell Tech and New York University argues that the bill originally required companies to make the AI source code available to the public for inspection, and that there be simulations of its decisionmaking using actual data. After criticism of those provisions, however, former Councilman James Vacca dropped the requirements in favor of a task force studying these issues. He and other city officials were concerned that publication of proprietary information on algorithms would slow innovation and make it difficult to find AI vendors who would work with the city.62 It remains to be seen how this local task force will balance issues of innovation, privacy, and transparency.\n Regulate broad objectives more than specific algorithms\nThe European Union has taken a restrictive stance on these issues of data collection and analysis.63 It has rules limiting the ability of companies from collecting data on road conditions and mapping street views. Because many of these countries worry that people’s personal information in unencrypted Wi-Fi networks are swept up in overall data collection, the EU has fined technology firms, demanded copies of data, and placed limits on the material collected.64 This has made it more difficult for technology companies operating there to develop the high-definition maps required for autonomous vehicles.\nThe GDPR being implemented in Europe place severe restrictions on the use of artificial intelligence and machine learning. According to published guidelines, “Regulations prohibit any automated decision that ‘significantly affects’ EU citizens. This includes techniques that evaluates a person’s ‘performance at work, economic situation, health, personal preferences, interests, reliability, behavior, location, or movements.’”65 In addition, these new rules give citizens the right to review how digital services made specific algorithmic choices affecting people.\nBy taking a restrictive stance on issues of data collection and analysis, the European Union is putting its manufacturers and software designers at a significant disadvantage to the rest of the world.\nIf interpreted stringently, these rules will make it difficult for European software designers (and American designers who work with European counterparts) to incorporate artificial intelligence and high-definition mapping in autonomous vehicles. Central to navigation in these cars and trucks is tracking location and movements. Without high-definition maps containing geo-coded data and the deep learning that makes use of this information, fully autonomous driving will stagnate in Europe. Through this and other data protection actions, the European Union is putting its manufacturers and software designers at a significant disadvantage to the rest of the world.\nIt makes more sense to think about the broad objectives desired in AI and enact policies that advance them, as opposed to governments trying to crack open the “black boxes” and see exactly how specific algorithms operate. Regulating individual algorithms will limit innovation and make it difficult for companies to make use of artificial intelligence.\n', '<3-hop>\n\nImproving data access\nThe United States should develop a data strategy that promotes innovation and consumer protection. Right now, there are no uniform standards in terms of data access, data sharing, or data protection. Almost all the data are proprietary in nature and not shared very broadly with the research community, and this limits innovation and system design. AI requires data to test and improve its learning capacity.50 Without structured and unstructured data sets, it will be nearly impossible to gain the full benefits of artificial intelligence.\nIn general, the research community needs better access to government and business data, although with appropriate safeguards to make sure researchers do not misuse data in the way Cambridge Analytica did with Facebook information. There is a variety of ways researchers could gain data access. One is through voluntary agreements with companies holding proprietary data. Facebook, for example, recently announced a partnership with Stanford economist Raj Chetty to use its social media data to explore inequality.51 As part of the arrangement, researchers were required to undergo background checks and could only access data from secured sites in order to protect user privacy and security.\nIn the U.S., there are no uniform standards in terms of data access, data sharing, or data protection. Almost all the data are proprietary in nature and not shared very broadly with the research community, and this limits innovation and system design.\nGoogle long has made available search results in aggregated form for researchers and the general public. Through its “Trends” site, scholars can analyze topics such as interest in Trump, views about democracy, and perspectives on the overall economy.52 That helps people track movements in public interest and identify topics that galvanize the general public.\nTwitter makes much of its tweets available to researchers through application programming interfaces, commonly referred to as APIs. These tools help people outside the company build application software and make use of data from its social media platform. They can study patterns of social media communications and see how people are commenting on or reacting to current events.\nIn some sectors where there is a discernible public benefit, governments can facilitate collaboration by building infrastructure that shares data. For example, the National Cancer Institute has pioneered a data-sharing protocol where certified researchers can query health data it has using de-identified information drawn from clinical data, claims information, and drug therapies. That enables researchers to evaluate efficacy and effectiveness, and make recommendations regarding the best medical approaches, without compromising the privacy of individual patients.\nThere could be public-private data partnerships that combine government and business data sets to improve system performance. For example, cities could integrate information from ride-sharing services with its own material on social service locations, bus lines, mass transit, and highway congestion to improve transportation. That would help metropolitan areas deal with traffic tie-ups and assist in highway and mass transit planning.\nSome combination of these approaches would improve data access for researchers, the government, and the business community, without impinging on personal privacy. As noted by Ian Buck, the vice president of NVIDIA, “Data is the fuel that drives the AI engine. The federal government has access to vast sources of information. Opening access to that data will help us get insights that will transform the U.S. economy.”53 Through its Data.gov portal, the federal government already has put over 230,000 data sets into the public domain, and this has propelled innovation and aided improvements in AI and data analytic technologies.54 The private sector also needs to facilitate research data access so that society can achieve the full benefits of artificial intelligence.\n', '<4-hop>\n\nCreate a federal AI advisory committee\nFederal officials need to think about how they deal with artificial intelligence. ']","The establishment of a federal AI advisory committee is crucial for addressing the broader implications of artificial intelligence, including the need for AI transparency and accountability. The committee is tasked with examining various issues, including bias and discrimination in AI, which aligns with local government actions like those of the New York City Council. The Council's bill aimed to monitor the fairness and validity of algorithms used by municipal agencies, reflecting a growing concern for transparency in AI applications. Both initiatives highlight the importance of ensuring that AI technologies are developed and implemented responsibly, with oversight to protect citizens' rights and promote ethical standards.",multi_hop_abstract_query_synthesizer
