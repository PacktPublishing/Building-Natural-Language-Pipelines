user_input,reference_contexts,reference,synthesizer_name
What support will be provided for Haystack 1.0 users after the release of Haystack 2.0?,"['Haystack 2.0: The Composable Open-Source LLM Framework\nMeet Haystack 2.0, a more flexible, customizable LLM framework\nMarch 11, 2024Today we are happy to announce the stable release of Haystack 2.0 - we‚Äôve been working on this for a while, and some of you have already been testing the beta since its first release in December 2023.\nHaystack is an open-source Python framework for building production-ready LLM applications, with integrations to almost all major model providers and databases.\nAt its core, Haystack 2.0 is a major rework of the previous version with a very clear goal in mind: making it possible to implement composable AI systems that are easy to use, customize, extend, optimise, evaluate and ultimately deploy to production.\nWe encourage you to start using Haystack 2.0 as of today, whether you‚Äôve been a Haystack user before or not. You can get started by installing haystack-ai\n, our new package for Haystack 2.0\n‚≠êÔ∏è To get started:\npip install haystack-ai\nand follow the get started instructions to build your first LLM app with just a few lines of code.\nIf you‚Äôre already using Haystack 1.0 in production, don‚Äôt worry! If your applications depend on farm-haystack\nand you‚Äôre not ready to migrate just yet, you don‚Äôt have to take any action: we will keep supporting Haystack 1.0, releasing security updates and critical bug fixes, giving everybody enough time to migrate. In the coming weeks, we will also start sharing some migration guides to help you along the way.\n Why Haystack 2.0?\nHaystack was first officially released in 2020, in the good old days when the forefront of NLP was semantic search, retrieval, and extractive question-answering. During this time, we established the core of what makes Haystack Haystack: Components and Pipelines. These allowed users to build end-to-end applications by combining their desired language models (embedding, extractive QA, ranking) with their database of choice.\nThe boom of LLMs in 2023 made two things clear:\n- üëç The pipeline-component structure is a great abstraction for building composable LLM applications with many moving parts.\n- üëé Haystack 1.0 often assumed that you would be doing retrieval and extractive QA over a set of documents, imposing limitations and providing a developer experience far from ideal when building LLM applications.\nSo, we decided that the best thing we could do for Haystack and our community was to rewrite the component and pipeline architecture to keep up with the fast-paced AI industry. While Haystack 2.0 is a complete rewrite, the underlying principle of composing components into flexible pipelines remains the same.\nWith that, let‚Äôs take a look at the pillars of Haystack 2.0:\n- Composable and customizable pipelines\n- A common interface for storing data\n- A clear path to production\n- Optimization and Evaluation for Retrieval Augmentation\n']","After the release of Haystack 2.0, users of Haystack 1.0 will continue to receive support, including security updates and critical bug fixes, allowing them enough time to migrate to the new version. Migration guides will also be shared in the coming weeks to assist users in the transition.",single_hop_specific_query_synthesizer
What limitations did Haystack 1.0 have regarding pipeline structure?,"['Composable and customizable Pipelines\nModern LLM applications comprise many moving parts: retrievers, rankers, LLMs, and many more such as entity extractors, summarizers, format converters and data cleaners. Each one of these ‚Äòsubtasks‚Äô is a component in Haystack.\nWith the first version of Haystack we proved that pipelines are a good abstraction for connecting all those moving parts, but some of the assumptions we made in Haystack 1.0 dated back to a pre-LLM era and needed rethinking.\nOne important limitation in Haystack 1.0 is that loops are not allowed, and the pipeline graph has to be acyclic. This makes it difficult to implement, for example, agents, which are often designed with a reasoning flow that loops until a task is resolved.\nIn Haystack 2.0 the pipeline graph can have cycles. Combined with decision components (think about if-then-else clauses in the execution flow) and routers (components that direct the execution flow towards a specific subgraph depending on the input) this can be used to build sophisticated loops that model agentic behavior.\n Customizable Components\nWe believe that the design of an AI framework should meet the following requirements:\n- Be technology agnostic: Allow users the flexibility to decide what vendor or technology they want for each of these components and make it easy to switch out any component for another.\n- Be explicit: Make it transparent as to how these components can ‚Äútalk‚Äù to each other.\n- Be flexible: Make it possible to create custom components whenever custom behavior is desirable.\n- Be extensible: Provide a uniform and easy way for the community and third parties to build their own components and foster an open ecosystem around Haystack.\nAll components in Haystack 2.0 (including Haystack Integrations) are built with a common ‚Äúcomponent‚Äù interface. The principle is simple:\n- A component implements some logic in a method called\nrun\n- The\nrun\nmethod receives one or more input values - The\nrun\nmethod returns one or more output values\nTake embedders as an example: these components expect text as input and create vector representations (embeddings) that they return as output. On the other hand, retrievers may need embeddings as input and return documents as output. When creating a new component, to decide what inputs and outputs it should have is part of the ideation process.\nWhile there are many ready-made components built into Haystack, we want to highlight that building your own custom components is also a core functionality of Haystack 2.0.\nIn fact, we‚Äôve taken advantage of this ourselves. For example, you can read about how to use the latest optimization techniques (like HyDE) in Haystack pipelines with custom components.\n']","One important limitation in Haystack 1.0 is that loops are not allowed, and the pipeline graph has to be acyclic. This restriction makes it difficult to implement agents, which often require a reasoning flow that loops until a task is resolved.",single_hop_specific_query_synthesizer
What is Haystack 2.0?,"['A common interface for storing data - A clear path to production - Optimization and Evaluation for Retrieval Augmentation Composable and customizable Pipelines Modern LLM applications comprise many moving parts: retrievers, rankers, LLMs, and many more such as entity extractors, summarizers, format converters and data cleaners. Each one of these ‚Äòsubtasks‚Äô is a component in Haystack. With the first version of Haystack we proved that pipelines are a good abstraction for connecting all those moving parts, but some of the assumptions we made in Haystack 1.0 dated back to a pre-LLM era and needed rethinking. One important limitation in Haystack 1.0 is that loops are not allowed, and the pipeline graph has to be acyclic. This makes it difficult to implement, for example, agents, which are often designed with a reasoning flow that loops until a task is resolved. In Haystack 2.0 the pipeline graph can have cycles. Combined with decision components (think about if-then-else clauses in the execution flow) and routers (components that direct the execution flow towards a specific subgraph depending on the input) this can be used to build sophisticated loops that model agentic behavior. Customizable Components We believe that the design of an AI framework should meet the following requirements: - Be technology agnostic: Allow users the flexibility to decide what vendor or technology they want for each of these components and make it easy to switch out any component for another. - Be explicit: Make it transparent as to how these components can ‚Äútalk‚Äù to each other. - Be flexible: Make it possible to create custom components whenever custom behavior is desirable. - Be extensible: Provide a uniform and easy way for the community and third parties to build their own components and foster an open ecosystem around Haystack. All components in Haystack 2.0 (including Haystack Integrations) are built with a common ‚Äúcomponent‚Äù interface. The principle is simple: - A component implements some logic in a method called run - The run method receives one or more input values - The run method returns one or more output values Take embedders as an example: these components expect text as input and create vector representations (embeddings) that they return as output. On the other hand, retrievers may need embeddings as input and return documents as output. When creating a new component, to decide what inputs and outputs it should have is part of the ideation process. While there are many ready-made components built into Haystack, we want to highlight that building your own custom components is also a core functionality of Haystack 2.0. In fact, we‚Äôve taken advantage of this ourselves. For example, you can read about how to use the latest optimization techniques (like HyDE) in Haystack pipelines with custom components. Sharing Custom Components Since the release of Haystack 2.0-Beta, we‚Äôve seen the benefits of having a well-defined simple interface for components. We, our community, and third parties have already created many components, available as additional packages for you to install. We share these on the Haystack Integrations page, which has expanded to include all sorts of components over the last few months (with contributions from Assembly AI, Jina AI, mixedbread ai and more). We will continue to expand this page with new integrations and you can help us by creating a PR on haystack-integrations if you‚Äôd like to share a component with the community. To learn more about integrations and how to share them, you can check out our ‚ÄúIntroduction to Integrations‚Äù documentation. A common interface for storing data Most NLP applications work on large amounts of data. A common design pattern is to connect your internal knowledge base to a Large Language Model (LLM) so that it can answer questions, summarize or translate documents, and extract specific information. For example, in retrieval-augment generative pipelines (RAG), you often use an LLM to answer questions about some data that was previously retrieved. This data has to come from somewhere, and Haystack 2.0 provides a common interface to access it in a consistent way, independently from where data comes from. This interface is called ‚ÄúDocument Store‚Äù, and it‚Äôs implemented for many different storage services, to make data easily available from within Haystack pipelines. Today, we are releasing Haystack 2.0 with a large selection of database and vector store integrations. These include Chroma, Weaviate, Pinecone, Qdrant, Elasticsearch, Open Search, pgvector, MongoDB, AstraDB, Neo4j, Marqo DB, and the list will keep growing. And if your storage service is not supported yet, or should you need a high degree of customization on top of an existing one, by following our guide to creating custom document stores, you can connect your Haystack pipelines to your']","Haystack 2.0 is an advanced version of the AI framework that allows for customizable and composable pipelines, enabling sophisticated loops and decision components in the execution flow. It provides a common interface for storing data, integrates with various storage services, and supports the creation of custom components.",single_hop_specific_query_synthesizer
"What limitations of Haystack 1.0 were addressed in Haystack 2.0, particularly regarding the deployment of AI applications?","['<1-hop>\n\nComposable and customizable Pipelines\nModern LLM applications comprise many moving parts: retrievers, rankers, LLMs, and many more such as entity extractors, summarizers, format converters and data cleaners. Each one of these ‚Äòsubtasks‚Äô is a component in Haystack.\nWith the first version of Haystack we proved that pipelines are a good abstraction for connecting all those moving parts, but some of the assumptions we made in Haystack 1.0 dated back to a pre-LLM era and needed rethinking.\nOne important limitation in Haystack 1.0 is that loops are not allowed, and the pipeline graph has to be acyclic. This makes it difficult to implement, for example, agents, which are often designed with a reasoning flow that loops until a task is resolved.\nIn Haystack 2.0 the pipeline graph can have cycles. Combined with decision components (think about if-then-else clauses in the execution flow) and routers (components that direct the execution flow towards a specific subgraph depending on the input) this can be used to build sophisticated loops that model agentic behavior.\n Customizable Components\nWe believe that the design of an AI framework should meet the following requirements:\n- Be technology agnostic: Allow users the flexibility to decide what vendor or technology they want for each of these components and make it easy to switch out any component for another.\n- Be explicit: Make it transparent as to how these components can ‚Äútalk‚Äù to each other.\n- Be flexible: Make it possible to create custom components whenever custom behavior is desirable.\n- Be extensible: Provide a uniform and easy way for the community and third parties to build their own components and foster an open ecosystem around Haystack.\nAll components in Haystack 2.0 (including Haystack Integrations) are built with a common ‚Äúcomponent‚Äù interface. The principle is simple:\n- A component implements some logic in a method called\nrun\n- The\nrun\nmethod receives one or more input values - The\nrun\nmethod returns one or more output values\nTake embedders as an example: these components expect text as input and create vector representations (embeddings) that they return as output. On the other hand, retrievers may need embeddings as input and return documents as output. When creating a new component, to decide what inputs and outputs it should have is part of the ideation process.\nWhile there are many ready-made components built into Haystack, we want to highlight that building your own custom components is also a core functionality of Haystack 2.0.\nIn fact, we‚Äôve taken advantage of this ourselves. For example, you can read about how to use the latest optimization techniques (like HyDE) in Haystack pipelines with custom components.\n', '<2-hop>\n\ndata from pretty much any storage service. A clear path to production The experience we got over the last couple of years, working on Haystack 1.0 and interacting with its community, taught us two things: - It‚Äôs essential for any AI application framework to be feature-complete and developer-friendly. - It‚Äôs only after the deployment phase that AI-based applications can truly make an impact. While rewriting the framework from scratch, we took the opportunity to incorporate specific features that would simplify the deployment of Haystack-based AI applications in a production-grade environment: - A customizable logging system that supports structured logging and tracing correlation out of the box.']","Haystack 1.0 had a significant limitation in that it did not allow loops in its pipeline graph, which had to be acyclic. This restriction made it challenging to implement agents that require a reasoning flow with loops until a task is resolved. In contrast, Haystack 2.0 addressed this limitation by allowing cycles in the pipeline graph, enabling the creation of sophisticated loops and decision components that can model agentic behavior. Additionally, the experience gained from working on Haystack 1.0 highlighted the importance of being feature-complete and developer-friendly for AI application frameworks, which influenced the design of Haystack 2.0 to simplify the deployment of AI applications in production-grade environments.",multi_hop_specific_query_synthesizer
What is Haystack and how does it help with data storage in AI applications?,"['<1-hop>\n\nA common interface for storing data - A clear path to production - Optimization and Evaluation for Retrieval Augmentation Composable and customizable Pipelines Modern LLM applications comprise many moving parts: retrievers, rankers, LLMs, and many more such as entity extractors, summarizers, format converters and data cleaners. Each one of these ‚Äòsubtasks‚Äô is a component in Haystack. With the first version of Haystack we proved that pipelines are a good abstraction for connecting all those moving parts, but some of the assumptions we made in Haystack 1.0 dated back to a pre-LLM era and needed rethinking. One important limitation in Haystack 1.0 is that loops are not allowed, and the pipeline graph has to be acyclic. This makes it difficult to implement, for example, agents, which are often designed with a reasoning flow that loops until a task is resolved. In Haystack 2.0 the pipeline graph can have cycles. Combined with decision components (think about if-then-else clauses in the execution flow) and routers (components that direct the execution flow towards a specific subgraph depending on the input) this can be used to build sophisticated loops that model agentic behavior. Customizable Components We believe that the design of an AI framework should meet the following requirements: - Be technology agnostic: Allow users the flexibility to decide what vendor or technology they want for each of these components and make it easy to switch out any component for another. - Be explicit: Make it transparent as to how these components can ‚Äútalk‚Äù to each other. - Be flexible: Make it possible to create custom components whenever custom behavior is desirable. - Be extensible: Provide a uniform and easy way for the community and third parties to build their own components and foster an open ecosystem around Haystack. All components in Haystack 2.0 (including Haystack Integrations) are built with a common ‚Äúcomponent‚Äù interface. The principle is simple: - A component implements some logic in a method called run - The run method receives one or more input values - The run method returns one or more output values Take embedders as an example: these components expect text as input and create vector representations (embeddings) that they return as output. On the other hand, retrievers may need embeddings as input and return documents as output. When creating a new component, to decide what inputs and outputs it should have is part of the ideation process. While there are many ready-made components built into Haystack, we want to highlight that building your own custom components is also a core functionality of Haystack 2.0. In fact, we‚Äôve taken advantage of this ourselves. For example, you can read about how to use the latest optimization techniques (like HyDE) in Haystack pipelines with custom components. Sharing Custom Components Since the release of Haystack 2.0-Beta, we‚Äôve seen the benefits of having a well-defined simple interface for components. We, our community, and third parties have already created many components, available as additional packages for you to install. We share these on the Haystack Integrations page, which has expanded to include all sorts of components over the last few months (with contributions from Assembly AI, Jina AI, mixedbread ai and more). We will continue to expand this page with new integrations and you can help us by creating a PR on haystack-integrations if you‚Äôd like to share a component with the community. To learn more about integrations and how to share them, you can check out our ‚ÄúIntroduction to Integrations‚Äù documentation. A common interface for storing data Most NLP applications work on large amounts of data. A common design pattern is to connect your internal knowledge base to a Large Language Model (LLM) so that it can answer questions, summarize or translate documents, and extract specific information. For example, in retrieval-augment generative pipelines (RAG), you often use an LLM to answer questions about some data that was previously retrieved. This data has to come from somewhere, and Haystack 2.0 provides a common interface to access it in a consistent way, independently from where data comes from. This interface is called ‚ÄúDocument Store‚Äù, and it‚Äôs implemented for many different storage services, to make data easily available from within Haystack pipelines. Today, we are releasing Haystack 2.0 with a large selection of database and vector store integrations. These include Chroma, Weaviate, Pinecone, Qdrant, Elasticsearch, Open Search, pgvector, MongoDB, AstraDB, Neo4j, Marqo DB, and the list will keep growing. And if your storage service is not supported yet, or should you need a high degree of customization on top of an existing one, by following our guide to creating custom document stores, you can connect your Haystack pipelines to your']","Haystack is an AI framework that provides a common interface for storing data, allowing users to connect their internal knowledge base to a Large Language Model (LLM) for tasks like answering questions and summarizing documents. It supports various storage services through its Document Store, which is implemented for many databases and vector stores, making data easily accessible within Haystack pipelines.",multi_hop_specific_query_synthesizer
What role does Weaviate play in the context of Haystack 2.0's document storage solutions and how does it contribute to the optimization and evaluation of retrieval augmentation?,"['<1-hop>\n\nA common interface for storing data - A clear path to production - Optimization and Evaluation for Retrieval Augmentation Composable and customizable Pipelines Modern LLM applications comprise many moving parts: retrievers, rankers, LLMs, and many more such as entity extractors, summarizers, format converters and data cleaners. Each one of these ‚Äòsubtasks‚Äô is a component in Haystack. With the first version of Haystack we proved that pipelines are a good abstraction for connecting all those moving parts, but some of the assumptions we made in Haystack 1.0 dated back to a pre-LLM era and needed rethinking. One important limitation in Haystack 1.0 is that loops are not allowed, and the pipeline graph has to be acyclic. This makes it difficult to implement, for example, agents, which are often designed with a reasoning flow that loops until a task is resolved. In Haystack 2.0 the pipeline graph can have cycles. Combined with decision components (think about if-then-else clauses in the execution flow) and routers (components that direct the execution flow towards a specific subgraph depending on the input) this can be used to build sophisticated loops that model agentic behavior. Customizable Components We believe that the design of an AI framework should meet the following requirements: - Be technology agnostic: Allow users the flexibility to decide what vendor or technology they want for each of these components and make it easy to switch out any component for another. - Be explicit: Make it transparent as to how these components can ‚Äútalk‚Äù to each other. - Be flexible: Make it possible to create custom components whenever custom behavior is desirable. - Be extensible: Provide a uniform and easy way for the community and third parties to build their own components and foster an open ecosystem around Haystack. All components in Haystack 2.0 (including Haystack Integrations) are built with a common ‚Äúcomponent‚Äù interface. The principle is simple: - A component implements some logic in a method called run - The run method receives one or more input values - The run method returns one or more output values Take embedders as an example: these components expect text as input and create vector representations (embeddings) that they return as output. On the other hand, retrievers may need embeddings as input and return documents as output. When creating a new component, to decide what inputs and outputs it should have is part of the ideation process. While there are many ready-made components built into Haystack, we want to highlight that building your own custom components is also a core functionality of Haystack 2.0. In fact, we‚Äôve taken advantage of this ourselves. For example, you can read about how to use the latest optimization techniques (like HyDE) in Haystack pipelines with custom components. Sharing Custom Components Since the release of Haystack 2.0-Beta, we‚Äôve seen the benefits of having a well-defined simple interface for components. We, our community, and third parties have already created many components, available as additional packages for you to install. We share these on the Haystack Integrations page, which has expanded to include all sorts of components over the last few months (with contributions from Assembly AI, Jina AI, mixedbread ai and more). We will continue to expand this page with new integrations and you can help us by creating a PR on haystack-integrations if you‚Äôd like to share a component with the community. To learn more about integrations and how to share them, you can check out our ‚ÄúIntroduction to Integrations‚Äù documentation. A common interface for storing data Most NLP applications work on large amounts of data. A common design pattern is to connect your internal knowledge base to a Large Language Model (LLM) so that it can answer questions, summarize or translate documents, and extract specific information. For example, in retrieval-augment generative pipelines (RAG), you often use an LLM to answer questions about some data that was previously retrieved. This data has to come from somewhere, and Haystack 2.0 provides a common interface to access it in a consistent way, independently from where data comes from. This interface is called ‚ÄúDocument Store‚Äù, and it‚Äôs implemented for many different storage services, to make data easily available from within Haystack pipelines. Today, we are releasing Haystack 2.0 with a large selection of database and vector store integrations. These include Chroma, Weaviate, Pinecone, Qdrant, Elasticsearch, Open Search, pgvector, MongoDB, AstraDB, Neo4j, Marqo DB, and the list will keep growing. And if your storage service is not supported yet, or should you need a high degree of customization on top of an existing one, by following our guide to creating custom document stores, you can connect your Haystack pipelines to your', '<2-hop>\n\nThese include Chroma, Weaviate, Pinecone, Qdrant, Elasticsearch, Open Search, pgvector, MongoDB, AstraDB, Neo4j, Marqo DB, and the list will keep growing. And if your storage service is not supported yet, or should you need a high degree of customization on top of an existing one, by following our guide to creating custom document stores, you can connect your Haystack pipelines to your data from pretty much any storage service.\n A clear path to production\nThe experience we got over the last couple of years, working on Haystack 1.0 and interacting with its community, taught us two things:\n- It‚Äôs essential for any AI application framework to be feature-complete and developer-friendly.\n- It‚Äôs only after the deployment phase that AI-based applications can truly make an impact.\nWhile rewriting the framework from scratch, we took the opportunity to incorporate specific features that would simplify the deployment of Haystack-based AI applications in a production-grade environment:\n- A customizable logging system that supports structured logging and tracing correlation out of the box.\n- Code instrumentation collecting spans and traces in strategic points of the execution path, with support for Open Telemetry and Datadog already in place.\nIn addition, we decided to start a dedicated project to simplify deploying Haystack pipelines behind a RESTful API: Hayhooks.\nHayhooks is a client-server application that allows you to deploy Haystack pipelines, serving them through HTTP endpoints dynamically spawned. Two foundational features of Haystack 2.0 made this possible:\n- The ability to introspect a pipeline, determining its inputs and outputs at runtime. This means that every REST endpoint has well-defined, dynamically generated schemas for the request and response body, all depending on the specific pipeline structure.\n- A robust serialization mechanism. This allows for the conversion of Haystack pipelines from Python to a preferred data serialization format, and vice versa. The default format is YAML but Haystack is designed to easily extend support for additional serialization formats.\n Optimization and Evaluation of Retrieval Augmentation\nWe‚Äôve already been seeing the benefits of the new Haystack design, with pipeline optimization and evaluation being good examples of how we‚Äôve been leveraging Haystack 2.0. How?:\n- It‚Äôs easier to extend the capabilities of Haystack\n- It‚Äôs easy to implement new integrations\n  Evaluation of Retrieval Augmentation\nWe‚Äôve already been seeing the benefits of the new Haystack design, with pipeline optimization and evaluation being good examples of how we‚Äôve been leveraging Haystack 2.0. How?:\n- It‚Äôs easier to extend the capabilities of Haystack\n- It‚Äôs easy to implement new integrations\nImplementing the latest retrieval optimizations\nRetrieval is a crucial step for successful RAG pipelines. And there‚Äôs been a lot of work to optimize this step. With Haystack 2.0, we‚Äôve been able to:\n- Implement Hypothetical Document Embeddings (HyDE) easily, and we‚Äôve already published a guide to HyDE along with an example walkthrough\n- Added an integration for Optimum embedders by Hugging Face\nAnd we will be able to add more optimization techniques along the way!\nEvaluation\nHaystack 2.0 is being released with a few evaluation framework integrations in place:\nAlong with a guide to model-based evaluation.\n']","Weaviate is one of the many storage services integrated into Haystack 2.0, which provides a common interface for accessing data in a consistent way. This integration allows users to connect their Haystack pipelines to various storage services, including Weaviate, enabling efficient data retrieval for NLP applications. The optimization and evaluation of retrieval augmentation in Haystack 2.0 benefit from this integration as it simplifies the implementation of new integrations and enhances the capabilities of the framework, making it easier to extend and optimize retrieval processes.",multi_hop_specific_query_synthesizer
How does Haystack 2.0 enhance customization and flexibility for integrating MongoDB as a storage solution?,"['<1-hop>\n\nThese include Chroma, Weaviate, Pinecone, Qdrant, Elasticsearch, Open Search, pgvector, MongoDB, AstraDB, Neo4j, Marqo DB, and the list will keep growing. And if your storage service is not supported yet, or should you need a high degree of customization on top of an existing one, by following our guide to creating custom document stores, you can connect your Haystack pipelines to your data from pretty much any storage service.\n A clear path to production\nThe experience we got over the last couple of years, working on Haystack 1.0 and interacting with its community, taught us two things:\n- It‚Äôs essential for any AI application framework to be feature-complete and developer-friendly.\n- It‚Äôs only after the deployment phase that AI-based applications can truly make an impact.\nWhile rewriting the framework from scratch, we took the opportunity to incorporate specific features that would simplify the deployment of Haystack-based AI applications in a production-grade environment:\n- A customizable logging system that supports structured logging and tracing correlation out of the box.\n- Code instrumentation collecting spans and traces in strategic points of the execution path, with support for Open Telemetry and Datadog already in place.\nIn addition, we decided to start a dedicated project to simplify deploying Haystack pipelines behind a RESTful API: Hayhooks.\nHayhooks is a client-server application that allows you to deploy Haystack pipelines, serving them through HTTP endpoints dynamically spawned. Two foundational features of Haystack 2.0 made this possible:\n- The ability to introspect a pipeline, determining its inputs and outputs at runtime. This means that every REST endpoint has well-defined, dynamically generated schemas for the request and response body, all depending on the specific pipeline structure.\n- A robust serialization mechanism. This allows for the conversion of Haystack pipelines from Python to a preferred data serialization format, and vice versa. The default format is YAML but Haystack is designed to easily extend support for additional serialization formats.\n Optimization and Evaluation of Retrieval Augmentation\nWe‚Äôve already been seeing the benefits of the new Haystack design, with pipeline optimization and evaluation being good examples of how we‚Äôve been leveraging Haystack 2.0. How?:\n- It‚Äôs easier to extend the capabilities of Haystack\n- It‚Äôs easy to implement new integrations\n  Evaluation of Retrieval Augmentation\nWe‚Äôve already been seeing the benefits of the new Haystack design, with pipeline optimization and evaluation being good examples of how we‚Äôve been leveraging Haystack 2.0. How?:\n- It‚Äôs easier to extend the capabilities of Haystack\n- It‚Äôs easy to implement new integrations\nImplementing the latest retrieval optimizations\nRetrieval is a crucial step for successful RAG pipelines. And there‚Äôs been a lot of work to optimize this step. With Haystack 2.0, we‚Äôve been able to:\n- Implement Hypothetical Document Embeddings (HyDE) easily, and we‚Äôve already published a guide to HyDE along with an example walkthrough\n- Added an integration for Optimum embedders by Hugging Face\nAnd we will be able to add more optimization techniques along the way!\nEvaluation\nHaystack 2.0 is being released with a few evaluation framework integrations in place:\nAlong with a guide to model-based evaluation.\n', '<2-hop>\n\nHaystack 2.0: The Composable Open-Source LLM Framework\nMeet Haystack 2.0, a more flexible, customizable LLM framework\nMarch 11, 2024Today we are happy to announce the stable release of Haystack 2.0 - we‚Äôve been working on this for a while, and some of you have already been testing the beta since its first release in December 2023.\nHaystack is an open-source Python framework for building production-ready LLM applications, with integrations to almost all major model providers and databases.\nAt its core, Haystack 2.0 is a major rework of the previous version with a very clear goal in mind: making it possible to implement composable AI systems that are easy to use, customize, extend, optimise, evaluate and ultimately deploy to production.\nWe encourage you to start using Haystack 2.0 as of today, whether you‚Äôve been a Haystack user before or not. You can get started by installing haystack-ai\n, our new package for Haystack 2.0\n‚≠êÔ∏è To get started:\npip install haystack-ai\nand follow the get started instructions to build your first LLM app with just a few lines of code.\nIf you‚Äôre already using Haystack 1.0 in production, don‚Äôt worry! If your applications depend on farm-haystack\nand you‚Äôre not ready to migrate just yet, you don‚Äôt have to take any action: we will keep supporting Haystack 1.0, releasing security updates and critical bug fixes, giving everybody enough time to migrate. In the coming weeks, we will also start sharing some migration guides to help you along the way.\n Why Haystack 2.0?\nHaystack was first officially released in 2020, in the good old days when the forefront of NLP was semantic search, retrieval, and extractive question-answering. During this time, we established the core of what makes Haystack Haystack: Components and Pipelines. These allowed users to build end-to-end applications by combining their desired language models (embedding, extractive QA, ranking) with their database of choice.\nThe boom of LLMs in 2023 made two things clear:\n- üëç The pipeline-component structure is a great abstraction for building composable LLM applications with many moving parts.\n- üëé Haystack 1.0 often assumed that you would be doing retrieval and extractive QA over a set of documents, imposing limitations and providing a developer experience far from ideal when building LLM applications.\nSo, we decided that the best thing we could do for Haystack and our community was to rewrite the component and pipeline architecture to keep up with the fast-paced AI industry. While Haystack 2.0 is a complete rewrite, the underlying principle of composing components into flexible pipelines remains the same.\nWith that, let‚Äôs take a look at the pillars of Haystack 2.0:\n- Composable and customizable pipelines\n- A common interface for storing data\n- A clear path to production\n- Optimization and Evaluation for Retrieval Augmentation\n']","Haystack 2.0 enhances customization and flexibility for integrating MongoDB as a storage solution by allowing users to connect their Haystack pipelines to various storage services, including MongoDB. The framework is designed to be feature-complete and developer-friendly, enabling high degrees of customization on top of existing storage solutions. This is facilitated through a guide for creating custom document stores, which ensures that users can tailor their integration to meet specific needs while leveraging the capabilities of MongoDB.",multi_hop_abstract_query_synthesizer
"What are the key features of Haystack 2.0 that make it suitable for building production-ready LLM applications, and how does it integrate with storage solutions like Pinecone?","['<1-hop>\n\nThese include Chroma, Weaviate, Pinecone, Qdrant, Elasticsearch, Open Search, pgvector, MongoDB, AstraDB, Neo4j, Marqo DB, and the list will keep growing. And if your storage service is not supported yet, or should you need a high degree of customization on top of an existing one, by following our guide to creating custom document stores, you can connect your Haystack pipelines to your data from pretty much any storage service.\n A clear path to production\nThe experience we got over the last couple of years, working on Haystack 1.0 and interacting with its community, taught us two things:\n- It‚Äôs essential for any AI application framework to be feature-complete and developer-friendly.\n- It‚Äôs only after the deployment phase that AI-based applications can truly make an impact.\nWhile rewriting the framework from scratch, we took the opportunity to incorporate specific features that would simplify the deployment of Haystack-based AI applications in a production-grade environment:\n- A customizable logging system that supports structured logging and tracing correlation out of the box.\n- Code instrumentation collecting spans and traces in strategic points of the execution path, with support for Open Telemetry and Datadog already in place.\nIn addition, we decided to start a dedicated project to simplify deploying Haystack pipelines behind a RESTful API: Hayhooks.\nHayhooks is a client-server application that allows you to deploy Haystack pipelines, serving them through HTTP endpoints dynamically spawned. Two foundational features of Haystack 2.0 made this possible:\n- The ability to introspect a pipeline, determining its inputs and outputs at runtime. This means that every REST endpoint has well-defined, dynamically generated schemas for the request and response body, all depending on the specific pipeline structure.\n- A robust serialization mechanism. This allows for the conversion of Haystack pipelines from Python to a preferred data serialization format, and vice versa. The default format is YAML but Haystack is designed to easily extend support for additional serialization formats.\n Optimization and Evaluation of Retrieval Augmentation\nWe‚Äôve already been seeing the benefits of the new Haystack design, with pipeline optimization and evaluation being good examples of how we‚Äôve been leveraging Haystack 2.0. How?:\n- It‚Äôs easier to extend the capabilities of Haystack\n- It‚Äôs easy to implement new integrations\n  Evaluation of Retrieval Augmentation\nWe‚Äôve already been seeing the benefits of the new Haystack design, with pipeline optimization and evaluation being good examples of how we‚Äôve been leveraging Haystack 2.0. How?:\n- It‚Äôs easier to extend the capabilities of Haystack\n- It‚Äôs easy to implement new integrations\nImplementing the latest retrieval optimizations\nRetrieval is a crucial step for successful RAG pipelines. And there‚Äôs been a lot of work to optimize this step. With Haystack 2.0, we‚Äôve been able to:\n- Implement Hypothetical Document Embeddings (HyDE) easily, and we‚Äôve already published a guide to HyDE along with an example walkthrough\n- Added an integration for Optimum embedders by Hugging Face\nAnd we will be able to add more optimization techniques along the way!\nEvaluation\nHaystack 2.0 is being released with a few evaluation framework integrations in place:\nAlong with a guide to model-based evaluation.\n', '<2-hop>\n\nHaystack 2.0: The Composable Open-Source LLM Framework\nMeet Haystack 2.0, a more flexible, customizable LLM framework\nMarch 11, 2024Today we are happy to announce the stable release of Haystack 2.0 - we‚Äôve been working on this for a while, and some of you have already been testing the beta since its first release in December 2023.\nHaystack is an open-source Python framework for building production-ready LLM applications, with integrations to almost all major model providers and databases.\nAt its core, Haystack 2.0 is a major rework of the previous version with a very clear goal in mind: making it possible to implement composable AI systems that are easy to use, customize, extend, optimise, evaluate and ultimately deploy to production.\nWe encourage you to start using Haystack 2.0 as of today, whether you‚Äôve been a Haystack user before or not. You can get started by installing haystack-ai\n, our new package for Haystack 2.0\n‚≠êÔ∏è To get started:\npip install haystack-ai\nand follow the get started instructions to build your first LLM app with just a few lines of code.\nIf you‚Äôre already using Haystack 1.0 in production, don‚Äôt worry! If your applications depend on farm-haystack\nand you‚Äôre not ready to migrate just yet, you don‚Äôt have to take any action: we will keep supporting Haystack 1.0, releasing security updates and critical bug fixes, giving everybody enough time to migrate. In the coming weeks, we will also start sharing some migration guides to help you along the way.\n Why Haystack 2.0?\nHaystack was first officially released in 2020, in the good old days when the forefront of NLP was semantic search, retrieval, and extractive question-answering. During this time, we established the core of what makes Haystack Haystack: Components and Pipelines. These allowed users to build end-to-end applications by combining their desired language models (embedding, extractive QA, ranking) with their database of choice.\nThe boom of LLMs in 2023 made two things clear:\n- üëç The pipeline-component structure is a great abstraction for building composable LLM applications with many moving parts.\n- üëé Haystack 1.0 often assumed that you would be doing retrieval and extractive QA over a set of documents, imposing limitations and providing a developer experience far from ideal when building LLM applications.\nSo, we decided that the best thing we could do for Haystack and our community was to rewrite the component and pipeline architecture to keep up with the fast-paced AI industry. While Haystack 2.0 is a complete rewrite, the underlying principle of composing components into flexible pipelines remains the same.\nWith that, let‚Äôs take a look at the pillars of Haystack 2.0:\n- Composable and customizable pipelines\n- A common interface for storing data\n- A clear path to production\n- Optimization and Evaluation for Retrieval Augmentation\n']","Haystack 2.0 is designed to be a flexible and customizable framework for building production-ready LLM applications. Key features include composable and customizable pipelines, a common interface for storing data, and a clear path to production. It allows for easy integration with various storage solutions, including Pinecone, enabling developers to connect their Haystack pipelines to data from almost any storage service. Additionally, Haystack 2.0 incorporates a customizable logging system, code instrumentation for tracing, and a robust serialization mechanism, which collectively enhance the deployment and optimization of AI applications.",multi_hop_abstract_query_synthesizer
"What are the key features of Haystack 2.0 that make it suitable for building production-ready LLM applications, particularly in relation to storage integration like Pinecone?","['<1-hop>\n\nThese include Chroma, Weaviate, Pinecone, Qdrant, Elasticsearch, Open Search, pgvector, MongoDB, AstraDB, Neo4j, Marqo DB, and the list will keep growing. And if your storage service is not supported yet, or should you need a high degree of customization on top of an existing one, by following our guide to creating custom document stores, you can connect your Haystack pipelines to your data from pretty much any storage service.\n A clear path to production\nThe experience we got over the last couple of years, working on Haystack 1.0 and interacting with its community, taught us two things:\n- It‚Äôs essential for any AI application framework to be feature-complete and developer-friendly.\n- It‚Äôs only after the deployment phase that AI-based applications can truly make an impact.\nWhile rewriting the framework from scratch, we took the opportunity to incorporate specific features that would simplify the deployment of Haystack-based AI applications in a production-grade environment:\n- A customizable logging system that supports structured logging and tracing correlation out of the box.\n- Code instrumentation collecting spans and traces in strategic points of the execution path, with support for Open Telemetry and Datadog already in place.\nIn addition, we decided to start a dedicated project to simplify deploying Haystack pipelines behind a RESTful API: Hayhooks.\nHayhooks is a client-server application that allows you to deploy Haystack pipelines, serving them through HTTP endpoints dynamically spawned. Two foundational features of Haystack 2.0 made this possible:\n- The ability to introspect a pipeline, determining its inputs and outputs at runtime. This means that every REST endpoint has well-defined, dynamically generated schemas for the request and response body, all depending on the specific pipeline structure.\n- A robust serialization mechanism. This allows for the conversion of Haystack pipelines from Python to a preferred data serialization format, and vice versa. The default format is YAML but Haystack is designed to easily extend support for additional serialization formats.\n Optimization and Evaluation of Retrieval Augmentation\nWe‚Äôve already been seeing the benefits of the new Haystack design, with pipeline optimization and evaluation being good examples of how we‚Äôve been leveraging Haystack 2.0. How?:\n- It‚Äôs easier to extend the capabilities of Haystack\n- It‚Äôs easy to implement new integrations\n  Evaluation of Retrieval Augmentation\nWe‚Äôve already been seeing the benefits of the new Haystack design, with pipeline optimization and evaluation being good examples of how we‚Äôve been leveraging Haystack 2.0. How?:\n- It‚Äôs easier to extend the capabilities of Haystack\n- It‚Äôs easy to implement new integrations\nImplementing the latest retrieval optimizations\nRetrieval is a crucial step for successful RAG pipelines. And there‚Äôs been a lot of work to optimize this step. With Haystack 2.0, we‚Äôve been able to:\n- Implement Hypothetical Document Embeddings (HyDE) easily, and we‚Äôve already published a guide to HyDE along with an example walkthrough\n- Added an integration for Optimum embedders by Hugging Face\nAnd we will be able to add more optimization techniques along the way!\nEvaluation\nHaystack 2.0 is being released with a few evaluation framework integrations in place:\nAlong with a guide to model-based evaluation.\n', '<2-hop>\n\nHaystack 2.0: The Composable Open-Source LLM Framework\nMeet Haystack 2.0, a more flexible, customizable LLM framework\nMarch 11, 2024Today we are happy to announce the stable release of Haystack 2.0 - we‚Äôve been working on this for a while, and some of you have already been testing the beta since its first release in December 2023.\nHaystack is an open-source Python framework for building production-ready LLM applications, with integrations to almost all major model providers and databases.\nAt its core, Haystack 2.0 is a major rework of the previous version with a very clear goal in mind: making it possible to implement composable AI systems that are easy to use, customize, extend, optimise, evaluate and ultimately deploy to production.\nWe encourage you to start using Haystack 2.0 as of today, whether you‚Äôve been a Haystack user before or not. You can get started by installing haystack-ai\n, our new package for Haystack 2.0\n‚≠êÔ∏è To get started:\npip install haystack-ai\nand follow the get started instructions to build your first LLM app with just a few lines of code.\nIf you‚Äôre already using Haystack 1.0 in production, don‚Äôt worry! If your applications depend on farm-haystack\nand you‚Äôre not ready to migrate just yet, you don‚Äôt have to take any action: we will keep supporting Haystack 1.0, releasing security updates and critical bug fixes, giving everybody enough time to migrate. In the coming weeks, we will also start sharing some migration guides to help you along the way.\n Why Haystack 2.0?\nHaystack was first officially released in 2020, in the good old days when the forefront of NLP was semantic search, retrieval, and extractive question-answering. During this time, we established the core of what makes Haystack Haystack: Components and Pipelines. These allowed users to build end-to-end applications by combining their desired language models (embedding, extractive QA, ranking) with their database of choice.\nThe boom of LLMs in 2023 made two things clear:\n- üëç The pipeline-component structure is a great abstraction for building composable LLM applications with many moving parts.\n- üëé Haystack 1.0 often assumed that you would be doing retrieval and extractive QA over a set of documents, imposing limitations and providing a developer experience far from ideal when building LLM applications.\nSo, we decided that the best thing we could do for Haystack and our community was to rewrite the component and pipeline architecture to keep up with the fast-paced AI industry. While Haystack 2.0 is a complete rewrite, the underlying principle of composing components into flexible pipelines remains the same.\nWith that, let‚Äôs take a look at the pillars of Haystack 2.0:\n- Composable and customizable pipelines\n- A common interface for storing data\n- A clear path to production\n- Optimization and Evaluation for Retrieval Augmentation\n']","Haystack 2.0 is designed to be a flexible and customizable framework for building production-ready LLM applications. Key features include composable and customizable pipelines, which allow developers to easily integrate various components and storage solutions like Pinecone. The framework emphasizes a clear path to production, ensuring that AI applications can be effectively deployed. Additionally, it includes a robust logging system, code instrumentation for tracing, and a serialization mechanism that supports various data formats. These enhancements facilitate the optimization and evaluation of retrieval augmentation, making it easier to implement new integrations and extend capabilities within the framework.",multi_hop_abstract_query_synthesizer
"What are the key features of Haystack 2.0 that make it suitable for building production-ready LLM applications, particularly in relation to storage integration with Pinecone?","['<1-hop>\n\nThese include Chroma, Weaviate, Pinecone, Qdrant, Elasticsearch, Open Search, pgvector, MongoDB, AstraDB, Neo4j, Marqo DB, and the list will keep growing. And if your storage service is not supported yet, or should you need a high degree of customization on top of an existing one, by following our guide to creating custom document stores, you can connect your Haystack pipelines to your data from pretty much any storage service.\n A clear path to production\nThe experience we got over the last couple of years, working on Haystack 1.0 and interacting with its community, taught us two things:\n- It‚Äôs essential for any AI application framework to be feature-complete and developer-friendly.\n- It‚Äôs only after the deployment phase that AI-based applications can truly make an impact.\nWhile rewriting the framework from scratch, we took the opportunity to incorporate specific features that would simplify the deployment of Haystack-based AI applications in a production-grade environment:\n- A customizable logging system that supports structured logging and tracing correlation out of the box.\n- Code instrumentation collecting spans and traces in strategic points of the execution path, with support for Open Telemetry and Datadog already in place.\nIn addition, we decided to start a dedicated project to simplify deploying Haystack pipelines behind a RESTful API: Hayhooks.\nHayhooks is a client-server application that allows you to deploy Haystack pipelines, serving them through HTTP endpoints dynamically spawned. Two foundational features of Haystack 2.0 made this possible:\n- The ability to introspect a pipeline, determining its inputs and outputs at runtime. This means that every REST endpoint has well-defined, dynamically generated schemas for the request and response body, all depending on the specific pipeline structure.\n- A robust serialization mechanism. This allows for the conversion of Haystack pipelines from Python to a preferred data serialization format, and vice versa. The default format is YAML but Haystack is designed to easily extend support for additional serialization formats.\n Optimization and Evaluation of Retrieval Augmentation\nWe‚Äôve already been seeing the benefits of the new Haystack design, with pipeline optimization and evaluation being good examples of how we‚Äôve been leveraging Haystack 2.0. How?:\n- It‚Äôs easier to extend the capabilities of Haystack\n- It‚Äôs easy to implement new integrations\n  Evaluation of Retrieval Augmentation\nWe‚Äôve already been seeing the benefits of the new Haystack design, with pipeline optimization and evaluation being good examples of how we‚Äôve been leveraging Haystack 2.0. How?:\n- It‚Äôs easier to extend the capabilities of Haystack\n- It‚Äôs easy to implement new integrations\nImplementing the latest retrieval optimizations\nRetrieval is a crucial step for successful RAG pipelines. And there‚Äôs been a lot of work to optimize this step. With Haystack 2.0, we‚Äôve been able to:\n- Implement Hypothetical Document Embeddings (HyDE) easily, and we‚Äôve already published a guide to HyDE along with an example walkthrough\n- Added an integration for Optimum embedders by Hugging Face\nAnd we will be able to add more optimization techniques along the way!\nEvaluation\nHaystack 2.0 is being released with a few evaluation framework integrations in place:\nAlong with a guide to model-based evaluation.\n', '<2-hop>\n\nHaystack 2.0: The Composable Open-Source LLM Framework\nMeet Haystack 2.0, a more flexible, customizable LLM framework\nMarch 11, 2024Today we are happy to announce the stable release of Haystack 2.0 - we‚Äôve been working on this for a while, and some of you have already been testing the beta since its first release in December 2023.\nHaystack is an open-source Python framework for building production-ready LLM applications, with integrations to almost all major model providers and databases.\nAt its core, Haystack 2.0 is a major rework of the previous version with a very clear goal in mind: making it possible to implement composable AI systems that are easy to use, customize, extend, optimise, evaluate and ultimately deploy to production.\nWe encourage you to start using Haystack 2.0 as of today, whether you‚Äôve been a Haystack user before or not. You can get started by installing haystack-ai\n, our new package for Haystack 2.0\n‚≠êÔ∏è To get started:\npip install haystack-ai\nand follow the get started instructions to build your first LLM app with just a few lines of code.\nIf you‚Äôre already using Haystack 1.0 in production, don‚Äôt worry! If your applications depend on farm-haystack\nand you‚Äôre not ready to migrate just yet, you don‚Äôt have to take any action: we will keep supporting Haystack 1.0, releasing security updates and critical bug fixes, giving everybody enough time to migrate. In the coming weeks, we will also start sharing some migration guides to help you along the way.\n Why Haystack 2.0?\nHaystack was first officially released in 2020, in the good old days when the forefront of NLP was semantic search, retrieval, and extractive question-answering. During this time, we established the core of what makes Haystack Haystack: Components and Pipelines. These allowed users to build end-to-end applications by combining their desired language models (embedding, extractive QA, ranking) with their database of choice.\nThe boom of LLMs in 2023 made two things clear:\n- üëç The pipeline-component structure is a great abstraction for building composable LLM applications with many moving parts.\n- üëé Haystack 1.0 often assumed that you would be doing retrieval and extractive QA over a set of documents, imposing limitations and providing a developer experience far from ideal when building LLM applications.\nSo, we decided that the best thing we could do for Haystack and our community was to rewrite the component and pipeline architecture to keep up with the fast-paced AI industry. While Haystack 2.0 is a complete rewrite, the underlying principle of composing components into flexible pipelines remains the same.\nWith that, let‚Äôs take a look at the pillars of Haystack 2.0:\n- Composable and customizable pipelines\n- A common interface for storing data\n- A clear path to production\n- Optimization and Evaluation for Retrieval Augmentation\n']","Haystack 2.0 is designed to be a flexible and customizable framework for building production-ready LLM applications. Key features include a common interface for storing data, which allows integration with various storage services like Pinecone. Additionally, it emphasizes a clear path to production, ensuring that AI applications can be effectively deployed. The framework also supports composable and customizable pipelines, making it easier to implement and optimize integrations, including those with Pinecone.",multi_hop_abstract_query_synthesizer
"What are the key features of Haystack 2.0 that make it suitable for building production-ready LLM applications, particularly in relation to storage integration like Pinecone?","['<1-hop>\n\nThese include Chroma, Weaviate, Pinecone, Qdrant, Elasticsearch, Open Search, pgvector, MongoDB, AstraDB, Neo4j, Marqo DB, and the list will keep growing. And if your storage service is not supported yet, or should you need a high degree of customization on top of an existing one, by following our guide to creating custom document stores, you can connect your Haystack pipelines to your data from pretty much any storage service.\n A clear path to production\nThe experience we got over the last couple of years, working on Haystack 1.0 and interacting with its community, taught us two things:\n- It‚Äôs essential for any AI application framework to be feature-complete and developer-friendly.\n- It‚Äôs only after the deployment phase that AI-based applications can truly make an impact.\nWhile rewriting the framework from scratch, we took the opportunity to incorporate specific features that would simplify the deployment of Haystack-based AI applications in a production-grade environment:\n- A customizable logging system that supports structured logging and tracing correlation out of the box.\n- Code instrumentation collecting spans and traces in strategic points of the execution path, with support for Open Telemetry and Datadog already in place.\nIn addition, we decided to start a dedicated project to simplify deploying Haystack pipelines behind a RESTful API: Hayhooks.\nHayhooks is a client-server application that allows you to deploy Haystack pipelines, serving them through HTTP endpoints dynamically spawned. Two foundational features of Haystack 2.0 made this possible:\n- The ability to introspect a pipeline, determining its inputs and outputs at runtime. This means that every REST endpoint has well-defined, dynamically generated schemas for the request and response body, all depending on the specific pipeline structure.\n- A robust serialization mechanism. This allows for the conversion of Haystack pipelines from Python to a preferred data serialization format, and vice versa. The default format is YAML but Haystack is designed to easily extend support for additional serialization formats.\n Optimization and Evaluation of Retrieval Augmentation\nWe‚Äôve already been seeing the benefits of the new Haystack design, with pipeline optimization and evaluation being good examples of how we‚Äôve been leveraging Haystack 2.0. How?:\n- It‚Äôs easier to extend the capabilities of Haystack\n- It‚Äôs easy to implement new integrations\n  Evaluation of Retrieval Augmentation\nWe‚Äôve already been seeing the benefits of the new Haystack design, with pipeline optimization and evaluation being good examples of how we‚Äôve been leveraging Haystack 2.0. How?:\n- It‚Äôs easier to extend the capabilities of Haystack\n- It‚Äôs easy to implement new integrations\nImplementing the latest retrieval optimizations\nRetrieval is a crucial step for successful RAG pipelines. And there‚Äôs been a lot of work to optimize this step. With Haystack 2.0, we‚Äôve been able to:\n- Implement Hypothetical Document Embeddings (HyDE) easily, and we‚Äôve already published a guide to HyDE along with an example walkthrough\n- Added an integration for Optimum embedders by Hugging Face\nAnd we will be able to add more optimization techniques along the way!\nEvaluation\nHaystack 2.0 is being released with a few evaluation framework integrations in place:\nAlong with a guide to model-based evaluation.\n', '<2-hop>\n\nHaystack 2.0: The Composable Open-Source LLM Framework\nMeet Haystack 2.0, a more flexible, customizable LLM framework\nMarch 11, 2024Today we are happy to announce the stable release of Haystack 2.0 - we‚Äôve been working on this for a while, and some of you have already been testing the beta since its first release in December 2023.\nHaystack is an open-source Python framework for building production-ready LLM applications, with integrations to almost all major model providers and databases.\nAt its core, Haystack 2.0 is a major rework of the previous version with a very clear goal in mind: making it possible to implement composable AI systems that are easy to use, customize, extend, optimise, evaluate and ultimately deploy to production.\nWe encourage you to start using Haystack 2.0 as of today, whether you‚Äôve been a Haystack user before or not. You can get started by installing haystack-ai\n, our new package for Haystack 2.0\n‚≠êÔ∏è To get started:\npip install haystack-ai\nand follow the get started instructions to build your first LLM app with just a few lines of code.\nIf you‚Äôre already using Haystack 1.0 in production, don‚Äôt worry! If your applications depend on farm-haystack\nand you‚Äôre not ready to migrate just yet, you don‚Äôt have to take any action: we will keep supporting Haystack 1.0, releasing security updates and critical bug fixes, giving everybody enough time to migrate. In the coming weeks, we will also start sharing some migration guides to help you along the way.\n Why Haystack 2.0?\nHaystack was first officially released in 2020, in the good old days when the forefront of NLP was semantic search, retrieval, and extractive question-answering. During this time, we established the core of what makes Haystack Haystack: Components and Pipelines. These allowed users to build end-to-end applications by combining their desired language models (embedding, extractive QA, ranking) with their database of choice.\nThe boom of LLMs in 2023 made two things clear:\n- üëç The pipeline-component structure is a great abstraction for building composable LLM applications with many moving parts.\n- üëé Haystack 1.0 often assumed that you would be doing retrieval and extractive QA over a set of documents, imposing limitations and providing a developer experience far from ideal when building LLM applications.\nSo, we decided that the best thing we could do for Haystack and our community was to rewrite the component and pipeline architecture to keep up with the fast-paced AI industry. While Haystack 2.0 is a complete rewrite, the underlying principle of composing components into flexible pipelines remains the same.\nWith that, let‚Äôs take a look at the pillars of Haystack 2.0:\n- Composable and customizable pipelines\n- A common interface for storing data\n- A clear path to production\n- Optimization and Evaluation for Retrieval Augmentation\n']","Haystack 2.0 is designed to be a flexible and customizable framework for building production-ready LLM applications. Key features include composable and customizable pipelines, which allow users to integrate various components easily. It supports almost all major model providers and databases, including Pinecone, facilitating seamless storage integration. Additionally, Haystack 2.0 emphasizes a clear path to production, with features such as a customizable logging system, code instrumentation for tracing, and a robust serialization mechanism for data formats. These enhancements make it easier to deploy Haystack pipelines effectively in a production environment.",multi_hop_abstract_query_synthesizer
