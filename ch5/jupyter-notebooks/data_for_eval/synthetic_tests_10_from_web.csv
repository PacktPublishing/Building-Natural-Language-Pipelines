user_input,reference_contexts,reference,synthesizer_name
What are the key features and improvements introduced in Haystack 2.0 for developing AI applications?,"['Haystack 2.0: The Composable Open-Source LLM Framework\nMeet Haystack 2.0, a more flexible, customizable LLM framework\nMarch 11, 2024Today we are happy to announce the stable release of Haystack 2.0 - we‚Äôve been working on this for a while, and some of you have already been testing the beta since its first release in December 2023.\nHaystack is an open-source Python framework for building production-ready LLM applications, with integrations to almost all major model providers and databases.\nAt its core, Haystack 2.0 is a major rework of the previous version with a very clear goal in mind: making it possible to implement composable AI systems that are easy to use, customize, extend, optimise, evaluate and ultimately deploy to production.\nWe encourage you to start using Haystack 2.0 as of today, whether you‚Äôve been a Haystack user before or not. You can get started by installing haystack-ai\n, our new package for Haystack 2.0\n‚≠êÔ∏è To get started:\npip install haystack-ai\nand follow the get started instructions to build your first LLM app with just a few lines of code.\nIf you‚Äôre already using Haystack 1.0 in production, don‚Äôt worry! If your applications depend on farm-haystack\nand you‚Äôre not ready to migrate just yet, you don‚Äôt have to take any action: we will keep supporting Haystack 1.0, releasing security updates and critical bug fixes, giving everybody enough time to migrate. In the coming weeks, we will also start sharing some migration guides to help you along the way.\n Why Haystack 2.0?\nHaystack was first officially released in 2020, in the good old days when the forefront of NLP was semantic search, retrieval, and extractive question-answering. During this time, we established the core of what makes Haystack Haystack: Components and Pipelines. These allowed users to build end-to-end applications by combining their desired language models (embedding, extractive QA, ranking) with their database of choice.\nThe boom of LLMs in 2023 made two things clear:\n- üëç The pipeline-component structure is a great abstraction for building composable LLM applications with many moving parts.\n- üëé Haystack 1.0 often assumed that you would be doing retrieval and extractive QA over a set of documents, imposing limitations and providing a developer experience far from ideal when building LLM applications.\nSo, we decided that the best thing we could do for Haystack and our community was to rewrite the component and pipeline architecture to keep up with the fast-paced AI industry. While Haystack 2.0 is a complete rewrite, the underlying principle of composing components into flexible pipelines remains the same.\nWith that, let‚Äôs take a look at the pillars of Haystack 2.0:\n- Composable and customizable pipelines\n- A common interface for storing data\n- A clear path to production\n- Optimization and Evaluation for Retrieval Augmentation\n']","Haystack 2.0 introduces several key features and improvements for developing AI applications, including composable and customizable pipelines, a common interface for storing data, a clear path to production, and optimization and evaluation for retrieval augmentation. It is designed to be flexible and customizable, allowing developers to implement composable AI systems that are easy to use, extend, and deploy to production.",single_hop_specific_query_synthesizer
Wut is AI in Haystack 2.0?,"['Composable and customizable Pipelines\nModern LLM applications comprise many moving parts: retrievers, rankers, LLMs, and many more such as entity extractors, summarizers, format converters and data cleaners. Each one of these ‚Äòsubtasks‚Äô is a component in Haystack.\nWith the first version of Haystack we proved that pipelines are a good abstraction for connecting all those moving parts, but some of the assumptions we made in Haystack 1.0 dated back to a pre-LLM era and needed rethinking.\nOne important limitation in Haystack 1.0 is that loops are not allowed, and the pipeline graph has to be acyclic. This makes it difficult to implement, for example, agents, which are often designed with a reasoning flow that loops until a task is resolved.\nIn Haystack 2.0 the pipeline graph can have cycles. Combined with decision components (think about if-then-else clauses in the execution flow) and routers (components that direct the execution flow towards a specific subgraph depending on the input) this can be used to build sophisticated loops that model agentic behavior.\n Customizable Components\nWe believe that the design of an AI framework should meet the following requirements:\n- Be technology agnostic: Allow users the flexibility to decide what vendor or technology they want for each of these components and make it easy to switch out any component for another.\n- Be explicit: Make it transparent as to how these components can ‚Äútalk‚Äù to each other.\n- Be flexible: Make it possible to create custom components whenever custom behavior is desirable.\n- Be extensible: Provide a uniform and easy way for the community and third parties to build their own components and foster an open ecosystem around Haystack.\nAll components in Haystack 2.0 (including Haystack Integrations) are built with a common ‚Äúcomponent‚Äù interface. The principle is simple:\n- A component implements some logic in a method called\nrun\n- The\nrun\nmethod receives one or more input values - The\nrun\nmethod returns one or more output values\nTake embedders as an example: these components expect text as input and create vector representations (embeddings) that they return as output. On the other hand, retrievers may need embeddings as input and return documents as output. When creating a new component, to decide what inputs and outputs it should have is part of the ideation process.\nWhile there are many ready-made components built into Haystack, we want to highlight that building your own custom components is also a core functionality of Haystack 2.0.\nIn fact, we‚Äôve taken advantage of this ourselves. For example, you can read about how to use the latest optimization techniques (like HyDE) in Haystack pipelines with custom components.\n']","AI in Haystack 2.0 refers to the customizable and composable pipelines that allow for the integration of various components such as retrievers, rankers, and LLMs. The framework is designed to be technology agnostic, explicit in component communication, flexible for custom components, and extensible for community contributions.",single_hop_specific_query_synthesizer
What role do LLMs play in modern AI applications?,"['A common interface for storing data - A clear path to production - Optimization and Evaluation for Retrieval Augmentation Composable and customizable Pipelines Modern LLM applications comprise many moving parts: retrievers, rankers, LLMs, and many more such as entity extractors, summarizers, format converters and data cleaners. Each one of these ‚Äòsubtasks‚Äô is a component in Haystack. With the first version of Haystack we proved that pipelines are a good abstraction for connecting all those moving parts, but some of the assumptions we made in Haystack 1.0 dated back to a pre-LLM era and needed rethinking. One important limitation in Haystack 1.0 is that loops are not allowed, and the pipeline graph has to be acyclic. This makes it difficult to implement, for example, agents, which are often designed with a reasoning flow that loops until a task is resolved. In Haystack 2.0 the pipeline graph can have cycles. Combined with decision components (think about if-then-else clauses in the execution flow) and routers (components that direct the execution flow towards a specific subgraph depending on the input) this can be used to build sophisticated loops that model agentic behavior. Customizable Components We believe that the design of an AI framework should meet the following requirements: - Be technology agnostic: Allow users the flexibility to decide what vendor or technology they want for each of these components and make it easy to switch out any component for another. - Be explicit: Make it transparent as to how these components can ‚Äútalk‚Äù to each other. - Be flexible: Make it possible to create custom components whenever custom behavior is desirable. - Be extensible: Provide a uniform and easy way for the community and third parties to build their own components and foster an open ecosystem around Haystack. All components in Haystack 2.0 (including Haystack Integrations) are built with a common ‚Äúcomponent‚Äù interface. The principle is simple: - A component implements some logic in a method called run - The run method receives one or more input values - The run method returns one or more output values Take embedders as an example: these components expect text as input and create vector representations (embeddings) that they return as output. On the other hand, retrievers may need embeddings as input and return documents as output. When creating a new component, to decide what inputs and outputs it should have is part of the ideation process. While there are many ready-made components built into Haystack, we want to highlight that building your own custom components is also a core functionality of Haystack 2.0. In fact, we‚Äôve taken advantage of this ourselves. For example, you can read about how to use the latest optimization techniques (like HyDE) in Haystack pipelines with custom components. Sharing Custom Components Since the release of Haystack 2.0-Beta, we‚Äôve seen the benefits of having a well-defined simple interface for components. We, our community, and third parties have already created many components, available as additional packages for you to install. We share these on the Haystack Integrations page, which has expanded to include all sorts of components over the last few months (with contributions from Assembly AI, Jina AI, mixedbread ai and more). We will continue to expand this page with new integrations and you can help us by creating a PR on haystack-integrations if you‚Äôd like to share a component with the community. To learn more about integrations and how to share them, you can check out our ‚ÄúIntroduction to Integrations‚Äù documentation. A common interface for storing data Most NLP applications work on large amounts of data. A common design pattern is to connect your internal knowledge base to a Large Language Model (LLM) so that it can answer questions, summarize or translate documents, and extract specific information. For example, in retrieval-augment generative pipelines (RAG), you often use an LLM to answer questions about some data that was previously retrieved. This data has to come from somewhere, and Haystack 2.0 provides a common interface to access it in a consistent way, independently from where data comes from. This interface is called ‚ÄúDocument Store‚Äù, and it‚Äôs implemented for many different storage services, to make data easily available from within Haystack pipelines. Today, we are releasing Haystack 2.0 with a large selection of database and vector store integrations. These include Chroma, Weaviate, Pinecone, Qdrant, Elasticsearch, Open Search, pgvector, MongoDB, AstraDB, Neo4j, Marqo DB, and the list will keep growing. And if your storage service is not supported yet, or should you need a high degree of customization on top of an existing one, by following our guide to creating custom document stores, you can connect your Haystack pipelines to your']","In modern AI applications, LLMs are used to answer questions, summarize or translate documents, and extract specific information, often in conjunction with a retrieval-augment generative pipeline (RAG).",single_hop_specific_query_synthesizer
What are the key improvements in Haystack 2.0 regarding customizable components and the ability to create loops in pipelines?,"['<1-hop>\n\nA common interface for storing data - A clear path to production - Optimization and Evaluation for Retrieval Augmentation Composable and customizable Pipelines Modern LLM applications comprise many moving parts: retrievers, rankers, LLMs, and many more such as entity extractors, summarizers, format converters and data cleaners. Each one of these ‚Äòsubtasks‚Äô is a component in Haystack. With the first version of Haystack we proved that pipelines are a good abstraction for connecting all those moving parts, but some of the assumptions we made in Haystack 1.0 dated back to a pre-LLM era and needed rethinking. One important limitation in Haystack 1.0 is that loops are not allowed, and the pipeline graph has to be acyclic. This makes it difficult to implement, for example, agents, which are often designed with a reasoning flow that loops until a task is resolved. In Haystack 2.0 the pipeline graph can have cycles. Combined with decision components (think about if-then-else clauses in the execution flow) and routers (components that direct the execution flow towards a specific subgraph depending on the input) this can be used to build sophisticated loops that model agentic behavior. Customizable Components We believe that the design of an AI framework should meet the following requirements: - Be technology agnostic: Allow users the flexibility to decide what vendor or technology they want for each of these components and make it easy to switch out any component for another. - Be explicit: Make it transparent as to how these components can ‚Äútalk‚Äù to each other. - Be flexible: Make it possible to create custom components whenever custom behavior is desirable. - Be extensible: Provide a uniform and easy way for the community and third parties to build their own components and foster an open ecosystem around Haystack. All components in Haystack 2.0 (including Haystack Integrations) are built with a common ‚Äúcomponent‚Äù interface. The principle is simple: - A component implements some logic in a method called run - The run method receives one or more input values - The run method returns one or more output values Take embedders as an example: these components expect text as input and create vector representations (embeddings) that they return as output. On the other hand, retrievers may need embeddings as input and return documents as output. When creating a new component, to decide what inputs and outputs it should have is part of the ideation process. While there are many ready-made components built into Haystack, we want to highlight that building your own custom components is also a core functionality of Haystack 2.0. In fact, we‚Äôve taken advantage of this ourselves. For example, you can read about how to use the latest optimization techniques (like HyDE) in Haystack pipelines with custom components. Sharing Custom Components Since the release of Haystack 2.0-Beta, we‚Äôve seen the benefits of having a well-defined simple interface for components. We, our community, and third parties have already created many components, available as additional packages for you to install. We share these on the Haystack Integrations page, which has expanded to include all sorts of components over the last few months (with contributions from Assembly AI, Jina AI, mixedbread ai and more). We will continue to expand this page with new integrations and you can help us by creating a PR on haystack-integrations if you‚Äôd like to share a component with the community. To learn more about integrations and how to share them, you can check out our ‚ÄúIntroduction to Integrations‚Äù documentation. A common interface for storing data Most NLP applications work on large amounts of data. A common design pattern is to connect your internal knowledge base to a Large Language Model (LLM) so that it can answer questions, summarize or translate documents, and extract specific information. For example, in retrieval-augment generative pipelines (RAG), you often use an LLM to answer questions about some data that was previously retrieved. This data has to come from somewhere, and Haystack 2.0 provides a common interface to access it in a consistent way, independently from where data comes from. This interface is called ‚ÄúDocument Store‚Äù, and it‚Äôs implemented for many different storage services, to make data easily available from within Haystack pipelines. Today, we are releasing Haystack 2.0 with a large selection of database and vector store integrations. These include Chroma, Weaviate, Pinecone, Qdrant, Elasticsearch, Open Search, pgvector, MongoDB, AstraDB, Neo4j, Marqo DB, and the list will keep growing. And if your storage service is not supported yet, or should you need a high degree of customization on top of an existing one, by following our guide to creating custom document stores, you can connect your Haystack pipelines to your', '<2-hop>\n\nComposable and customizable Pipelines\nModern LLM applications comprise many moving parts: retrievers, rankers, LLMs, and many more such as entity extractors, summarizers, format converters and data cleaners. Each one of these ‚Äòsubtasks‚Äô is a component in Haystack.\nWith the first version of Haystack we proved that pipelines are a good abstraction for connecting all those moving parts, but some of the assumptions we made in Haystack 1.0 dated back to a pre-LLM era and needed rethinking.\nOne important limitation in Haystack 1.0 is that loops are not allowed, and the pipeline graph has to be acyclic. This makes it difficult to implement, for example, agents, which are often designed with a reasoning flow that loops until a task is resolved.\nIn Haystack 2.0 the pipeline graph can have cycles. Combined with decision components (think about if-then-else clauses in the execution flow) and routers (components that direct the execution flow towards a specific subgraph depending on the input) this can be used to build sophisticated loops that model agentic behavior.\n Customizable Components\nWe believe that the design of an AI framework should meet the following requirements:\n- Be technology agnostic: Allow users the flexibility to decide what vendor or technology they want for each of these components and make it easy to switch out any component for another.\n- Be explicit: Make it transparent as to how these components can ‚Äútalk‚Äù to each other.\n- Be flexible: Make it possible to create custom components whenever custom behavior is desirable.\n- Be extensible: Provide a uniform and easy way for the community and third parties to build their own components and foster an open ecosystem around Haystack.\nAll components in Haystack 2.0 (including Haystack Integrations) are built with a common ‚Äúcomponent‚Äù interface. The principle is simple:\n- A component implements some logic in a method called\nrun\n- The\nrun\nmethod receives one or more input values - The\nrun\nmethod returns one or more output values\nTake embedders as an example: these components expect text as input and create vector representations (embeddings) that they return as output. On the other hand, retrievers may need embeddings as input and return documents as output. When creating a new component, to decide what inputs and outputs it should have is part of the ideation process.\nWhile there are many ready-made components built into Haystack, we want to highlight that building your own custom components is also a core functionality of Haystack 2.0.\nIn fact, we‚Äôve taken advantage of this ourselves. For example, you can read about how to use the latest optimization techniques (like HyDE) in Haystack pipelines with custom components.\n']","Haystack 2.0 introduces significant improvements over its predecessor, particularly in the area of customizable components and the ability to create loops in pipelines. One of the main enhancements is that the pipeline graph can now have cycles, allowing for more complex reasoning flows that can loop until a task is resolved. This is facilitated by the introduction of decision components and routers, which direct the execution flow based on input. Additionally, Haystack 2.0 emphasizes a technology-agnostic design, enabling users to choose and switch out components easily. It also promotes flexibility and extensibility, allowing for the creation of custom components and fostering an open ecosystem around Haystack.",multi_hop_specific_query_synthesizer
"What are the benefits of using Chroma in Haystack 2.0, and how does it relate to the customizable components and the common interface for storing data?","['<1-hop>\n\nThese include Chroma, Weaviate, Pinecone, Qdrant, Elasticsearch, Open Search, pgvector, MongoDB, AstraDB, Neo4j, Marqo DB, and the list will keep growing. And if your storage service is not supported yet, or should you need a high degree of customization on top of an existing one, by following our guide to creating custom document stores, you can connect your Haystack pipelines to your data from pretty much any storage service.\n A clear path to production\nThe experience we got over the last couple of years, working on Haystack 1.0 and interacting with its community, taught us two things:\n- It‚Äôs essential for any AI application framework to be feature-complete and developer-friendly.\n- It‚Äôs only after the deployment phase that AI-based applications can truly make an impact.\nWhile rewriting the framework from scratch, we took the opportunity to incorporate specific features that would simplify the deployment of Haystack-based AI applications in a production-grade environment:\n- A customizable logging system that supports structured logging and tracing correlation out of the box.\n- Code instrumentation collecting spans and traces in strategic points of the execution path, with support for Open Telemetry and Datadog already in place.\nIn addition, we decided to start a dedicated project to simplify deploying Haystack pipelines behind a RESTful API: Hayhooks.\nHayhooks is a client-server application that allows you to deploy Haystack pipelines, serving them through HTTP endpoints dynamically spawned. Two foundational features of Haystack 2.0 made this possible:\n- The ability to introspect a pipeline, determining its inputs and outputs at runtime. This means that every REST endpoint has well-defined, dynamically generated schemas for the request and response body, all depending on the specific pipeline structure.\n- A robust serialization mechanism. This allows for the conversion of Haystack pipelines from Python to a preferred data serialization format, and vice versa. The default format is YAML but Haystack is designed to easily extend support for additional serialization formats.\n Optimization and Evaluation of Retrieval Augmentation\nWe‚Äôve already been seeing the benefits of the new Haystack design, with pipeline optimization and evaluation being good examples of how we‚Äôve been leveraging Haystack 2.0. How?:\n- It‚Äôs easier to extend the capabilities of Haystack\n- It‚Äôs easy to implement new integrations\n  Evaluation of Retrieval Augmentation\nWe‚Äôve already been seeing the benefits of the new Haystack design, with pipeline optimization and evaluation being good examples of how we‚Äôve been leveraging Haystack 2.0. How?:\n- It‚Äôs easier to extend the capabilities of Haystack\n- It‚Äôs easy to implement new integrations\nImplementing the latest retrieval optimizations\nRetrieval is a crucial step for successful RAG pipelines. And there‚Äôs been a lot of work to optimize this step. With Haystack 2.0, we‚Äôve been able to:\n- Implement Hypothetical Document Embeddings (HyDE) easily, and we‚Äôve already published a guide to HyDE along with an example walkthrough\n- Added an integration for Optimum embedders by Hugging Face\nAnd we will be able to add more optimization techniques along the way!\nEvaluation\nHaystack 2.0 is being released with a few evaluation framework integrations in place:\nAlong with a guide to model-based evaluation.\n', '<2-hop>\n\nA common interface for storing data - A clear path to production - Optimization and Evaluation for Retrieval Augmentation Composable and customizable Pipelines Modern LLM applications comprise many moving parts: retrievers, rankers, LLMs, and many more such as entity extractors, summarizers, format converters and data cleaners. Each one of these ‚Äòsubtasks‚Äô is a component in Haystack. With the first version of Haystack we proved that pipelines are a good abstraction for connecting all those moving parts, but some of the assumptions we made in Haystack 1.0 dated back to a pre-LLM era and needed rethinking. One important limitation in Haystack 1.0 is that loops are not allowed, and the pipeline graph has to be acyclic. This makes it difficult to implement, for example, agents, which are often designed with a reasoning flow that loops until a task is resolved. In Haystack 2.0 the pipeline graph can have cycles. Combined with decision components (think about if-then-else clauses in the execution flow) and routers (components that direct the execution flow towards a specific subgraph depending on the input) this can be used to build sophisticated loops that model agentic behavior. Customizable Components We believe that the design of an AI framework should meet the following requirements: - Be technology agnostic: Allow users the flexibility to decide what vendor or technology they want for each of these components and make it easy to switch out any component for another. - Be explicit: Make it transparent as to how these components can ‚Äútalk‚Äù to each other. - Be flexible: Make it possible to create custom components whenever custom behavior is desirable. - Be extensible: Provide a uniform and easy way for the community and third parties to build their own components and foster an open ecosystem around Haystack. All components in Haystack 2.0 (including Haystack Integrations) are built with a common ‚Äúcomponent‚Äù interface. The principle is simple: - A component implements some logic in a method called run - The run method receives one or more input values - The run method returns one or more output values Take embedders as an example: these components expect text as input and create vector representations (embeddings) that they return as output. On the other hand, retrievers may need embeddings as input and return documents as output. When creating a new component, to decide what inputs and outputs it should have is part of the ideation process. While there are many ready-made components built into Haystack, we want to highlight that building your own custom components is also a core functionality of Haystack 2.0. In fact, we‚Äôve taken advantage of this ourselves. For example, you can read about how to use the latest optimization techniques (like HyDE) in Haystack pipelines with custom components. Sharing Custom Components Since the release of Haystack 2.0-Beta, we‚Äôve seen the benefits of having a well-defined simple interface for components. We, our community, and third parties have already created many components, available as additional packages for you to install. We share these on the Haystack Integrations page, which has expanded to include all sorts of components over the last few months (with contributions from Assembly AI, Jina AI, mixedbread ai and more). We will continue to expand this page with new integrations and you can help us by creating a PR on haystack-integrations if you‚Äôd like to share a component with the community. To learn more about integrations and how to share them, you can check out our ‚ÄúIntroduction to Integrations‚Äù documentation. A common interface for storing data Most NLP applications work on large amounts of data. A common design pattern is to connect your internal knowledge base to a Large Language Model (LLM) so that it can answer questions, summarize or translate documents, and extract specific information. For example, in retrieval-augment generative pipelines (RAG), you often use an LLM to answer questions about some data that was previously retrieved. This data has to come from somewhere, and Haystack 2.0 provides a common interface to access it in a consistent way, independently from where data comes from. This interface is called ‚ÄúDocument Store‚Äù, and it‚Äôs implemented for many different storage services, to make data easily available from within Haystack pipelines. Today, we are releasing Haystack 2.0 with a large selection of database and vector store integrations. These include Chroma, Weaviate, Pinecone, Qdrant, Elasticsearch, Open Search, pgvector, MongoDB, AstraDB, Neo4j, Marqo DB, and the list will keep growing. And if your storage service is not supported yet, or should you need a high degree of customization on top of an existing one, by following our guide to creating custom document stores, you can connect your Haystack pipelines to your']","Chroma is one of the many storage services integrated into Haystack 2.0, which provides a common interface for accessing data in a consistent manner, regardless of the source. The benefits of using Chroma include its ability to work seamlessly with Haystack pipelines, allowing for easy retrieval and management of data. Additionally, Haystack 2.0 emphasizes customizable components, enabling developers to create and integrate their own components into the framework. This flexibility allows users to tailor their AI applications to specific needs while leveraging the robust capabilities of Chroma and other storage services.",multi_hop_specific_query_synthesizer
"What limitations did Haystack 1.0 have that were addressed in Haystack 2.0, especially regarding the pipeline structure?","['<1-hop>\n\nHaystack 2.0: The Composable Open-Source LLM Framework\nMeet Haystack 2.0, a more flexible, customizable LLM framework\nMarch 11, 2024Today we are happy to announce the stable release of Haystack 2.0 - we‚Äôve been working on this for a while, and some of you have already been testing the beta since its first release in December 2023.\nHaystack is an open-source Python framework for building production-ready LLM applications, with integrations to almost all major model providers and databases.\nAt its core, Haystack 2.0 is a major rework of the previous version with a very clear goal in mind: making it possible to implement composable AI systems that are easy to use, customize, extend, optimise, evaluate and ultimately deploy to production.\nWe encourage you to start using Haystack 2.0 as of today, whether you‚Äôve been a Haystack user before or not. You can get started by installing haystack-ai\n, our new package for Haystack 2.0\n‚≠êÔ∏è To get started:\npip install haystack-ai\nand follow the get started instructions to build your first LLM app with just a few lines of code.\nIf you‚Äôre already using Haystack 1.0 in production, don‚Äôt worry! If your applications depend on farm-haystack\nand you‚Äôre not ready to migrate just yet, you don‚Äôt have to take any action: we will keep supporting Haystack 1.0, releasing security updates and critical bug fixes, giving everybody enough time to migrate. In the coming weeks, we will also start sharing some migration guides to help you along the way.\n Why Haystack 2.0?\nHaystack was first officially released in 2020, in the good old days when the forefront of NLP was semantic search, retrieval, and extractive question-answering. During this time, we established the core of what makes Haystack Haystack: Components and Pipelines. These allowed users to build end-to-end applications by combining their desired language models (embedding, extractive QA, ranking) with their database of choice.\nThe boom of LLMs in 2023 made two things clear:\n- üëç The pipeline-component structure is a great abstraction for building composable LLM applications with many moving parts.\n- üëé Haystack 1.0 often assumed that you would be doing retrieval and extractive QA over a set of documents, imposing limitations and providing a developer experience far from ideal when building LLM applications.\nSo, we decided that the best thing we could do for Haystack and our community was to rewrite the component and pipeline architecture to keep up with the fast-paced AI industry. While Haystack 2.0 is a complete rewrite, the underlying principle of composing components into flexible pipelines remains the same.\nWith that, let‚Äôs take a look at the pillars of Haystack 2.0:\n- Composable and customizable pipelines\n- A common interface for storing data\n- A clear path to production\n- Optimization and Evaluation for Retrieval Augmentation\n', '<2-hop>\n\nA common interface for storing data - A clear path to production - Optimization and Evaluation for Retrieval Augmentation Composable and customizable Pipelines Modern LLM applications comprise many moving parts: retrievers, rankers, LLMs, and many more such as entity extractors, summarizers, format converters and data cleaners. Each one of these ‚Äòsubtasks‚Äô is a component in Haystack. With the first version of Haystack we proved that pipelines are a good abstraction for connecting all those moving parts, but some of the assumptions we made in Haystack 1.0 dated back to a pre-LLM era and needed rethinking. One important limitation in Haystack 1.0 is that loops are not allowed, and the pipeline graph has to be acyclic. This makes it difficult to implement, for example, agents, which are often designed with a reasoning flow that loops until a task is resolved. In Haystack 2.0 the pipeline graph can have cycles. Combined with decision components (think about if-then-else clauses in the execution flow) and routers (components that direct the execution flow towards a specific subgraph depending on the input) this can be used to build sophisticated loops that model agentic behavior. Customizable Components We believe that the design of an AI framework should meet the following requirements: - Be technology agnostic: Allow users the flexibility to decide what vendor or technology they want for each of these components and make it easy to switch out any component for another. - Be explicit: Make it transparent as to how these components can ‚Äútalk‚Äù to each other. - Be flexible: Make it possible to create custom components whenever custom behavior is desirable. - Be extensible: Provide a uniform and easy way for the community and third parties to build their own components and foster an open ecosystem around Haystack. All components in Haystack 2.0 (including Haystack Integrations) are built with a common ‚Äúcomponent‚Äù interface. The principle is simple: - A component implements some logic in a method called run - The run method receives one or more input values - The run method returns one or more output values Take embedders as an example: these components expect text as input and create vector representations (embeddings) that they return as output. On the other hand, retrievers may need embeddings as input and return documents as output. When creating a new component, to decide what inputs and outputs it should have is part of the ideation process. While there are many ready-made components built into Haystack, we want to highlight that building your own custom components is also a core functionality of Haystack 2.0. In fact, we‚Äôve taken advantage of this ourselves. For example, you can read about how to use the latest optimization techniques (like HyDE) in Haystack pipelines with custom components. Sharing Custom Components Since the release of Haystack 2.0-Beta, we‚Äôve seen the benefits of having a well-defined simple interface for components. We, our community, and third parties have already created many components, available as additional packages for you to install. We share these on the Haystack Integrations page, which has expanded to include all sorts of components over the last few months (with contributions from Assembly AI, Jina AI, mixedbread ai and more). We will continue to expand this page with new integrations and you can help us by creating a PR on haystack-integrations if you‚Äôd like to share a component with the community. To learn more about integrations and how to share them, you can check out our ‚ÄúIntroduction to Integrations‚Äù documentation. A common interface for storing data Most NLP applications work on large amounts of data. A common design pattern is to connect your internal knowledge base to a Large Language Model (LLM) so that it can answer questions, summarize or translate documents, and extract specific information. For example, in retrieval-augment generative pipelines (RAG), you often use an LLM to answer questions about some data that was previously retrieved. This data has to come from somewhere, and Haystack 2.0 provides a common interface to access it in a consistent way, independently from where data comes from. This interface is called ‚ÄúDocument Store‚Äù, and it‚Äôs implemented for many different storage services, to make data easily available from within Haystack pipelines. Today, we are releasing Haystack 2.0 with a large selection of database and vector store integrations. These include Chroma, Weaviate, Pinecone, Qdrant, Elasticsearch, Open Search, pgvector, MongoDB, AstraDB, Neo4j, Marqo DB, and the list will keep growing. And if your storage service is not supported yet, or should you need a high degree of customization on top of an existing one, by following our guide to creating custom document stores, you can connect your Haystack pipelines to your']","Haystack 1.0 had limitations such as not allowing loops in the pipeline graph, which had to be acyclic. This restriction made it difficult to implement agents that require a reasoning flow with loops until a task is resolved. In contrast, Haystack 2.0 addressed this by allowing the pipeline graph to have cycles, enabling the creation of sophisticated loops and decision components that model agentic behavior.",multi_hop_specific_query_synthesizer
How Haystack 2.0 make user-friendly installation and setup while also being developer-friendly?,"['<1-hop>\n\nHaystack 2.0: The Composable Open-Source LLM Framework\nMeet Haystack 2.0, a more flexible, customizable LLM framework\nMarch 11, 2024Today we are happy to announce the stable release of Haystack 2.0 - we‚Äôve been working on this for a while, and some of you have already been testing the beta since its first release in December 2023.\nHaystack is an open-source Python framework for building production-ready LLM applications, with integrations to almost all major model providers and databases.\nAt its core, Haystack 2.0 is a major rework of the previous version with a very clear goal in mind: making it possible to implement composable AI systems that are easy to use, customize, extend, optimise, evaluate and ultimately deploy to production.\nWe encourage you to start using Haystack 2.0 as of today, whether you‚Äôve been a Haystack user before or not. You can get started by installing haystack-ai\n, our new package for Haystack 2.0\n‚≠êÔ∏è To get started:\npip install haystack-ai\nand follow the get started instructions to build your first LLM app with just a few lines of code.\nIf you‚Äôre already using Haystack 1.0 in production, don‚Äôt worry! If your applications depend on farm-haystack\nand you‚Äôre not ready to migrate just yet, you don‚Äôt have to take any action: we will keep supporting Haystack 1.0, releasing security updates and critical bug fixes, giving everybody enough time to migrate. In the coming weeks, we will also start sharing some migration guides to help you along the way.\n Why Haystack 2.0?\nHaystack was first officially released in 2020, in the good old days when the forefront of NLP was semantic search, retrieval, and extractive question-answering. During this time, we established the core of what makes Haystack Haystack: Components and Pipelines. These allowed users to build end-to-end applications by combining their desired language models (embedding, extractive QA, ranking) with their database of choice.\nThe boom of LLMs in 2023 made two things clear:\n- üëç The pipeline-component structure is a great abstraction for building composable LLM applications with many moving parts.\n- üëé Haystack 1.0 often assumed that you would be doing retrieval and extractive QA over a set of documents, imposing limitations and providing a developer experience far from ideal when building LLM applications.\nSo, we decided that the best thing we could do for Haystack and our community was to rewrite the component and pipeline architecture to keep up with the fast-paced AI industry. While Haystack 2.0 is a complete rewrite, the underlying principle of composing components into flexible pipelines remains the same.\nWith that, let‚Äôs take a look at the pillars of Haystack 2.0:\n- Composable and customizable pipelines\n- A common interface for storing data\n- A clear path to production\n- Optimization and Evaluation for Retrieval Augmentation\n', '<2-hop>\n\ndata from pretty much any storage service. A clear path to production The experience we got over the last couple of years, working on Haystack 1.0 and interacting with its community, taught us two things: - It‚Äôs essential for any AI application framework to be feature-complete and developer-friendly. - It‚Äôs only after the deployment phase that AI-based applications can truly make an impact. While rewriting the framework from scratch, we took the opportunity to incorporate specific features that would simplify the deployment of Haystack-based AI applications in a production-grade environment: - A customizable logging system that supports structured logging and tracing correlation out of the box.']","Haystack 2.0 makes user-friendly installation and setup possible by providing a simple command to install the new package, 'pip install haystack-ai', and offering clear get started instructions to build your first LLM app with just a few lines of code. Additionally, it is designed to be developer-friendly by incorporating a customizable logging system and a common interface for storing data, which simplifies the deployment of Haystack-based AI applications in a production-grade environment.",multi_hop_abstract_query_synthesizer
How does Haystack 2.0 facilitate the development of production-ready LLM applications while enabling agentic behavior through customizable pipelines?,"['<1-hop>\n\nHaystack 2.0: The Composable Open-Source LLM Framework\nMeet Haystack 2.0, a more flexible, customizable LLM framework\nMarch 11, 2024Today we are happy to announce the stable release of Haystack 2.0 - we‚Äôve been working on this for a while, and some of you have already been testing the beta since its first release in December 2023.\nHaystack is an open-source Python framework for building production-ready LLM applications, with integrations to almost all major model providers and databases.\nAt its core, Haystack 2.0 is a major rework of the previous version with a very clear goal in mind: making it possible to implement composable AI systems that are easy to use, customize, extend, optimise, evaluate and ultimately deploy to production.\nWe encourage you to start using Haystack 2.0 as of today, whether you‚Äôve been a Haystack user before or not. You can get started by installing haystack-ai\n, our new package for Haystack 2.0\n‚≠êÔ∏è To get started:\npip install haystack-ai\nand follow the get started instructions to build your first LLM app with just a few lines of code.\nIf you‚Äôre already using Haystack 1.0 in production, don‚Äôt worry! If your applications depend on farm-haystack\nand you‚Äôre not ready to migrate just yet, you don‚Äôt have to take any action: we will keep supporting Haystack 1.0, releasing security updates and critical bug fixes, giving everybody enough time to migrate. In the coming weeks, we will also start sharing some migration guides to help you along the way.\n Why Haystack 2.0?\nHaystack was first officially released in 2020, in the good old days when the forefront of NLP was semantic search, retrieval, and extractive question-answering. During this time, we established the core of what makes Haystack Haystack: Components and Pipelines. These allowed users to build end-to-end applications by combining their desired language models (embedding, extractive QA, ranking) with their database of choice.\nThe boom of LLMs in 2023 made two things clear:\n- üëç The pipeline-component structure is a great abstraction for building composable LLM applications with many moving parts.\n- üëé Haystack 1.0 often assumed that you would be doing retrieval and extractive QA over a set of documents, imposing limitations and providing a developer experience far from ideal when building LLM applications.\nSo, we decided that the best thing we could do for Haystack and our community was to rewrite the component and pipeline architecture to keep up with the fast-paced AI industry. While Haystack 2.0 is a complete rewrite, the underlying principle of composing components into flexible pipelines remains the same.\nWith that, let‚Äôs take a look at the pillars of Haystack 2.0:\n- Composable and customizable pipelines\n- A common interface for storing data\n- A clear path to production\n- Optimization and Evaluation for Retrieval Augmentation\n', '<2-hop>\n\nComposable and customizable Pipelines\nModern LLM applications comprise many moving parts: retrievers, rankers, LLMs, and many more such as entity extractors, summarizers, format converters and data cleaners. Each one of these ‚Äòsubtasks‚Äô is a component in Haystack.\nWith the first version of Haystack we proved that pipelines are a good abstraction for connecting all those moving parts, but some of the assumptions we made in Haystack 1.0 dated back to a pre-LLM era and needed rethinking.\nOne important limitation in Haystack 1.0 is that loops are not allowed, and the pipeline graph has to be acyclic. This makes it difficult to implement, for example, agents, which are often designed with a reasoning flow that loops until a task is resolved.\nIn Haystack 2.0 the pipeline graph can have cycles. Combined with decision components (think about if-then-else clauses in the execution flow) and routers (components that direct the execution flow towards a specific subgraph depending on the input) this can be used to build sophisticated loops that model agentic behavior.\n Customizable Components\nWe believe that the design of an AI framework should meet the following requirements:\n- Be technology agnostic: Allow users the flexibility to decide what vendor or technology they want for each of these components and make it easy to switch out any component for another.\n- Be explicit: Make it transparent as to how these components can ‚Äútalk‚Äù to each other.\n- Be flexible: Make it possible to create custom components whenever custom behavior is desirable.\n- Be extensible: Provide a uniform and easy way for the community and third parties to build their own components and foster an open ecosystem around Haystack.\nAll components in Haystack 2.0 (including Haystack Integrations) are built with a common ‚Äúcomponent‚Äù interface. The principle is simple:\n- A component implements some logic in a method called\nrun\n- The\nrun\nmethod receives one or more input values - The\nrun\nmethod returns one or more output values\nTake embedders as an example: these components expect text as input and create vector representations (embeddings) that they return as output. On the other hand, retrievers may need embeddings as input and return documents as output. When creating a new component, to decide what inputs and outputs it should have is part of the ideation process.\nWhile there are many ready-made components built into Haystack, we want to highlight that building your own custom components is also a core functionality of Haystack 2.0.\nIn fact, we‚Äôve taken advantage of this ourselves. For example, you can read about how to use the latest optimization techniques (like HyDE) in Haystack pipelines with custom components.\n']","Haystack 2.0 facilitates the development of production-ready LLM applications by providing a flexible and customizable framework that allows developers to build composable AI systems. The core of Haystack 2.0 is its ability to implement customizable pipelines that can integrate various components such as retrievers, rankers, and LLMs. This flexibility is crucial for adapting to the fast-paced AI industry. Additionally, Haystack 2.0 introduces the capability for pipeline graphs to have cycles, which is essential for modeling agentic behavior. This allows for the creation of sophisticated loops and decision components that can direct execution flow based on input, enabling more complex reasoning processes. Overall, Haystack 2.0 not only supports the creation of production-ready applications but also enhances the ability to implement agentic behavior through its innovative pipeline architecture.",multi_hop_abstract_query_synthesizer
What features in Haystack 2.0 contribute to its user-friendly installation and developer-friendly design for AI applications?,"['<1-hop>\n\nHaystack 2.0: The Composable Open-Source LLM Framework\nMeet Haystack 2.0, a more flexible, customizable LLM framework\nMarch 11, 2024Today we are happy to announce the stable release of Haystack 2.0 - we‚Äôve been working on this for a while, and some of you have already been testing the beta since its first release in December 2023.\nHaystack is an open-source Python framework for building production-ready LLM applications, with integrations to almost all major model providers and databases.\nAt its core, Haystack 2.0 is a major rework of the previous version with a very clear goal in mind: making it possible to implement composable AI systems that are easy to use, customize, extend, optimise, evaluate and ultimately deploy to production.\nWe encourage you to start using Haystack 2.0 as of today, whether you‚Äôve been a Haystack user before or not. You can get started by installing haystack-ai\n, our new package for Haystack 2.0\n‚≠êÔ∏è To get started:\npip install haystack-ai\nand follow the get started instructions to build your first LLM app with just a few lines of code.\nIf you‚Äôre already using Haystack 1.0 in production, don‚Äôt worry! If your applications depend on farm-haystack\nand you‚Äôre not ready to migrate just yet, you don‚Äôt have to take any action: we will keep supporting Haystack 1.0, releasing security updates and critical bug fixes, giving everybody enough time to migrate. In the coming weeks, we will also start sharing some migration guides to help you along the way.\n Why Haystack 2.0?\nHaystack was first officially released in 2020, in the good old days when the forefront of NLP was semantic search, retrieval, and extractive question-answering. During this time, we established the core of what makes Haystack Haystack: Components and Pipelines. These allowed users to build end-to-end applications by combining their desired language models (embedding, extractive QA, ranking) with their database of choice.\nThe boom of LLMs in 2023 made two things clear:\n- üëç The pipeline-component structure is a great abstraction for building composable LLM applications with many moving parts.\n- üëé Haystack 1.0 often assumed that you would be doing retrieval and extractive QA over a set of documents, imposing limitations and providing a developer experience far from ideal when building LLM applications.\nSo, we decided that the best thing we could do for Haystack and our community was to rewrite the component and pipeline architecture to keep up with the fast-paced AI industry. While Haystack 2.0 is a complete rewrite, the underlying principle of composing components into flexible pipelines remains the same.\nWith that, let‚Äôs take a look at the pillars of Haystack 2.0:\n- Composable and customizable pipelines\n- A common interface for storing data\n- A clear path to production\n- Optimization and Evaluation for Retrieval Augmentation\n', '<2-hop>\n\ndata from pretty much any storage service. A clear path to production The experience we got over the last couple of years, working on Haystack 1.0 and interacting with its community, taught us two things: - It‚Äôs essential for any AI application framework to be feature-complete and developer-friendly. - It‚Äôs only after the deployment phase that AI-based applications can truly make an impact. While rewriting the framework from scratch, we took the opportunity to incorporate specific features that would simplify the deployment of Haystack-based AI applications in a production-grade environment: - A customizable logging system that supports structured logging and tracing correlation out of the box.']","Haystack 2.0 offers a user-friendly installation process through the command 'pip install haystack-ai' and provides clear get started instructions to build LLM applications with just a few lines of code. Additionally, it emphasizes a developer-friendly design by incorporating a customizable logging system that supports structured logging and tracing correlation, ensuring that the framework is feature-complete and simplifies the deployment of AI applications in a production-grade environment.",multi_hop_abstract_query_synthesizer
"How does Haystack 2.0, as an open-source framework, facilitate the use of data storage services in building production-ready AI applications?","['<1-hop>\n\nHaystack 2.0: The Composable Open-Source LLM Framework\nMeet Haystack 2.0, a more flexible, customizable LLM framework\nMarch 11, 2024Today we are happy to announce the stable release of Haystack 2.0 - we‚Äôve been working on this for a while, and some of you have already been testing the beta since its first release in December 2023.\nHaystack is an open-source Python framework for building production-ready LLM applications, with integrations to almost all major model providers and databases.\nAt its core, Haystack 2.0 is a major rework of the previous version with a very clear goal in mind: making it possible to implement composable AI systems that are easy to use, customize, extend, optimise, evaluate and ultimately deploy to production.\nWe encourage you to start using Haystack 2.0 as of today, whether you‚Äôve been a Haystack user before or not. You can get started by installing haystack-ai\n, our new package for Haystack 2.0\n‚≠êÔ∏è To get started:\npip install haystack-ai\nand follow the get started instructions to build your first LLM app with just a few lines of code.\nIf you‚Äôre already using Haystack 1.0 in production, don‚Äôt worry! If your applications depend on farm-haystack\nand you‚Äôre not ready to migrate just yet, you don‚Äôt have to take any action: we will keep supporting Haystack 1.0, releasing security updates and critical bug fixes, giving everybody enough time to migrate. In the coming weeks, we will also start sharing some migration guides to help you along the way.\n Why Haystack 2.0?\nHaystack was first officially released in 2020, in the good old days when the forefront of NLP was semantic search, retrieval, and extractive question-answering. During this time, we established the core of what makes Haystack Haystack: Components and Pipelines. These allowed users to build end-to-end applications by combining their desired language models (embedding, extractive QA, ranking) with their database of choice.\nThe boom of LLMs in 2023 made two things clear:\n- üëç The pipeline-component structure is a great abstraction for building composable LLM applications with many moving parts.\n- üëé Haystack 1.0 often assumed that you would be doing retrieval and extractive QA over a set of documents, imposing limitations and providing a developer experience far from ideal when building LLM applications.\nSo, we decided that the best thing we could do for Haystack and our community was to rewrite the component and pipeline architecture to keep up with the fast-paced AI industry. While Haystack 2.0 is a complete rewrite, the underlying principle of composing components into flexible pipelines remains the same.\nWith that, let‚Äôs take a look at the pillars of Haystack 2.0:\n- Composable and customizable pipelines\n- A common interface for storing data\n- A clear path to production\n- Optimization and Evaluation for Retrieval Augmentation\n', '<2-hop>\n\ndata from pretty much any storage service. A clear path to production The experience we got over the last couple of years, working on Haystack 1.0 and interacting with its community, taught us two things: - It‚Äôs essential for any AI application framework to be feature-complete and developer-friendly. - It‚Äôs only after the deployment phase that AI-based applications can truly make an impact. While rewriting the framework from scratch, we took the opportunity to incorporate specific features that would simplify the deployment of Haystack-based AI applications in a production-grade environment: - A customizable logging system that supports structured logging and tracing correlation out of the box.']","Haystack 2.0 is an open-source Python framework designed for building production-ready LLM applications, which integrates with almost all major model providers and databases. It emphasizes a common interface for storing data, allowing developers to utilize data from various storage services seamlessly. This flexibility is crucial for creating composable and customizable pipelines that can be easily optimized and evaluated, ultimately simplifying the deployment of AI applications in a production-grade environment.",multi_hop_abstract_query_synthesizer
How can I create customizable pipelines in Haystack 2.0 to enhance my AI applications?,"['<1-hop>\n\nHaystack 2.0: The Composable Open-Source LLM Framework\nMeet Haystack 2.0, a more flexible, customizable LLM framework\nMarch 11, 2024Today we are happy to announce the stable release of Haystack 2.0 - we‚Äôve been working on this for a while, and some of you have already been testing the beta since its first release in December 2023.\nHaystack is an open-source Python framework for building production-ready LLM applications, with integrations to almost all major model providers and databases.\nAt its core, Haystack 2.0 is a major rework of the previous version with a very clear goal in mind: making it possible to implement composable AI systems that are easy to use, customize, extend, optimise, evaluate and ultimately deploy to production.\nWe encourage you to start using Haystack 2.0 as of today, whether you‚Äôve been a Haystack user before or not. You can get started by installing haystack-ai\n, our new package for Haystack 2.0\n‚≠êÔ∏è To get started:\npip install haystack-ai\nand follow the get started instructions to build your first LLM app with just a few lines of code.\nIf you‚Äôre already using Haystack 1.0 in production, don‚Äôt worry! If your applications depend on farm-haystack\nand you‚Äôre not ready to migrate just yet, you don‚Äôt have to take any action: we will keep supporting Haystack 1.0, releasing security updates and critical bug fixes, giving everybody enough time to migrate. In the coming weeks, we will also start sharing some migration guides to help you along the way.\n Why Haystack 2.0?\nHaystack was first officially released in 2020, in the good old days when the forefront of NLP was semantic search, retrieval, and extractive question-answering. During this time, we established the core of what makes Haystack Haystack: Components and Pipelines. These allowed users to build end-to-end applications by combining their desired language models (embedding, extractive QA, ranking) with their database of choice.\nThe boom of LLMs in 2023 made two things clear:\n- üëç The pipeline-component structure is a great abstraction for building composable LLM applications with many moving parts.\n- üëé Haystack 1.0 often assumed that you would be doing retrieval and extractive QA over a set of documents, imposing limitations and providing a developer experience far from ideal when building LLM applications.\nSo, we decided that the best thing we could do for Haystack and our community was to rewrite the component and pipeline architecture to keep up with the fast-paced AI industry. While Haystack 2.0 is a complete rewrite, the underlying principle of composing components into flexible pipelines remains the same.\nWith that, let‚Äôs take a look at the pillars of Haystack 2.0:\n- Composable and customizable pipelines\n- A common interface for storing data\n- A clear path to production\n- Optimization and Evaluation for Retrieval Augmentation\n', '<2-hop>\n\nComposable and customizable Pipelines\nModern LLM applications comprise many moving parts: retrievers, rankers, LLMs, and many more such as entity extractors, summarizers, format converters and data cleaners. Each one of these ‚Äòsubtasks‚Äô is a component in Haystack.\nWith the first version of Haystack we proved that pipelines are a good abstraction for connecting all those moving parts, but some of the assumptions we made in Haystack 1.0 dated back to a pre-LLM era and needed rethinking.\nOne important limitation in Haystack 1.0 is that loops are not allowed, and the pipeline graph has to be acyclic. This makes it difficult to implement, for example, agents, which are often designed with a reasoning flow that loops until a task is resolved.\nIn Haystack 2.0 the pipeline graph can have cycles. Combined with decision components (think about if-then-else clauses in the execution flow) and routers (components that direct the execution flow towards a specific subgraph depending on the input) this can be used to build sophisticated loops that model agentic behavior.\n Customizable Components\nWe believe that the design of an AI framework should meet the following requirements:\n- Be technology agnostic: Allow users the flexibility to decide what vendor or technology they want for each of these components and make it easy to switch out any component for another.\n- Be explicit: Make it transparent as to how these components can ‚Äútalk‚Äù to each other.\n- Be flexible: Make it possible to create custom components whenever custom behavior is desirable.\n- Be extensible: Provide a uniform and easy way for the community and third parties to build their own components and foster an open ecosystem around Haystack.\nAll components in Haystack 2.0 (including Haystack Integrations) are built with a common ‚Äúcomponent‚Äù interface. The principle is simple:\n- A component implements some logic in a method called\nrun\n- The\nrun\nmethod receives one or more input values - The\nrun\nmethod returns one or more output values\nTake embedders as an example: these components expect text as input and create vector representations (embeddings) that they return as output. On the other hand, retrievers may need embeddings as input and return documents as output. When creating a new component, to decide what inputs and outputs it should have is part of the ideation process.\nWhile there are many ready-made components built into Haystack, we want to highlight that building your own custom components is also a core functionality of Haystack 2.0.\nIn fact, we‚Äôve taken advantage of this ourselves. For example, you can read about how to use the latest optimization techniques (like HyDE) in Haystack pipelines with custom components.\n', '<3-hop>\n\nStart using Haystack 2.0\nAlongside Haystack 2.0, today we are also releasing a whole set of new tutorials, documentation, resources and more to help you get started:\n- Documentation: full technical documentation on all Haystack concepts and components\n- Tutorials: step-by-step, runnable Colab notebooks. Start with our first 2.0 tutorial ‚ÄúCreating Your First QA Pipeline with Retrieval-Augmentation‚Äù\n- Cookbooks: A collection of useful notebooks that showcase Haystack in various scenarios, using a number of our integrations.\nAnd, as always, keep an eye out on our blog and integrations for updates and new content.\nJoin the Community\nStay up-to-date with Haystack:']","To create customizable pipelines in Haystack 2.0, you can utilize the framework's flexible architecture that allows for the integration of various components such as retrievers, rankers, and LLMs. The new version supports cycles in the pipeline graph, enabling the implementation of sophisticated loops and decision components that can model agentic behavior. Additionally, you can build your own custom components by following the common 'component' interface, where each component implements logic in a method called 'run', which processes input values and returns output values. This flexibility allows you to tailor the pipelines to meet specific requirements of your AI applications.",multi_hop_abstract_query_synthesizer
