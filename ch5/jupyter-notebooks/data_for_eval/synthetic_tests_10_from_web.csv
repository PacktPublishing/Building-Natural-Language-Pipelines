user_input,reference_contexts,reference,synthesizer_name
Wut is a Document Store in data pipelines?,"['You can check them on the documentation page for a specific component or in the component\'s run()\nmethod. For more information, see Components: Input and Output.\n Steps to Create a Pipeline\n 1. Import dependencies\nImport all the dependencies, like pipeline, documents, Document Store, and all the components you want to use in your pipeline.\nFor example, to create a semantic document search pipelines, you need the Document\nobject, the pipeline, the Document Store, Embedders, and a Retriever:\nfrom haystack import Document, Pipeline\nfrom haystack.document_stores.in_memory import InMemoryDocumentStore\nfrom haystack.components.embedders import SentenceTransformersTextEmbedder\nfrom haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n 2. Initialize components\nInitialize the components, passing any parameters you want to configure:\ndocument_store = InMemoryDocumentStore(embedding_similarity_function=""cosine"")\ntext_embedder = SentenceTransformersTextEmbedder()\nretriever = InMemoryEmbeddingRetriever(document_store=document_store)\n']","A Document Store is a component used in data pipelines, specifically for managing and storing documents. It is essential for creating semantic document search pipelines, where it works alongside other components like the Document object, pipeline, Embedders, and a Retriever.",single_hop_specific_query_synthesizer
How is the llm component integrated into the data pipeline?,"['3. Create the pipeline\nquery_pipeline = Pipeline()\n4. Add components\nAdd components to the pipeline one by one. The order in which you do this doesn\'t matter:\nquery_pipeline.add_component(""component_name"", component_type)\n# Here is an example of how you\'d add the components initialized in step 2 above:\nquery_pipeline.add_component(""text_embedder"", text_embedder)\nquery_pipeline.add_component(""retriever"", retriever)\n# You could also add components without initializing them before:\nquery_pipeline.add_component(""text_embedder"", SentenceTransformersTextEmbedder())\nquery_pipeline.add_component(""retriever"", InMemoryEmbeddingRetriever(document_store=document_store))\n5. Connect components\nConnect the components by indicating which output of a component should be connected to the input of the next component. If a component has only one input or output and the connection is obvious, you can just pass the component name without specifying the input or output.\nTo understand what inputs are expected to run your pipeline, use an .inputs()\npipeline function. See a detailed examples in the Pipeline Inputs section below.\nHere\'s a more visual explanation within the code:\n# This is the syntax to connect components. Here you\'re connecting output1 of component1 to input1 of component2:\npipeline.connect(""component1.output1"", ""component2.input1"")\n# If both components have only one output and input, you can just pass their names:\npipeline.connect(""component1"", ""component2"")\n# If one of the components has only one output but the other has multiple inputs,\n# you can pass just the name of the component with a single output, but for the component with\n# multiple inputs, you must specify which input you want to connect\n# Here, component1 has only one output, but component2 has mulitiple inputs:\npipeline.connect(""component1"", ""component2.input1"")\n# And here\'s how it should look like for the semantic document search pipeline we\'re using as an example:\npipeline.connect(""text_embedder.embedding"", ""retriever.query_embedding"")\n# Because the InMemoryEmbeddingRetriever only has one input, this is also correct:\npipeline.connect(""text_embedder.embedding"", ""retriever"")\nYou need to link all the components together, connecting them gradually in pairs. Here\'s an explicit example for the pipeline we\'re assembling:\n# Imagine this pipeline has four components: text_embedder, retriever, prompt_builder and llm.\n# Here\'s how you would connect them into a pipeline:\nquery_pipeline.connect(""text_embedder.embedding"", ""retriever"")\nquery_pipeline.connect(""retriever"",""prompt_builder.documents"")\nquery_pipeline.connect(""prompt_builder"", ""llm"")\n']","The llm component is integrated into the data pipeline by connecting it to the prompt_builder component, which in turn is connected to the retriever component. The connections are established as follows: query_pipeline.connect(""prompt_builder"", ""llm"").",single_hop_specific_query_synthesizer
How do I use HTMLToDocument in my pipeline?,"['Pipeline.run()\ncan be called in two ways, either passing a dictionary of the component names and their inputs, or by directly passing just the inputs. When passed directly, the pipeline resolves inputs to the correct components.\n# Here\'s one way of calling the run() method\nresults = pipeline.run({""component1"": {""input1_value"": value1, ""input2_value"": value2}})\n# The inputs can also be passed directly without specifying component names\nresults = pipeline.run({""input1_value"": value1, ""input2_value"": value2})\n# This is how you\'d run the semantic document search pipeline we\'re using as an example:\nquery = ""Here comes the query text""\nresults = query_pipeline.run({""text_embedder"": {""text"": query}})\n Pipeline Inputs\nIf you need to understand what component inputs are expected to run your pipeline, Haystack features a useful pipeline function .inputs()\nthat lists all the required inputs for the components.\nThis is how it works:\n# A short pipeline example that converts webpages into documents\nfrom haystack import Pipeline\nfrom haystack.document_stores.in_memory import InMemoryDocumentStore\nfrom haystack.components.fetchers import LinkContentFetcher\nfrom haystack.components.converters import HTMLToDocument\nfrom haystack.components.writers import DocumentWriter\ndocument_store = InMemoryDocumentStore()\nfetcher = LinkContentFetcher()\nconverter = HTMLToDocument()\nwriter = DocumentWriter(document_store = document_store)\npipeline = Pipeline()\npipeline.add_component(instance=fetcher, name=""fetcher"")\npipeline.add_component(instance=converter, name=""converter"")\npipeline.add_component(instance=writer, name=""writer"")\npipeline.connect(""fetcher.streams"", ""converter.sources"")\npipeline.connect(""converter.documents"", ""writer.documents"")\n# Requesting a list of required inputs\npipeline.inputs()\n# {\'fetcher\': {\'urls\': {\'type\': typing.List[str], \'is_mandatory\': True}},\n# \'converter\': {\'meta\': {\'type\': typing.Union[typing.Dict[str, typing.Any], typing.List[typing.Dict[str, typing.Any]], NoneType],\n# \'is_mandatory\': False,\n# \'default_value\': None},\n# \'extraction_kwargs\': {\'type\': typing.Optional[typing.Dict[str, typing.Any]],\n# \'is_mandatory\': False,\n# \'default_value\': None}},\n# \'writer\': {\'policy\': {\'type\': typing.Optional[haystack.document_stores.types.policy.DuplicatePolicy],\n# \'is_mandatory\': False,\n# \'default_value\': None}}}\nFrom the above response, you can see that the urls\ninput is mandatory for LinkContentFetcher\n. This is how you would then run this pipeline:\npipeline.run(data=\n{""fetcher"":\n{""urls"": [""https://docs.haystack.deepset.ai/docs/pipelines""]}\n}\n)\n']","To use HTMLToDocument in your pipeline, you first need to import it from the haystack.components.converters module. Then, create an instance of HTMLToDocument and add it to your pipeline using the add_component method. For example, you can set up your pipeline as follows: converter = HTMLToDocument() and pipeline.add_component(instance=converter, name=""converter"").",single_hop_specific_query_synthesizer
What are the steps to run a pipeline using DocumentWriter and how do you specify the required inputs for the components?,"['<1-hop>\n\nPipeline.run()\ncan be called in two ways, either passing a dictionary of the component names and their inputs, or by directly passing just the inputs. When passed directly, the pipeline resolves inputs to the correct components.\n# Here\'s one way of calling the run() method\nresults = pipeline.run({""component1"": {""input1_value"": value1, ""input2_value"": value2}})\n# The inputs can also be passed directly without specifying component names\nresults = pipeline.run({""input1_value"": value1, ""input2_value"": value2})\n# This is how you\'d run the semantic document search pipeline we\'re using as an example:\nquery = ""Here comes the query text""\nresults = query_pipeline.run({""text_embedder"": {""text"": query}})\n Pipeline Inputs\nIf you need to understand what component inputs are expected to run your pipeline, Haystack features a useful pipeline function .inputs()\nthat lists all the required inputs for the components.\nThis is how it works:\n# A short pipeline example that converts webpages into documents\nfrom haystack import Pipeline\nfrom haystack.document_stores.in_memory import InMemoryDocumentStore\nfrom haystack.components.fetchers import LinkContentFetcher\nfrom haystack.components.converters import HTMLToDocument\nfrom haystack.components.writers import DocumentWriter\ndocument_store = InMemoryDocumentStore()\nfetcher = LinkContentFetcher()\nconverter = HTMLToDocument()\nwriter = DocumentWriter(document_store = document_store)\npipeline = Pipeline()\npipeline.add_component(instance=fetcher, name=""fetcher"")\npipeline.add_component(instance=converter, name=""converter"")\npipeline.add_component(instance=writer, name=""writer"")\npipeline.connect(""fetcher.streams"", ""converter.sources"")\npipeline.connect(""converter.documents"", ""writer.documents"")\n# Requesting a list of required inputs\npipeline.inputs()\n# {\'fetcher\': {\'urls\': {\'type\': typing.List[str], \'is_mandatory\': True}},\n# \'converter\': {\'meta\': {\'type\': typing.Union[typing.Dict[str, typing.Any], typing.List[typing.Dict[str, typing.Any]], NoneType],\n# \'is_mandatory\': False,\n# \'default_value\': None},\n# \'extraction_kwargs\': {\'type\': typing.Optional[typing.Dict[str, typing.Any]],\n# \'is_mandatory\': False,\n# \'default_value\': None}},\n# \'writer\': {\'policy\': {\'type\': typing.Optional[haystack.document_stores.types.policy.DuplicatePolicy],\n# \'is_mandatory\': False,\n# \'default_value\': None}}}\nFrom the above response, you can see that the urls\ninput is mandatory for LinkContentFetcher\n. This is how you would then run this pipeline:\npipeline.run(data=\n{""fetcher"":\n{""urls"": [""https://docs.haystack.deepset.ai/docs/pipelines""]}\n}\n)\n']","To run a pipeline using DocumentWriter, you first need to set up the pipeline by adding components such as LinkContentFetcher, HTMLToDocument, and DocumentWriter. You can do this by creating an instance of each component and then connecting them appropriately. For example, you would connect the fetcher's streams to the converter's sources and the converter's documents to the writer's documents. To specify the required inputs for the components, you can use the pipeline function .inputs(), which lists all the expected inputs. For instance, the urls input is mandatory for the LinkContentFetcher component.",multi_hop_specific_query_synthesizer
"What steps should be followed to create a pipeline that utilizes documents, and where can one find additional information about the components involved?","['<1-hop>\n\nYou can check them on the documentation page for a specific component or in the component\'s run()\nmethod. For more information, see Components: Input and Output.\n Steps to Create a Pipeline\n 1. Import dependencies\nImport all the dependencies, like pipeline, documents, Document Store, and all the components you want to use in your pipeline.\nFor example, to create a semantic document search pipelines, you need the Document\nobject, the pipeline, the Document Store, Embedders, and a Retriever:\nfrom haystack import Document, Pipeline\nfrom haystack.document_stores.in_memory import InMemoryDocumentStore\nfrom haystack.components.embedders import SentenceTransformersTextEmbedder\nfrom haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n 2. Initialize components\nInitialize the components, passing any parameters you want to configure:\ndocument_store = InMemoryDocumentStore(embedding_similarity_function=""cosine"")\ntext_embedder = SentenceTransformersTextEmbedder()\nretriever = InMemoryEmbeddingRetriever(document_store=document_store)\n']","To create a pipeline that utilizes documents, you should follow these steps: First, import all the necessary dependencies, including pipeline, documents, Document Store, and the components you want to use. For instance, to create a semantic document search pipeline, you need to import the Document object, the pipeline, the Document Store, Embedders, and a Retriever. The relevant code for this step includes importing from the haystack library. Second, initialize the components by passing any parameters you want to configure, such as creating an InMemoryDocumentStore with a specified embedding similarity function, initializing a SentenceTransformersTextEmbedder, and setting up an InMemoryEmbeddingRetriever with the document store. For more information about these components, you can check the documentation page for a specific component or refer to the component's run() method.",multi_hop_specific_query_synthesizer
What are the steps to initialize and connect the SentenceTransformersTextEmbedder in a data pipeline?,"['<1-hop>\n\nYou can check them on the documentation page for a specific component or in the component\'s run()\nmethod. For more information, see Components: Input and Output.\n Steps to Create a Pipeline\n 1. Import dependencies\nImport all the dependencies, like pipeline, documents, Document Store, and all the components you want to use in your pipeline.\nFor example, to create a semantic document search pipelines, you need the Document\nobject, the pipeline, the Document Store, Embedders, and a Retriever:\nfrom haystack import Document, Pipeline\nfrom haystack.document_stores.in_memory import InMemoryDocumentStore\nfrom haystack.components.embedders import SentenceTransformersTextEmbedder\nfrom haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n 2. Initialize components\nInitialize the components, passing any parameters you want to configure:\ndocument_store = InMemoryDocumentStore(embedding_similarity_function=""cosine"")\ntext_embedder = SentenceTransformersTextEmbedder()\nretriever = InMemoryEmbeddingRetriever(document_store=document_store)\n', '<2-hop>\n\n3. Create the pipeline\nquery_pipeline = Pipeline()\n4. Add components\nAdd components to the pipeline one by one. The order in which you do this doesn\'t matter:\nquery_pipeline.add_component(""component_name"", component_type)\n# Here is an example of how you\'d add the components initialized in step 2 above:\nquery_pipeline.add_component(""text_embedder"", text_embedder)\nquery_pipeline.add_component(""retriever"", retriever)\n# You could also add components without initializing them before:\nquery_pipeline.add_component(""text_embedder"", SentenceTransformersTextEmbedder())\nquery_pipeline.add_component(""retriever"", InMemoryEmbeddingRetriever(document_store=document_store))\n5. Connect components\nConnect the components by indicating which output of a component should be connected to the input of the next component. If a component has only one input or output and the connection is obvious, you can just pass the component name without specifying the input or output.\nTo understand what inputs are expected to run your pipeline, use an .inputs()\npipeline function. See a detailed examples in the Pipeline Inputs section below.\nHere\'s a more visual explanation within the code:\n# This is the syntax to connect components. Here you\'re connecting output1 of component1 to input1 of component2:\npipeline.connect(""component1.output1"", ""component2.input1"")\n# If both components have only one output and input, you can just pass their names:\npipeline.connect(""component1"", ""component2"")\n# If one of the components has only one output but the other has multiple inputs,\n# you can pass just the name of the component with a single output, but for the component with\n# multiple inputs, you must specify which input you want to connect\n# Here, component1 has only one output, but component2 has mulitiple inputs:\npipeline.connect(""component1"", ""component2.input1"")\n# And here\'s how it should look like for the semantic document search pipeline we\'re using as an example:\npipeline.connect(""text_embedder.embedding"", ""retriever.query_embedding"")\n# Because the InMemoryEmbeddingRetriever only has one input, this is also correct:\npipeline.connect(""text_embedder.embedding"", ""retriever"")\nYou need to link all the components together, connecting them gradually in pairs. Here\'s an explicit example for the pipeline we\'re assembling:\n# Imagine this pipeline has four components: text_embedder, retriever, prompt_builder and llm.\n# Here\'s how you would connect them into a pipeline:\nquery_pipeline.connect(""text_embedder.embedding"", ""retriever"")\nquery_pipeline.connect(""retriever"",""prompt_builder.documents"")\nquery_pipeline.connect(""prompt_builder"", ""llm"")\n']","To initialize the SentenceTransformersTextEmbedder in a data pipeline, first import the necessary dependencies, including the embedder. Then, initialize the embedder with the line: text_embedder = SentenceTransformersTextEmbedder(). After initializing the embedder, create the pipeline using query_pipeline = Pipeline(). Next, add the text embedder to the pipeline with the command: query_pipeline.add_component(""text_embedder"", text_embedder). Finally, connect the text embedder to other components in the pipeline, for example, by using query_pipeline.connect(""text_embedder.embedding"", ""retriever"").",multi_hop_specific_query_synthesizer
How do you connect components in a query pipeline that utilizes a document store?,"['<1-hop>\n\n3. Create the pipeline\nquery_pipeline = Pipeline()\n4. Add components\nAdd components to the pipeline one by one. The order in which you do this doesn\'t matter:\nquery_pipeline.add_component(""component_name"", component_type)\n# Here is an example of how you\'d add the components initialized in step 2 above:\nquery_pipeline.add_component(""text_embedder"", text_embedder)\nquery_pipeline.add_component(""retriever"", retriever)\n# You could also add components without initializing them before:\nquery_pipeline.add_component(""text_embedder"", SentenceTransformersTextEmbedder())\nquery_pipeline.add_component(""retriever"", InMemoryEmbeddingRetriever(document_store=document_store))\n5. Connect components\nConnect the components by indicating which output of a component should be connected to the input of the next component. If a component has only one input or output and the connection is obvious, you can just pass the component name without specifying the input or output.\nTo understand what inputs are expected to run your pipeline, use an .inputs()\npipeline function. See a detailed examples in the Pipeline Inputs section below.\nHere\'s a more visual explanation within the code:\n# This is the syntax to connect components. Here you\'re connecting output1 of component1 to input1 of component2:\npipeline.connect(""component1.output1"", ""component2.input1"")\n# If both components have only one output and input, you can just pass their names:\npipeline.connect(""component1"", ""component2"")\n# If one of the components has only one output but the other has multiple inputs,\n# you can pass just the name of the component with a single output, but for the component with\n# multiple inputs, you must specify which input you want to connect\n# Here, component1 has only one output, but component2 has mulitiple inputs:\npipeline.connect(""component1"", ""component2.input1"")\n# And here\'s how it should look like for the semantic document search pipeline we\'re using as an example:\npipeline.connect(""text_embedder.embedding"", ""retriever.query_embedding"")\n# Because the InMemoryEmbeddingRetriever only has one input, this is also correct:\npipeline.connect(""text_embedder.embedding"", ""retriever"")\nYou need to link all the components together, connecting them gradually in pairs. Here\'s an explicit example for the pipeline we\'re assembling:\n# Imagine this pipeline has four components: text_embedder, retriever, prompt_builder and llm.\n# Here\'s how you would connect them into a pipeline:\nquery_pipeline.connect(""text_embedder.embedding"", ""retriever"")\nquery_pipeline.connect(""retriever"",""prompt_builder.documents"")\nquery_pipeline.connect(""prompt_builder"", ""llm"")\n']","To connect components in a query pipeline that utilizes a document store, you would use the syntax 'pipeline.connect(""component1.output1"", ""component2.input1"")'. For example, in the semantic document search pipeline, you would connect the text embedder's output to the retriever's input by using 'query_pipeline.connect(""text_embedder.embedding"", ""retriever"")'. This ensures that the output from the text embedder is correctly passed to the retriever, which is essential for effective data processing and retrieval.",multi_hop_specific_query_synthesizer
How do you create a pipeline and what are the component inputs required for it?,"['<1-hop>\n\nPipeline.run()\ncan be called in two ways, either passing a dictionary of the component names and their inputs, or by directly passing just the inputs. When passed directly, the pipeline resolves inputs to the correct components.\n# Here\'s one way of calling the run() method\nresults = pipeline.run({""component1"": {""input1_value"": value1, ""input2_value"": value2}})\n# The inputs can also be passed directly without specifying component names\nresults = pipeline.run({""input1_value"": value1, ""input2_value"": value2})\n# This is how you\'d run the semantic document search pipeline we\'re using as an example:\nquery = ""Here comes the query text""\nresults = query_pipeline.run({""text_embedder"": {""text"": query}})\n Pipeline Inputs\nIf you need to understand what component inputs are expected to run your pipeline, Haystack features a useful pipeline function .inputs()\nthat lists all the required inputs for the components.\nThis is how it works:\n# A short pipeline example that converts webpages into documents\nfrom haystack import Pipeline\nfrom haystack.document_stores.in_memory import InMemoryDocumentStore\nfrom haystack.components.fetchers import LinkContentFetcher\nfrom haystack.components.converters import HTMLToDocument\nfrom haystack.components.writers import DocumentWriter\ndocument_store = InMemoryDocumentStore()\nfetcher = LinkContentFetcher()\nconverter = HTMLToDocument()\nwriter = DocumentWriter(document_store = document_store)\npipeline = Pipeline()\npipeline.add_component(instance=fetcher, name=""fetcher"")\npipeline.add_component(instance=converter, name=""converter"")\npipeline.add_component(instance=writer, name=""writer"")\npipeline.connect(""fetcher.streams"", ""converter.sources"")\npipeline.connect(""converter.documents"", ""writer.documents"")\n# Requesting a list of required inputs\npipeline.inputs()\n# {\'fetcher\': {\'urls\': {\'type\': typing.List[str], \'is_mandatory\': True}},\n# \'converter\': {\'meta\': {\'type\': typing.Union[typing.Dict[str, typing.Any], typing.List[typing.Dict[str, typing.Any]], NoneType],\n# \'is_mandatory\': False,\n# \'default_value\': None},\n# \'extraction_kwargs\': {\'type\': typing.Optional[typing.Dict[str, typing.Any]],\n# \'is_mandatory\': False,\n# \'default_value\': None}},\n# \'writer\': {\'policy\': {\'type\': typing.Optional[haystack.document_stores.types.policy.DuplicatePolicy],\n# \'is_mandatory\': False,\n# \'default_value\': None}}}\nFrom the above response, you can see that the urls\ninput is mandatory for LinkContentFetcher\n. This is how you would then run this pipeline:\npipeline.run(data=\n{""fetcher"":\n{""urls"": [""https://docs.haystack.deepset.ai/docs/pipelines""]}\n}\n)\n', '<2-hop>\n\nYou can check them on the documentation page for a specific component or in the component\'s run()\nmethod. For more information, see Components: Input and Output.\n Steps to Create a Pipeline\n 1. Import dependencies\nImport all the dependencies, like pipeline, documents, Document Store, and all the components you want to use in your pipeline.\nFor example, to create a semantic document search pipelines, you need the Document\nobject, the pipeline, the Document Store, Embedders, and a Retriever:\nfrom haystack import Document, Pipeline\nfrom haystack.document_stores.in_memory import InMemoryDocumentStore\nfrom haystack.components.embedders import SentenceTransformersTextEmbedder\nfrom haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n 2. Initialize components\nInitialize the components, passing any parameters you want to configure:\ndocument_store = InMemoryDocumentStore(embedding_similarity_function=""cosine"")\ntext_embedder = SentenceTransformersTextEmbedder()\nretriever = InMemoryEmbeddingRetriever(document_store=document_store)\n', '<3-hop>\n\n3. Create the pipeline\nquery_pipeline = Pipeline()\n4. Add components\nAdd components to the pipeline one by one. The order in which you do this doesn\'t matter:\nquery_pipeline.add_component(""component_name"", component_type)\n# Here is an example of how you\'d add the components initialized in step 2 above:\nquery_pipeline.add_component(""text_embedder"", text_embedder)\nquery_pipeline.add_component(""retriever"", retriever)\n# You could also add components without initializing them before:\nquery_pipeline.add_component(""text_embedder"", SentenceTransformersTextEmbedder())\nquery_pipeline.add_component(""retriever"", InMemoryEmbeddingRetriever(document_store=document_store))\n5. Connect components\nConnect the components by indicating which output of a component should be connected to the input of the next component. If a component has only one input or output and the connection is obvious, you can just pass the component name without specifying the input or output.\nTo understand what inputs are expected to run your pipeline, use an .inputs()\npipeline function. See a detailed examples in the Pipeline Inputs section below.\nHere\'s a more visual explanation within the code:\n# This is the syntax to connect components. Here you\'re connecting output1 of component1 to input1 of component2:\npipeline.connect(""component1.output1"", ""component2.input1"")\n# If both components have only one output and input, you can just pass their names:\npipeline.connect(""component1"", ""component2"")\n# If one of the components has only one output but the other has multiple inputs,\n# you can pass just the name of the component with a single output, but for the component with\n# multiple inputs, you must specify which input you want to connect\n# Here, component1 has only one output, but component2 has mulitiple inputs:\npipeline.connect(""component1"", ""component2.input1"")\n# And here\'s how it should look like for the semantic document search pipeline we\'re using as an example:\npipeline.connect(""text_embedder.embedding"", ""retriever.query_embedding"")\n# Because the InMemoryEmbeddingRetriever only has one input, this is also correct:\npipeline.connect(""text_embedder.embedding"", ""retriever"")\nYou need to link all the components together, connecting them gradually in pairs. Here\'s an explicit example for the pipeline we\'re assembling:\n# Imagine this pipeline has four components: text_embedder, retriever, prompt_builder and llm.\n# Here\'s how you would connect them into a pipeline:\nquery_pipeline.connect(""text_embedder.embedding"", ""retriever"")\nquery_pipeline.connect(""retriever"",""prompt_builder.documents"")\nquery_pipeline.connect(""prompt_builder"", ""llm"")\n']","To create a pipeline, you first need to import all the necessary dependencies, such as the pipeline, documents, Document Store, and the components you want to use. For example, to create a semantic document search pipeline, you would import the Document object, the pipeline, the Document Store, Embedders, and a Retriever. After importing, you initialize the components by passing any parameters you want to configure. For instance, you would create an InMemoryDocumentStore and initialize a text embedder and a retriever. Once the components are initialized, you create the pipeline and add the components one by one. The order of adding components does not matter. Finally, you connect the components by indicating which output of a component should be connected to the input of the next component. To understand what inputs are expected to run your pipeline, you can use the .inputs() function, which lists all the required inputs for the components.",multi_hop_abstract_query_synthesizer
How does the InMemoryDocumentStore relate to the validation process when running the pipeline?,"['<1-hop>\n\nPipeline.run()\ncan be called in two ways, either passing a dictionary of the component names and their inputs, or by directly passing just the inputs. When passed directly, the pipeline resolves inputs to the correct components.\n# Here\'s one way of calling the run() method\nresults = pipeline.run({""component1"": {""input1_value"": value1, ""input2_value"": value2}})\n# The inputs can also be passed directly without specifying component names\nresults = pipeline.run({""input1_value"": value1, ""input2_value"": value2})\n# This is how you\'d run the semantic document search pipeline we\'re using as an example:\nquery = ""Here comes the query text""\nresults = query_pipeline.run({""text_embedder"": {""text"": query}})\n Pipeline Inputs\nIf you need to understand what component inputs are expected to run your pipeline, Haystack features a useful pipeline function .inputs()\nthat lists all the required inputs for the components.\nThis is how it works:\n# A short pipeline example that converts webpages into documents\nfrom haystack import Pipeline\nfrom haystack.document_stores.in_memory import InMemoryDocumentStore\nfrom haystack.components.fetchers import LinkContentFetcher\nfrom haystack.components.converters import HTMLToDocument\nfrom haystack.components.writers import DocumentWriter\ndocument_store = InMemoryDocumentStore()\nfetcher = LinkContentFetcher()\nconverter = HTMLToDocument()\nwriter = DocumentWriter(document_store = document_store)\npipeline = Pipeline()\npipeline.add_component(instance=fetcher, name=""fetcher"")\npipeline.add_component(instance=converter, name=""converter"")\npipeline.add_component(instance=writer, name=""writer"")\npipeline.connect(""fetcher.streams"", ""converter.sources"")\npipeline.connect(""converter.documents"", ""writer.documents"")\n# Requesting a list of required inputs\npipeline.inputs()\n# {\'fetcher\': {\'urls\': {\'type\': typing.List[str], \'is_mandatory\': True}},\n# \'converter\': {\'meta\': {\'type\': typing.Union[typing.Dict[str, typing.Any], typing.List[typing.Dict[str, typing.Any]], NoneType],\n# \'is_mandatory\': False,\n# \'default_value\': None},\n# \'extraction_kwargs\': {\'type\': typing.Optional[typing.Dict[str, typing.Any]],\n# \'is_mandatory\': False,\n# \'default_value\': None}},\n# \'writer\': {\'policy\': {\'type\': typing.Optional[haystack.document_stores.types.policy.DuplicatePolicy],\n# \'is_mandatory\': False,\n# \'default_value\': None}}}\nFrom the above response, you can see that the urls\ninput is mandatory for LinkContentFetcher\n. This is how you would then run this pipeline:\npipeline.run(data=\n{""fetcher"":\n{""urls"": [""https://docs.haystack.deepset.ai/docs/pipelines""]}\n}\n)\n', '<2-hop>\n\n6. Run the pipeline\nWait for the pipeline to validate the components and connections. If everything is OK, you can now run the pipeline. Pipeline.run()\ncan be called in two ways, either passing a dictionary of the component names and their inputs, or by directly passing just the inputs. ']","The InMemoryDocumentStore is a component used in the pipeline that stores documents in memory. When running the pipeline, it is important to validate the components and their connections to ensure everything is functioning correctly. The validation process checks if the required inputs for each component, including the InMemoryDocumentStore, are provided before executing the pipeline.",multi_hop_abstract_query_synthesizer
How do you create a pipeline and what component inputs are necessary for its execution?,"['<1-hop>\n\nPipeline.run()\ncan be called in two ways, either passing a dictionary of the component names and their inputs, or by directly passing just the inputs. When passed directly, the pipeline resolves inputs to the correct components.\n# Here\'s one way of calling the run() method\nresults = pipeline.run({""component1"": {""input1_value"": value1, ""input2_value"": value2}})\n# The inputs can also be passed directly without specifying component names\nresults = pipeline.run({""input1_value"": value1, ""input2_value"": value2})\n# This is how you\'d run the semantic document search pipeline we\'re using as an example:\nquery = ""Here comes the query text""\nresults = query_pipeline.run({""text_embedder"": {""text"": query}})\n Pipeline Inputs\nIf you need to understand what component inputs are expected to run your pipeline, Haystack features a useful pipeline function .inputs()\nthat lists all the required inputs for the components.\nThis is how it works:\n# A short pipeline example that converts webpages into documents\nfrom haystack import Pipeline\nfrom haystack.document_stores.in_memory import InMemoryDocumentStore\nfrom haystack.components.fetchers import LinkContentFetcher\nfrom haystack.components.converters import HTMLToDocument\nfrom haystack.components.writers import DocumentWriter\ndocument_store = InMemoryDocumentStore()\nfetcher = LinkContentFetcher()\nconverter = HTMLToDocument()\nwriter = DocumentWriter(document_store = document_store)\npipeline = Pipeline()\npipeline.add_component(instance=fetcher, name=""fetcher"")\npipeline.add_component(instance=converter, name=""converter"")\npipeline.add_component(instance=writer, name=""writer"")\npipeline.connect(""fetcher.streams"", ""converter.sources"")\npipeline.connect(""converter.documents"", ""writer.documents"")\n# Requesting a list of required inputs\npipeline.inputs()\n# {\'fetcher\': {\'urls\': {\'type\': typing.List[str], \'is_mandatory\': True}},\n# \'converter\': {\'meta\': {\'type\': typing.Union[typing.Dict[str, typing.Any], typing.List[typing.Dict[str, typing.Any]], NoneType],\n# \'is_mandatory\': False,\n# \'default_value\': None},\n# \'extraction_kwargs\': {\'type\': typing.Optional[typing.Dict[str, typing.Any]],\n# \'is_mandatory\': False,\n# \'default_value\': None}},\n# \'writer\': {\'policy\': {\'type\': typing.Optional[haystack.document_stores.types.policy.DuplicatePolicy],\n# \'is_mandatory\': False,\n# \'default_value\': None}}}\nFrom the above response, you can see that the urls\ninput is mandatory for LinkContentFetcher\n. This is how you would then run this pipeline:\npipeline.run(data=\n{""fetcher"":\n{""urls"": [""https://docs.haystack.deepset.ai/docs/pipelines""]}\n}\n)\n', '<2-hop>\n\nYou can check them on the documentation page for a specific component or in the component\'s run()\nmethod. For more information, see Components: Input and Output.\n Steps to Create a Pipeline\n 1. Import dependencies\nImport all the dependencies, like pipeline, documents, Document Store, and all the components you want to use in your pipeline.\nFor example, to create a semantic document search pipelines, you need the Document\nobject, the pipeline, the Document Store, Embedders, and a Retriever:\nfrom haystack import Document, Pipeline\nfrom haystack.document_stores.in_memory import InMemoryDocumentStore\nfrom haystack.components.embedders import SentenceTransformersTextEmbedder\nfrom haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n 2. Initialize components\nInitialize the components, passing any parameters you want to configure:\ndocument_store = InMemoryDocumentStore(embedding_similarity_function=""cosine"")\ntext_embedder = SentenceTransformersTextEmbedder()\nretriever = InMemoryEmbeddingRetriever(document_store=document_store)\n', '<3-hop>\n\n3. Create the pipeline\nquery_pipeline = Pipeline()\n4. Add components\nAdd components to the pipeline one by one. The order in which you do this doesn\'t matter:\nquery_pipeline.add_component(""component_name"", component_type)\n# Here is an example of how you\'d add the components initialized in step 2 above:\nquery_pipeline.add_component(""text_embedder"", text_embedder)\nquery_pipeline.add_component(""retriever"", retriever)\n# You could also add components without initializing them before:\nquery_pipeline.add_component(""text_embedder"", SentenceTransformersTextEmbedder())\nquery_pipeline.add_component(""retriever"", InMemoryEmbeddingRetriever(document_store=document_store))\n5. Connect components\nConnect the components by indicating which output of a component should be connected to the input of the next component. If a component has only one input or output and the connection is obvious, you can just pass the component name without specifying the input or output.\nTo understand what inputs are expected to run your pipeline, use an .inputs()\npipeline function. See a detailed examples in the Pipeline Inputs section below.\nHere\'s a more visual explanation within the code:\n# This is the syntax to connect components. Here you\'re connecting output1 of component1 to input1 of component2:\npipeline.connect(""component1.output1"", ""component2.input1"")\n# If both components have only one output and input, you can just pass their names:\npipeline.connect(""component1"", ""component2"")\n# If one of the components has only one output but the other has multiple inputs,\n# you can pass just the name of the component with a single output, but for the component with\n# multiple inputs, you must specify which input you want to connect\n# Here, component1 has only one output, but component2 has mulitiple inputs:\npipeline.connect(""component1"", ""component2.input1"")\n# And here\'s how it should look like for the semantic document search pipeline we\'re using as an example:\npipeline.connect(""text_embedder.embedding"", ""retriever.query_embedding"")\n# Because the InMemoryEmbeddingRetriever only has one input, this is also correct:\npipeline.connect(""text_embedder.embedding"", ""retriever"")\nYou need to link all the components together, connecting them gradually in pairs. Here\'s an explicit example for the pipeline we\'re assembling:\n# Imagine this pipeline has four components: text_embedder, retriever, prompt_builder and llm.\n# Here\'s how you would connect them into a pipeline:\nquery_pipeline.connect(""text_embedder.embedding"", ""retriever"")\nquery_pipeline.connect(""retriever"",""prompt_builder.documents"")\nquery_pipeline.connect(""prompt_builder"", ""llm"")\n']","To create a pipeline, you first need to import all the necessary dependencies, such as the pipeline, documents, Document Store, and the components you want to use. For example, to create a semantic document search pipeline, you would need to import the Document object, the pipeline, the Document Store, Embedders, and a Retriever. After importing, you initialize the components by passing any parameters you want to configure. For instance, you would initialize an InMemoryDocumentStore and a SentenceTransformersTextEmbedder. Once the components are initialized, you create the pipeline and add the components one by one. The order of adding components does not matter. Finally, you connect the components by indicating which output of a component should be connected to the input of the next component. To understand what inputs are expected to run your pipeline, you can use the .inputs() function, which lists all the required inputs for the components.",multi_hop_abstract_query_synthesizer
What steps are involved in validating the components of a pipeline that uses an InMemoryDocumentStore before running it?,"['<1-hop>\n\nPipeline.run()\ncan be called in two ways, either passing a dictionary of the component names and their inputs, or by directly passing just the inputs. When passed directly, the pipeline resolves inputs to the correct components.\n# Here\'s one way of calling the run() method\nresults = pipeline.run({""component1"": {""input1_value"": value1, ""input2_value"": value2}})\n# The inputs can also be passed directly without specifying component names\nresults = pipeline.run({""input1_value"": value1, ""input2_value"": value2})\n# This is how you\'d run the semantic document search pipeline we\'re using as an example:\nquery = ""Here comes the query text""\nresults = query_pipeline.run({""text_embedder"": {""text"": query}})\n Pipeline Inputs\nIf you need to understand what component inputs are expected to run your pipeline, Haystack features a useful pipeline function .inputs()\nthat lists all the required inputs for the components.\nThis is how it works:\n# A short pipeline example that converts webpages into documents\nfrom haystack import Pipeline\nfrom haystack.document_stores.in_memory import InMemoryDocumentStore\nfrom haystack.components.fetchers import LinkContentFetcher\nfrom haystack.components.converters import HTMLToDocument\nfrom haystack.components.writers import DocumentWriter\ndocument_store = InMemoryDocumentStore()\nfetcher = LinkContentFetcher()\nconverter = HTMLToDocument()\nwriter = DocumentWriter(document_store = document_store)\npipeline = Pipeline()\npipeline.add_component(instance=fetcher, name=""fetcher"")\npipeline.add_component(instance=converter, name=""converter"")\npipeline.add_component(instance=writer, name=""writer"")\npipeline.connect(""fetcher.streams"", ""converter.sources"")\npipeline.connect(""converter.documents"", ""writer.documents"")\n# Requesting a list of required inputs\npipeline.inputs()\n# {\'fetcher\': {\'urls\': {\'type\': typing.List[str], \'is_mandatory\': True}},\n# \'converter\': {\'meta\': {\'type\': typing.Union[typing.Dict[str, typing.Any], typing.List[typing.Dict[str, typing.Any]], NoneType],\n# \'is_mandatory\': False,\n# \'default_value\': None},\n# \'extraction_kwargs\': {\'type\': typing.Optional[typing.Dict[str, typing.Any]],\n# \'is_mandatory\': False,\n# \'default_value\': None}},\n# \'writer\': {\'policy\': {\'type\': typing.Optional[haystack.document_stores.types.policy.DuplicatePolicy],\n# \'is_mandatory\': False,\n# \'default_value\': None}}}\nFrom the above response, you can see that the urls\ninput is mandatory for LinkContentFetcher\n. This is how you would then run this pipeline:\npipeline.run(data=\n{""fetcher"":\n{""urls"": [""https://docs.haystack.deepset.ai/docs/pipelines""]}\n}\n)\n', '<2-hop>\n\n6. Run the pipeline\nWait for the pipeline to validate the components and connections. If everything is OK, you can now run the pipeline. Pipeline.run()\ncan be called in two ways, either passing a dictionary of the component names and their inputs, or by directly passing just the inputs. ']","To validate the components of a pipeline that uses an InMemoryDocumentStore before running it, you first need to ensure that all components are correctly connected and that the required inputs for each component are specified. You can use the pipeline function .inputs() to list all the required inputs for the components. Once you have confirmed that the inputs are correct and all components are connected properly, you can then run the pipeline using Pipeline.run(). The validation process checks if everything is OK before executing the pipeline.",multi_hop_abstract_query_synthesizer
