user_input,reference_contexts,reference,synthesizer_name
What role does OpenAI play in the development of ChatGPT?,"['NBER WORKING PAPER SERIES\nHOW PEOPLE USE CHATGPT\nAaron Chatterji\nThomas Cunningham\nDavid J. Deming\nZoe Hitzig\nChristopher Ong\nCarl Yan Shan\nKevin Wadman\nWorking Paper 34255\nhttp://www.nber.org/papers/w34255\nNATIONAL BUREAU OF ECONOMIC RESEARCH\n1050 Massachusetts Avenue\nCambridge, MA 02138\nSeptember 2025\nWe acknowledge help and comments from Joshua Achiam, Hemanth Asirvatham, Ryan Beiermeister, Rachel Brown, Cassandra Duchan Solis, Jason Kwon, Elliott Mokski, Kevin Rao, Harrison Satcher, Gawesha Weeratunga, Hannah Wong, and Analytics & Insights team. We especially thank Tyna Eloundou and Pamela Mishkin who in several ways laid the foundation for this work. This study was approved by Harvard IRB (IRB25-0983). A repository containing all code run to produce the analyses in this paper is available on request. The views expressed herein are those of the authors and do not necessarily reflect the views of the National Bureau of Economic Research.\nAt least one co-author has disclosed additional relationships of potential relevance for this research. Further information is available online at http://www.nber.org/papers/w34255\nNBER working papers are circulated for discussion and comment purposes. They have not been peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies official NBER publications.\n© 2025 by Aaron Chatterji, Thomas Cunningham, David J. Deming, Zoe Hitzig, Christopher Ong, Carl Yan Shan, and Kevin Wadman. All rights reserved. Short sections of text, not to exceed two paragraphs, may be quoted without explicit permission provided that full credit, including © notice, is given to the source.How People Use ChatGPT\nAaron Chatterji, Thomas Cunningham, David J. Deming, Zoe Hitzig, Christopher Ong, Carl\nYan Shan, and Kevin Wadman\nNBER Working Paper No. 34255\nSeptember 2025\nJEL No. J01, O3, O4\nABSTRACT\nDespite the rapid adoption of LLM chatbots, little is known about how they are used. We document the growth of ChatGPT’s consumer product from its launch in November 2022 through July 2025, when it had been adopted by around 10% of the world’s adult population. Early adopters were disproportionately male but the gender gap has narrowed dramatically, and we find higher growth rates in lower-income countries. Using a privacy-preserving automated pipeline, we classify usage patterns within a representative sample of ChatGPT conversations. We find steady growth in work-related messages bu t even faster growth in non-work-related messages, which have grown from 53% to more than 70% of all usage. Work usage is more common for educated users in highly-paid professional occupations. We classify messages by conversation top\nic and find that “Practical Guidance,” “Seeking Information,” and “Writing” are the three most common topics and collectively account for nearly 80% of all conversations. Writing dominates work-related tasks, highlighting chatbots’ unique ability to generate digital outputs compared to traditional sear ch engines. Computer programming and self-expression both represent relatively small shares of use. Overall, we find that ChatGPT provides economic value through decision support, which is especially important in knowledge-intensive jobs.\nAaron Chatterji\nDuke University\nFuqua School of Business and OpenAI\nronnie@duke.edu\nThomas Cunningham OpenAI\ntom.cunningham@gmail.com\nDavid J. Deming\nHarvard University\nHarvard Kennedy School and NBER\ndavid_deming@harvard.edu\nZoe Hitzig\nOpenAI\nand Harvard Society of Fellows\nzhitzig@g.harvard.edu\nChristopher Ong\nHarvard University\nand OpenAI\nchristopherong@hks.harvard.edu\nCarl Yan Shan\nOpenAI\ncshan@openai.com\nKevin Wadman\nOpenAI\nkevin.wadman@c-openai.com']","OpenAI is involved in the development of ChatGPT, as indicated by the affiliations of several authors of the working paper, including Thomas Cunningham, Zoe Hitzig, Christopher Ong, Carl Yan Shan, and Kevin Wadman, who are associated with OpenAI.",single_hop_specific_query_synthesizer
What is the connection between Duke University and the study of ChatGPT usage?,"['ABSTRACT Despite the rapid adoption of LLM chatbots, little is known about how they are used. We document the growth of ChatGPT’s consumer product from its launch in November 2022 through July 2025, when it had been adopted by around 10% of the world’s adult population. Early adopters were disproportionately male but the gender gap has narrowed dramatically, and we find higher growth rates in lower-income countries. Using a privacy-preserving automated pipeline, we classify usage patterns within a representative sample of ChatGPT conversations. We find steady growth in work-related messages bu t even faster growth in non-work-related messages, which have grown from 53% to more than 70% of all usage. Work usage is more common for educated users in highly-paid professional occupations. We classify messages by conversation top ic and find that “Practical Guidance,” “Seeking Information,” and “Writing” are the three most common topics and collectively account for nearly 80% of all conversations. Writing dominates work-related tasks, highlighting chatbots’ unique ability to generate digital outputs compared to traditional sear ch engines. Computer programming and self-expression both represent relatively small shares of use. Overall, we find that ChatGPT provides economic value through decision support, which is especially important in knowledge-intensive jobs. Aaron Chatterji Duke University Fuqua School of Business and OpenAI ronnie@duke.edu Thomas Cunningham OpenAI tom.cunningham@gmail.com David J. Deming Harvard University Harvard Kennedy School and NBER david_deming@harvard.edu Zoe Hitzig OpenAI and Harvard Society of Fellows zhitzig@g.harvard.edu Christopher Ong Harvard University and OpenAI christopherong@hks.harvard.edu Carl Yan Shan OpenAI cshan@openai.com Kevin Wadman OpenAI kevin.wadman@c-openai.com1 Introduction ChatGPT launched in November 2022. By July 2025, 18 billion messages were being sent each week by 700 million users, representing around 10% of the global adult population. 1 For a new technology, this speed of global diffusion has no precedent (Bick et al., 2024). This paper studies consumer usage of ChatGPT, the first mass-market chatbot and likely the largest.2 ChatGPT is based on a Large Language Model (LLM), a type of Artificial Intelligence (AI) developed over the last decade and generally considered to represent an acceleration in AI capabilities.3 The sudden growth in LLM abilities and adoption has intensified interest in the effects of artificial intelligence on economic growth (Acemoglu, 2024; Korinek and Suh, 2024); employment (Eloundou et al., 2025); and society (Kulveit et al., 2025). However, despite the rapid adoption of LLMs, there is limited public information on how they are used. A number of surveys have measured self-reported adoption of LLMs (Bick et al., 2024; Pew Research Center, 2025); however there are reasons to expect bias in self-reports (Ling and Imas, 2025), and none of these papers have been able to directly track the quantity or nature of chatbot conversations. Two recent papers do report statistics on chatbot conversations, classified in a variety of ways (Handa et al., 2025; Tomlinson et al., 2025). We build on this work in several respects. First, the pool of users on ChatGPT is far larger, meaning we expect our data to be a closer approximation to the average chatbot user.4 Second, we use automated classifiers to report on the types of messages that users send using new classification taxonomies relative to the existing literature. Third, we report the diffusion of chatbot use across populations and the growth of different types of usage within cohorts. Fourth, we use a secure data clean room protocol to analyze aggregated employment and education categories for a sample of our users, lending new insights about differences in the types of messages sent by different groups while protecting user privacy. Our primary sample is a random selection of messages sent to ChatGPT on consumer plans (Free, Plus, Pro) between May 2024 and June 2025. 5 Messages from the user to chatbot are classified automatically using a number of different taxonomies: whether the message is used for paid work, the topic of conversation, and the type of interaction (asking, doing, or expressing), and the O*NET task the user is performing. Each taxonomy is defined in a prompt passed to an LLM, allowing us to classify messages without']",Aaron Chatterji from Duke University Fuqua School of Business is one of the authors of the paper studying consumer usage of ChatGPT.,single_hop_specific_query_synthesizer
How does the usage of Claude compare to ChatGPT in terms of visitor numbers as reported in the context?,"['any human seeing them. We give the text of most prompts in Appendix A along with details about how the prompts were validated in Appendix B.6 The classification pipeline is protected by a series of privacy measures, detailed below, to ensure no leakage of sensitive information during the automated analysis. In a secure data clean room, we relate taxonomies of messages to aggregated employment and education categories. Table 1 shows the growth in total message volume for work and non-work usage. Both types of 1Reuters (2025), Roth (2025) 2Bick et al. (2024) report that 28% of US adults used ChatGPT in late 2024, higher than any other chatbot. 3We use the term LLM loosely here and give more details in the following section. 4Wiggers (2025) reports estimates that in April 2025 ChatGPT was receiving more than 10 times as many visitors as either Claude or Copilot. 5Our sample includes the three consumer plans (Free, Plus, or Pro). OpenAI also offers a variety of other ChatGPT plans (Business fka. Teams, Enterprise, Education), which we do not include in our sample. 6Our classifiers take into account not just the randomly-selected user message, but also a portion of the preceding messages in that conversation. 1Month Non-Work (M)(%)Work (M)(%)Total Messages (M) Jun 2024 238 53% 213 47% 451 Jun 2025 1,911 73% 716 27% 2,627 Table 1:ChatGPT daily message counts (millions), broken down by likely work-related or non-work-related. Total daily counts are exact measurements of message volume from all consumer plans. Daily counts of work and non-work related messages are estimated by classifying a random sample of conversations from that day. Sampling is done to exclude users who opt-out of sharing their messages for model training, users who self- report their age as under 18, logged-out users, deleted conversations, and accounts which have been deactivated or banned (details available in Section 3).']","According to the context, Wiggers (2025) reports estimates that in April 2025, ChatGPT was receiving more than 10 times as many visitors as either Claude or Copilot.",single_hop_specific_query_synthesizer
"What insights does the research from Harvard University provide regarding the usage patterns of ChatGPT among different demographics, particularly in relation to work-related and non-work-related messages?","['<1-hop>\n\nABSTRACT Despite the rapid adoption of LLM chatbots, little is known about how they are used. We document the growth of ChatGPT’s consumer product from its launch in November 2022 through July 2025, when it had been adopted by around 10% of the world’s adult population. Early adopters were disproportionately male but the gender gap has narrowed dramatically, and we find higher growth rates in lower-income countries. Using a privacy-preserving automated pipeline, we classify usage patterns within a representative sample of ChatGPT conversations. We find steady growth in work-related messages bu t even faster growth in non-work-related messages, which have grown from 53% to more than 70% of all usage. Work usage is more common for educated users in highly-paid professional occupations. We classify messages by conversation top ic and find that “Practical Guidance,” “Seeking Information,” and “Writing” are the three most common topics and collectively account for nearly 80% of all conversations. Writing dominates work-related tasks, highlighting chatbots’ unique ability to generate digital outputs compared to traditional sear ch engines. Computer programming and self-expression both represent relatively small shares of use. Overall, we find that ChatGPT provides economic value through decision support, which is especially important in knowledge-intensive jobs. Aaron Chatterji Duke University Fuqua School of Business and OpenAI ronnie@duke.edu Thomas Cunningham OpenAI tom.cunningham@gmail.com David J. Deming Harvard University Harvard Kennedy School and NBER david_deming@harvard.edu Zoe Hitzig OpenAI and Harvard Society of Fellows zhitzig@g.harvard.edu Christopher Ong Harvard University and OpenAI christopherong@hks.harvard.edu Carl Yan Shan OpenAI cshan@openai.com Kevin Wadman OpenAI kevin.wadman@c-openai.com1 Introduction ChatGPT launched in November 2022. By July 2025, 18 billion messages were being sent each week by 700 million users, representing around 10% of the global adult population. 1 For a new technology, this speed of global diffusion has no precedent (Bick et al., 2024). This paper studies consumer usage of ChatGPT, the first mass-market chatbot and likely the largest.2 ChatGPT is based on a Large Language Model (LLM), a type of Artificial Intelligence (AI) developed over the last decade and generally considered to represent an acceleration in AI capabilities.3 The sudden growth in LLM abilities and adoption has intensified interest in the effects of artificial intelligence on economic growth (Acemoglu, 2024; Korinek and Suh, 2024); employment (Eloundou et al., 2025); and society (Kulveit et al., 2025). However, despite the rapid adoption of LLMs, there is limited public information on how they are used. A number of surveys have measured self-reported adoption of LLMs (Bick et al., 2024; Pew Research Center, 2025); however there are reasons to expect bias in self-reports (Ling and Imas, 2025), and none of these papers have been able to directly track the quantity or nature of chatbot conversations. Two recent papers do report statistics on chatbot conversations, classified in a variety of ways (Handa et al., 2025; Tomlinson et al., 2025). We build on this work in several respects. First, the pool of users on ChatGPT is far larger, meaning we expect our data to be a closer approximation to the average chatbot user.4 Second, we use automated classifiers to report on the types of messages that users send using new classification taxonomies relative to the existing literature. Third, we report the diffusion of chatbot use across populations and the growth of different types of usage within cohorts. Fourth, we use a secure data clean room protocol to analyze aggregated employment and education categories for a sample of our users, lending new insights about differences in the types of messages sent by different groups while protecting user privacy. Our primary sample is a random selection of messages sent to ChatGPT on consumer plans (Free, Plus, Pro) between May 2024 and June 2025. 5 Messages from the user to chatbot are classified automatically using a number of different taxonomies: whether the message is used for paid work, the topic of conversation, and the type of interaction (asking, doing, or expressing), and the O*NET task the user is performing. Each taxonomy is defined in a prompt passed to an LLM, allowing us to classify messages without']","The research from Harvard University highlights that despite the rapid adoption of ChatGPT, there is limited public information on its usage patterns. The study documents that by July 2025, ChatGPT had been adopted by around 10% of the global adult population, with a notable increase in non-work-related messages, which grew from 53% to more than 70% of all usage. It was found that work-related usage is more common among educated users in highly-paid professional occupations. The most common topics of conversation classified in the study include 'Practical Guidance,' 'Seeking Information,' and 'Writing,' which collectively account for nearly 80% of all conversations. This indicates that while ChatGPT is utilized for work-related tasks, its unique ability to generate digital outputs has led to a significant increase in non-work-related interactions.",multi_hop_specific_query_synthesizer
"What details about the prompts used in the classification pipeline are provided in Appendix A, and how does this relate to the privacy measures mentioned in the context?","['<1-hop>\n\n3.2 Classified Messages\nTo understand usage while preserving user privacy, we construct message-level datasets without any\nhuman ever reading the contents of a message. See Figure 1 for an overview of the privacy-preserving\nclassification pipeline. Messages are categorized according to 5 different LLM-based classifiers. The\nclassifiers are introduced in more detail in Section 5, their exact text is reproduced in Appendix A,\nand our validation procedure is described in Appendix B.\nSampled From All ChatGPT Users.We uniformly sampled approximately 1.1 million conver-\nsations, and then sampled one message within each conversation, with the following restrictions:\n1. We only include messages from May 2024 to July 2025.\n2. We exclude conversations from users who opted out of sharing their messages for model training.\n3. We exclude users who self-report their age as under 18.\n4. We exclude conversations that users have deleted and from users whose accounts have been\ndeactivated or banned.\n5. We exclude logged-out users, 16 which represented a minority share of ChatGPT users over the\nsample period.\nOur sample is drawn from a table that is itself sampled, where the sampling rate varied over time.\nWe thus adjust our sampling weights to maintain a fixed ratio with aggregate messages sent.\nSampled From a Subset of ChatGPT Users.We construct two samples of classified messages\nfrom a subset of ChatGPT users (approximately 130,000 users). This sample of users does not include\nany users who opted out of sharing their messages for training, nor does it include users whose self-\nreported age is below 18, nor does it include users who have been banned or deleted their accounts.\nThe first sample contains classifications of 1.58 million messages from this subset of users, sampled\nat the conversation level (a conversation is a series of messages between the user and chatbot). This\nsample is constructed such that the user’s representation in the data is proportional to overall message\nvolume. The second sample contains messages sent from this subset of users, sampled at the user level\nwith up to six messages from each user in the group.\n16ChatGPT became available to logged-out users in April 2024, i.e., users could use ChatGPT without signing up\nfor an account with an email address. However, messages from logged-out users are only available in our dataset from\nMarch 2025, thus for consistency we drop all messages from logged-out users.\n6Figure 1:Illustration of Privacy-Preserving Automated Classification Pipeline (Synthetic Example). Mes-\nsages are first stripped of PII via an internal LLM-based tool calledPrivacy Filter. Then they are classified by\nLLM-based automated classifiers, described in detail in Appendices A and B. Humans do not see raw messages\nor PII-scrubbed messages, only the final classifications of messages.\n', '<2-hop>\n\nany human seeing them. We give the text of most prompts in Appendix A along with details about how the prompts were validated in Appendix B.6 The classification pipeline is protected by a series of privacy measures, detailed below, to ensure no leakage of sensitive information during the automated analysis. In a secure data clean room, we relate taxonomies of messages to aggregated employment and education categories. Table 1 shows the growth in total message volume for work and non-work usage. Both types of 1Reuters (2025), Roth (2025) 2Bick et al. (2024) report that 28% of US adults used ChatGPT in late 2024, higher than any other chatbot. 3We use the term LLM loosely here and give more details in the following section. 4Wiggers (2025) reports estimates that in April 2025 ChatGPT was receiving more than 10 times as many visitors as either Claude or Copilot. 5Our sample includes the three consumer plans (Free, Plus, or Pro). OpenAI also offers a variety of other ChatGPT plans (Business fka. Teams, Enterprise, Education), which we do not include in our sample. 6Our classifiers take into account not just the randomly-selected user message, but also a portion of the preceding messages in that conversation. 1Month Non-Work (M)(%)Work (M)(%)Total Messages (M) Jun 2024 238 53% 213 47% 451 Jun 2025 1,911 73% 716 27% 2,627 Table 1:ChatGPT daily message counts (millions), broken down by likely work-related or non-work-related. Total daily counts are exact measurements of message volume from all consumer plans. Daily counts of work and non-work related messages are estimated by classifying a random sample of conversations from that day. Sampling is done to exclude users who opt-out of sharing their messages for model training, users who self- report their age as under 18, logged-out users, deleted conversations, and accounts which have been deactivated or banned (details available in Section 3).']","Appendix A contains the text of most prompts used in the classification pipeline, along with details about how these prompts were validated. The classification pipeline is designed with a series of privacy measures to ensure that no sensitive information is leaked during the automated analysis. This includes the use of an internal LLM-based tool called Privacy Filter to strip messages of personally identifiable information (PII) before classification, ensuring that no human ever sees the raw or PII-scrubbed messages, only the final classifications.",multi_hop_specific_query_synthesizer
What insights do Collis and Brynjolfsson (2025) provide regarding the increasing non-work usage of generative AI and its implications for user welfare?,"['<1-hop>\n\n6Our classifiers take into account not just the randomly-selected user message, but also a portion of the preceding messages in that conversation. 1Month Non-Work (M)(%)Work (M)(%)Total Messages (M) Jun 2024 238 53% 213 47% 451 Jun 2025 1,911 73% 716 27% 2,627 Table 1:ChatGPT daily message counts (millions), broken down by likely work-related or non-work-related. Total daily counts are exact measurements of message volume from all consumer plans. Daily counts of work and non-work related messages are estimated by classifying a random sample of conversations from that day. Sampling is done to exclude users who opt-out of sharing their messages for model training, users who self- report their age as under 18, logged-out users, deleted conversations, and accounts which have been deactivated or banned (details available in Section 3). Reported values are 7-day averages (to smooth weekly fluctuation) ending on the 26th of June 2024 and 26th of June 2025. messages have grown continuously, but non-work messages have grown faster and now represent more than 70% of all consumer ChatGPT messages. While most economic analysis of AI has focused on its impact on productivity in paid work, the impact on activity outside of work (home production) is on a similar scale and possibly larger. The decrease in the share of work-related messages is primarily due to changing usage within each cohort of users rather than a change in the composition of new ChatGPT users. This finding is consistent with Collis and Brynjolfsson (2025), who use choice experiments to uncover willingness-to-pay for generative AI and estimate a consumer surplus of at least$97 billion in 2024 alone in the US. We next report on a classification of messages using a taxonomy developed at OpenAI for un- derstanding product usage (“conversation classifier”). Nearly 80% of all ChatGPT usage falls into three broad categories, which we callPractical Guidance,Seeking Information, andWriting.Practical Guidanceis the most common use case and includes activities like tutoring and teaching, how-to advice about a variety of topics, and creative ideation. 7 Seeking Informationincludes searching for information about people, current events, products, and recipes, and appears to be a very close sub- stitute for web search.Writingincludes the automated production of emails, documents and other communications, but also editing, critiquing, summarizing, and translating text provided by the user. Writingis the most common use case at work, accounting for 40% of work-related messages on average in June 2025. About two-thirds of allWritingmessages ask ChatGPT to modify user text (editing, critiquing, translating, etc.) rather than creating new text from scratch. About 10% of all messages are requests for tutoring or teaching, suggesting that education is a key use case for ChatGPT. Two of our findings stand in contrast to other work. First, we find the share of messages related to computer coding is relatively small: only 4.2% of ChatGPT messages are related to computer programming, compared to 33% of work-related Claude conversations Handa et al. (2025).8 Second, we find the share of messages related to companionship or social-emotional issues is fairly small: only 1.9% of ChatGPT messages are on the topic ofRelationships and Personal Reflectionand 0.4% are related 7The difference betweenPractical GuidanceandSeeking Informationis that the former is highly customized to the user and can be adapted based on conversation and follow-up, whereas the latter is factual information that should be the same for all users. For example, users interested in running might ask ChatGPT for the Boston Marathon qualifying times by age and gender (Seeking Information), or they might ask for a customized workout plan that matches their goals and current level of fitness (Practical Guidance). 8Handa et al. (2025) report that 37% of conversations are mapped to a “computer and mathematical” occupation category, and their Figure 12 shows 30% or more of all imputed tasks are programming or IT-related. We believe the discrepancy is partly due to the difference in types of users between Claude and ChatGPT, additionally Handa et al. (2025) only includes queries that ”possibly involve an occupational task”. 2toGames and Role Play. In contrast, Zao-Sanders (2025) estimates thatTherapy/Companionshipis the most prevalent use case for generative AI. 9 We also document several important facts about demographic variation in ChatGPT usage.', '<2-hop>\n\nThe fact that non-work usage is increasing faster suggests that the welfare gains from generative AI\nusage could be substantial. Collis and Brynjolfsson (2025) estimate that US users would have to\nbe paid$98 to forgo using generative AI for a month, implying a surplus of at least$97 billion a\nyear. Within work usage, we find that users currently appear to derive value from using ChatGPT\nas an advisor or research assistant, not just a technology that performs job tasks directly. Still,\nChatGPT likely improves worker output by providingdecision support, which is especially important\nin knowledge-intensive jobs where productivity is increasing in the quality of decision-making.\n36']","Collis and Brynjolfsson (2025) provide insights that the increasing non-work usage of generative AI suggests substantial welfare gains for users. They estimate that US users would require a payment of $98 to forgo using generative AI for a month, indicating a consumer surplus of at least $97 billion annually. This finding highlights that users derive significant value from generative AI not only in performing job tasks but also as advisors or research assistants, particularly in knowledge-intensive jobs where decision-making quality enhances productivity.",multi_hop_specific_query_synthesizer
How does message classification relate to user privacy in the analysis of ChatGPT interactions?,"['<1-hop>\n\nThe left column\nshows a standalone message to be classified, and the right column shows the prior context included in the\nclassification of the message on the left.\nWe truncate each message to a maximum of 5,000 characters, because long context windows could\ninduce variability in the quality of the classification (Liu et al., 2023). We classify each message\nwith the “gpt-5-mini” model, with the exception ofInteraction Quality,which uses “gpt-5,” using the\nprompts listed in Appendix A.\n17Internal analyses show that the tool,Privacy Filter, has substantial alignment with human judgment.\n18In the case ofInteraction Quality,we additionally include the next two messages in the conversation as context.\n7We validated each of the classification prompts by comparing model classification decisions against\nhuman-judged classifications of a sample of conversations from the publicly available WildChat dataset\n(Zhao et al., 2024), a set of conversations with a third-party chatbot which users affirmatively gave\ntheir assent to share publicly for research purposes. 19 Appendix B provides detail on our validation\napproach and performance relative to human judgment. For additional transparency, we classify\na sample of 100,000 public WildChat messages and provide those data in this paper’s replication\npackage.\n 3.3 Employment Dataset\nWe conduct limited analyses of aggregated employment categories based on publicly available data\nfor a sample of consumer ChatGPT users. This sample included approximately 130,000 Free, Plus,\nand Pro users, and the employment categories were aggregated by a vendor working through a secure\nData Clean Room (DCR). For this analysis, we use the same exclusion criteria as for the message-level\ndatasets: we exclude deactivated users, banned users, users who have opted out of training, and users\nwhose self-reported age is under 18. Because the data was only available for a subset of users the\nresults may not be representative of the full pool of users.\nDescription.The employment data, which is aggregated from publicly available sources, includes\nindustry, occupations coarsened to O*NET categories, seniority level, company size, and education\ninformation that is limited to the degree attained. A vendor working within a DCR procured this\ndataset, restricted us to running only aggregated queries against it through the DCR, and deleted it\nupon the study’s completion.\n Privacy via a Data Clean Room.We never directly accessed user-level demographic records.\nAll analysis of employment data was executed exclusively within a secure DCR that permits only\npre-approved aggregate computations across independently held datasets; neither party can view or\nexport the other party’s underlying records. We governed the DCR with strict protocols: To execute\nany query that touched the external demographic data, we first obtained explicit sign-off from a\ncommittee of 6 coauthors and then submitted the notebook to our data partner for approval; only\napproved notebooks could run in the DCR (see Figure 2).\nOur partner enforced strict aggregation limits: they only approved code that returned cells meeting\na threshold of 100 users. Consequently, no individual rows or narrowly defined categories were ever\nvisible to researchers. For example, if 99 users had the occupation “anesthesiologist,” any occupation-\nlevel output would place those users into a “suppressed” category, or place these observations in a\ncoarsened category (e.g. “medical professionals”) rather than reporting a separate cell of anesthesiol-\nogists.\n19The dataset was collected from a third party chatbot using OpenAI’s LLMs via their API.\n8Figure 2:Illustration of Aggregated Employment Category Analysis via a Data Clean Room. All queries run\nin the Data Clean Room must be approved by our data partner, enforcing a strict aggregation threshold (100\nobservations). As a result, researchers cannot access user-level employment data, only aggregated employment\ncategories.\n', '<2-hop>\n\n3.4 Summarizing Our Approach to Privacy\nWe took measures to safeguard user privacy at every stage of analysis. To summarize, the key elements\nof our approach are:\nAutomated classification of messages.In the course of analysis, no one ever looked directly\nat the content of user messages: all of our analysis of the content of user messages is done\nthrough output of automated classifiers run on de-identified and PII-scrubbed usage data.\nAggregated employment data via a data clean room.We analyze and report aggregated\nemployment data through a secure data clean room environment: no one on the research\nteam had direct access to user-level demographic data and none of our analyses report\naggregates for groups with less than 100 users.\nIn following these measures, we aim to match or exceed the privacy protection precedents set by\nother social scientists studying chatbots and those linking digital platform data to external sources.\nWe follow the precedent established in recent analyses of chatbot conversations (Phang et al.\n(2025), Eloundou et al. (2025), Handa et al. (2025), Tomlinson et al. (2025)) that rely on automated\nclassification rather than human inspection of raw transcripts. In particular, Phang et al. (2025)’s\nstudy of affective use of ChatGPT and Eloundou et al. (2025) investigation of first-person fairness in\nchatbots both analyze ChatGPT message content via automated classifiers and emphasize classifier-\nbased labeling as a scalable, privacy-preserving approach. Anthropic’s Handa et al. (2025) used a\nsimilar approach: theirCliomethodology applies automated classifiers to large collections of conver-\nsations, classifying conversations into thousands of topics, and in their appendix they describe manual\nvalidation on sampled conversations (100 user conversations flagged for review and 100 randomly sam-\npled calibrations). Like Eloundou et al., we validate our classifiers using WildChat, a public dataset\nof user conversations.\n9Other papers have analyzed digital behavior and demographic data; we mention a few relevant\nprecedents here. Humlum and Vestergaard (2025b) and Humlum and Vestergaard (2025a), for exam-\nple, analyze large-scale surveys on chatbot use along with Danish administrative labor market data.\nChetty et al. (2022) analyze de-identified Facebook friendship graphs and anonymized IRS tax records,\naggregated at the zip code level.\n', '<3-hop>\n\nThose differences are cut roughly in half\nafter adjusting for other characteristics, but they are still statistically significant at the less than 1\npercent level. Educated users are more likely to send work-related messages.\nPanel B explores variation by education in user intent.Askingconstitutes about 49% of messages\nfor users with less than a bachelor’s degree, with little variation for more educated users. After\nregression adjustment, we find that users with a graduate degree are about two percentage points\nmore likely to use ChatGPT forAskingmessages, a difference that is statistically significant at the\n5% level. Prior to regression adjustment, the frequency ofDoingmessages is increasing in education.\nHowever, this pattern reverses after adjusting for other characteristics such as occupation. Users with\na graduate degree are about 1.6 percentage points less likely to sendDoingmessages than users with\nless than a bachelor’s degree, and the difference is statistically significant at the 10% level.\nPanel C studies variation by education in the frequency of four different conversation topics –\nPractical Guidance,Seeking Information,Technical Help, andWriting. We find only modest differ-\nences by education across most of these categories. The one exception is that the share of messages\nrelated toWritingis increasing in relation to education.\n28Panel A.Work Related\nPanel B1.Asking. Panel B2.Doing.\nPanel B3.Expressing.\nFigure 22:(continued on next page)\n29Panel C1.Writing. Panel C2.Technical Help.\nPanel C3.Seeking Information. Panel C4.Practical Guidance.\nFigure 22:Variation in ChatGPT usage by education. Each plot shows unadjusted vs. regression-adjusted\nestimates, with 95% confidence intervals. We regress each message share on education and occupation, control-\nling for the following covariates: age, whether the name was typically masculine or feminine, seniority within\nrole, company size, and industry. (To guarantee user privacy, we coarsen all covariates to broad categories and\nprogrammatically enforce that each group has at least 100 members prior to running the regression) We add\nthe coefficients on each education and occupation category to the unadjusted value for the reference category\nand compute 95% confidence intervals using the standard errors from the regression coefficients. The sample\nfor this regression is the approximately 40,000 users of the original 130,000 sample whose publicly available\noccupation was not blank or consisted of strictly special characters (as determined by a classification script).\nShares for each user are calculated by randomly sampling up to six conversations attributed to the user from\nMay 2024 through July 2025.\n306.5 ']","Message classification is conducted through automated classifiers that analyze user messages without direct human inspection, ensuring user privacy. The analysis is performed on de-identified and PII-scrubbed data, which aligns with the measures taken to safeguard user privacy at every stage of analysis. Additionally, aggregated employment data is processed in a secure data clean room, where no individual user-level data is accessible, further protecting user privacy while allowing for effective message classification.",multi_hop_abstract_query_synthesizer
How does the automated classification of messages ensure user privacy while analyzing message content and employment data?,"['<1-hop>\n\nThe left column\nshows a standalone message to be classified, and the right column shows the prior context included in the\nclassification of the message on the left.\nWe truncate each message to a maximum of 5,000 characters, because long context windows could\ninduce variability in the quality of the classification (Liu et al., 2023). We classify each message\nwith the “gpt-5-mini” model, with the exception ofInteraction Quality,which uses “gpt-5,” using the\nprompts listed in Appendix A.\n17Internal analyses show that the tool,Privacy Filter, has substantial alignment with human judgment.\n18In the case ofInteraction Quality,we additionally include the next two messages in the conversation as context.\n7We validated each of the classification prompts by comparing model classification decisions against\nhuman-judged classifications of a sample of conversations from the publicly available WildChat dataset\n(Zhao et al., 2024), a set of conversations with a third-party chatbot which users affirmatively gave\ntheir assent to share publicly for research purposes. 19 Appendix B provides detail on our validation\napproach and performance relative to human judgment. For additional transparency, we classify\na sample of 100,000 public WildChat messages and provide those data in this paper’s replication\npackage.\n 3.3 Employment Dataset\nWe conduct limited analyses of aggregated employment categories based on publicly available data\nfor a sample of consumer ChatGPT users. This sample included approximately 130,000 Free, Plus,\nand Pro users, and the employment categories were aggregated by a vendor working through a secure\nData Clean Room (DCR). For this analysis, we use the same exclusion criteria as for the message-level\ndatasets: we exclude deactivated users, banned users, users who have opted out of training, and users\nwhose self-reported age is under 18. Because the data was only available for a subset of users the\nresults may not be representative of the full pool of users.\nDescription.The employment data, which is aggregated from publicly available sources, includes\nindustry, occupations coarsened to O*NET categories, seniority level, company size, and education\ninformation that is limited to the degree attained. A vendor working within a DCR procured this\ndataset, restricted us to running only aggregated queries against it through the DCR, and deleted it\nupon the study’s completion.\n Privacy via a Data Clean Room.We never directly accessed user-level demographic records.\nAll analysis of employment data was executed exclusively within a secure DCR that permits only\npre-approved aggregate computations across independently held datasets; neither party can view or\nexport the other party’s underlying records. We governed the DCR with strict protocols: To execute\nany query that touched the external demographic data, we first obtained explicit sign-off from a\ncommittee of 6 coauthors and then submitted the notebook to our data partner for approval; only\napproved notebooks could run in the DCR (see Figure 2).\nOur partner enforced strict aggregation limits: they only approved code that returned cells meeting\na threshold of 100 users. Consequently, no individual rows or narrowly defined categories were ever\nvisible to researchers. For example, if 99 users had the occupation “anesthesiologist,” any occupation-\nlevel output would place those users into a “suppressed” category, or place these observations in a\ncoarsened category (e.g. “medical professionals”) rather than reporting a separate cell of anesthesiol-\nogists.\n19The dataset was collected from a third party chatbot using OpenAI’s LLMs via their API.\n8Figure 2:Illustration of Aggregated Employment Category Analysis via a Data Clean Room. All queries run\nin the Data Clean Room must be approved by our data partner, enforcing a strict aggregation threshold (100\nobservations). As a result, researchers cannot access user-level employment data, only aggregated employment\ncategories.\n', '<2-hop>\n\n3.4 Summarizing Our Approach to Privacy\nWe took measures to safeguard user privacy at every stage of analysis. To summarize, the key elements\nof our approach are:\nAutomated classification of messages.In the course of analysis, no one ever looked directly\nat the content of user messages: all of our analysis of the content of user messages is done\nthrough output of automated classifiers run on de-identified and PII-scrubbed usage data.\nAggregated employment data via a data clean room.We analyze and report aggregated\nemployment data through a secure data clean room environment: no one on the research\nteam had direct access to user-level demographic data and none of our analyses report\naggregates for groups with less than 100 users.\nIn following these measures, we aim to match or exceed the privacy protection precedents set by\nother social scientists studying chatbots and those linking digital platform data to external sources.\nWe follow the precedent established in recent analyses of chatbot conversations (Phang et al.\n(2025), Eloundou et al. (2025), Handa et al. (2025), Tomlinson et al. (2025)) that rely on automated\nclassification rather than human inspection of raw transcripts. In particular, Phang et al. (2025)’s\nstudy of affective use of ChatGPT and Eloundou et al. (2025) investigation of first-person fairness in\nchatbots both analyze ChatGPT message content via automated classifiers and emphasize classifier-\nbased labeling as a scalable, privacy-preserving approach. Anthropic’s Handa et al. (2025) used a\nsimilar approach: theirCliomethodology applies automated classifiers to large collections of conver-\nsations, classifying conversations into thousands of topics, and in their appendix they describe manual\nvalidation on sampled conversations (100 user conversations flagged for review and 100 randomly sam-\npled calibrations). Like Eloundou et al., we validate our classifiers using WildChat, a public dataset\nof user conversations.\n9Other papers have analyzed digital behavior and demographic data; we mention a few relevant\nprecedents here. Humlum and Vestergaard (2025b) and Humlum and Vestergaard (2025a), for exam-\nple, analyze large-scale surveys on chatbot use along with Danish administrative labor market data.\nChetty et al. (2022) analyze de-identified Facebook friendship graphs and anonymized IRS tax records,\naggregated at the zip code level.\n']","The automated classification of messages ensures user privacy by analyzing the content of user messages through output from automated classifiers that operate on de-identified and PII-scrubbed usage data. This means that no one on the research team directly accesses the content of user messages. Additionally, employment data is analyzed and reported through a secure data clean room environment, where researchers do not have direct access to user-level demographic data. All analyses are conducted in a manner that adheres to strict aggregation limits, ensuring that no individual records are visible and that groups with fewer than 100 users are not reported, thereby safeguarding user privacy throughout the analysis process.",multi_hop_abstract_query_synthesizer
How does the automated classification of messages ensure user privacy while analyzing message content and what role does the data clean room play in this process?,"['<1-hop>\n\nThe left column\nshows a standalone message to be classified, and the right column shows the prior context included in the\nclassification of the message on the left.\nWe truncate each message to a maximum of 5,000 characters, because long context windows could\ninduce variability in the quality of the classification (Liu et al., 2023). We classify each message\nwith the “gpt-5-mini” model, with the exception ofInteraction Quality,which uses “gpt-5,” using the\nprompts listed in Appendix A.\n17Internal analyses show that the tool,Privacy Filter, has substantial alignment with human judgment.\n18In the case ofInteraction Quality,we additionally include the next two messages in the conversation as context.\n7We validated each of the classification prompts by comparing model classification decisions against\nhuman-judged classifications of a sample of conversations from the publicly available WildChat dataset\n(Zhao et al., 2024), a set of conversations with a third-party chatbot which users affirmatively gave\ntheir assent to share publicly for research purposes. 19 Appendix B provides detail on our validation\napproach and performance relative to human judgment. For additional transparency, we classify\na sample of 100,000 public WildChat messages and provide those data in this paper’s replication\npackage.\n 3.3 Employment Dataset\nWe conduct limited analyses of aggregated employment categories based on publicly available data\nfor a sample of consumer ChatGPT users. This sample included approximately 130,000 Free, Plus,\nand Pro users, and the employment categories were aggregated by a vendor working through a secure\nData Clean Room (DCR). For this analysis, we use the same exclusion criteria as for the message-level\ndatasets: we exclude deactivated users, banned users, users who have opted out of training, and users\nwhose self-reported age is under 18. Because the data was only available for a subset of users the\nresults may not be representative of the full pool of users.\nDescription.The employment data, which is aggregated from publicly available sources, includes\nindustry, occupations coarsened to O*NET categories, seniority level, company size, and education\ninformation that is limited to the degree attained. A vendor working within a DCR procured this\ndataset, restricted us to running only aggregated queries against it through the DCR, and deleted it\nupon the study’s completion.\n Privacy via a Data Clean Room.We never directly accessed user-level demographic records.\nAll analysis of employment data was executed exclusively within a secure DCR that permits only\npre-approved aggregate computations across independently held datasets; neither party can view or\nexport the other party’s underlying records. We governed the DCR with strict protocols: To execute\nany query that touched the external demographic data, we first obtained explicit sign-off from a\ncommittee of 6 coauthors and then submitted the notebook to our data partner for approval; only\napproved notebooks could run in the DCR (see Figure 2).\nOur partner enforced strict aggregation limits: they only approved code that returned cells meeting\na threshold of 100 users. Consequently, no individual rows or narrowly defined categories were ever\nvisible to researchers. For example, if 99 users had the occupation “anesthesiologist,” any occupation-\nlevel output would place those users into a “suppressed” category, or place these observations in a\ncoarsened category (e.g. “medical professionals”) rather than reporting a separate cell of anesthesiol-\nogists.\n19The dataset was collected from a third party chatbot using OpenAI’s LLMs via their API.\n8Figure 2:Illustration of Aggregated Employment Category Analysis via a Data Clean Room. All queries run\nin the Data Clean Room must be approved by our data partner, enforcing a strict aggregation threshold (100\nobservations). As a result, researchers cannot access user-level employment data, only aggregated employment\ncategories.\n', '<2-hop>\n\n3.4 Summarizing Our Approach to Privacy\nWe took measures to safeguard user privacy at every stage of analysis. To summarize, the key elements\nof our approach are:\nAutomated classification of messages.In the course of analysis, no one ever looked directly\nat the content of user messages: all of our analysis of the content of user messages is done\nthrough output of automated classifiers run on de-identified and PII-scrubbed usage data.\nAggregated employment data via a data clean room.We analyze and report aggregated\nemployment data through a secure data clean room environment: no one on the research\nteam had direct access to user-level demographic data and none of our analyses report\naggregates for groups with less than 100 users.\nIn following these measures, we aim to match or exceed the privacy protection precedents set by\nother social scientists studying chatbots and those linking digital platform data to external sources.\nWe follow the precedent established in recent analyses of chatbot conversations (Phang et al.\n(2025), Eloundou et al. (2025), Handa et al. (2025), Tomlinson et al. (2025)) that rely on automated\nclassification rather than human inspection of raw transcripts. In particular, Phang et al. (2025)’s\nstudy of affective use of ChatGPT and Eloundou et al. (2025) investigation of first-person fairness in\nchatbots both analyze ChatGPT message content via automated classifiers and emphasize classifier-\nbased labeling as a scalable, privacy-preserving approach. Anthropic’s Handa et al. (2025) used a\nsimilar approach: theirCliomethodology applies automated classifiers to large collections of conver-\nsations, classifying conversations into thousands of topics, and in their appendix they describe manual\nvalidation on sampled conversations (100 user conversations flagged for review and 100 randomly sam-\npled calibrations). Like Eloundou et al., we validate our classifiers using WildChat, a public dataset\nof user conversations.\n9Other papers have analyzed digital behavior and demographic data; we mention a few relevant\nprecedents here. Humlum and Vestergaard (2025b) and Humlum and Vestergaard (2025a), for exam-\nple, analyze large-scale surveys on chatbot use along with Danish administrative labor market data.\nChetty et al. (2022) analyze de-identified Facebook friendship graphs and anonymized IRS tax records,\naggregated at the zip code level.\n']","The automated classification of messages ensures user privacy by analyzing the content of user messages through output from automated classifiers that operate on de-identified and PII-scrubbed usage data. This means that no one on the research team directly accesses the content of user messages. Additionally, the analysis of aggregated employment data is conducted within a secure data clean room environment, where researchers do not have direct access to user-level demographic data. The data clean room enforces strict aggregation limits, ensuring that analyses report aggregates only for groups with at least 100 users, thus protecting individual user privacy. These measures aim to match or exceed privacy protection standards established by other studies in the field.",multi_hop_abstract_query_synthesizer
How does message classification relate to user privacy in the analysis of ChatGPT interactions?,"['<1-hop>\n\nThe left column\nshows a standalone message to be classified, and the right column shows the prior context included in the\nclassification of the message on the left.\nWe truncate each message to a maximum of 5,000 characters, because long context windows could\ninduce variability in the quality of the classification (Liu et al., 2023). We classify each message\nwith the “gpt-5-mini” model, with the exception ofInteraction Quality,which uses “gpt-5,” using the\nprompts listed in Appendix A.\n17Internal analyses show that the tool,Privacy Filter, has substantial alignment with human judgment.\n18In the case ofInteraction Quality,we additionally include the next two messages in the conversation as context.\n7We validated each of the classification prompts by comparing model classification decisions against\nhuman-judged classifications of a sample of conversations from the publicly available WildChat dataset\n(Zhao et al., 2024), a set of conversations with a third-party chatbot which users affirmatively gave\ntheir assent to share publicly for research purposes. 19 Appendix B provides detail on our validation\napproach and performance relative to human judgment. For additional transparency, we classify\na sample of 100,000 public WildChat messages and provide those data in this paper’s replication\npackage.\n 3.3 Employment Dataset\nWe conduct limited analyses of aggregated employment categories based on publicly available data\nfor a sample of consumer ChatGPT users. This sample included approximately 130,000 Free, Plus,\nand Pro users, and the employment categories were aggregated by a vendor working through a secure\nData Clean Room (DCR). For this analysis, we use the same exclusion criteria as for the message-level\ndatasets: we exclude deactivated users, banned users, users who have opted out of training, and users\nwhose self-reported age is under 18. Because the data was only available for a subset of users the\nresults may not be representative of the full pool of users.\nDescription.The employment data, which is aggregated from publicly available sources, includes\nindustry, occupations coarsened to O*NET categories, seniority level, company size, and education\ninformation that is limited to the degree attained. A vendor working within a DCR procured this\ndataset, restricted us to running only aggregated queries against it through the DCR, and deleted it\nupon the study’s completion.\n Privacy via a Data Clean Room.We never directly accessed user-level demographic records.\nAll analysis of employment data was executed exclusively within a secure DCR that permits only\npre-approved aggregate computations across independently held datasets; neither party can view or\nexport the other party’s underlying records. We governed the DCR with strict protocols: To execute\nany query that touched the external demographic data, we first obtained explicit sign-off from a\ncommittee of 6 coauthors and then submitted the notebook to our data partner for approval; only\napproved notebooks could run in the DCR (see Figure 2).\nOur partner enforced strict aggregation limits: they only approved code that returned cells meeting\na threshold of 100 users. Consequently, no individual rows or narrowly defined categories were ever\nvisible to researchers. For example, if 99 users had the occupation “anesthesiologist,” any occupation-\nlevel output would place those users into a “suppressed” category, or place these observations in a\ncoarsened category (e.g. “medical professionals”) rather than reporting a separate cell of anesthesiol-\nogists.\n19The dataset was collected from a third party chatbot using OpenAI’s LLMs via their API.\n8Figure 2:Illustration of Aggregated Employment Category Analysis via a Data Clean Room. All queries run\nin the Data Clean Room must be approved by our data partner, enforcing a strict aggregation threshold (100\nobservations). As a result, researchers cannot access user-level employment data, only aggregated employment\ncategories.\n', '<2-hop>\n\n3.4 Summarizing Our Approach to Privacy\nWe took measures to safeguard user privacy at every stage of analysis. To summarize, the key elements\nof our approach are:\nAutomated classification of messages.In the course of analysis, no one ever looked directly\nat the content of user messages: all of our analysis of the content of user messages is done\nthrough output of automated classifiers run on de-identified and PII-scrubbed usage data.\nAggregated employment data via a data clean room.We analyze and report aggregated\nemployment data through a secure data clean room environment: no one on the research\nteam had direct access to user-level demographic data and none of our analyses report\naggregates for groups with less than 100 users.\nIn following these measures, we aim to match or exceed the privacy protection precedents set by\nother social scientists studying chatbots and those linking digital platform data to external sources.\nWe follow the precedent established in recent analyses of chatbot conversations (Phang et al.\n(2025), Eloundou et al. (2025), Handa et al. (2025), Tomlinson et al. (2025)) that rely on automated\nclassification rather than human inspection of raw transcripts. In particular, Phang et al. (2025)’s\nstudy of affective use of ChatGPT and Eloundou et al. (2025) investigation of first-person fairness in\nchatbots both analyze ChatGPT message content via automated classifiers and emphasize classifier-\nbased labeling as a scalable, privacy-preserving approach. Anthropic’s Handa et al. (2025) used a\nsimilar approach: theirCliomethodology applies automated classifiers to large collections of conver-\nsations, classifying conversations into thousands of topics, and in their appendix they describe manual\nvalidation on sampled conversations (100 user conversations flagged for review and 100 randomly sam-\npled calibrations). Like Eloundou et al., we validate our classifiers using WildChat, a public dataset\nof user conversations.\n9Other papers have analyzed digital behavior and demographic data; we mention a few relevant\nprecedents here. Humlum and Vestergaard (2025b) and Humlum and Vestergaard (2025a), for exam-\nple, analyze large-scale surveys on chatbot use along with Danish administrative labor market data.\nChetty et al. (2022) analyze de-identified Facebook friendship graphs and anonymized IRS tax records,\naggregated at the zip code level.\n']","Message classification is conducted through automated classifiers that analyze user messages without direct human inspection, ensuring user privacy. The analysis is performed on de-identified and PII-scrubbed data, which aligns with the measures taken to safeguard user privacy at every stage. Additionally, aggregated employment data is analyzed through a secure data clean room, where no individual user-level data is accessible, further protecting user privacy while allowing for effective message classification.",multi_hop_abstract_query_synthesizer
"How does the automated classification of messages ensure user privacy while analyzing message content, and what role does the data clean room play in safeguarding user information?","['<1-hop>\n\nThe left column\nshows a standalone message to be classified, and the right column shows the prior context included in the\nclassification of the message on the left.\nWe truncate each message to a maximum of 5,000 characters, because long context windows could\ninduce variability in the quality of the classification (Liu et al., 2023). We classify each message\nwith the “gpt-5-mini” model, with the exception ofInteraction Quality,which uses “gpt-5,” using the\nprompts listed in Appendix A.\n17Internal analyses show that the tool,Privacy Filter, has substantial alignment with human judgment.\n18In the case ofInteraction Quality,we additionally include the next two messages in the conversation as context.\n7We validated each of the classification prompts by comparing model classification decisions against\nhuman-judged classifications of a sample of conversations from the publicly available WildChat dataset\n(Zhao et al., 2024), a set of conversations with a third-party chatbot which users affirmatively gave\ntheir assent to share publicly for research purposes. 19 Appendix B provides detail on our validation\napproach and performance relative to human judgment. For additional transparency, we classify\na sample of 100,000 public WildChat messages and provide those data in this paper’s replication\npackage.\n 3.3 Employment Dataset\nWe conduct limited analyses of aggregated employment categories based on publicly available data\nfor a sample of consumer ChatGPT users. This sample included approximately 130,000 Free, Plus,\nand Pro users, and the employment categories were aggregated by a vendor working through a secure\nData Clean Room (DCR). For this analysis, we use the same exclusion criteria as for the message-level\ndatasets: we exclude deactivated users, banned users, users who have opted out of training, and users\nwhose self-reported age is under 18. Because the data was only available for a subset of users the\nresults may not be representative of the full pool of users.\nDescription.The employment data, which is aggregated from publicly available sources, includes\nindustry, occupations coarsened to O*NET categories, seniority level, company size, and education\ninformation that is limited to the degree attained. A vendor working within a DCR procured this\ndataset, restricted us to running only aggregated queries against it through the DCR, and deleted it\nupon the study’s completion.\n Privacy via a Data Clean Room.We never directly accessed user-level demographic records.\nAll analysis of employment data was executed exclusively within a secure DCR that permits only\npre-approved aggregate computations across independently held datasets; neither party can view or\nexport the other party’s underlying records. We governed the DCR with strict protocols: To execute\nany query that touched the external demographic data, we first obtained explicit sign-off from a\ncommittee of 6 coauthors and then submitted the notebook to our data partner for approval; only\napproved notebooks could run in the DCR (see Figure 2).\nOur partner enforced strict aggregation limits: they only approved code that returned cells meeting\na threshold of 100 users. Consequently, no individual rows or narrowly defined categories were ever\nvisible to researchers. For example, if 99 users had the occupation “anesthesiologist,” any occupation-\nlevel output would place those users into a “suppressed” category, or place these observations in a\ncoarsened category (e.g. “medical professionals”) rather than reporting a separate cell of anesthesiol-\nogists.\n19The dataset was collected from a third party chatbot using OpenAI’s LLMs via their API.\n8Figure 2:Illustration of Aggregated Employment Category Analysis via a Data Clean Room. All queries run\nin the Data Clean Room must be approved by our data partner, enforcing a strict aggregation threshold (100\nobservations). As a result, researchers cannot access user-level employment data, only aggregated employment\ncategories.\n', '<2-hop>\n\n3.4 Summarizing Our Approach to Privacy\nWe took measures to safeguard user privacy at every stage of analysis. To summarize, the key elements\nof our approach are:\nAutomated classification of messages.In the course of analysis, no one ever looked directly\nat the content of user messages: all of our analysis of the content of user messages is done\nthrough output of automated classifiers run on de-identified and PII-scrubbed usage data.\nAggregated employment data via a data clean room.We analyze and report aggregated\nemployment data through a secure data clean room environment: no one on the research\nteam had direct access to user-level demographic data and none of our analyses report\naggregates for groups with less than 100 users.\nIn following these measures, we aim to match or exceed the privacy protection precedents set by\nother social scientists studying chatbots and those linking digital platform data to external sources.\nWe follow the precedent established in recent analyses of chatbot conversations (Phang et al.\n(2025), Eloundou et al. (2025), Handa et al. (2025), Tomlinson et al. (2025)) that rely on automated\nclassification rather than human inspection of raw transcripts. In particular, Phang et al. (2025)’s\nstudy of affective use of ChatGPT and Eloundou et al. (2025) investigation of first-person fairness in\nchatbots both analyze ChatGPT message content via automated classifiers and emphasize classifier-\nbased labeling as a scalable, privacy-preserving approach. Anthropic’s Handa et al. (2025) used a\nsimilar approach: theirCliomethodology applies automated classifiers to large collections of conver-\nsations, classifying conversations into thousands of topics, and in their appendix they describe manual\nvalidation on sampled conversations (100 user conversations flagged for review and 100 randomly sam-\npled calibrations). Like Eloundou et al., we validate our classifiers using WildChat, a public dataset\nof user conversations.\n9Other papers have analyzed digital behavior and demographic data; we mention a few relevant\nprecedents here. Humlum and Vestergaard (2025b) and Humlum and Vestergaard (2025a), for exam-\nple, analyze large-scale surveys on chatbot use along with Danish administrative labor market data.\nChetty et al. (2022) analyze de-identified Facebook friendship graphs and anonymized IRS tax records,\naggregated at the zip code level.\n']","The automated classification of messages ensures user privacy by analyzing the content of user messages through output from automated classifiers that operate on de-identified and PII-scrubbed usage data. This means that no one on the research team directly accesses the content of user messages, thereby maintaining privacy. Additionally, the data clean room plays a crucial role in safeguarding user information by allowing the analysis and reporting of aggregated employment data without direct access to user-level demographic data. All queries run in the data clean room must meet strict aggregation thresholds, ensuring that no individual user data is visible to researchers. This approach aligns with privacy protection precedents set by other studies in the field, emphasizing the importance of automated classification as a scalable and privacy-preserving method.",multi_hop_abstract_query_synthesizer
