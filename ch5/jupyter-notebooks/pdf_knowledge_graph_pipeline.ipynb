{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20afb80e",
   "metadata": {},
   "source": [
    "üîß **Setup Required**: Before running this notebook, please follow the [setup instructions](../README.md#setup-instructions) to configure your environment and API keys."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d0d577",
   "metadata": {},
   "source": [
    "# PDF Knowledge Graph and Synthetic Data Generation Pipeline\n",
    "\n",
    "This notebook demonstrates how to build a comprehensive pipeline for PDF document processing that:\n",
    "1. **Extracts content** from PDF files using Haystack's PyPDFToDocument converter\n",
    "2. **Preprocesses the text** with cleaning and splitting components\n",
    "3. **Creates a knowledge graph** from the processed documents\n",
    "4. **Generates synthetic test data** using the knowledge graph\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "- How to build end-to-end Haystack pipelines for PDF processing\n",
    "- The relationship between knowledge graphs and synthetic test data generation\n",
    "- Best practices for PDF document preprocessing\n",
    "- How to evaluate synthetic datasets generated from PDF content\n",
    "\n",
    "## Key Components\n",
    "- **PyPDFToDocument**: Converts PDF files to Haystack Document objects\n",
    "- **DocumentCleaner**: Removes extra whitespaces and empty lines\n",
    "- **DocumentSplitter**: Breaks documents into manageable chunks\n",
    "- **KnowledgeGraphGenerator**: Creates structured knowledge representations\n",
    "- **SyntheticTestGenerator**: Produces question-answer pairs for evaluation\n",
    "\n",
    "## Why This Approach?\n",
    "Using knowledge graphs as an intermediate step improves the quality of synthetic test generation because:\n",
    "- Knowledge graphs capture relationships between entities\n",
    "- They provide structured context for question generation\n",
    "- The resulting questions are more coherent and factually grounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b77acac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurafunderburk/Documents/GitHub/Building-Natural-Language-Pipelines/ch5/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x31cd0d610>\n",
       "üöÖ Components\n",
       "  - pdf_converter: PyPDFToDocument\n",
       "  - doc_cleaner: DocumentCleaner\n",
       "  - doc_splitter: DocumentSplitter\n",
       "  - doc_converter: DocumentToLangChainConverter\n",
       "  - kg_generator: KnowledgeGraphGenerator\n",
       "  - test_generator: SyntheticTestGenerator\n",
       "  - test_saver: TestDatasetSaver\n",
       "üõ§Ô∏è Connections\n",
       "  - pdf_converter.documents -> doc_cleaner.documents (list[Document])\n",
       "  - doc_cleaner.documents -> doc_splitter.documents (list[Document])\n",
       "  - doc_splitter.documents -> doc_converter.documents (list[Document])\n",
       "  - doc_converter.langchain_documents -> kg_generator.documents (List[Document])\n",
       "  - doc_converter.langchain_documents -> test_generator.documents (List[Document])\n",
       "  - kg_generator.knowledge_graph -> test_generator.knowledge_graph (KnowledgeGraph)\n",
       "  - test_generator.testset -> test_saver.testset (DataFrame)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from haystack import Pipeline\n",
    "from haystack.components.converters import PyPDFToDocument\n",
    "from haystack.components.preprocessors import (\n",
    "    DocumentCleaner,\n",
    "    DocumentSplitter)\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.components.embedders.openai_text_embedder import OpenAITextEmbedder\n",
    "from haystack.utils import Secret\n",
    "from pathlib import Path\n",
    "from scripts.knowledge_graph_component import KnowledgeGraphGenerator\n",
    "from scripts.langchaindocument_component import DocumentToLangChainConverter\n",
    "from scripts.synthetic_test_components import SyntheticTestGenerator, TestDatasetSaver\n",
    "                                                    \n",
    "# Load environment variables\n",
    "load_dotenv(\"./.env\")\n",
    "\n",
    "# Helper function to create fresh generator and embedder instances\n",
    "def create_llm_components():\n",
    "    \"\"\"Create fresh instances of generator and embedder.\"\"\"\n",
    "    # You can use OpenAI models:\n",
    "    generator = OpenAIGenerator(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        api_key=Secret.from_token(os.getenv(\"OPENAI_API_KEY\"))\n",
    "    )\n",
    "    embedder = OpenAITextEmbedder(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        api_key=Secret.from_token(os.getenv(\"OPENAI_API_KEY\"))\n",
    "    )\n",
    "    \n",
    "    # Or use Ollama models (uncomment to use):\n",
    "    # from haystack_integrations.components.generators.ollama import OllamaGenerator\n",
    "    # from haystack_integrations.components.embedders.ollama import OllamaTextEmbedder\n",
    "    # \n",
    "    # generator = OllamaGenerator(\n",
    "    #     model=\"mistral-nemo:12b\",\n",
    "    #     generation_kwargs={\n",
    "    #         \"num_predict\": 100,\n",
    "    #         \"temperature\": 0.9,\n",
    "    #     }\n",
    "    # )\n",
    "    # embedder = OllamaTextEmbedder(model=\"nomic-embed-text\")\n",
    "    \n",
    "    return generator, embedder\n",
    "        \n",
    "# Create pipeline components\n",
    "pdf_converter = PyPDFToDocument()\n",
    "doc_cleaner = DocumentCleaner(\n",
    "    remove_empty_lines=True,\n",
    "    remove_extra_whitespaces=True,\n",
    ")\n",
    "doc_splitter = DocumentSplitter(split_by=\"sentence\",\n",
    "                                split_length=5,\n",
    "                                split_overlap=1)\n",
    "doc_converter = DocumentToLangChainConverter()\n",
    "\n",
    "# Create knowledge graph component with its own generator and embedder instances\n",
    "kg_gen, kg_embed = create_llm_components()\n",
    "kg_generator = KnowledgeGraphGenerator(\n",
    "    generator=kg_gen,\n",
    "    embedder=kg_embed,\n",
    "    apply_transforms=True\n",
    ")\n",
    "\n",
    "# Create test generator component with its own generator and embedder instances\n",
    "test_gen, test_embed = create_llm_components()\n",
    "test_generator = SyntheticTestGenerator(\n",
    "    generator=test_gen,\n",
    "    embedder=test_embed,\n",
    "    test_size=10,\n",
    "    query_distribution=[\n",
    "        (\"single_hop\", 0.3),\n",
    "        (\"multi_hop_specific\", 0.3),\n",
    "        (\"multi_hop_abstract\", 0.4)\n",
    "    ]\n",
    ")\n",
    "test_saver = TestDatasetSaver(\"data_for_eval/synthetic_tests_10_from_pdf.csv\")\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline()\n",
    "pipeline.add_component(\"pdf_converter\", pdf_converter)\n",
    "pipeline.add_component(\"doc_cleaner\", doc_cleaner)\n",
    "pipeline.add_component(\"doc_splitter\", doc_splitter)\n",
    "pipeline.add_component(\"doc_converter\", doc_converter)\n",
    "pipeline.add_component(\"kg_generator\", kg_generator)\n",
    "pipeline.add_component(\"test_generator\", test_generator)\n",
    "pipeline.add_component(\"test_saver\", test_saver)\n",
    "\n",
    "# Connect components in sequence\n",
    "pipeline.connect(\"pdf_converter.documents\", \"doc_cleaner.documents\")\n",
    "pipeline.connect(\"doc_cleaner.documents\", \"doc_splitter.documents\")\n",
    "pipeline.connect(\"doc_splitter.documents\", \"doc_converter.documents\")\n",
    "pipeline.connect(\"doc_converter.langchain_documents\", \"kg_generator.documents\")\n",
    "pipeline.connect(\"kg_generator.knowledge_graph\", \"test_generator.knowledge_graph\")\n",
    "pipeline.connect(\"doc_converter.langchain_documents\", \"test_generator.documents\")\n",
    "pipeline.connect(\"test_generator.testset\", \"test_saver.testset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78722540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying SummaryExtractor: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 163/163 [00:40<00:00,  4.01it/s]\n",
      "Applying CustomNodeFilter:  10%|‚ñâ         | 18/185 [00:05<00:37,  4.41it/s]Node 2b41eae4-c8fc-47f4-bea2-0c4df514fa87 does not have a summary. Skipping filtering.\n",
      "Node 4fd81ab9-f797-4b10-8767-02e13a22be46 does not have a summary. Skipping filtering.\n",
      "Node 0dc5ffdc-d76c-44ad-a519-a747ceebab60 does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:  22%|‚ñà‚ñà‚ñè       | 41/185 [00:11<00:36,  3.96it/s]Node aa017d86-b472-400f-af93-f26ac4d1e36f does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:  28%|‚ñà‚ñà‚ñä       | 51/185 [00:12<00:27,  4.80it/s]Node 48416ab7-775c-4f58-82cd-ef8d5a455eae does not have a summary. Skipping filtering.\n",
      "Node 9742dd49-3487-4991-bf44-2c493fea6132 does not have a summary. Skipping filtering.\n",
      "Node 73e00305-3349-472e-9966-4d4778d6b62c does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:  31%|‚ñà‚ñà‚ñà‚ñè      | 58/185 [00:15<00:30,  4.14it/s]Node 14648f96-8295-4efd-9a44-51c2a3e4d084 does not have a summary. Skipping filtering.\n",
      "Node dd4768ca-e691-4441-80f3-24ad5cfed52d does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:  38%|‚ñà‚ñà‚ñà‚ñä      | 71/185 [00:16<00:20,  5.46it/s]Node e0ee6113-16c1-432a-8c82-8a71042d677f does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 89/185 [00:20<00:18,  5.30it/s]Node e28d0df2-2927-4aed-9d6c-d1ae6ef132fc does not have a summary. Skipping filtering.\n",
      "Node 54889a00-0fc2-4b84-9da5-7ce06f5ccecc does not have a summary. Skipping filtering.\n",
      "Node 86867d41-963d-4c51-b2f3-6b56b5e52a29 does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 96/185 [00:22<00:19,  4.65it/s]Node 16b4202e-e689-4fed-a116-e1f272143815 does not have a summary. Skipping filtering.\n",
      "Node f008f028-33d3-450a-8006-08889169d584 does not have a summary. Skipping filtering.\n",
      "Node 5fd96d31-1ce3-4694-b854-f5b664315f7d does not have a summary. Skipping filtering.\n",
      "Node a90ba90f-3237-4c08-96f2-06d25d6794e4 does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 128/185 [00:27<00:10,  5.45it/s]Node fce94d42-991e-4a20-8b26-aa5286a73165 does not have a summary. Skipping filtering.\n",
      "Node e843f4e3-5e54-4a8e-a8a2-603fcfc2ad22 does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 135/185 [00:29<00:10,  4.76it/s]Node 226a296a-409e-4703-ac6a-bf911dfc3039 does not have a summary. Skipping filtering.\n",
      "Node 505b29a8-45c0-46cd-b8ed-ede5a69b9fe6 does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 155/185 [00:33<00:05,  5.02it/s]Node f6353b0b-a676-4e08-ba5e-b69eaf131aca does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 185/185 [00:37<00:00,  4.87it/s]\n",
      "Applying EmbeddingExtractor: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 162/162 [00:04<00:00, 36.93it/s]\n",
      "Applying ThemesExtractor: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 184/184 [00:44<00:00,  4.16it/s]\n",
      "Applying NERExtractor: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 184/184 [00:43<00:00,  4.23it/s]\n",
      "Applying CosineSimilarityBuilder: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 40.32it/s]\n",
      "Applying OverlapScoreBuilder: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.79it/s]\n",
      "Generating personas: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  1.78it/s]\n",
      "Generating Scenarios: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:10<00:00,  3.57s/it]\n",
      "Generating Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:04<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Pipeline Results:\n",
      "  üìÑ Documents Processed: 185\n",
      "  üß† Knowledge Graph Nodes: 185\n",
      "  üß™ Test Cases Generated: 10\n",
      "  üîß Generation Method: knowledge_graph\n"
     ]
    }
   ],
   "source": [
    "# Prepare input data - convert PDF files to ByteStream objects\n",
    "pdf_sources = [Path(\"./data_for_indexing/howpeopleuseai.pdf\")]\n",
    "\n",
    "try:\n",
    "    # Run pipeline with both input types\n",
    "    result = pipeline.run({\n",
    "            \"pdf_converter\": {\"sources\": pdf_sources}\n",
    "        })\n",
    "    print(\"\\nüìä Pipeline Results:\")\n",
    "    print(f\"  üìÑ Documents Processed: {result['doc_converter']['document_count']}\")\n",
    "    print(f\"  üß† Knowledge Graph Nodes: {result['kg_generator']['node_count']}\")\n",
    "    print(f\"  üß™ Test Cases Generated: {result['test_generator']['testset_size']}\")\n",
    "    print(f\"  üîß Generation Method: {result['test_generator']['generation_method']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error processing web content: {str(e)}\")\n",
    "    print(\"This might be due to network issues or website access restrictions.\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6201635",
   "metadata": {},
   "source": [
    "### Understanding the PDF Processing Pipeline Architecture\n",
    "\n",
    "The pipeline we're building follows this flow:\n",
    "\n",
    "```\n",
    "PDF File ‚Üí PDF Converter ‚Üí Document Cleaner ‚Üí Document Splitter \n",
    "    ‚Üì\n",
    "Document Converter ‚Üí Knowledge Graph Generator\n",
    "    ‚Üì                         ‚Üì\n",
    "Test Generator ‚Üê ‚Üê ‚Üê ‚Üê ‚Üê ‚Üê ‚Üê ‚Üê\n",
    "    ‚Üì\n",
    "Test Dataset Saver\n",
    "```\n",
    "\n",
    "**Key Design Decisions:**\n",
    "\n",
    "1. **Document Processing Chain**: We clean and split documents before knowledge graph generation to ensure high-quality input\n",
    "2. **Dual Input to Test Generator**: Both the knowledge graph and original documents are provided to enable fallback generation methods\n",
    "3. **Configurable Test Distribution**: We can control the types of questions generated (single-hop vs multi-hop)\n",
    "\n",
    "**Pipeline Parameters Explained:**\n",
    "- `test_size=10`: Number of question-answer pairs to generate\n",
    "- `split_length=5`: Number of sentences per document chunk\n",
    "- `query_distribution`: Controls complexity of generated questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02bbe757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì∏ Pipeline diagram saved to: ./images/pdf_knowledge_graph_pipeline.png\n"
     ]
    }
   ],
   "source": [
    "pipeline.draw(path=\"./images/pdf_knowledge_graph_pipeline.png\")\n",
    "print(\"üì∏ Pipeline diagram saved to: ./images/pdf_knowledge_graph_pipeline.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ec75012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Synthetic Tests Sample:\n",
      "First 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What NBER do?</td>\n",
       "      <td>['NBER WORKING PAPER SERIES\\nHOW PEOPLE USE CH...</td>\n",
       "      <td>The National Bureau of Economic Research (NBER...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the role of Zoe Hitzig in the context ...</td>\n",
       "      <td>['The views expressed herein are those of the ...</td>\n",
       "      <td>Zoe Hitzig is one of the co-authors of the NBE...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who are the authors of the NBER Working Paper ...</td>\n",
       "      <td>['¬© 2025 by Aaron Chatterji, Thomas Cunningham...</td>\n",
       "      <td>The authors of the NBER Working Paper No. 3425...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wht is the breakdown of convrsation topics in ...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nShares are calculated from a samp...</td>\n",
       "      <td>Figure 11 presents the breakdown of conversati...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the differences in user demographics ...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\n8Handa et al. (2025) report that ...</td>\n",
       "      <td>Handa et al. (2025) report that the discrepanc...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0                                      What NBER do?   \n",
       "1  What is the role of Zoe Hitzig in the context ...   \n",
       "2  Who are the authors of the NBER Working Paper ...   \n",
       "3  Wht is the breakdown of convrsation topics in ...   \n",
       "4  What are the differences in user demographics ...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  ['NBER WORKING PAPER SERIES\\nHOW PEOPLE USE CH...   \n",
       "1  ['The views expressed herein are those of the ...   \n",
       "2  ['¬© 2025 by Aaron Chatterji, Thomas Cunningham...   \n",
       "3  ['<1-hop>\\n\\nShares are calculated from a samp...   \n",
       "4  ['<1-hop>\\n\\n8Handa et al. (2025) report that ...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  The National Bureau of Economic Research (NBER...   \n",
       "1  Zoe Hitzig is one of the co-authors of the NBE...   \n",
       "2  The authors of the NBER Working Paper No. 3425...   \n",
       "3  Figure 11 presents the breakdown of conversati...   \n",
       "4  Handa et al. (2025) report that the discrepanc...   \n",
       "\n",
       "                        synthesizer_name  \n",
       "0  single_hop_specific_query_synthesizer  \n",
       "1  single_hop_specific_query_synthesizer  \n",
       "2  single_hop_specific_query_synthesizer  \n",
       "3   multi_hop_specific_query_synthesizer  \n",
       "4   multi_hop_specific_query_synthesizer  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What does Table 5 reveal about the model-plura...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nA development set (46 items) was ...</td>\n",
       "      <td>Table 5 reveals that the model-plurality agree...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How did the adoption of ChatGPT vary between l...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nThe figure below plots this propo...</td>\n",
       "      <td>The adoption of ChatGPT grew dramatically in l...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What impact did the adoption of ChatGPT have o...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nThe figure below plots this propo...</td>\n",
       "      <td>By July 2025, the adoption of ChatGPT had a si...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What are the total daily counts of messages an...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nTotal daily counts are exact meas...</td>\n",
       "      <td>Total daily counts are exact measurements of m...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How does the quality of user interaction relat...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nWe do not show the shares for the...</td>\n",
       "      <td>The quality of user interaction is assessed th...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "5  What does Table 5 reveal about the model-plura...   \n",
       "6  How did the adoption of ChatGPT vary between l...   \n",
       "7  What impact did the adoption of ChatGPT have o...   \n",
       "8  What are the total daily counts of messages an...   \n",
       "9  How does the quality of user interaction relat...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "5  ['<1-hop>\\n\\nA development set (46 items) was ...   \n",
       "6  ['<1-hop>\\n\\nThe figure below plots this propo...   \n",
       "7  ['<1-hop>\\n\\nThe figure below plots this propo...   \n",
       "8  ['<1-hop>\\n\\nTotal daily counts are exact meas...   \n",
       "9  ['<1-hop>\\n\\nWe do not show the shares for the...   \n",
       "\n",
       "                                           reference  \\\n",
       "5  Table 5 reveals that the model-plurality agree...   \n",
       "6  The adoption of ChatGPT grew dramatically in l...   \n",
       "7  By July 2025, the adoption of ChatGPT had a si...   \n",
       "8  Total daily counts are exact measurements of m...   \n",
       "9  The quality of user interaction is assessed th...   \n",
       "\n",
       "                       synthesizer_name  \n",
       "5  multi_hop_specific_query_synthesizer  \n",
       "6  multi_hop_abstract_query_synthesizer  \n",
       "7  multi_hop_abstract_query_synthesizer  \n",
       "8  multi_hop_abstract_query_synthesizer  \n",
       "9  multi_hop_abstract_query_synthesizer  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load and display the generated synthetic tests\n",
    "test_file_path = \"data_for_eval/synthetic_tests_10_from_pdf.csv\"\n",
    "\n",
    "if os.path.exists(test_file_path):\n",
    "    synthetic_tests_df = pd.read_csv(test_file_path)\n",
    "    print(\"\\nüß™ Synthetic Tests Sample:\")\n",
    "    print(\"First 5 rows:\")\n",
    "    display(synthetic_tests_df.head())\n",
    "    print(\"Last 5 rows:\")\n",
    "    display(synthetic_tests_df.tail())\n",
    "else:\n",
    "    print(\"‚ùå Synthetic test file not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa35cba",
   "metadata": {},
   "source": [
    "### Analyzing the Generated Test Dataset\n",
    "\n",
    "Now let's examine the synthetic test data that was generated from our PDF processing pipeline.\n",
    "\n",
    "**What to Look For:**\n",
    "- **Question Quality**: Are the questions grammatically correct and meaningful?\n",
    "- **Answer Accuracy**: Do the answers correctly reflect the source material?\n",
    "- **Question Types**: Notice the variety of single-hop and multi-hop questions\n",
    "- **Context Relevance**: Check if the reference contexts support the answers\n",
    "\n",
    "**Common Question Types You'll See:**\n",
    "1. **Single-hop questions**: Direct factual queries (e.g., \"What is X?\")\n",
    "2. **Multi-hop specific**: Questions requiring connecting specific facts\n",
    "3. **Multi-hop abstract**: Questions requiring broader reasoning across multiple concepts\n",
    "\n",
    "**PDF-Specific Considerations:**\n",
    "- **Text Extraction Quality**: PDFs may have formatting artifacts that affect question quality\n",
    "- **Document Structure**: Well-structured PDFs tend to produce better knowledge graphs\n",
    "- **Content Density**: Dense technical content may result in more complex questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9495df",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We've Accomplished\n",
    "\n",
    "In this notebook, we successfully:\n",
    "\n",
    "1. **Built a PDF Processing Pipeline**: Created an end-to-end pipeline specifically optimized for PDF documents\n",
    "2. **Generated Knowledge Graphs**: Converted unstructured PDF content into structured knowledge representations\n",
    "3. **Produced Synthetic Test Data**: Created question-answer pairs for evaluation and testing purposes\n",
    "4. **Analyzed Results**: Examined the quality and characteristics of the generated synthetic dataset\n",
    "\n",
    "### Key Benefits of This Approach\n",
    "\n",
    "- **Automated Processing**: No manual intervention required for PDF to test data conversion\n",
    "- **Scalable**: Can process multiple PDF documents in batch\n",
    "- **Quality-Driven**: Knowledge graphs act as a quality filter for better synthetic questions\n",
    "- **Configurable**: Easy to adjust parameters for different use cases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c369101b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-with-haystack-ch5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
