{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20afb80e",
   "metadata": {},
   "source": [
    "üîß **Setup Required**: Before running this notebook, please follow the [setup instructions](../README.md#setup-instructions) to configure your environment and API keys."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d0d577",
   "metadata": {},
   "source": [
    "# PDF Knowledge Graph and Synthetic Data Generation Pipeline\n",
    "\n",
    "This notebook demonstrates how to build a comprehensive pipeline for PDF document processing that:\n",
    "1. **Extracts content** from PDF files using Haystack's PyPDFToDocument converter\n",
    "2. **Preprocesses the text** with cleaning and splitting components\n",
    "3. **Creates a knowledge graph** from the processed documents\n",
    "4. **Generates synthetic test data** using the knowledge graph\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "- How to build end-to-end Haystack pipelines for PDF processing\n",
    "- The relationship between knowledge graphs and synthetic test data generation\n",
    "- Best practices for PDF document preprocessing\n",
    "- How to evaluate synthetic datasets generated from PDF content\n",
    "\n",
    "## Key Components\n",
    "- **PyPDFToDocument**: Converts PDF files to Haystack Document objects\n",
    "- **DocumentCleaner**: Removes extra whitespaces and empty lines\n",
    "- **DocumentSplitter**: Breaks documents into manageable chunks\n",
    "- **KnowledgeGraphGenerator**: Creates structured knowledge representations\n",
    "- **SyntheticTestGenerator**: Produces question-answer pairs for evaluation\n",
    "\n",
    "## Why This Approach?\n",
    "Using knowledge graphs as an intermediate step improves the quality of synthetic test generation because:\n",
    "- Knowledge graphs capture relationships between entities\n",
    "- They provide structured context for question generation\n",
    "- The resulting questions are more coherent and factually grounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b77acac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurafunderburk/Documents/GitHub/Building-Natural-Language-Pipelines/ch5/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x168c43920>\n",
       "üöÖ Components\n",
       "  - pdf_converter: PyPDFToDocument\n",
       "  - doc_cleaner: DocumentCleaner\n",
       "  - doc_splitter: DocumentSplitter\n",
       "  - doc_converter: DocumentToLangChainConverter\n",
       "  - kg_generator: KnowledgeGraphGenerator\n",
       "  - test_generator: SyntheticTestGenerator\n",
       "  - test_saver: TestDatasetSaver\n",
       "üõ§Ô∏è Connections\n",
       "  - pdf_converter.documents -> doc_cleaner.documents (list[Document])\n",
       "  - doc_cleaner.documents -> doc_splitter.documents (list[Document])\n",
       "  - doc_splitter.documents -> doc_converter.documents (list[Document])\n",
       "  - doc_converter.langchain_documents -> kg_generator.documents (List[Document])\n",
       "  - doc_converter.langchain_documents -> test_generator.documents (List[Document])\n",
       "  - kg_generator.knowledge_graph -> test_generator.knowledge_graph (KnowledgeGraph)\n",
       "  - test_generator.testset -> test_saver.testset (DataFrame)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from haystack import Pipeline\n",
    "from haystack.components.converters import PyPDFToDocument\n",
    "from haystack.components.preprocessors import (\n",
    "    DocumentCleaner,\n",
    "    DocumentSplitter)\n",
    "from pathlib import Path\n",
    "from scripts.knowledge_graph_component import KnowledgeGraphGenerator\n",
    "from scripts.langchaindocument_component import DocumentToLangChainConverter\n",
    "from scripts.synthetic_test_components import SyntheticTestGenerator, TestDatasetSaver\n",
    "                                                    \n",
    "# Load environment variables\n",
    "load_dotenv(\"./.env\")\n",
    "\n",
    "        \n",
    "# Create pipeline components\n",
    "pdf_converter = PyPDFToDocument()\n",
    "doc_cleaner = DocumentCleaner(\n",
    "    remove_empty_lines=True,\n",
    "    remove_extra_whitespaces=True,\n",
    ")\n",
    "doc_splitter = DocumentSplitter(split_by=\"sentence\",\n",
    "                                split_length=50,\n",
    "                                split_overlap=5)\n",
    "doc_converter = DocumentToLangChainConverter()\n",
    "kg_generator = KnowledgeGraphGenerator(apply_transforms=True)\n",
    "\n",
    "\n",
    "test_generator = SyntheticTestGenerator(\n",
    "            test_size=10,\n",
    "            llm_model=\"gpt-4o-mini\",\n",
    "            embedder_model=\"text-embedding-ada-002\",\n",
    "            query_distribution=[\n",
    "                (\"single_hop\", 0.3),\n",
    "                (\"multi_hop_specific\", 0.3),\n",
    "                (\"multi_hop_abstract\", 0.4)\n",
    "            ],\n",
    "            openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "        )\n",
    "test_saver = TestDatasetSaver(\"data_for_eval/synthetic_tests_10_from_pdf.csv\")\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline()\n",
    "pipeline.add_component(\"pdf_converter\", pdf_converter)\n",
    "pipeline.add_component(\"doc_cleaner\", doc_cleaner)\n",
    "pipeline.add_component(\"doc_splitter\", doc_splitter)\n",
    "pipeline.add_component(\"doc_converter\", doc_converter)\n",
    "pipeline.add_component(\"kg_generator\", kg_generator)\n",
    "pipeline.add_component(\"test_generator\", test_generator)\n",
    "pipeline.add_component(\"test_saver\", test_saver)\n",
    "\n",
    "# Connect components in sequence\n",
    "pipeline.connect(\"pdf_converter.documents\", \"doc_cleaner.documents\")\n",
    "pipeline.connect(\"doc_cleaner.documents\", \"doc_splitter.documents\")\n",
    "pipeline.connect(\"doc_splitter.documents\", \"doc_converter.documents\")\n",
    "pipeline.connect(\"doc_converter.langchain_documents\", \"kg_generator.documents\")\n",
    "pipeline.connect(\"kg_generator.knowledge_graph\", \"test_generator.knowledge_graph\")\n",
    "pipeline.connect(\"doc_converter.langchain_documents\", \"test_generator.documents\")\n",
    "pipeline.connect(\"test_generator.testset\", \"test_saver.testset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78722540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying HeadlinesExtractor: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [00:05<00:00,  3.34it/s]\n",
      "Applying HeadlinesExtractor: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [00:05<00:00,  3.34it/s]\n",
      "Applying HeadlineSplitter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [00:00<00:00, 511.67it/s]\n",
      "Applying SummaryExtractor:   0%|          | 0/17 [00:00<?, ?it/s]\n",
      "Applying SummaryExtractor: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [00:06<00:00,  2.73it/s]\n",
      "Applying SummaryExtractor: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [00:06<00:00,  2.73it/s]\n",
      "Applying CustomNodeFilter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [00:11<00:00,  4.16it/s]\n",
      "Applying CustomNodeFilter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [00:11<00:00,  4.16it/s]\n",
      "Applying EmbeddingExtractor: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [00:01<00:00, 11.93it/s]\n",
      "Applying EmbeddingExtractor: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [00:01<00:00, 11.93it/s]\n",
      "Applying ThemesExtractor: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:11<00:00,  3.85it/s]\n",
      "Applying ThemesExtractor: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:11<00:00,  3.85it/s]\n",
      "Applying NERExtractor: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:12<00:00,  3.68it/s]\n",
      "Applying NERExtractor: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:12<00:00,  3.68it/s]\n",
      "Applying CosineSimilarityBuilder: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 471.75it/s]\n",
      "Applying CosineSimilarityBuilder: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 471.75it/s]\n",
      "Applying OverlapScoreBuilder: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 95.33it/s]\n",
      "Applying OverlapScoreBuilder: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 95.33it/s]\n",
      "Generating personas: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.38it/s]\n",
      "Generating personas: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.38it/s]\n",
      "Generating Scenarios: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:12<00:00,  4.14s/it]\n",
      "Generating Scenarios: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:12<00:00,  4.14s/it]\n",
      "Generating Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:06<00:00,  1.49it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Pipeline Results:\n",
      "  üìÑ Documents Processed: 17\n",
      "  üß† Knowledge Graph Nodes: 17\n",
      "  üß™ Test Cases Generated: 10\n",
      "  üîß Generation Method: knowledge_graph\n"
     ]
    }
   ],
   "source": [
    "# Prepare input data - convert PDF files to ByteStream objects\n",
    "pdf_sources = [Path(\"./data_for_indexing/howpeopleuseai.pdf\")]\n",
    "\n",
    "try:\n",
    "    # Run pipeline with both input types\n",
    "    result = pipeline.run({\n",
    "            \"pdf_converter\": {\"sources\": pdf_sources}\n",
    "        })\n",
    "    print(\"\\nüìä Pipeline Results:\")\n",
    "    print(f\"  üìÑ Documents Processed: {result['doc_converter']['document_count']}\")\n",
    "    print(f\"  üß† Knowledge Graph Nodes: {result['kg_generator']['node_count']}\")\n",
    "    print(f\"  üß™ Test Cases Generated: {result['test_generator']['testset_size']}\")\n",
    "    print(f\"  üîß Generation Method: {result['test_generator']['generation_method']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error processing web content: {str(e)}\")\n",
    "    print(\"This might be due to network issues or website access restrictions.\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6201635",
   "metadata": {},
   "source": [
    "### Understanding the PDF Processing Pipeline Architecture\n",
    "\n",
    "The pipeline we're building follows this flow:\n",
    "\n",
    "```\n",
    "PDF File ‚Üí PDF Converter ‚Üí Document Cleaner ‚Üí Document Splitter \n",
    "    ‚Üì\n",
    "Document Converter ‚Üí Knowledge Graph Generator\n",
    "    ‚Üì                         ‚Üì\n",
    "Test Generator ‚Üê ‚Üê ‚Üê ‚Üê ‚Üê ‚Üê ‚Üê ‚Üê\n",
    "    ‚Üì\n",
    "Test Dataset Saver\n",
    "```\n",
    "\n",
    "**Key Design Decisions:**\n",
    "\n",
    "1. **Document Processing Chain**: We clean and split documents before knowledge graph generation to ensure high-quality input\n",
    "2. **Dual Input to Test Generator**: Both the knowledge graph and original documents are provided to enable fallback generation methods\n",
    "3. **Configurable Test Distribution**: We can control the types of questions generated (single-hop vs multi-hop)\n",
    "\n",
    "**Pipeline Parameters Explained:**\n",
    "- `test_size=10`: Number of question-answer pairs to generate\n",
    "- `split_length=50`: Number of sentences per document chunk\n",
    "- `query_distribution`: Controls complexity of generated questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02bbe757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì∏ Pipeline diagram saved to: ./images/pdf_knowledge_graph_pipeline.png\n"
     ]
    }
   ],
   "source": [
    "pipeline.draw(path=\"./images/pdf_knowledge_graph_pipeline.png\")\n",
    "print(\"üì∏ Pipeline diagram saved to: ./images/pdf_knowledge_graph_pipeline.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d60f85",
   "metadata": {},
   "source": [
    "![](./images/pdf_knowledge_graph_pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ec75012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Synthetic Tests Sample:\n",
      "First 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who is Kevin Wadman and what is his role in th...</td>\n",
       "      <td>['NBER WORKING PAPER SERIES\\nHOW PEOPLE USE CH...</td>\n",
       "      <td>Kevin Wadman is one of the co-authors of the N...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wen did ChatGPT launch and how has it grown si...</td>\n",
       "      <td>['ABSTRACT Despite the rapid adoption of LLM c...</td>\n",
       "      <td>ChatGPT launched in November 2022. By July 202...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How many US adults used ChatGPT in late 2024?</td>\n",
       "      <td>['to classify messages without any human seein...</td>\n",
       "      <td>28% of US adults used ChatGPT in late 2024, wh...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are the primary work activities associate...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\n5.4 O*NET Work Activities\\nWe map...</td>\n",
       "      <td>The primary work activities associated with Ch...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What datasets were used in the analysis and wh...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nData and Privacy\\nIn this section...</td>\n",
       "      <td>The analysis utilized several datasets, includ...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  Who is Kevin Wadman and what is his role in th...   \n",
       "1  Wen did ChatGPT launch and how has it grown si...   \n",
       "2      How many US adults used ChatGPT in late 2024?   \n",
       "3  What are the primary work activities associate...   \n",
       "4  What datasets were used in the analysis and wh...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  ['NBER WORKING PAPER SERIES\\nHOW PEOPLE USE CH...   \n",
       "1  ['ABSTRACT Despite the rapid adoption of LLM c...   \n",
       "2  ['to classify messages without any human seein...   \n",
       "3  ['<1-hop>\\n\\n5.4 O*NET Work Activities\\nWe map...   \n",
       "4  ['<1-hop>\\n\\nData and Privacy\\nIn this section...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  Kevin Wadman is one of the co-authors of the N...   \n",
       "1  ChatGPT launched in November 2022. By July 202...   \n",
       "2  28% of US adults used ChatGPT in late 2024, wh...   \n",
       "3  The primary work activities associated with Ch...   \n",
       "4  The analysis utilized several datasets, includ...   \n",
       "\n",
       "                        synthesizer_name  \n",
       "0  single_hop_specific_query_synthesizer  \n",
       "1  single_hop_specific_query_synthesizer  \n",
       "2  single_hop_specific_query_synthesizer  \n",
       "3   multi_hop_specific_query_synthesizer  \n",
       "4   multi_hop_specific_query_synthesizer  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What does Zao-Sanders (2025) say about the dis...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nThis is consistent with the fact ...</td>\n",
       "      <td>Zao-Sanders (2025) indicates that their findin...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How has the usage of ChatGPT evolved since its...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nNBER WORKING PAPER SERIES\\nHOW PE...</td>\n",
       "      <td>Since its launch in November 2022, the usage o...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What patterns of ChatGPT usage have been obser...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nNBER WORKING PAPER SERIES\\nHOW PE...</td>\n",
       "      <td>Since its launch in November 2022, ChatGPT has...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How has the usage of ChatGPT evolved since its...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nNBER WORKING PAPER SERIES\\nHOW PE...</td>\n",
       "      <td>Since its launch in November 2022, the usage o...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How does ChatGPT usage differ between work-rel...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nNBER WORKING PAPER SERIES\\nHOW PE...</td>\n",
       "      <td>ChatGPT usage shows a significant difference b...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "5  What does Zao-Sanders (2025) say about the dis...   \n",
       "6  How has the usage of ChatGPT evolved since its...   \n",
       "7  What patterns of ChatGPT usage have been obser...   \n",
       "8  How has the usage of ChatGPT evolved since its...   \n",
       "9  How does ChatGPT usage differ between work-rel...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "5  ['<1-hop>\\n\\nThis is consistent with the fact ...   \n",
       "6  ['<1-hop>\\n\\nNBER WORKING PAPER SERIES\\nHOW PE...   \n",
       "7  ['<1-hop>\\n\\nNBER WORKING PAPER SERIES\\nHOW PE...   \n",
       "8  ['<1-hop>\\n\\nNBER WORKING PAPER SERIES\\nHOW PE...   \n",
       "9  ['<1-hop>\\n\\nNBER WORKING PAPER SERIES\\nHOW PE...   \n",
       "\n",
       "                                           reference  \\\n",
       "5  Zao-Sanders (2025) indicates that their findin...   \n",
       "6  Since its launch in November 2022, the usage o...   \n",
       "7  Since its launch in November 2022, ChatGPT has...   \n",
       "8  Since its launch in November 2022, the usage o...   \n",
       "9  ChatGPT usage shows a significant difference b...   \n",
       "\n",
       "                       synthesizer_name  \n",
       "5  multi_hop_specific_query_synthesizer  \n",
       "6  multi_hop_abstract_query_synthesizer  \n",
       "7  multi_hop_abstract_query_synthesizer  \n",
       "8  multi_hop_abstract_query_synthesizer  \n",
       "9  multi_hop_abstract_query_synthesizer  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load and display the generated synthetic tests\n",
    "test_file_path = \"data_for_eval/synthetic_tests_10_from_pdf.csv\"\n",
    "\n",
    "if os.path.exists(test_file_path):\n",
    "    synthetic_tests_df = pd.read_csv(test_file_path)\n",
    "    print(\"\\nüß™ Synthetic Tests Sample:\")\n",
    "    print(\"First 5 rows:\")\n",
    "    display(synthetic_tests_df.head())\n",
    "    print(\"Last 5 rows:\")\n",
    "    display(synthetic_tests_df.tail())\n",
    "else:\n",
    "    print(\"‚ùå Synthetic test file not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa35cba",
   "metadata": {},
   "source": [
    "### Analyzing the Generated Test Dataset\n",
    "\n",
    "Now let's examine the synthetic test data that was generated from our PDF processing pipeline.\n",
    "\n",
    "**What to Look For:**\n",
    "- **Question Quality**: Are the questions grammatically correct and meaningful?\n",
    "- **Answer Accuracy**: Do the answers correctly reflect the source material?\n",
    "- **Question Types**: Notice the variety of single-hop and multi-hop questions\n",
    "- **Context Relevance**: Check if the reference contexts support the answers\n",
    "\n",
    "**Common Question Types You'll See:**\n",
    "1. **Single-hop questions**: Direct factual queries (e.g., \"What is X?\")\n",
    "2. **Multi-hop specific**: Questions requiring connecting specific facts\n",
    "3. **Multi-hop abstract**: Questions requiring broader reasoning across multiple concepts\n",
    "\n",
    "**PDF-Specific Considerations:**\n",
    "- **Text Extraction Quality**: PDFs may have formatting artifacts that affect question quality\n",
    "- **Document Structure**: Well-structured PDFs tend to produce better knowledge graphs\n",
    "- **Content Density**: Dense technical content may result in more complex questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9495df",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We've Accomplished\n",
    "\n",
    "In this notebook, we successfully:\n",
    "\n",
    "1. **Built a PDF Processing Pipeline**: Created an end-to-end pipeline specifically optimized for PDF documents\n",
    "2. **Generated Knowledge Graphs**: Converted unstructured PDF content into structured knowledge representations\n",
    "3. **Produced Synthetic Test Data**: Created question-answer pairs for evaluation and testing purposes\n",
    "4. **Analyzed Results**: Examined the quality and characteristics of the generated synthetic dataset\n",
    "\n",
    "### Key Benefits of This Approach\n",
    "\n",
    "- **Automated Processing**: No manual intervention required for PDF to test data conversion\n",
    "- **Scalable**: Can process multiple PDF documents in batch\n",
    "- **Quality-Driven**: Knowledge graphs act as a quality filter for better synthetic questions\n",
    "- **Configurable**: Easy to adjust parameters for different use cases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c369101b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-with-haystack-ch5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
