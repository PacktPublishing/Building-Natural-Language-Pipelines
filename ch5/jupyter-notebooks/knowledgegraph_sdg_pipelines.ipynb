{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0de4ea5e",
   "metadata": {},
   "source": [
    "🔧 **Setup Required**: Before running this notebook, please follow the [setup instructions](../README.md#setup-instructions) to configure your environment and API keys."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dad9b8",
   "metadata": {},
   "source": [
    "# Knowledge Graph and Synthetic Data Generation Pipelines\n",
    "\n",
    "This notebook demonstrates how to build comprehensive pipelines for:\n",
    "1. **Knowledge Graph Creation** - Converting documents into structured knowledge representations\n",
    "2. **Synthetic Test Data Generation** - Creating question-answer pairs for evaluation\n",
    "3. **Multi-Source Processing** - Working with PDFs, web content, and other document types\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "- How to build end-to-end Haystack pipelines for knowledge extraction\n",
    "- The relationship between knowledge graphs and test data generation\n",
    "- Best practices for processing different document formats\n",
    "- How to evaluate and validate synthetic datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb2f063",
   "metadata": {},
   "source": [
    "## Part 1: PDF Processing Pipeline\n",
    "\n",
    "### Overview\n",
    "In this section, we'll build a comprehensive pipeline that:\n",
    "1. **Extracts content** from PDF files using Haystack's PyPDFToDocument converter\n",
    "2. **Preprocesses the text** with cleaning and splitting components\n",
    "3. **Creates a knowledge graph** from the processed documents\n",
    "4. **Generates synthetic test data** using the knowledge graph\n",
    "\n",
    "### Key Components\n",
    "- **PyPDFToDocument**: Converts PDF files to Haystack Document objects\n",
    "- **DocumentCleaner**: Removes extra whitespaces and empty lines\n",
    "- **DocumentSplitter**: Breaks documents into manageable chunks\n",
    "- **KnowledgeGraphGenerator**: Creates structured knowledge representations\n",
    "- **SyntheticTestGenerator**: Produces question-answer pairs for evaluation\n",
    "\n",
    "### Why This Approach?\n",
    "Using knowledge graphs as an intermediate step improves the quality of synthetic test generation because:\n",
    "- Knowledge graphs capture relationships between entities\n",
    "- They provide structured context for question generation\n",
    "- The resulting questions are more coherent and factually grounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c86671d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurafunderburk/Documents/GitHub/Building-Natural-Language-Pipelines/ch5/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x362fddeb0>\n",
       "🚅 Components\n",
       "  - pdf_converter: PyPDFToDocument\n",
       "  - doc_cleaner: DocumentCleaner\n",
       "  - doc_splitter: DocumentSplitter\n",
       "  - doc_converter: DocumentToLangChainConverter\n",
       "  - kg_generator: KnowledgeGraphGenerator\n",
       "  - test_generator: SyntheticTestGenerator\n",
       "  - test_saver: TestDatasetSaver\n",
       "🛤️ Connections\n",
       "  - pdf_converter.documents -> doc_cleaner.documents (list[Document])\n",
       "  - doc_cleaner.documents -> doc_splitter.documents (list[Document])\n",
       "  - doc_splitter.documents -> doc_converter.documents (list[Document])\n",
       "  - doc_converter.langchain_documents -> kg_generator.documents (List[Document])\n",
       "  - doc_converter.langchain_documents -> test_generator.documents (List[Document])\n",
       "  - kg_generator.knowledge_graph -> test_generator.knowledge_graph (KnowledgeGraph)\n",
       "  - test_generator.testset -> test_saver.testset (DataFrame)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from haystack import Pipeline\n",
    "from haystack.components.converters import PyPDFToDocument\n",
    "from haystack.components.preprocessors import (\n",
    "    DocumentCleaner,\n",
    "    DocumentSplitter)\n",
    "from pathlib import Path\n",
    "from scripts.knowledge_graph_component import KnowledgeGraphGenerator,\\\n",
    "                                                DocumentToLangChainConverter\n",
    "from scripts.synthetic_test_components import SyntheticTestGenerator,\\\n",
    "                                                TestDatasetSaver\n",
    "                                                    \n",
    "# Load environment variables\n",
    "load_dotenv(\"./.env\")\n",
    "\n",
    "        \n",
    "# Create pipeline components\n",
    "pdf_converter = PyPDFToDocument()\n",
    "doc_cleaner = DocumentCleaner(remove_empty_lines=True,\n",
    "                                remove_extra_whitespaces=True)\n",
    "doc_splitter = DocumentSplitter(split_by=\"sentence\",\n",
    "                                split_length=50,\n",
    "                                split_overlap=5)\n",
    "doc_converter = DocumentToLangChainConverter()\n",
    "kg_generator = KnowledgeGraphGenerator(apply_transforms=True)\n",
    "\n",
    "\n",
    "test_generator = SyntheticTestGenerator(\n",
    "    testset_size=10,  \n",
    "    llm_model=\"gpt-4o-mini\",\n",
    "    query_distribution=[\n",
    "        (\"single_hop\", 0.25), \n",
    "        (\"multi_hop_specific\", 0.25),\n",
    "        (\"multi_hop_abstract\", 0.5)\n",
    "    ],\n",
    "    # Optional: Add max_testset_size=5 if you want to limit due to API constraints\n",
    "    # max_testset_size=5  # Uncomment this line if you experience API timeouts\n",
    ")\n",
    "test_saver = TestDatasetSaver(\"data_for_eval/synthetic_tests_10_from_pdf.csv\")\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline()\n",
    "pipeline.add_component(\"pdf_converter\", pdf_converter)\n",
    "pipeline.add_component(\"doc_cleaner\", doc_cleaner)\n",
    "pipeline.add_component(\"doc_splitter\", doc_splitter)\n",
    "pipeline.add_component(\"doc_converter\", doc_converter)\n",
    "pipeline.add_component(\"kg_generator\", kg_generator)\n",
    "pipeline.add_component(\"test_generator\", test_generator)\n",
    "pipeline.add_component(\"test_saver\", test_saver)\n",
    "\n",
    "# Connect components in sequence\n",
    "pipeline.connect(\"pdf_converter.documents\", \"doc_cleaner.documents\")\n",
    "pipeline.connect(\"doc_cleaner.documents\", \"doc_splitter.documents\")\n",
    "pipeline.connect(\"doc_splitter.documents\", \"doc_converter.documents\")\n",
    "pipeline.connect(\"doc_converter.langchain_documents\", \"kg_generator.documents\")\n",
    "pipeline.connect(\"kg_generator.knowledge_graph\", \"test_generator.knowledge_graph\")\n",
    "pipeline.connect(\"doc_converter.langchain_documents\", \"test_generator.documents\")\n",
    "pipeline.connect(\"test_generator.testset\", \"test_saver.testset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24b94f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying HeadlinesExtractor: 100%|██████████| 17/17 [00:09<00:00,  1.71it/s]\n",
      "Applying HeadlineSplitter: 100%|██████████| 17/17 [00:00<00:00, 546.84it/s]\n",
      "Applying SummaryExtractor: 100%|██████████| 17/17 [00:10<00:00,  1.70it/s]\n",
      "Applying CustomNodeFilter: 100%|██████████| 50/50 [00:26<00:00,  1.85it/s]\n",
      "Applying EmbeddingExtractor: 100%|██████████| 17/17 [00:04<00:00,  3.61it/s]\n",
      "Applying ThemesExtractor: 100%|██████████| 45/45 [00:27<00:00,  1.61it/s]\n",
      "Applying NERExtractor: 100%|██████████| 45/45 [00:24<00:00,  1.86it/s]\n",
      "Applying CosineSimilarityBuilder: 100%|██████████| 1/1 [00:00<00:00, 124.77it/s]\n",
      "Applying OverlapScoreBuilder: 100%|██████████| 1/1 [00:00<00:00, 73.70it/s]\n",
      "Generating personas: 100%|██████████| 3/3 [00:02<00:00,  1.28it/s]\n",
      "Generating Scenarios: 100%|██████████| 3/3 [00:15<00:00,  5.21s/it]\n",
      "Generating Samples: 100%|██████████| 11/11 [00:06<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Pipeline Results:\n",
      "  📄 Documents Processed: 17\n",
      "  🧠 Knowledge Graph Nodes: 17\n",
      "  🧪 Test Cases Generated: 11\n",
      "  🔧 Generation Method: knowledge_graph\n"
     ]
    }
   ],
   "source": [
    "# Prepare input data - convert PDF files to ByteStream objects\n",
    "pdf_sources = [Path(\"./data_for_indexing/howpeopleuseai.pdf\")]\n",
    "result = pipeline.run({\n",
    "            \"pdf_converter\": {\"sources\": pdf_sources}\n",
    "        })\n",
    "        \n",
    "print(\"\\n📊 Pipeline Results:\")\n",
    "print(f\"  📄 Documents Processed: {result['doc_converter']['document_count']}\")\n",
    "print(f\"  🧠 Knowledge Graph Nodes: {result['kg_generator']['node_count']}\")\n",
    "print(f\"  🧪 Test Cases Generated: {result['test_generator']['testset_size']}\")\n",
    "print(f\"  🔧 Generation Method: {result['test_generator']['generation_method']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60c699f",
   "metadata": {},
   "source": [
    "### Understanding the Pipeline Architecture\n",
    "\n",
    "The pipeline we're building follows this flow:\n",
    "\n",
    "```\n",
    "PDF File → PDF Converter → Document Cleaner → Document Splitter \n",
    "    ↓\n",
    "Document Converter → Knowledge Graph Generator\n",
    "    ↓                         ↓\n",
    "Test Generator ← ← ← ← ← ← ← ←\n",
    "    ↓\n",
    "Test Dataset Saver\n",
    "```\n",
    "\n",
    "**Key Design Decisions:**\n",
    "\n",
    "1. **Document Processing Chain**: We clean and split documents before knowledge graph generation to ensure high-quality input\n",
    "2. **Dual Input to Test Generator**: Both the knowledge graph and original documents are provided to enable fallback generation methods\n",
    "3. **Configurable Test Distribution**: We can control the types of questions generated (single-hop vs multi-hop)\n",
    "\n",
    "**Pipeline Parameters Explained:**\n",
    "- `testset_size=10`: Number of question-answer pairs to generate\n",
    "- `split_length=50`: Number of sentences per document chunk\n",
    "- `query_distribution`: Controls complexity of generated questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5689774",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.draw(path=\"./images/knowledgegraph_sdg_pipeline_pdf.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e83c9a",
   "metadata": {},
   "source": [
    "![](./images/knowledgegraph_sdg_pipeline_pdf.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1429d2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 Synthetic Tests Sample:\n",
      "First 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What role does Christopher Ong play in the res...</td>\n",
       "      <td>['NBER WORKING PAPER SERIES\\nHOW PEOPLE USE CH...</td>\n",
       "      <td>Christopher Ong is one of the co-authors of th...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What role does David J. Deming play in the stu...</td>\n",
       "      <td>['ABSTRACT Despite the rapid adoption of LLM c...</td>\n",
       "      <td>David J. Deming is affiliated with Harvard Uni...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the different consumer plans availabl...</td>\n",
       "      <td>['to classify messages without any human seein...</td>\n",
       "      <td>OpenAI offers a variety of consumer plans for ...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the estimated economic surplus generat...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nto classify messages without any ...</td>\n",
       "      <td>The estimated economic surplus generated by US...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What demographic trends in ChatGPT usage are h...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nTeams, Enterprise, Education), wh...</td>\n",
       "      <td>Zao-Sanders (2025) highlights several demograp...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  What role does Christopher Ong play in the res...   \n",
       "1  What role does David J. Deming play in the stu...   \n",
       "2  What are the different consumer plans availabl...   \n",
       "3  What is the estimated economic surplus generat...   \n",
       "4  What demographic trends in ChatGPT usage are h...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  ['NBER WORKING PAPER SERIES\\nHOW PEOPLE USE CH...   \n",
       "1  ['ABSTRACT Despite the rapid adoption of LLM c...   \n",
       "2  ['to classify messages without any human seein...   \n",
       "3  ['<1-hop>\\n\\nto classify messages without any ...   \n",
       "4  ['<1-hop>\\n\\nTeams, Enterprise, Education), wh...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  Christopher Ong is one of the co-authors of th...   \n",
       "1  David J. Deming is affiliated with Harvard Uni...   \n",
       "2  OpenAI offers a variety of consumer plans for ...   \n",
       "3  The estimated economic surplus generated by US...   \n",
       "4  Zao-Sanders (2025) highlights several demograp...   \n",
       "\n",
       "                        synthesizer_name  \n",
       "0  single_hop_specific_query_synthesizer  \n",
       "1  single_hop_specific_query_synthesizer  \n",
       "2  single_hop_specific_query_synthesizer  \n",
       "3   multi_hop_specific_query_synthesizer  \n",
       "4   multi_hop_specific_query_synthesizer  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What are the differences in ChatGPT usage for ...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\n37% of messages are work-related\\...</td>\n",
       "      <td>Users with different education levels exhibit ...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How do the conversation topics identified in u...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nFigure 9 disaggregates four of th...</td>\n",
       "      <td>The conversation topics identified in user int...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What percentage of work-related messages are s...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\n37% of messages are work-related\\...</td>\n",
       "      <td>Users in computer-related occupations send 57%...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What are the main conversation topics in ChatG...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nFigure 9 disaggregates four of th...</td>\n",
       "      <td>The main conversation topics in ChatGPT messag...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What percentage of work-related messages are c...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nNearly 35% of all work-related qu...</td>\n",
       "      <td>Nearly 56% of work-related queries are classif...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           user_input  \\\n",
       "6   What are the differences in ChatGPT usage for ...   \n",
       "7   How do the conversation topics identified in u...   \n",
       "8   What percentage of work-related messages are s...   \n",
       "9   What are the main conversation topics in ChatG...   \n",
       "10  What percentage of work-related messages are c...   \n",
       "\n",
       "                                   reference_contexts  \\\n",
       "6   ['<1-hop>\\n\\n37% of messages are work-related\\...   \n",
       "7   ['<1-hop>\\n\\nFigure 9 disaggregates four of th...   \n",
       "8   ['<1-hop>\\n\\n37% of messages are work-related\\...   \n",
       "9   ['<1-hop>\\n\\nFigure 9 disaggregates four of th...   \n",
       "10  ['<1-hop>\\n\\nNearly 35% of all work-related qu...   \n",
       "\n",
       "                                            reference  \\\n",
       "6   Users with different education levels exhibit ...   \n",
       "7   The conversation topics identified in user int...   \n",
       "8   Users in computer-related occupations send 57%...   \n",
       "9   The main conversation topics in ChatGPT messag...   \n",
       "10  Nearly 56% of work-related queries are classif...   \n",
       "\n",
       "                        synthesizer_name  \n",
       "6   multi_hop_abstract_query_synthesizer  \n",
       "7   multi_hop_abstract_query_synthesizer  \n",
       "8   multi_hop_abstract_query_synthesizer  \n",
       "9   multi_hop_abstract_query_synthesizer  \n",
       "10  multi_hop_abstract_query_synthesizer  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load and display the generated synthetic tests\n",
    "test_file_path = \"data_for_eval/synthetic_tests_10_from_pdf.csv\"\n",
    "\n",
    "if os.path.exists(test_file_path):\n",
    "    synthetic_tests_df = pd.read_csv(test_file_path)\n",
    "    print(\"\\n🧪 Synthetic Tests Sample:\")\n",
    "    print(\"First 5 rows:\")\n",
    "    display(synthetic_tests_df.head())\n",
    "    print(\"Last 5 rows:\")\n",
    "    display(synthetic_tests_df.tail())\n",
    "else:\n",
    "    print(\"❌ Synthetic test file not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced14565",
   "metadata": {},
   "source": [
    "### Analyzing the Generated Test Dataset\n",
    "\n",
    "Now let's examine the synthetic test data that was generated from our PDF processing pipeline.\n",
    "\n",
    "**What to Look For:**\n",
    "- **Question Quality**: Are the questions grammatically correct and meaningful?\n",
    "- **Answer Accuracy**: Do the answers correctly reflect the source material?\n",
    "- **Question Types**: Notice the variety of single-hop and multi-hop questions\n",
    "- **Context Relevance**: Check if the reference contexts support the answers\n",
    "\n",
    "**Common Question Types You'll See:**\n",
    "1. **Single-hop questions**: Direct factual queries (e.g., \"What is X?\")\n",
    "2. **Multi-hop specific**: Questions requiring connecting specific facts\n",
    "3. **Multi-hop abstract**: Questions requiring broader reasoning across multiple concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ebd922",
   "metadata": {},
   "source": [
    "## Part 2: Web Content Processing Pipeline\n",
    "\n",
    "### Overview\n",
    "In this section, we'll adapt our pipeline to work with web content instead of PDF files. This demonstrates the flexibility of Haystack pipelines and how the same knowledge graph generation approach can work across different content sources.\n",
    "\n",
    "### Key Differences from PDF Processing\n",
    "1. **LinkContentFetcher**: Retrieves content directly from URLs\n",
    "2. **HTMLToDocument**: Converts HTML content to Haystack Documents\n",
    "3. **Same Processing Chain**: The rest of the pipeline remains identical\n",
    "\n",
    "### Real-World Applications\n",
    "This approach is particularly useful for:\n",
    "- **Documentation Analysis**: Processing online documentation and creating test datasets\n",
    "- **Content Monitoring**: Regularly generating tests from updated web content  \n",
    "- **Multi-Source Knowledge**: Combining web content with other document types\n",
    "- **Research Applications**: Creating datasets from academic papers, blog posts, etc.\n",
    "\n",
    "### Technical Considerations\n",
    "- **Rate Limiting**: Be mindful of website rate limits when fetching content\n",
    "- **Content Quality**: Web content may require more aggressive cleaning\n",
    "- **Dynamic Content**: Some websites use JavaScript; static HTML fetching may miss content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67f162b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x381684680>\n",
       "🚅 Components\n",
       "  - fetcher: LinkContentFetcher\n",
       "  - converter: HTMLToDocument\n",
       "  - doc_cleaner: DocumentCleaner\n",
       "  - doc_splitter: DocumentSplitter\n",
       "  - doc_converter: DocumentToLangChainConverter\n",
       "  - kg_generator: KnowledgeGraphGenerator\n",
       "  - test_generator: SyntheticTestGenerator\n",
       "  - test_saver: TestDatasetSaver\n",
       "🛤️ Connections\n",
       "  - fetcher.streams -> converter.sources (list[ByteStream])\n",
       "  - converter.documents -> doc_cleaner.documents (list[Document])\n",
       "  - doc_cleaner.documents -> doc_splitter.documents (list[Document])\n",
       "  - doc_splitter.documents -> doc_converter.documents (list[Document])\n",
       "  - doc_converter.langchain_documents -> kg_generator.documents (List[Document])\n",
       "  - doc_converter.langchain_documents -> test_generator.documents (List[Document])\n",
       "  - kg_generator.knowledge_graph -> test_generator.knowledge_graph (KnowledgeGraph)\n",
       "  - test_generator.testset -> test_saver.testset (DataFrame)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from haystack.components.fetchers import LinkContentFetcher\n",
    "from haystack.components.converters import HTMLToDocument\n",
    "\n",
    "fetcher = LinkContentFetcher()\n",
    "converter = HTMLToDocument()\n",
    "doc_cleaner = DocumentCleaner(remove_empty_lines=True,\n",
    "                            remove_extra_whitespaces=True)\n",
    "doc_splitter = DocumentSplitter(split_by=\"sentence\",\n",
    "                                split_length=50,\n",
    "                                split_overlap=5)\n",
    "doc_converter = DocumentToLangChainConverter()\n",
    "kg_generator = KnowledgeGraphGenerator(apply_transforms=True)\n",
    "test_generator = SyntheticTestGenerator(\n",
    "            testset_size=10,  \n",
    "            llm_model=\"gpt-4o-mini\",\n",
    "            query_distribution=[\n",
    "                (\"single_hop\", 0.25), \n",
    "                (\"multi_hop_specific\", 0.25),\n",
    "                (\"multi_hop_abstract\", 0.5)\n",
    "            ]\n",
    "        )\n",
    "test_saver = TestDatasetSaver(\"data_for_eval/synthetic_tests_10_from_html_page.csv\")\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline()\n",
    "pipeline.add_component(\"fetcher\", fetcher)\n",
    "pipeline.add_component(\"converter\", converter)\n",
    "pipeline.add_component(\"doc_cleaner\", doc_cleaner)\n",
    "pipeline.add_component(\"doc_splitter\", doc_splitter)\n",
    "pipeline.add_component(\"doc_converter\", doc_converter)\n",
    "pipeline.add_component(\"kg_generator\", kg_generator)\n",
    "pipeline.add_component(\"test_generator\", test_generator)\n",
    "pipeline.add_component(\"test_saver\", test_saver)\n",
    "\n",
    "# Connect components in sequence\n",
    "pipeline.connect(\"fetcher.streams\", \"converter.sources\")\n",
    "pipeline.connect(\"converter.documents\", \"doc_cleaner.documents\")\n",
    "pipeline.connect(\"doc_cleaner.documents\", \"doc_splitter.documents\")\n",
    "pipeline.connect(\"doc_splitter.documents\", \"doc_converter.documents\")\n",
    "pipeline.connect(\"doc_converter.langchain_documents\", \"kg_generator.documents\")\n",
    "pipeline.connect(\"kg_generator.knowledge_graph\", \"test_generator.knowledge_graph\")\n",
    "pipeline.connect(\"doc_converter.langchain_documents\", \"test_generator.documents\")\n",
    "pipeline.connect(\"test_generator.testset\", \"test_saver.testset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90dfe9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying HeadlinesExtractor: 100%|██████████| 2/2 [00:02<00:00,  1.43s/it]\n",
      "Applying HeadlineSplitter: 100%|██████████| 2/2 [00:00<00:00, 278.59it/s]\n",
      "Applying SummaryExtractor: 100%|██████████| 2/2 [00:03<00:00,  1.59s/it]\n",
      "Applying CustomNodeFilter: 100%|██████████| 6/6 [00:03<00:00,  1.65it/s]\n",
      "Applying EmbeddingExtractor: 100%|██████████| 2/2 [00:00<00:00,  2.66it/s]\n",
      "Applying ThemesExtractor: 100%|██████████| 6/6 [00:05<00:00,  1.12it/s]\n",
      "Applying NERExtractor: 100%|██████████| 6/6 [00:03<00:00,  1.55it/s]\n",
      "Applying CosineSimilarityBuilder: 100%|██████████| 1/1 [00:00<00:00, 660.94it/s]\n",
      "Applying OverlapScoreBuilder: 100%|██████████| 1/1 [00:00<00:00, 1093.12it/s]\n",
      "Generating personas: 100%|██████████| 2/2 [00:01<00:00,  1.05it/s]\n",
      "Generating Scenarios: 100%|██████████| 3/3 [00:05<00:00,  1.85s/it]\n",
      "Generating Samples: 100%|██████████| 11/11 [00:07<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Pipeline Results:\n",
      "  📄 Documents Processed: 2\n",
      "  🧠 Knowledge Graph Nodes: 2\n",
      "  🧪 Test Cases Generated: 11\n",
      "  🔧 Generation Method: knowledge_graph\n"
     ]
    }
   ],
   "source": [
    "web_url = \"https://haystack.deepset.ai/blog/haystack-2-release\"\n",
    "\n",
    "result = pipeline.run({\n",
    "    \"fetcher\": {\"urls\": [web_url]}\n",
    "})\n",
    "\n",
    "print(\"\\n📊 Pipeline Results:\")\n",
    "print(f\"  📄 Documents Processed: {result['doc_converter']['document_count']}\")\n",
    "print(f\"  🧠 Knowledge Graph Nodes: {result['kg_generator']['node_count']}\")\n",
    "print(f\"  🧪 Test Cases Generated: {result['test_generator']['testset_size']}\")\n",
    "print(f\"  🔧 Generation Method: {result['test_generator']['generation_method']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa853c0",
   "metadata": {},
   "source": [
    "### Web Pipeline Architecture\n",
    "\n",
    "The web processing pipeline follows a similar structure but with adapted input components:\n",
    "\n",
    "```\n",
    "Web URL → Link Fetcher → HTML Converter → Document Cleaner → Document Splitter\n",
    "    ↓\n",
    "Document Converter → Knowledge Graph Generator  \n",
    "    ↓                         ↓\n",
    "Test Generator ← ← ← ← ← ← ← ←\n",
    "    ↓\n",
    "Test Dataset Saver\n",
    "```\n",
    "\n",
    "**Why This Works:**\n",
    "- The knowledge graph generation is **content-agnostic** - it works the same whether input comes from PDFs, web pages, or other sources\n",
    "- Document preprocessing steps ensure consistent quality regardless of input format\n",
    "- The same test generation logic produces comparable quality across all sources\n",
    "\n",
    "**Pipeline Reusability:**\n",
    "Notice how we can reuse the same components (`doc_cleaner`, `doc_splitter`, `kg_generator`, etc.) with different input sources. This demonstrates the modularity and flexibility of Haystack's component architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaea2c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.draw(path=\"./images/knowledgegraph_sdg_pipeline_html.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e528d45",
   "metadata": {},
   "source": [
    "![](./images/knowledgegraph_sdg_pipeline_html.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "508384b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 Synthetic Tests Sample:\n",
      "First 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What support will be provided for Haystack 1.0...</td>\n",
       "      <td>['Haystack 2.0: The Composable Open-Source LLM...</td>\n",
       "      <td>After the release of Haystack 2.0, users of Ha...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What limitations did Haystack 1.0 have regardi...</td>\n",
       "      <td>['Composable and customizable Pipelines\\nModer...</td>\n",
       "      <td>One important limitation in Haystack 1.0 is th...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is Haystack 2.0?</td>\n",
       "      <td>['A common interface for storing data - A clea...</td>\n",
       "      <td>Haystack 2.0 is an advanced version of the AI ...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What limitations of Haystack 1.0 were addresse...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nComposable and customizable Pipel...</td>\n",
       "      <td>Haystack 1.0 had a significant limitation in t...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is Haystack and how does it help with dat...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nA common interface for storing da...</td>\n",
       "      <td>Haystack is an AI framework that provides a co...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  What support will be provided for Haystack 1.0...   \n",
       "1  What limitations did Haystack 1.0 have regardi...   \n",
       "2                              What is Haystack 2.0?   \n",
       "3  What limitations of Haystack 1.0 were addresse...   \n",
       "4  What is Haystack and how does it help with dat...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  ['Haystack 2.0: The Composable Open-Source LLM...   \n",
       "1  ['Composable and customizable Pipelines\\nModer...   \n",
       "2  ['A common interface for storing data - A clea...   \n",
       "3  ['<1-hop>\\n\\nComposable and customizable Pipel...   \n",
       "4  ['<1-hop>\\n\\nA common interface for storing da...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  After the release of Haystack 2.0, users of Ha...   \n",
       "1  One important limitation in Haystack 1.0 is th...   \n",
       "2  Haystack 2.0 is an advanced version of the AI ...   \n",
       "3  Haystack 1.0 had a significant limitation in t...   \n",
       "4  Haystack is an AI framework that provides a co...   \n",
       "\n",
       "                        synthesizer_name  \n",
       "0  single_hop_specific_query_synthesizer  \n",
       "1  single_hop_specific_query_synthesizer  \n",
       "2  single_hop_specific_query_synthesizer  \n",
       "3   multi_hop_specific_query_synthesizer  \n",
       "4   multi_hop_specific_query_synthesizer  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How does Haystack 2.0 enhance customization an...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nThese include Chroma, Weaviate, P...</td>\n",
       "      <td>Haystack 2.0 enhances customization and flexib...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What are the key features of Haystack 2.0 that...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nThese include Chroma, Weaviate, P...</td>\n",
       "      <td>Haystack 2.0 is designed to be a flexible and ...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What are the key features of Haystack 2.0 that...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nThese include Chroma, Weaviate, P...</td>\n",
       "      <td>Haystack 2.0 is designed to be a flexible and ...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What are the key features of Haystack 2.0 that...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nThese include Chroma, Weaviate, P...</td>\n",
       "      <td>Haystack 2.0 is designed to be a flexible and ...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What are the key features of Haystack 2.0 that...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nThese include Chroma, Weaviate, P...</td>\n",
       "      <td>Haystack 2.0 is designed to be a flexible and ...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           user_input  \\\n",
       "6   How does Haystack 2.0 enhance customization an...   \n",
       "7   What are the key features of Haystack 2.0 that...   \n",
       "8   What are the key features of Haystack 2.0 that...   \n",
       "9   What are the key features of Haystack 2.0 that...   \n",
       "10  What are the key features of Haystack 2.0 that...   \n",
       "\n",
       "                                   reference_contexts  \\\n",
       "6   ['<1-hop>\\n\\nThese include Chroma, Weaviate, P...   \n",
       "7   ['<1-hop>\\n\\nThese include Chroma, Weaviate, P...   \n",
       "8   ['<1-hop>\\n\\nThese include Chroma, Weaviate, P...   \n",
       "9   ['<1-hop>\\n\\nThese include Chroma, Weaviate, P...   \n",
       "10  ['<1-hop>\\n\\nThese include Chroma, Weaviate, P...   \n",
       "\n",
       "                                            reference  \\\n",
       "6   Haystack 2.0 enhances customization and flexib...   \n",
       "7   Haystack 2.0 is designed to be a flexible and ...   \n",
       "8   Haystack 2.0 is designed to be a flexible and ...   \n",
       "9   Haystack 2.0 is designed to be a flexible and ...   \n",
       "10  Haystack 2.0 is designed to be a flexible and ...   \n",
       "\n",
       "                        synthesizer_name  \n",
       "6   multi_hop_abstract_query_synthesizer  \n",
       "7   multi_hop_abstract_query_synthesizer  \n",
       "8   multi_hop_abstract_query_synthesizer  \n",
       "9   multi_hop_abstract_query_synthesizer  \n",
       "10  multi_hop_abstract_query_synthesizer  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and display the generated synthetic tests\n",
    "test_file_path = \"data_for_eval/synthetic_tests_10_from_html_page.csv\"\n",
    "\n",
    "if os.path.exists(test_file_path):\n",
    "    synthetic_tests_df = pd.read_csv(test_file_path)\n",
    "    print(\"\\n🧪 Synthetic Tests Sample:\")\n",
    "    print(\"First 5 rows:\")\n",
    "    display(synthetic_tests_df.head())\n",
    "    print(\"Last 5 rows:\")\n",
    "    display(synthetic_tests_df.tail())\n",
    "else:\n",
    "    print(\"❌ Synthetic test file not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658c5727",
   "metadata": {},
   "source": [
    "### Comparing Results Across Sources\n",
    "\n",
    "Let's examine how the synthetic test generation performs when using web content versus PDF content.\n",
    "\n",
    "**Expected Differences:**\n",
    "- **Content Structure**: Web content may have different formatting and structure\n",
    "- **Question Complexity**: Depending on the source material's complexity\n",
    "- **Context Quality**: Web content might include navigation elements or ads that need filtering\n",
    "\n",
    "**Quality Assessment Checklist:**\n",
    "- [ ] Questions are grammatically correct\n",
    "- [ ] Answers are factually accurate based on the source\n",
    "- [ ] Context excerpts support the provided answers\n",
    "- [ ] Questions test different levels of comprehension\n",
    "- [ ] No duplicate or overly similar questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab8b538",
   "metadata": {},
   "source": [
    "## Part 3: Unified Branching Pipeline - PDF or Web Content\n",
    "\n",
    "### Overview\n",
    "In this section, we'll create a more sophisticated pipeline that can intelligently route inputs based on their type. This pipeline demonstrates:\n",
    "\n",
    "1. **Input Flexibility**: Accept either PDF files OR web URLs as input\n",
    "2. **Intelligent Routing**: Automatically determine the processing path based on input type  \n",
    "3. **Unified Processing**: Both input types follow the same knowledge graph and synthetic data generation workflow\n",
    "4. **Production-Ready Design**: Handle multiple input sources in a single, cohesive pipeline\n",
    "\n",
    "### Key Components for Branching\n",
    "- **Conditional Logic**: Python conditionals to determine input type\n",
    "- **Modular Design**: Reusable component instances across different input paths\n",
    "- **Error Handling**: Graceful handling of different input scenarios\n",
    "- **Consistent Output**: Same output format regardless of input source\n",
    "\n",
    "### Real-World Benefits\n",
    "This approach is valuable because:\n",
    "- **Operational Efficiency**: One pipeline handles multiple input scenarios\n",
    "- **Maintenance Simplicity**: Single codebase for different input types  \n",
    "- **Flexible Deployment**: Can be easily extended to handle additional input formats\n",
    "- **Cost Optimization**: Shared components reduce resource requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3efc151",
   "metadata": {},
   "source": [
    "### Advanced Branching Pipeline with FileTypeRouter\n",
    "\n",
    "Now let's create an even more sophisticated version using Haystack's `FileTypeRouter` component, which can automatically detect input types and route them appropriately. This approach is more scalable and follows the example pattern you provided.\n",
    "\n",
    "**Key Advantages of FileTypeRouter Approach:**\n",
    "- **Automatic Detection**: No manual input type checking required\n",
    "- **Extensibility**: Easy to add new input types (CSV, Word docs, etc.)\n",
    "- **Production Ready**: Built-in error handling and type validation  \n",
    "- **Performance**: Efficient routing without conditional logic overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71c991b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x382403c50>\n",
       "🚅 Components\n",
       "  - file_router: FileTypeRouter\n",
       "  - link_fetcher: LinkContentFetcher\n",
       "  - pdf_converter: PyPDFToDocument\n",
       "  - html_converter: HTMLToDocument\n",
       "  - doc_joiner: DocumentJoiner\n",
       "  - doc_cleaner: DocumentCleaner\n",
       "  - doc_splitter: DocumentSplitter\n",
       "  - doc_converter: DocumentToLangChainConverter\n",
       "  - kg_generator: KnowledgeGraphGenerator\n",
       "  - test_generator: SyntheticTestGenerator\n",
       "  - test_saver: TestDatasetSaver\n",
       "🛤️ Connections\n",
       "  - file_router.application/pdf -> pdf_converter.sources (list[Union[str, Path, ByteStream]])\n",
       "  - link_fetcher.streams -> html_converter.sources (list[ByteStream])\n",
       "  - pdf_converter.documents -> doc_joiner.documents (list[Document])\n",
       "  - html_converter.documents -> doc_joiner.documents (list[Document])\n",
       "  - doc_joiner.documents -> doc_cleaner.documents (list[Document])\n",
       "  - doc_cleaner.documents -> doc_splitter.documents (list[Document])\n",
       "  - doc_splitter.documents -> doc_converter.documents (list[Document])\n",
       "  - doc_converter.langchain_documents -> kg_generator.documents (List[Document])\n",
       "  - doc_converter.langchain_documents -> test_generator.documents (List[Document])\n",
       "  - kg_generator.knowledge_graph -> test_generator.knowledge_graph (KnowledgeGraph)\n",
       "  - test_generator.testset -> test_saver.testset (DataFrame)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from haystack.components.routers import FileTypeRouter\n",
    "from haystack.components.joiners import DocumentJoiner\n",
    "\n",
    "website = [\"https://www.tableau.com/data-insights/ai/examples\"]\n",
    "\n",
    "\n",
    "\n",
    "# Initialize pipeline\n",
    "advanced_pipeline = Pipeline()\n",
    "\n",
    "# Core routing and joining components  \n",
    "file_router = FileTypeRouter(mime_types=[\"text/plain\", \"application/pdf\", \"text/html\"])\n",
    "doc_joiner = DocumentJoiner()  # Joins documents from different branches\n",
    "\n",
    "# Input converters for each file type\n",
    "pdf_converter = PyPDFToDocument()\n",
    "html_converter = HTMLToDocument()  \n",
    "link_fetcher = LinkContentFetcher()\n",
    "\n",
    "# Shared processing components\n",
    "doc_cleaner = DocumentCleaner(remove_empty_lines=True, remove_extra_whitespaces=True)\n",
    "doc_splitter = DocumentSplitter(split_by=\"sentence\", split_length=50, split_overlap=5)\n",
    "doc_converter = DocumentToLangChainConverter()\n",
    "kg_generator = KnowledgeGraphGenerator(apply_transforms=True)\n",
    "test_generator = SyntheticTestGenerator(\n",
    "    testset_size=15,  # Larger test set for multiple sources\n",
    "    llm_model=\"gpt-4o-mini\",\n",
    "    query_distribution=[\n",
    "        (\"single_hop\", 0.3),\n",
    "        (\"multi_hop_specific\", 0.3), \n",
    "        (\"multi_hop_abstract\", 0.4)\n",
    "    ]\n",
    ")\n",
    "test_saver = TestDatasetSaver(\"data_for_eval/synthetic_tests_advanced_branching.csv\")\n",
    "\n",
    "# Add all components to pipeline\n",
    "advanced_pipeline.add_component(\"file_router\", file_router)\n",
    "advanced_pipeline.add_component(\"link_fetcher\", link_fetcher)\n",
    "advanced_pipeline.add_component(\"pdf_converter\", pdf_converter) \n",
    "advanced_pipeline.add_component(\"html_converter\", html_converter)\n",
    "advanced_pipeline.add_component(\"doc_joiner\", doc_joiner)\n",
    "advanced_pipeline.add_component(\"doc_cleaner\", doc_cleaner)\n",
    "advanced_pipeline.add_component(\"doc_splitter\", doc_splitter)\n",
    "advanced_pipeline.add_component(\"doc_converter\", doc_converter)\n",
    "advanced_pipeline.add_component(\"kg_generator\", kg_generator)\n",
    "advanced_pipeline.add_component(\"test_generator\", test_generator)\n",
    "advanced_pipeline.add_component(\"test_saver\", test_saver)\n",
    "\n",
    "# Connect file routing branches\n",
    "advanced_pipeline.connect(\"file_router.application/pdf\", \"pdf_converter.sources\") \n",
    "advanced_pipeline.connect(\"link_fetcher.streams\", \"html_converter.sources\")\n",
    "\n",
    "# Connect converters to joiner\n",
    "advanced_pipeline.connect(\"pdf_converter.documents\", \"doc_joiner.documents\")\n",
    "advanced_pipeline.connect(\"html_converter.documents\", \"doc_joiner.documents\")\n",
    "\n",
    "# Connect main processing path\n",
    "advanced_pipeline.connect(\"doc_joiner.documents\", \"doc_cleaner.documents\")\n",
    "advanced_pipeline.connect(\"doc_cleaner.documents\", \"doc_splitter.documents\")\n",
    "advanced_pipeline.connect(\"doc_splitter.documents\", \"doc_converter.documents\")\n",
    "advanced_pipeline.connect(\"doc_converter.langchain_documents\", \"kg_generator.documents\")\n",
    "advanced_pipeline.connect(\"kg_generator.knowledge_graph\", \"test_generator.knowledge_graph\")\n",
    "advanced_pipeline.connect(\"doc_converter.langchain_documents\", \"test_generator.documents\")\n",
    "advanced_pipeline.connect(\"test_generator.testset\", \"test_saver.testset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f649e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying HeadlinesExtractor: 100%|██████████| 19/19 [00:10<00:00,  1.79it/s]\n",
      "Applying HeadlinesExtractor: 100%|██████████| 19/19 [00:10<00:00,  1.79it/s]\n",
      "Applying HeadlineSplitter: 100%|██████████| 20/20 [00:00<00:00, 525.50it/s]\n",
      "Applying SummaryExtractor:   0%|          | 0/19 [00:00<?, ?it/s]\n",
      "Applying SummaryExtractor: 100%|██████████| 19/19 [00:11<00:00,  1.61it/s]\n",
      "Applying SummaryExtractor: 100%|██████████| 19/19 [00:11<00:00,  1.61it/s]\n",
      "Applying CustomNodeFilter: 100%|██████████| 52/52 [00:27<00:00,  1.86it/s]\n",
      "Applying EmbeddingExtractor:   0%|          | 0/19 [00:00<?, ?it/s]\n",
      "Applying EmbeddingExtractor: 100%|██████████| 19/19 [00:05<00:00,  3.47it/s]\n",
      "Applying EmbeddingExtractor: 100%|██████████| 19/19 [00:05<00:00,  3.47it/s]\n",
      "Applying ThemesExtractor: 100%|██████████| 47/47 [00:29<00:00,  1.60it/s]\n",
      "Applying ThemesExtractor: 100%|██████████| 47/47 [00:29<00:00,  1.60it/s]\n",
      "Applying NERExtractor: 100%|██████████| 47/47 [00:27<00:00,  1.73it/s]\n",
      "Applying CosineSimilarityBuilder: 100%|██████████| 1/1 [00:00<00:00, 321.45it/s]\n",
      "Applying OverlapScoreBuilder:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Applying CosineSimilarityBuilder: 100%|██████████| 1/1 [00:00<00:00, 321.45it/s]\n",
      "Applying OverlapScoreBuilder: 100%|██████████| 1/1 [00:00<00:00, 68.63it/s]\n",
      "\n",
      "Generating personas: 100%|██████████| 3/3 [00:02<00:00,  1.26it/s]\n",
      "Generating personas: 100%|██████████| 3/3 [00:02<00:00,  1.26it/s]\n",
      "Generating Scenarios: 100%|██████████| 3/3 [00:25<00:00,  8.66s/it]\n",
      "Generating Scenarios: 100%|██████████| 3/3 [00:25<00:00,  8.66s/it]\n",
      "Generating Samples: 100%|██████████| 16/16 [00:09<00:00,  1.76it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Advanced Pipeline Results:\n",
      "  📄 Total Documents Processed: 20\n",
      "  🧠 Knowledge Graph Nodes: 20\n",
      "  🧪 Test Cases Generated: 16\n",
      "  🔧 Generation Method: knowledge_graph\n",
      "  💾 Combined output saved to: data_for_eval/synthetic_tests_advanced_branching.csv\n",
      "\n",
      "🔗 Document Joiner successfully combined:\n",
      "  - PDF content from: howpeopleuseai.pdf\n",
      "  - Web content from: https://www.tableau.com/data-insights/ai/examples\n",
      "  Into a unified knowledge graph and test dataset!\n"
     ]
    }
   ],
   "source": [
    "# Define inputs\n",
    "pdf_file = Path(\"./data_for_indexing/howpeopleuseai.pdf\")\n",
    "web_url = \"https://www.tableau.com/data-insights/ai/examples\"\n",
    "\n",
    "try:\n",
    "    # Run pipeline with both input types\n",
    "    result = advanced_pipeline.run({\n",
    "        \"file_router\": {\"sources\": [pdf_file]},  # PDF input through FileTypeRouter\n",
    "        \"link_fetcher\": {\"urls\": [web_url]}      # Web input through LinkContentFetcher\n",
    "    })\n",
    "    \n",
    "    print(\"\\n📊 Advanced Pipeline Results:\")\n",
    "    print(f\"  📄 Total Documents Processed: {result['doc_converter']['document_count']}\")\n",
    "    print(f\"  🧠 Knowledge Graph Nodes: {result['kg_generator']['node_count']}\")\n",
    "    print(f\"  🧪 Test Cases Generated: {result['test_generator']['testset_size']}\")\n",
    "    print(f\"  🔧 Generation Method: {result['test_generator']['generation_method']}\")\n",
    "    print(f\"  💾 Combined output saved to: data_for_eval/synthetic_tests_advanced_branching.csv\")\n",
    "    \n",
    "    # Show the power of the joiner\n",
    "    print(f\"\\n🔗 Document Joiner successfully combined:\")\n",
    "    print(f\"  - PDF content from: {pdf_file.name}\")\n",
    "    print(f\"  - Web content from: {web_url}\")\n",
    "    print(f\"  Into a unified knowledge graph and test dataset!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in advanced pipeline: {str(e)}\")\n",
    "    print(\"This might be due to network issues or file access problems.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a54d288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the advanced branching pipeline architecture\n",
    "advanced_pipeline.draw(path=\"./images/advanced_branching_kg_pipeline.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfa2c71",
   "metadata": {},
   "source": [
    "![](./images/advanced_branching_kg_pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc38dabc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How does Yahoo utilize AI in its search engine...</td>\n",
       "      <td>['You may or may not be aware of how pervasive...</td>\n",
       "      <td>Yahoo, as a popular search engine, utilizes AI...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How Facebook use AI for keep users engaged and...</td>\n",
       "      <td>['Social media\\nSocial media platforms are ano...</td>\n",
       "      <td>Facebook, like other major social media platfo...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does Grammarly help with text editing and ...</td>\n",
       "      <td>['Some examples of industrial robots include:\\...</td>\n",
       "      <td>Grammarly is an online text editor that takes ...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are some applications of AI in healthcare?</td>\n",
       "      <td>['Fraud prevention\\nIf you have an account wit...</td>\n",
       "      <td>AI has many uses in the field of healthcare, i...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who is Zoe Hitzig and what is her affiliation?</td>\n",
       "      <td>['NBER WORKING PAPER SERIES\\nHOW PEOPLE USE CH...</td>\n",
       "      <td>Zoe Hitzig is affiliated with OpenAI and the H...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  How does Yahoo utilize AI in its search engine...   \n",
       "1  How Facebook use AI for keep users engaged and...   \n",
       "2  How does Grammarly help with text editing and ...   \n",
       "3    What are some applications of AI in healthcare?   \n",
       "4     Who is Zoe Hitzig and what is her affiliation?   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  ['You may or may not be aware of how pervasive...   \n",
       "1  ['Social media\\nSocial media platforms are ano...   \n",
       "2  ['Some examples of industrial robots include:\\...   \n",
       "3  ['Fraud prevention\\nIf you have an account wit...   \n",
       "4  ['NBER WORKING PAPER SERIES\\nHOW PEOPLE USE CH...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  Yahoo, as a popular search engine, utilizes AI...   \n",
       "1  Facebook, like other major social media platfo...   \n",
       "2  Grammarly is an online text editor that takes ...   \n",
       "3  AI has many uses in the field of healthcare, i...   \n",
       "4  Zoe Hitzig is affiliated with OpenAI and the H...   \n",
       "\n",
       "                        synthesizer_name  \n",
       "0  single_hop_specific_query_synthesizer  \n",
       "1  single_hop_specific_query_synthesizer  \n",
       "2  single_hop_specific_query_synthesizer  \n",
       "3  single_hop_specific_query_synthesizer  \n",
       "4  single_hop_specific_query_synthesizer  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>How does the training of language models relat...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nOuyang, Long, Jeff Wu, Xu Jiang, ...</td>\n",
       "      <td>The training of language models, as discussed ...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What is the relationship between ChatGPT usage...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\n37% of messages are work-related\\...</td>\n",
       "      <td>ChatGPT usage for work-related messages varies...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What are the classifier prompts used for train...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nOuyang, Long, Jeff Wu, Xu Jiang, ...</td>\n",
       "      <td>The classifier prompts used for training langu...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>How generative AI is used for task automation ...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nOuyang, Long, Jeff Wu, Xu Jiang, ...</td>\n",
       "      <td>Generative AI is used for task automation by p...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>How does the usage of ChatGPT for work-related...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\n37% of messages are work-related\\...</td>\n",
       "      <td>The usage of ChatGPT for work-related messages...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           user_input  \\\n",
       "11  How does the training of language models relat...   \n",
       "12  What is the relationship between ChatGPT usage...   \n",
       "13  What are the classifier prompts used for train...   \n",
       "14  How generative AI is used for task automation ...   \n",
       "15  How does the usage of ChatGPT for work-related...   \n",
       "\n",
       "                                   reference_contexts  \\\n",
       "11  ['<1-hop>\\n\\nOuyang, Long, Jeff Wu, Xu Jiang, ...   \n",
       "12  ['<1-hop>\\n\\n37% of messages are work-related\\...   \n",
       "13  ['<1-hop>\\n\\nOuyang, Long, Jeff Wu, Xu Jiang, ...   \n",
       "14  ['<1-hop>\\n\\nOuyang, Long, Jeff Wu, Xu Jiang, ...   \n",
       "15  ['<1-hop>\\n\\n37% of messages are work-related\\...   \n",
       "\n",
       "                                            reference  \\\n",
       "11  The training of language models, as discussed ...   \n",
       "12  ChatGPT usage for work-related messages varies...   \n",
       "13  The classifier prompts used for training langu...   \n",
       "14  Generative AI is used for task automation by p...   \n",
       "15  The usage of ChatGPT for work-related messages...   \n",
       "\n",
       "                        synthesizer_name  \n",
       "11  multi_hop_abstract_query_synthesizer  \n",
       "12  multi_hop_abstract_query_synthesizer  \n",
       "13  multi_hop_abstract_query_synthesizer  \n",
       "14  multi_hop_abstract_query_synthesizer  \n",
       "15  multi_hop_abstract_query_synthesizer  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and compare results from the advanced branching pipeline\n",
    "advanced_test_file = \"data_for_eval/synthetic_tests_advanced_branching.csv\"\n",
    "\n",
    "if os.path.exists(advanced_test_file):\n",
    "    advanced_tests_df = pd.read_csv(advanced_test_file)\n",
    "    \n",
    "    display(advanced_tests_df.head())\n",
    "    display(advanced_tests_df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcfa57a",
   "metadata": {},
   "source": [
    "### Branching Pipeline Architecture Analysis\n",
    "\n",
    "The advanced branching pipeline we just built demonstrates several key architectural patterns:\n",
    "\n",
    "#### Pipeline Flow\n",
    "```\n",
    "Input Sources (PDF + Web URL)\n",
    "    ↓                    ↓\n",
    "FileTypeRouter    LinkContentFetcher\n",
    "    ↓                    ↓  \n",
    "PDFConverter      HTMLConverter\n",
    "    ↓                    ↓\n",
    "    └── DocumentJoiner ──┘\n",
    "            ↓\n",
    "    Document Processing Chain\n",
    "    (Cleaner → Splitter → Converter)\n",
    "            ↓\n",
    "    Knowledge Graph Generator\n",
    "            ↓\n",
    "    Synthetic Test Generator  \n",
    "            ↓\n",
    "    Test Dataset Saver\n",
    "```\n",
    "\n",
    "#### Key Architectural Benefits\n",
    "\n",
    "1. **Modularity**: Each component has a single responsibility and can be reused\n",
    "2. **Flexibility**: Easy to add new input types (CSV, Word docs, etc.) \n",
    "3. **Scalability**: DocumentJoiner allows processing multiple sources simultaneously\n",
    "4. **Consistency**: Same processing logic regardless of input source\n",
    "5. **Error Isolation**: Problems with one input source don't affect others\n",
    "\n",
    "#### Production Considerations\n",
    "\n",
    "**Advantages of This Approach:**\n",
    "- **Unified Output**: Single knowledge graph and test dataset from multiple sources\n",
    "- **Rich Context**: Cross-referencing information between different document types\n",
    "- **Operational Efficiency**: One pipeline deployment handles multiple scenarios\n",
    "- **Quality Improvement**: More diverse training data leads to better synthetic questions\n",
    "\n",
    "**When to Use Branching Pipelines:**\n",
    "- Processing heterogeneous document collections\n",
    "- Building comprehensive knowledge bases from multiple sources\n",
    "- Creating robust test datasets that cover various content types\n",
    "- Implementing production pipelines that need input flexibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc4ab33",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What We've Learned\n",
    "\n",
    "In this notebook, we explored:\n",
    "\n",
    "1. **Knowledge Graph-Driven Test Generation**: How structured knowledge representations improve synthetic data quality\n",
    "2. **Multi-Source Processing**: Adapting the same pipeline architecture for different input types (PDFs, web content)\n",
    "3. **Pipeline Modularity**: Reusing components across different use cases while maintaining consistency\n",
    "4. **Quality Assessment**: Evaluating synthetic test datasets for accuracy and usefulness\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Knowledge graphs act as a quality filter** for test generation, producing more coherent and factually grounded questions\n",
    "- **Haystack's component architecture** enables easy adaptation between different content sources\n",
    "- **Preprocessing matters** - cleaning and splitting documents appropriately affects downstream quality\n",
    "- **Synthetic test generation** can scale evaluation efforts but requires careful quality validation\n",
    "\n",
    "### Production Considerations\n",
    "\n",
    "When moving to production, consider:\n",
    "\n",
    "1. **Quality Control**: Implement automated quality checks (see the quality control components in other notebooks)\n",
    "2. **Scalability**: Use batch processing for large document collections\n",
    "3. **Monitoring**: Track generation success rates and quality metrics over time\n",
    "4. **Cost Management**: Balance test quantity with API usage costs\n",
    "5. **Validation**: Always human-review a sample of generated tests before deployment\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-with-haystack-ch5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
