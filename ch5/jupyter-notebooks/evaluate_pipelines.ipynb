{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c86671d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating synthetic test generation pipeline...\n",
      "Found 1 PDF files to process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying HeadlinesExtractor: 100%|██████████| 17/17 [00:08<00:00,  1.91it/s]\n",
      "Applying HeadlineSplitter: 100%|██████████| 17/17 [00:00<00:00, 457.22it/s]\n",
      "Applying SummaryExtractor: 100%|██████████| 17/17 [00:10<00:00,  1.70it/s]\n",
      "Applying CustomNodeFilter: 100%|██████████| 49/49 [00:22<00:00,  2.14it/s]\n",
      "Applying EmbeddingExtractor: 100%|██████████| 17/17 [00:04<00:00,  3.91it/s]\n",
      "Applying ThemesExtractor: 100%|██████████| 44/44 [00:22<00:00,  1.91it/s]\n",
      "Applying NERExtractor: 100%|██████████| 44/44 [00:21<00:00,  2.07it/s]\n",
      "Applying CosineSimilarityBuilder: 100%|██████████| 1/1 [00:00<00:00, 234.41it/s]\n",
      "Applying OverlapScoreBuilder: 100%|██████████| 1/1 [00:00<00:00, 68.50it/s]\n",
      "Generating personas: 100%|██████████| 3/3 [00:02<00:00,  1.33it/s]\n",
      "Generating Scenarios: 100%|██████████| 3/3 [00:17<00:00,  5.97s/it]\n",
      "Generating Samples: 100%|██████████| 11/11 [00:07<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Pipeline Results:\n",
      "  📄 Documents Processed: 17\n",
      "  🧠 Knowledge Graph Nodes: 17\n",
      "  🧪 Test Cases Generated: 11\n",
      "  🔧 Generation Method: knowledge_graph\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from haystack import Pipeline\n",
    "from haystack.components.converters import PyPDFToDocument\n",
    "from haystack.components.preprocessors import (\n",
    "    DocumentCleaner,\n",
    "    DocumentSplitter)\n",
    "from pathlib import Path\n",
    "from scripts.knowledge_graph_component import KnowledgeGraphGenerator\n",
    "from scripts.synthetic_test_components import SyntheticTestGenerator,\\\n",
    "                                                TestDatasetSaver,\\\n",
    "                                                    DocumentToLangChainConverter\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(\"./.env\")\n",
    "\n",
    "# Example: Create a complete pipeline for synthetic test generation\n",
    "data_path = \"data_for_indexing\"\n",
    "\n",
    "if os.path.exists(data_path):\n",
    "    print(\"Creating synthetic test generation pipeline...\")\n",
    "    \n",
    "    # Get PDF files from the directory\n",
    "    pdf_files = list(Path(data_path).glob(\"*.pdf\"))\n",
    "    \n",
    "    if pdf_files:\n",
    "        print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "        \n",
    "        # Create pipeline components\n",
    "        pdf_converter = PyPDFToDocument()\n",
    "        doc_cleaner = DocumentCleaner(remove_empty_lines=True,\n",
    "                                      remove_extra_whitespaces=True)\n",
    "        doc_splitter = DocumentSplitter(split_by=\"sentence\",\n",
    "                                       split_length=50,\n",
    "                                       split_overlap=5)\n",
    "        doc_converter = DocumentToLangChainConverter()\n",
    "        kg_generator = KnowledgeGraphGenerator(apply_transforms=True)\n",
    "        \n",
    "        # Updated: Removed artificial size limits - now generates requested 10 tests\n",
    "        test_generator = SyntheticTestGenerator(\n",
    "            testset_size=10,  # This will now generate 10 tests instead of 3\n",
    "            llm_model=\"gpt-4o-mini\",\n",
    "            query_distribution=[\n",
    "                (\"single_hop\", 0.25), \n",
    "                (\"multi_hop_specific\", 0.25),\n",
    "                (\"multi_hop_abstract\", 0.5)\n",
    "            ],\n",
    "            # Optional: Add max_testset_size=5 if you want to limit due to API constraints\n",
    "            # max_testset_size=5  # Uncomment this line if you experience API timeouts\n",
    "        )\n",
    "        test_saver = TestDatasetSaver(\"data_for_eval/synthetic_tests_10.csv\")\n",
    "        \n",
    "        # Create pipeline\n",
    "        pipeline = Pipeline()\n",
    "        pipeline.add_component(\"pdf_converter\", pdf_converter)\n",
    "        pipeline.add_component(\"doc_cleaner\", doc_cleaner)\n",
    "        pipeline.add_component(\"doc_splitter\", doc_splitter)\n",
    "        pipeline.add_component(\"doc_converter\", doc_converter)\n",
    "        pipeline.add_component(\"kg_generator\", kg_generator)\n",
    "        pipeline.add_component(\"test_generator\", test_generator)\n",
    "        pipeline.add_component(\"test_saver\", test_saver)\n",
    "        \n",
    "        # Connect components in sequence\n",
    "        pipeline.connect(\"pdf_converter.documents\", \"doc_cleaner.documents\")\n",
    "        pipeline.connect(\"doc_cleaner.documents\", \"doc_splitter.documents\")\n",
    "        pipeline.connect(\"doc_splitter.documents\", \"doc_converter.documents\")\n",
    "        pipeline.connect(\"doc_converter.langchain_documents\", \"kg_generator.documents\")\n",
    "        pipeline.connect(\"kg_generator.knowledge_graph\", \"test_generator.knowledge_graph\")\n",
    "        pipeline.connect(\"doc_converter.langchain_documents\", \"test_generator.documents\")\n",
    "        pipeline.connect(\"test_generator.testset\", \"test_saver.testset\")\n",
    "        \n",
    "        # Prepare input data - convert PDF files to ByteStream objects\n",
    "        pdf_sources = [Path(\"./data_for_indexing/howpeopleuseai.pdf\")]\n",
    "         \n",
    "        result = pipeline.run({\n",
    "            \"pdf_converter\": {\"sources\": pdf_sources}\n",
    "        })\n",
    "        \n",
    "        print(\"\\n📊 Pipeline Results:\")\n",
    "        print(f\"  📄 Documents Processed: {result['doc_converter']['document_count']}\")\n",
    "        print(f\"  🧠 Knowledge Graph Nodes: {result['kg_generator']['node_count']}\")\n",
    "        print(f\"  🧪 Test Cases Generated: {result['test_generator']['testset_size']}\")\n",
    "        print(f\"  🔧 Generation Method: {result['test_generator']['generation_method']}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ No PDF files found in data_for_indexing directory\")\n",
    "else:\n",
    "    print(\"❌ Data path 'data_for_indexing' not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1429d2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 Synthetic Tests Sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How do peple use OpenAI in their daily lives?</td>\n",
       "      <td>['NBER WORKING PAPER SERIES\\nHOW PEOPLE USE CH...</td>\n",
       "      <td>The study documents the growth of ChatGPT’s co...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who is Thomas Cunningham in relation to the st...</td>\n",
       "      <td>['ABSTRACT Despite the rapid adoption of LLM c...</td>\n",
       "      <td>Thomas Cunningham is associated with OpenAI an...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What does Roth (2025) report about ChatGPT usa...</td>\n",
       "      <td>['to classify messages without any human seein...</td>\n",
       "      <td>Roth (2025) reports that 28% of US adults used...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What trends in user behavior and message types...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nHowever, in the first half of 202...</td>\n",
       "      <td>By June 2025, it was observed that the share o...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the key privacy protections implement...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nOverall, the majority of ChatGPT ...</td>\n",
       "      <td>The key privacy protections implemented in the...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0      How do peple use OpenAI in their daily lives?   \n",
       "1  Who is Thomas Cunningham in relation to the st...   \n",
       "2  What does Roth (2025) report about ChatGPT usa...   \n",
       "3  What trends in user behavior and message types...   \n",
       "4  What are the key privacy protections implement...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  ['NBER WORKING PAPER SERIES\\nHOW PEOPLE USE CH...   \n",
       "1  ['ABSTRACT Despite the rapid adoption of LLM c...   \n",
       "2  ['to classify messages without any human seein...   \n",
       "3  ['<1-hop>\\n\\nHowever, in the first half of 202...   \n",
       "4  ['<1-hop>\\n\\nOverall, the majority of ChatGPT ...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  The study documents the growth of ChatGPT’s co...   \n",
       "1  Thomas Cunningham is associated with OpenAI an...   \n",
       "2  Roth (2025) reports that 28% of US adults used...   \n",
       "3  By June 2025, it was observed that the share o...   \n",
       "4  The key privacy protections implemented in the...   \n",
       "\n",
       "                        synthesizer_name  \n",
       "0  single_hop_specific_query_synthesizer  \n",
       "1  single_hop_specific_query_synthesizer  \n",
       "2  single_hop_specific_query_synthesizer  \n",
       "3   multi_hop_specific_query_synthesizer  \n",
       "4   multi_hop_specific_query_synthesizer  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load and display the generated synthetic tests\n",
    "test_file_path = \"data_for_eval/synthetic_tests_10.csv\"\n",
    "\n",
    "if os.path.exists(test_file_path):\n",
    "    synthetic_tests_df = pd.read_csv(test_file_path)\n",
    "    print(\"\\n🧪 Synthetic Tests Sample:\")\n",
    "    display(synthetic_tests_df.head())\n",
    "else:\n",
    "    print(\"❌ Synthetic test file not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f162b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-with-haystack-ch5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
