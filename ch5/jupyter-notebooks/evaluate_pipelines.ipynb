{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c86671d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating synthetic test generation pipeline...\n",
      "Checking environment setup...\n",
      "‚úÖ OPENAI_API_KEY found\n",
      "Running pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying HeadlinesExtractor: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:10<00:00,  2.02it/s]\n",
      "Applying HeadlineSplitter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:00<00:00, 3546.70it/s]\n",
      "Applying SummaryExtractor:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 20/35 [00:12<00:06,  2.34it/s]Property 'summary' already exists in node 'b813d7'. Skipping!\n",
      "Applying SummaryExtractor:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 22/35 [00:12<00:05,  2.55it/s]Property 'summary' already exists in node '372af7'. Skipping!\n",
      "Applying SummaryExtractor:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 23/35 [00:13<00:04,  2.50it/s]Property 'summary' already exists in node '1d93b5'. Skipping!\n",
      "Property 'summary' already exists in node '02a20b'. Skipping!\n",
      "Applying SummaryExtractor:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 24/35 [00:14<00:05,  2.11it/s]Property 'summary' already exists in node 'ae99e4'. Skipping!\n",
      "Property 'summary' already exists in node '695b25'. Skipping!\n",
      "Applying SummaryExtractor:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 26/35 [00:15<00:04,  2.06it/s]Property 'summary' already exists in node '693753'. Skipping!\n",
      "Property 'summary' already exists in node '05d831'. Skipping!\n",
      "Applying SummaryExtractor:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 28/35 [00:16<00:03,  2.12it/s]Property 'summary' already exists in node 'caa45e'. Skipping!\n",
      "Applying SummaryExtractor:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 30/35 [00:16<00:02,  2.45it/s]Property 'summary' already exists in node 'd51273'. Skipping!\n",
      "Property 'summary' already exists in node '0025cd'. Skipping!\n",
      "Property 'summary' already exists in node '51fcb1'. Skipping!\n",
      "Applying SummaryExtractor:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 31/35 [00:18<00:02,  1.76it/s]Property 'summary' already exists in node 'b36aa7'. Skipping!\n",
      "Applying SummaryExtractor:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 34/35 [00:18<00:00,  2.57it/s]Property 'summary' already exists in node '685616'. Skipping!\n",
      "Applying SummaryExtractor: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 35/35 [00:19<00:00,  1.84it/s]\n",
      "Applying CustomNodeFilter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [00:07<00:00,  1.90it/s]\n",
      "Applying EmbeddingExtractor:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 17/35 [00:04<00:03,  5.23it/s]Property 'summary_embedding' already exists in node '02a20b'. Skipping!\n",
      "Property 'summary_embedding' already exists in node 'ae99e4'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '51fcb1'. Skipping!\n",
      "Property 'summary_embedding' already exists in node 'b813d7'. Skipping!\n",
      "Property 'summary_embedding' already exists in node 'caa45e'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '1d93b5'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '05d831'. Skipping!\n",
      "Applying EmbeddingExtractor:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 19/35 [00:06<00:06,  2.51it/s]Property 'summary_embedding' already exists in node '372af7'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '0025cd'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '685616'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '695b25'. Skipping!\n",
      "Applying EmbeddingExtractor:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 29/35 [00:07<00:01,  4.57it/s]Property 'summary_embedding' already exists in node '693753'. Skipping!\n",
      "Property 'summary_embedding' already exists in node 'd51273'. Skipping!\n",
      "Applying EmbeddingExtractor:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 33/35 [00:08<00:00,  4.89it/s]Property 'summary_embedding' already exists in node 'b36aa7'. Skipping!\n",
      "Applying EmbeddingExtractor: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 35/35 [00:08<00:00,  4.13it/s]\n",
      "Applying ThemesExtractor: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:06<00:00,  1.68it/s]\n",
      "Applying NERExtractor: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:06<00:00,  1.81it/s]\n",
      "Applying CosineSimilarityBuilder: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 156.79it/s]\n",
      "Applying OverlapScoreBuilder: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 801.36it/s]\n",
      "Generating personas: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.18it/s]\n",
      "Generating Scenarios: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:16<00:00,  5.49s/it]\n",
      "Generating Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:07<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Pipeline Results:\n",
      "  üß† Knowledge Graph Nodes: 64\n",
      "  üß™ Test Cases Generated: 10\n",
      "  üîß Generation Method: knowledge_graph\n",
      "  üíæ Saved to: data_for_eval/synthetic_tests_10.csv\n",
      "  ‚úÖ Save Success: True\n",
      "  üìä Rows Saved: 10\n",
      "\n",
      "üìã Test cases successfully generated and saved!\n",
      "     Check the saved file: data_for_eval/synthetic_tests_10.csv\n",
      "\n",
      "üìã Sample Questions from saved file (showing 3 of 10):\n",
      "  Q1: N/A\n",
      "     A: N/A...\n",
      "\n",
      "  Q2: N/A\n",
      "     A: N/A...\n",
      "\n",
      "  Q3: N/A\n",
      "     A: N/A...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage and pipeline creation\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Example usage of the synthetic test generation components.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    from dotenv import load_dotenv\n",
    "    from haystack import Pipeline\n",
    "    from langchain_community.document_loaders import DirectoryLoader, PyMuPDFLoader\n",
    "    from scripts.knowledge_graph_component import KnowledgeGraphGenerator\n",
    "    from scripts.synthetic_test_components import SyntheticTestGenerator, TestDatasetSaver\n",
    "    \n",
    "    # Load environment variables\n",
    "    load_dotenv(\"./.env\")\n",
    "    \n",
    "    # Example: Create a complete pipeline for synthetic test generation\n",
    "    data_path = \"data_for_indexing\"\n",
    "    \n",
    "    if os.path.exists(data_path):\n",
    "        print(\"Creating synthetic test generation pipeline...\")\n",
    "        \n",
    "        # Load documents\n",
    "        loader = DirectoryLoader(data_path, glob=\"*.pdf\", loader_cls=PyMuPDFLoader)\n",
    "        docs = loader.load()\n",
    "        \n",
    "        if docs:\n",
    "            # Create pipeline components with corrected parameters\n",
    "            kg_generator = KnowledgeGraphGenerator(apply_transforms=True)\n",
    "            \n",
    "            # Updated: Removed artificial size limits - now generates requested 10 tests\n",
    "            test_generator = SyntheticTestGenerator(\n",
    "                testset_size=10,  # This will now generate 10 tests instead of 3\n",
    "                llm_model=\"gpt-4o-mini\",\n",
    "                query_distribution=[\n",
    "                    (\"single_hop\", 0.25), \n",
    "                    (\"multi_hop_specific\", 0.25),\n",
    "                    (\"multi_hop_abstract\", 0.5)\n",
    "                ],\n",
    "                # Optional: Add max_testset_size=5 if you want to limit due to API constraints\n",
    "                # max_testset_size=5  # Uncomment this line if you experience API timeouts\n",
    "            )\n",
    "            test_saver = TestDatasetSaver(\"data_for_eval/synthetic_tests_10.csv\")\n",
    "            \n",
    "            # Create pipeline\n",
    "            pipeline = Pipeline()\n",
    "            pipeline.add_component(\"kg_generator\", kg_generator)\n",
    "            pipeline.add_component(\"test_generator\", test_generator)\n",
    "            pipeline.add_component(\"test_saver\", test_saver)\n",
    "            \n",
    "            # Connect components\n",
    "            pipeline.connect(\"kg_generator.knowledge_graph\", \"test_generator.knowledge_graph\")\n",
    "            pipeline.connect(\"test_generator.testset\", \"test_saver.testset\")\n",
    "            \n",
    "            # Check environment first\n",
    "            print(\"Checking environment setup...\")\n",
    "            api_key = os.getenv('OPENAI_API_KEY')\n",
    "            if not api_key:\n",
    "                print(\"‚ùå Error: OPENAI_API_KEY not found in environment variables.\")\n",
    "                print(\"Please set your OpenAI API key in the .env file.\")\n",
    "                exit(1)\n",
    "            else:\n",
    "                print(\"‚úÖ OPENAI_API_KEY found\")\n",
    "            \n",
    "            # Run pipeline\n",
    "            try:\n",
    "                print(\"Running pipeline...\")\n",
    "                result = pipeline.run({\n",
    "                    \"kg_generator\": {\"documents\": docs},\n",
    "                    \"test_generator\": {\"documents\": docs}\n",
    "                })\n",
    "                \n",
    "                print(\"\\nüìä Pipeline Results:\")\n",
    "                print(f\"  üß† Knowledge Graph Nodes: {result['kg_generator']['node_count']}\")\n",
    "                print(f\"  üß™ Test Cases Generated: {result['test_generator']['testset_size']}\")\n",
    "                print(f\"  üîß Generation Method: {result['test_generator']['generation_method']}\")\n",
    "                \n",
    "                if result['test_generator']['success']:\n",
    "                    print(f\"  üíæ Saved to: {result['test_saver']['saved_path']}\")\n",
    "                    print(f\"  ‚úÖ Save Success: {result['test_saver']['success']}\")\n",
    "                    print(f\"  üìä Rows Saved: {result['test_saver']['row_count']}\")\n",
    "                    \n",
    "                    # Safely access testset - check if it exists in the result\n",
    "                    if 'testset' in result['test_generator']:\n",
    "                        testset_df = result['test_generator']['testset']\n",
    "                        print(f\"\\nüìã Sample Questions (showing 3 of {len(testset_df)}):\")\n",
    "                        for i, row in testset_df.head(3).iterrows():\n",
    "                            print(f\"  Q{i+1}: {row.get('question', 'N/A')}\")\n",
    "                            print(f\"     A: {row.get('ground_truth', row.get('answer', 'N/A'))[:100]}...\")\n",
    "                            print()\n",
    "                    else:\n",
    "                        print(f\"\\nüìã Test cases successfully generated and saved!\")\n",
    "                        print(f\"     Check the saved file: {result['test_saver']['saved_path']}\")\n",
    "                        # Try to read the saved file to display sample questions\n",
    "                        try:\n",
    "                            import pandas as pd\n",
    "                            saved_df = pd.read_csv(result['test_saver']['saved_path'])\n",
    "                            print(f\"\\nüìã Sample Questions from saved file (showing 3 of {len(saved_df)}):\")\n",
    "                            for i, row in saved_df.head(3).iterrows():\n",
    "                                print(f\"  Q{i+1}: {row.get('question', 'N/A')}\")\n",
    "                                print(f\"     A: {row.get('ground_truth', row.get('answer', 'N/A'))[:100]}...\")\n",
    "                                print()\n",
    "                        except Exception as read_error:\n",
    "                            print(f\"     Could not read saved file: {read_error}\")\n",
    "                else:\n",
    "                    print(f\"  ‚ùå Test generation failed: {result['test_generator']['generation_method']}\")\n",
    "                    if 'connection' in result['test_generator']['generation_method']:\n",
    "                        print(\"  üí° This appears to be a connection issue. Please check:\")\n",
    "                        print(\"     - Internet connection\")\n",
    "                        print(\"     - OpenAI API key validity\")\n",
    "                        print(\"     - OpenAI API quota/billing\")\n",
    "                        print(\"  üí° Try adding max_testset_size=3 to the SyntheticTestGenerator if API timeouts occur\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Pipeline execution failed: {e}\")\n",
    "                print(f\"Error type: {type(e).__name__}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "        else:\n",
    "            print(f\"No documents found in {data_path}\")\n",
    "    else:\n",
    "        print(f\"Data path {data_path} not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc57506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Test the components individually to understand the data flow\n",
    "print(\"üß™ Testing individual components...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyMuPDFLoader\n",
    "from scripts.knowledge_graph_component import KnowledgeGraphGenerator\n",
    "from scripts.synthetic_test_components import SyntheticTestGenerator, TestDatasetSaver\n",
    "\n",
    "load_dotenv(\"./.env\")\n",
    "\n",
    "# Test with a small sample\n",
    "data_path = \"data_for_indexing\"\n",
    "if os.path.exists(data_path):\n",
    "    # Load just one document for testing\n",
    "    loader = DirectoryLoader(data_path, glob=\"*.pdf\", loader_cls=PyMuPDFLoader)\n",
    "    docs = loader.load()\n",
    "    \n",
    "    if docs:\n",
    "        print(f\"‚úÖ Loaded {len(docs)} documents\")\n",
    "        \n",
    "        # Test individual components\n",
    "        print(\"\\n1. Testing KnowledgeGraphGenerator...\")\n",
    "        kg_generator = KnowledgeGraphGenerator(apply_transforms=False)  # Skip transforms for speed\n",
    "        kg_result = kg_generator.run(documents=docs[:1])  # Use just first document\n",
    "        print(f\"   KG Nodes: {kg_result['node_count']}\")\n",
    "        \n",
    "        print(\"\\n2. Testing SyntheticTestGenerator...\")\n",
    "        test_generator = SyntheticTestGenerator(\n",
    "            testset_size=3,  # Small number for testing\n",
    "            llm_model=\"gpt-4o-mini\",\n",
    "            max_testset_size=2  # Even smaller limit for testing\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            test_result = test_generator.run(\n",
    "                documents=docs[:1], \n",
    "                knowledge_graph=kg_result['knowledge_graph']\n",
    "            )\n",
    "            \n",
    "            print(f\"   Generation success: {test_result['success']}\")\n",
    "            print(f\"   Test cases: {test_result['testset_size']}\")\n",
    "            print(f\"   Method: {test_result['generation_method']}\")\n",
    "            \n",
    "            # Check what's actually in the result\n",
    "            print(f\"   Result keys: {list(test_result.keys())}\")\n",
    "            \n",
    "            if test_result['success'] and 'testset' in test_result:\n",
    "                testset_df = test_result['testset']\n",
    "                print(f\"   Testset type: {type(testset_df)}\")\n",
    "                print(f\"   Testset shape: {testset_df.shape if hasattr(testset_df, 'shape') else 'No shape'}\")\n",
    "                if hasattr(testset_df, 'columns'):\n",
    "                    print(f\"   Testset columns: {list(testset_df.columns)}\")\n",
    "                    \n",
    "                    # Show first question\n",
    "                    if len(testset_df) > 0:\n",
    "                        first_row = testset_df.iloc[0]\n",
    "                        print(f\"\\n   üìù Sample Question:\")\n",
    "                        print(f\"      Q: {first_row.get('question', 'N/A')}\")\n",
    "                        print(f\"      A: {first_row.get('ground_truth', first_row.get('answer', 'N/A'))[:100]}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Test generation failed: {e}\")\n",
    "            print(f\"   Error type: {type(e).__name__}\")\n",
    "    else:\n",
    "        print(\"‚ùå No documents found\")\n",
    "else:\n",
    "    print(\"‚ùå Data path not found\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"üí° This test helps identify where the issue occurs in the pipeline.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-with-haystack-ch5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
