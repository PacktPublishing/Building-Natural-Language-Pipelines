{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca1c5d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurafunderburk/Documents/GitHub/Building-Natural-Language-Pipelines/ch4/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running unified indexing pipeline for web, local files, and CSV...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing document 1384ec36dd6d99f90ab589732d5219b7371dac846d0f0bd89c6385189c4079c0. Keeping it, but skipping cleaning. Error: Error tokenizing data. C error: Expected 5 fields in line 5, saw 7\n",
      "\n",
      "Error processing document 1384ec36dd6d99f90ab589732d5219b7371dac846d0f0bd89c6385189c4079c0. Keeping it, but skipping splitting. Error: Error tokenizing data. C error: Expected 5 fields in line 5, saw 7\n",
      "\n",
      "Batches: 100%|██████████| 5/5 [00:03<00:00,  1.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# Continue from the previous script, assuming 'document_store' is populated.\n",
    "from scripts.indexing import document_store  # Adjust the import as necessary\n",
    "\n",
    "# Import necessary components for the query pipeline\n",
    "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.utils import Secret\n",
    "from haystack import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88910741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x3be143e00>\n",
       "🚅 Components\n",
       "  - text_embedder: SentenceTransformersTextEmbedder\n",
       "  - retriever: InMemoryEmbeddingRetriever\n",
       "  - prompt_builder: PromptBuilder\n",
       "  - llm: OpenAIGenerator\n",
       "🛤️ Connections\n",
       "  - text_embedder.embedding -> retriever.query_embedding (list[float])\n",
       "  - retriever.documents -> prompt_builder.documents (list[Document])\n",
       "  - prompt_builder.prompt -> llm.prompt (str)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive RAG Pipeline Construction\n",
    "\n",
    "# Text Embedder: To embed the user's query. Must be compatible with the document embedder.\n",
    "text_embedder = SentenceTransformersTextEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Retriever: Fetches documents from the DocumentStore based on vector similarity.\n",
    "retriever = InMemoryEmbeddingRetriever(document_store=document_store, top_k=3)\n",
    "\n",
    "# PromptBuilder: Creates a prompt using the retrieved documents and the query.\n",
    "# The Jinja2 template iterates through the documents and adds their content to the prompt.\n",
    "prompt_template_for_pipeline = \"\"\"\n",
    "Given the following information, answer the user's question.\n",
    "If the information is not available in the provided documents, say that you don't have enough information to answer.\n",
    "\n",
    "Context:\n",
    "{% for doc in documents %}\n",
    "    {{ doc.content }}\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{question}}\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt_builder_inst = PromptBuilder(template=prompt_template_for_pipeline,\n",
    "                                    required_variables=\"*\")\n",
    "llm_generator_inst = OpenAIGenerator(api_key=Secret.from_env_var(\"OPENAI_API_KEY\"), model=\"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "# --- 2. Build the Naive RAG Pipeline ---\n",
    "\n",
    "naive_rag_pipeline = Pipeline()\n",
    "\n",
    "# Add components to the pipeline\n",
    "naive_rag_pipeline.add_component(\"text_embedder\", text_embedder)\n",
    "naive_rag_pipeline.add_component(\"retriever\", retriever)\n",
    "naive_rag_pipeline.add_component(\"prompt_builder\", prompt_builder_inst)\n",
    "naive_rag_pipeline.add_component(\"llm\", llm_generator_inst)\n",
    "\n",
    "# --- 3. Connect the Components ---\n",
    "\n",
    "# The query embedding is sent to the retriever\n",
    "naive_rag_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "# The retriever's documents are sent to the prompt builder\n",
    "naive_rag_pipeline.connect(\"retriever.documents\", \"prompt_builder.documents\")\n",
    "# The final prompt is sent to the LLM\n",
    "naive_rag_pipeline.connect(\"prompt_builder.prompt\", \"llm.prompt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77fb36d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.retrievers.in_memory import InMemoryBM25Retriever\n",
    "from haystack.components.joiners import DocumentJoiner\n",
    "from haystack.components.rankers import TransformersSimilarityRanker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f54de6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TransformersSimilarityRanker is considered legacy and will no longer receive updates. It may be deprecated in a future release, with removal following after a deprecation period. Consider using SentenceTransformersSimilarityRanker instead, which provides the same functionality along with additional features.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x17c681880>\n",
       "🚅 Components\n",
       "  - text_embedder: SentenceTransformersTextEmbedder\n",
       "  - embedding_retriever: InMemoryEmbeddingRetriever\n",
       "  - bm25_retriever: InMemoryBM25Retriever\n",
       "  - document_joiner: DocumentJoiner\n",
       "  - ranker: TransformersSimilarityRanker\n",
       "  - prompt_builder: PromptBuilder\n",
       "  - llm: OpenAIGenerator\n",
       "🛤️ Connections\n",
       "  - text_embedder.embedding -> embedding_retriever.query_embedding (list[float])\n",
       "  - embedding_retriever.documents -> document_joiner.documents (list[Document])\n",
       "  - bm25_retriever.documents -> document_joiner.documents (list[Document])\n",
       "  - document_joiner.documents -> ranker.documents (list[Document])\n",
       "  - ranker.documents -> prompt_builder.documents (list[Document])\n",
       "  - prompt_builder.prompt -> llm.prompt (str)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hybrid RAG Pipeline Construction\n",
    "\n",
    "# Text Embedder: To embed the user's query. Must be compatible with the document embedder.\n",
    "text_embedder = SentenceTransformersTextEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Retriever: Fetches documents from the DocumentStore based on vector similarity.\n",
    "retriever = InMemoryEmbeddingRetriever(document_store=document_store, top_k=3)\n",
    "\n",
    "# PromptBuilder: Creates a prompt using the retrieved documents and the query.\n",
    "# The Jinja2 template iterates through the documents and adds their content to the prompt.\n",
    "prompt_template_for_pipeline = \"\"\"\n",
    "Given the following information, answer the user's question.\n",
    "If the information is not available in the provided documents, say that you don't have enough information to answer.\n",
    "\n",
    "Context:\n",
    "{% for doc in documents %}\n",
    "    {{ doc.content }}\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{question}}\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt_builder_inst = PromptBuilder(template=prompt_template_for_pipeline,\n",
    "                                    required_variables=\"*\")\n",
    "llm_generator_inst = OpenAIGenerator(api_key=Secret.from_env_var(\"OPENAI_API_KEY\"), model=\"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "\n",
    "# Sparse Retriever (BM25): For keyword-based search.\n",
    "# This retriever needs to be \"warmed up\" by calculating statistics on the documents in the store.\n",
    "bm25_retriever = InMemoryBM25Retriever(document_store=document_store, top_k=3)\n",
    "\n",
    "# DocumentJoiner: To merge the results from the two retrievers.\n",
    "# The default 'concatenate' mode works well here as the ranker will handle final ordering.\n",
    "document_joiner = DocumentJoiner()\n",
    "\n",
    "# Ranker: A cross-encoder model to re-rank the combined results for higher precision.\n",
    "# This model is highly effective at identifying the most relevant documents from a candidate set.\n",
    "ranker = TransformersSimilarityRanker(model=\"BAAI/bge-reranker-base\", top_k=3)\n",
    "\n",
    "\n",
    "# --- 2. Build the Hybrid RAG Pipeline ---\n",
    "\n",
    "hybrid_rag_pipeline = Pipeline()\n",
    "\n",
    "# Add all necessary components\n",
    "hybrid_rag_pipeline.add_component(\"text_embedder\", text_embedder)\n",
    "hybrid_rag_pipeline.add_component(\"embedding_retriever\", retriever) # Dense retriever\n",
    "hybrid_rag_pipeline.add_component(\"bm25_retriever\", bm25_retriever) # Sparse retriever\n",
    "hybrid_rag_pipeline.add_component(\"document_joiner\", document_joiner)\n",
    "hybrid_rag_pipeline.add_component(\"ranker\", ranker)\n",
    "hybrid_rag_pipeline.add_component(\"prompt_builder\", prompt_builder_inst)\n",
    "hybrid_rag_pipeline.add_component(\"llm\", llm_generator_inst)\n",
    "\n",
    "# --- 3. Connect the Components in a Graph ---\n",
    "\n",
    "# The query is embedded for the dense retriever\n",
    "hybrid_rag_pipeline.connect(\"text_embedder.embedding\", \"embedding_retriever.query_embedding\")\n",
    "\n",
    "# The raw query text is sent to the BM25 retriever and the ranker\n",
    "# Note: The query input for these components is the raw text string.\n",
    "\n",
    "# The outputs of both retrievers are fed into the document joiner\n",
    "hybrid_rag_pipeline.connect(\"embedding_retriever.documents\", \"document_joiner.documents\")\n",
    "hybrid_rag_pipeline.connect(\"bm25_retriever.documents\", \"document_joiner.documents\")\n",
    "\n",
    "# The joined documents are sent to the ranker\n",
    "hybrid_rag_pipeline.connect(\"document_joiner.documents\", \"ranker.documents\")\n",
    "\n",
    "# The ranked documents are sent to the prompt builder\n",
    "hybrid_rag_pipeline.connect(\"ranker.documents\", \"prompt_builder.documents\")\n",
    "\n",
    "# The final prompt is sent to the LLM\n",
    "hybrid_rag_pipeline.connect(\"prompt_builder.prompt\", \"llm.prompt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cca3ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Test Set:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is OpenAI?</td>\n",
       "      <td>['Introduction\\nChatGPT launched in November 2...</td>\n",
       "      <td>OpenAI is the developer of ChatGPT, a large la...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How many total messages were sent in Jun 2025?</td>\n",
       "      <td>['Month\\nNon-Work (M)\\n(%)\\nWork (M)\\n(%)\\nTot...</td>\n",
       "      <td>The total messages sent in Jun 2025 were 2,627.</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the significance of June 2025 in ChatG...</td>\n",
       "      <td>['Table 1: ChatGPT daily message counts (milli...</td>\n",
       "      <td>The context reports data ending on the 26th of...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does Caplin relate to AI's economic value?</td>\n",
       "      <td>['Doing, and that Asking messages are consiste...</td>\n",
       "      <td>The context discusses how ChatGPT improves wor...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How do inter-annotator agreement metrics like ...</td>\n",
       "      <td>[\"&lt;1-hop&gt;\\n\\nAppendix: Classifier Validation\\n...</td>\n",
       "      <td>The context shows that inter-annotator agreeme...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0                                    What is OpenAI?   \n",
       "1     How many total messages were sent in Jun 2025?   \n",
       "2  What is the significance of June 2025 in ChatG...   \n",
       "3     How does Caplin relate to AI's economic value?   \n",
       "4  How do inter-annotator agreement metrics like ...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  ['Introduction\\nChatGPT launched in November 2...   \n",
       "1  ['Month\\nNon-Work (M)\\n(%)\\nWork (M)\\n(%)\\nTot...   \n",
       "2  ['Table 1: ChatGPT daily message counts (milli...   \n",
       "3  ['Doing, and that Asking messages are consiste...   \n",
       "4  [\"<1-hop>\\n\\nAppendix: Classifier Validation\\n...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  OpenAI is the developer of ChatGPT, a large la...   \n",
       "1    The total messages sent in Jun 2025 were 2,627.   \n",
       "2  The context reports data ending on the 26th of...   \n",
       "3  The context discusses how ChatGPT improves wor...   \n",
       "4  The context shows that inter-annotator agreeme...   \n",
       "\n",
       "                        synthesizer_name  \n",
       "0  single_hop_specific_query_synthesizer  \n",
       "1  single_hop_specific_query_synthesizer  \n",
       "2  single_hop_specific_query_synthesizer  \n",
       "3  single_hop_specific_query_synthesizer  \n",
       "4   multi_hop_abstract_query_synthesizer  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "# Convert the generated test set to a pandas DataFrame for easier handling\n",
    "test_df = pd.read_csv(\"./data_for_eval/synthetic_qa_pairs.csv\")\n",
    "print(\"Generated Test Set:\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b41d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_rag_pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ca4da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Naive RAG pipeline...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Pipeline' object has no attribute 'components'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m ground_truths = test_df[\u001b[33m\"\u001b[39m\u001b[33mreference\u001b[39m\u001b[33m\"\u001b[39m].tolist()\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mRunning Naive RAG pipeline...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m naive_outputs = \u001b[43mget_pipeline_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnaive_rag_pipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRunning Hybrid RAG pipeline...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     33\u001b[39m hybrid_outputs = get_pipeline_outputs(hybrid_rag_pipeline, questions)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mget_pipeline_outputs\u001b[39m\u001b[34m(pipeline, questions)\u001b[39m\n\u001b[32m      3\u001b[39m outputs = []\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m questions:\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mbm25_retriever\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcomponents\u001b[49m: \u001b[38;5;66;03m# Hybrid pipeline\u001b[39;00m\n\u001b[32m      6\u001b[39m         result = pipeline.run({\n\u001b[32m      7\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtext_embedder\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: q},\n\u001b[32m      8\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mbm25_retriever\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m: q},\n\u001b[32m      9\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mranker\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m: q},\n\u001b[32m     10\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mprompt_builder\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: q}\n\u001b[32m     11\u001b[39m         })\n\u001b[32m     12\u001b[39m         \u001b[38;5;66;03m# For hybrid, the final documents come from the ranker\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'Pipeline' object has no attribute 'components'"
     ]
    }
   ],
   "source": [
    "def get_pipeline_outputs(pipeline, questions):\n",
    "    \"\"\"Helper function to run a pipeline over a list of questions and collect outputs.\"\"\"\n",
    "    outputs = []\n",
    "    for q in questions:\n",
    "        if \"bm25_retriever\" in pipeline.: # Hybrid pipeline\n",
    "            result = pipeline.run({\n",
    "                \"text_embedder\": {\"text\": q},\n",
    "                \"bm25_retriever\": {\"query\": q},\n",
    "                \"ranker\": {\"query\": q},\n",
    "                \"prompt_builder\": {\"question\": q}\n",
    "            })\n",
    "            # For hybrid, the final documents come from the ranker\n",
    "            retrieved_docs = result[\"ranker\"][\"documents\"]\n",
    "        else: # Naive pipeline\n",
    "            result = pipeline.run({\n",
    "                \"text_embedder\": {\"text\": q},\n",
    "                \"prompt_builder\": {\"question\": q}\n",
    "            })\n",
    "            retrieved_docs = result[\"retriever\"][\"documents\"]\n",
    "        \n",
    "        answer = result[\"llm\"][\"replies\"].content\n",
    "        contexts = [doc.content for doc in retrieved_docs]\n",
    "        outputs.append({\"answer\": answer, \"contexts\": contexts})\n",
    "    return outputs\n",
    "\n",
    "questions = test_df[\"user_input\"].tolist()\n",
    "ground_truths = test_df[\"reference\"].tolist()\n",
    "\n",
    "print(\"\\nRunning Naive RAG pipeline...\")\n",
    "naive_outputs = get_pipeline_outputs(naive_rag_pipeline, questions)\n",
    "\n",
    "print(\"Running Hybrid RAG pipeline...\")\n",
    "hybrid_outputs = get_pipeline_outputs(hybrid_rag_pipeline, questions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5604271b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Build the Evaluation Pipeline ---\n",
    "\n",
    "from haystack_integrations.components.evaluators.ragas import RagasEvaluator, RagasMetric\n",
    "# Initialize RagasEvaluator for each metric\n",
    "faithfulness_evaluator = RagasEvaluator(metric=RagasMetric.FAITHFULNESS)\n",
    "context_recall_evaluator = RagasEvaluator(metric=RagasMetric.CONTEXT_RECALL)\n",
    "\n",
    "# Create the evaluation pipeline\n",
    "evaluation_pipeline = Pipeline()\n",
    "evaluation_pipeline.add_component(\"faithfulness\", faithfulness_evaluator)\n",
    "evaluation_pipeline.add_component(\"context_recall\", context_recall_evaluator)\n",
    "\n",
    "# --- 4. Execute Evaluation and Analyze Results ---\n",
    "\n",
    "def run_evaluation(pipeline_outputs):\n",
    "    \"\"\"Helper function to run the evaluation pipeline and return scores.\"\"\"\n",
    "    eval_results = evaluation_pipeline.run({\n",
    "        \"faithfulness\": {\n",
    "            \"questions\": questions,\n",
    "            \"contexts\": [out[\"contexts\"] for out in pipeline_outputs],\n",
    "            \"responses\": [out[\"answer\"] for out in pipeline_outputs]\n",
    "        },\n",
    "        \"context_recall\": {\n",
    "            \"questions\": questions,\n",
    "            \"contexts\": [out[\"contexts\"] for out in pipeline_outputs],\n",
    "            \"ground_truths\": ground_truths\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    # Calculate average scores\n",
    "    avg_faithfulness = sum(r[\"score\"] for r in eval_results[\"faithfulness\"][\"results\"]) / len(eval_results[\"faithfulness\"][\"results\"])\n",
    "    avg_context_recall = sum(r[\"score\"] for r in eval_results[\"context_recall\"][\"results\"]) / len(eval_results[\"context_recall\"][\"results\"])\n",
    "    \n",
    "    return {\"faithfulness\": avg_faithfulness, \"context_recall\": avg_context_recall}\n",
    "\n",
    "print(\"\\nEvaluating Naive RAG pipeline...\")\n",
    "naive_scores = run_evaluation(naive_outputs)\n",
    "\n",
    "print(\"Evaluating Hybrid RAG pipeline...\")\n",
    "hybrid_scores = run_evaluation(hybrid_outputs)\n",
    "\n",
    "# --- Display Results in a Table ---\n",
    "results_df = pd.DataFrame({\n",
    "    \"Metric\":,\n",
    "    \"Naive RAG Score\": [naive_scores[\"faithfulness\"], naive_scores[\"context_recall\"]],\n",
    "    \"Hybrid RAG Score\": [hybrid_scores[\"faithfulness\"], hybrid_scores[\"context_recall\"]]\n",
    "})\n",
    "\n",
    "print(\"\\n--- Ragas Evaluation Results ---\")\n",
    "print(results_df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-with-haystack-ch4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
