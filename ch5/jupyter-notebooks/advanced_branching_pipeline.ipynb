{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b16f9261",
   "metadata": {},
   "source": [
    "ðŸ”§ **Setup Required**: Before running this notebook, please follow the [setup instructions](../README.md#setup-instructions) to configure your environment and API keys."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788b42c4",
   "metadata": {},
   "source": [
    "# Advanced Branching Pipeline - Multi-Source Knowledge Graph Generation\n",
    "\n",
    "This notebook demonstrates how to build sophisticated branching pipelines that can:\n",
    "1. **Process Multiple Input Types**: Handle PDFs, web URLs, and other document formats simultaneously\n",
    "2. **Intelligent Routing**: Automatically route different content types through appropriate processing paths\n",
    "3. **Unified Knowledge Graphs**: Combine information from multiple sources into a single knowledge representation\n",
    "4. **Scalable Architecture**: Design patterns that can be extended to handle additional content types\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "- How to use Haystack's `FileTypeRouter` for automatic input type detection\n",
    "- How to design branching pipelines that process heterogeneous data sources\n",
    "- How to use `DocumentJoiner` to combine processed content from multiple branches\n",
    "- Best practices for production-ready multi-source processing pipelines\n",
    "\n",
    "## Key Architectural Components\n",
    "- **FileTypeRouter**: Automatically detects input types and routes them appropriately\n",
    "- **DocumentJoiner**: Combines documents from different processing branches\n",
    "- **LinkContentFetcher + HTMLToDocument**: Web content processing branch\n",
    "- **PyPDFToDocument**: PDF processing branch\n",
    "- **Shared Processing Components**: Unified cleaning, splitting, and knowledge graph generation\n",
    "\n",
    "## Real-World Applications\n",
    "This approach is essential for:\n",
    "- **Enterprise Knowledge Management**: Processing diverse document collections\n",
    "- **Research Data Integration**: Combining academic papers, web articles, and reports\n",
    "- **Multi-Modal Content Analysis**: Handling various content formats in a single workflow\n",
    "- **Automated Content Pipelines**: Production systems that need to handle varied input types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3afc4fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurafunderburk/Documents/GitHub/Building-Natural-Language-Pipelines/ch5/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ All components imported successfully!\n",
      "ðŸš€ Ready to build advanced branching pipeline\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from haystack import Pipeline\n",
    "from haystack.components.converters import PyPDFToDocument, HTMLToDocument\n",
    "from haystack.components.fetchers import LinkContentFetcher\n",
    "from haystack.components.routers import FileTypeRouter\n",
    "from haystack.components.joiners import DocumentJoiner\n",
    "from haystack.components.preprocessors import (\n",
    "    DocumentCleaner,\n",
    "    DocumentSplitter)\n",
    "from pathlib import Path\n",
    "from scripts.knowledge_graph_component import KnowledgeGraphGenerator,\\\n",
    "                                                DocumentToLangChainConverter\n",
    "from scripts.synthetic_test_components import SyntheticTestGenerator,\\\n",
    "                                                TestDatasetSaver\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(\"./.env\")\n",
    "\n",
    "print(\"ðŸ“¦ All components imported successfully!\")\n",
    "print(\"ðŸš€ Ready to build advanced branching pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c182d7",
   "metadata": {},
   "source": [
    "## Building the Advanced Branching Pipeline\n",
    "\n",
    "### Pipeline Architecture Overview\n",
    "\n",
    "Our advanced pipeline will follow this architecture:\n",
    "\n",
    "```\n",
    "Input Sources (PDF + Web URL)\n",
    "    â†“                    â†“\n",
    "FileTypeRouter    LinkContentFetcher\n",
    "    â†“                    â†“  \n",
    "PDFConverter      HTMLConverter\n",
    "    â†“                    â†“\n",
    "    â””â”€â”€ DocumentJoiner â”€â”€â”˜\n",
    "            â†“\n",
    "    Document Processing Chain\n",
    "    (Cleaner â†’ Splitter â†’ Converter)\n",
    "            â†“\n",
    "    Knowledge Graph Generator\n",
    "            â†“\n",
    "    Synthetic Test Generator  \n",
    "            â†“\n",
    "    Test Dataset Saver\n",
    "```\n",
    "\n",
    "### Key Design Principles\n",
    "\n",
    "1. **Separation of Concerns**: Each component has a single, well-defined responsibility\n",
    "2. **Flexible Input Handling**: Can process multiple input types simultaneously\n",
    "3. **Unified Processing**: Same downstream logic regardless of input source\n",
    "4. **Extensibility**: Easy to add new input types (CSV, Word docs, etc.)\n",
    "5. **Error Isolation**: Problems with one input source don't affect others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e5ffcd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x30be731a0>\n",
       "ðŸš… Components\n",
       "  - file_router: FileTypeRouter\n",
       "  - link_fetcher: LinkContentFetcher\n",
       "  - pdf_converter: PyPDFToDocument\n",
       "  - html_converter: HTMLToDocument\n",
       "  - doc_joiner: DocumentJoiner\n",
       "  - doc_cleaner: DocumentCleaner\n",
       "  - doc_splitter: DocumentSplitter\n",
       "  - doc_converter: DocumentToLangChainConverter\n",
       "  - kg_generator: KnowledgeGraphGenerator\n",
       "  - test_generator: SyntheticTestGenerator\n",
       "  - test_saver: TestDatasetSaver\n",
       "ðŸ›¤ï¸ Connections\n",
       "  - file_router.application/pdf -> pdf_converter.sources (list[Union[str, Path, ByteStream]])\n",
       "  - link_fetcher.streams -> html_converter.sources (list[ByteStream])\n",
       "  - pdf_converter.documents -> doc_joiner.documents (list[Document])\n",
       "  - html_converter.documents -> doc_joiner.documents (list[Document])\n",
       "  - doc_joiner.documents -> doc_cleaner.documents (list[Document])\n",
       "  - doc_cleaner.documents -> doc_splitter.documents (list[Document])\n",
       "  - doc_splitter.documents -> doc_converter.documents (list[Document])\n",
       "  - doc_converter.langchain_documents -> kg_generator.documents (List[Document])\n",
       "  - doc_converter.langchain_documents -> test_generator.documents (List[Document])\n",
       "  - kg_generator.knowledge_graph -> test_generator.knowledge_graph (KnowledgeGraph)\n",
       "  - test_generator.testset -> test_saver.testset (DataFrame)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize pipeline\n",
    "pipeline = Pipeline()\n",
    "\n",
    "# Core routing and joining components  \n",
    "file_router = FileTypeRouter(mime_types=[\"text/plain\", \"application/pdf\", \"text/html\"])\n",
    "doc_joiner = DocumentJoiner()  # Joins documents from different branches\n",
    "\n",
    "# Input converters for each file type\n",
    "pdf_converter = PyPDFToDocument()\n",
    "html_converter = HTMLToDocument()  \n",
    "link_fetcher = LinkContentFetcher()\n",
    "\n",
    "# Shared processing components\n",
    "doc_cleaner = DocumentCleaner(remove_empty_lines=True, remove_extra_whitespaces=True)\n",
    "doc_splitter = DocumentSplitter(split_by=\"sentence\", split_length=50, split_overlap=5)\n",
    "doc_converter = DocumentToLangChainConverter()\n",
    "kg_generator = KnowledgeGraphGenerator(apply_transforms=True)\n",
    "test_generator = SyntheticTestGenerator(\n",
    "    testset_size=15,  # Larger test set for multiple sources\n",
    "    llm_model=\"gpt-4o-mini\",\n",
    "    query_distribution=[\n",
    "        (\"single_hop\", 0.3),\n",
    "        (\"multi_hop_specific\", 0.3), \n",
    "        (\"multi_hop_abstract\", 0.4)\n",
    "    ]\n",
    ")\n",
    "test_saver = TestDatasetSaver(\"data_for_eval/synthetic_tests_advanced_branching.csv\")\n",
    "\n",
    "# Add all components to pipeline\n",
    "pipeline.add_component(\"file_router\", file_router)\n",
    "pipeline.add_component(\"link_fetcher\", link_fetcher)\n",
    "pipeline.add_component(\"pdf_converter\", pdf_converter) \n",
    "pipeline.add_component(\"html_converter\", html_converter)\n",
    "pipeline.add_component(\"doc_joiner\", doc_joiner)\n",
    "pipeline.add_component(\"doc_cleaner\", doc_cleaner)\n",
    "pipeline.add_component(\"doc_splitter\", doc_splitter)\n",
    "pipeline.add_component(\"doc_converter\", doc_converter)\n",
    "pipeline.add_component(\"kg_generator\", kg_generator)\n",
    "pipeline.add_component(\"test_generator\", test_generator)\n",
    "pipeline.add_component(\"test_saver\", test_saver)\n",
    "\n",
    "# Connect file routing branches\n",
    "pipeline.connect(\"file_router.application/pdf\", \"pdf_converter.sources\") \n",
    "pipeline.connect(\"link_fetcher.streams\", \"html_converter.sources\")\n",
    "\n",
    "# Connect converters to joiner\n",
    "pipeline.connect(\"pdf_converter.documents\", \"doc_joiner.documents\")\n",
    "pipeline.connect(\"html_converter.documents\", \"doc_joiner.documents\")\n",
    "\n",
    "# Connect main processing path\n",
    "pipeline.connect(\"doc_joiner.documents\", \"doc_cleaner.documents\")\n",
    "pipeline.connect(\"doc_cleaner.documents\", \"doc_splitter.documents\")\n",
    "pipeline.connect(\"doc_splitter.documents\", \"doc_converter.documents\")\n",
    "pipeline.connect(\"doc_converter.langchain_documents\", \"kg_generator.documents\")\n",
    "pipeline.connect(\"kg_generator.knowledge_graph\", \"test_generator.knowledge_graph\")\n",
    "pipeline.connect(\"doc_converter.langchain_documents\", \"test_generator.documents\")\n",
    "pipeline.connect(\"test_generator.testset\", \"test_saver.testset\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ee4b0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying HeadlinesExtractor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:10<00:00,  1.84it/s]\n",
      "Applying HeadlineSplitter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:00<00:00, 392.63it/s]\n",
      "Applying SummaryExtractor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:11<00:00,  1.59it/s]\n",
      "Applying CustomNodeFilter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:29<00:00,  1.78it/s]\n",
      "Applying EmbeddingExtractor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:05<00:00,  3.63it/s]\n",
      "Applying ThemesExtractor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:26<00:00,  1.81it/s]\n",
      "Applying NERExtractor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:25<00:00,  1.92it/s]\n",
      "Applying CosineSimilarityBuilder: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 202.92it/s]\n",
      "Applying OverlapScoreBuilder: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 64.18it/s]\n",
      "Generating personas: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.20it/s]\n",
      "Generating Scenarios: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:26<00:00,  8.74s/it]\n",
      "Generating Samples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:09<00:00,  1.70it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define inputs\n",
    "pdf_file = Path(\"./data_for_indexing/howpeopleuseai.pdf\")\n",
    "web_url = \"https://www.tableau.com/data-insights/ai/examples\"\n",
    "\n",
    "\n",
    "# Run pipeline with both input types\n",
    "result = advanced_pipeline.run({\n",
    "    \"file_router\": {\"sources\": [pdf_file]},  # PDF input through FileTypeRouter\n",
    "    \"link_fetcher\": {\"urls\": [web_url]}      # Web input through LinkContentFetcher\n",
    "})\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3775608a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¸ Pipeline diagram saved to: ./images/advanced_branching_kg_pipeline.png\n"
     ]
    }
   ],
   "source": [
    "# Visualize the advanced branching pipeline architecture\n",
    "advanced_pipeline.draw(path=\"./images/advanced_branching_kg_pipeline.png\")\n",
    "print(\"ðŸ“¸ Pipeline diagram saved to: ./images/advanced_branching_kg_pipeline.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04bb1e0",
   "metadata": {},
   "source": [
    "![Advanced Branching Pipeline](./images/advanced_branching_kg_pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0a81ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How does Google utilize AI in its search engin...</td>\n",
       "      <td>['You may or may not be aware of how pervasive...</td>\n",
       "      <td>Google's search engine algorithms utilize AI t...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wut r the robots in Star Wars used for?</td>\n",
       "      <td>['Social media\\nSocial media platforms are ano...</td>\n",
       "      <td>In Star Wars, robots are often depicted as hum...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How AI help in transportation and navigation?</td>\n",
       "      <td>['Some examples of industrial robots include:\\...</td>\n",
       "      <td>AI is used in transportation and navigation in...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How is AI utilized in fraud prevention by banks?</td>\n",
       "      <td>['Fraud prevention\\nIf you have an account wit...</td>\n",
       "      <td>Banks use AI in their fraud detection and prev...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What insights does Aaron Chatterji provide reg...</td>\n",
       "      <td>['NBER WORKING PAPER SERIES\\nHOW PEOPLE USE CH...</td>\n",
       "      <td>In the working paper titled 'How People Use Ch...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  How does Google utilize AI in its search engin...   \n",
       "1            Wut r the robots in Star Wars used for?   \n",
       "2      How AI help in transportation and navigation?   \n",
       "3   How is AI utilized in fraud prevention by banks?   \n",
       "4  What insights does Aaron Chatterji provide reg...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  ['You may or may not be aware of how pervasive...   \n",
       "1  ['Social media\\nSocial media platforms are ano...   \n",
       "2  ['Some examples of industrial robots include:\\...   \n",
       "3  ['Fraud prevention\\nIf you have an account wit...   \n",
       "4  ['NBER WORKING PAPER SERIES\\nHOW PEOPLE USE CH...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  Google's search engine algorithms utilize AI t...   \n",
       "1  In Star Wars, robots are often depicted as hum...   \n",
       "2  AI is used in transportation and navigation in...   \n",
       "3  Banks use AI in their fraud detection and prev...   \n",
       "4  In the working paper titled 'How People Use Ch...   \n",
       "\n",
       "                        synthesizer_name  \n",
       "0  single_hop_specific_query_synthesizer  \n",
       "1  single_hop_specific_query_synthesizer  \n",
       "2  single_hop_specific_query_synthesizer  \n",
       "3  single_hop_specific_query_synthesizer  \n",
       "4  single_hop_specific_query_synthesizer  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>How do classifier prompts relate to the traini...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nOuyang, Long, Jeff Wu, Xu Jiang, ...</td>\n",
       "      <td>Classifier prompts are essential in the traini...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>How do classifier prompts relate to the traini...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nOuyang, Long, Jeff Wu, Xu Jiang, ...</td>\n",
       "      <td>Classifier prompts are essential in the traini...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What are the classifier prompts used for deter...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nOuyang, Long, Jeff Wu, Xu Jiang, ...</td>\n",
       "      <td>The classifier prompts used for determining wo...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>How does the training of language models influ...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nOuyang, Long, Jeff Wu, Xu Jiang, ...</td>\n",
       "      <td>The training of language models, as discussed ...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>How do the privacy protections implemented in ...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nWe describe the contents of each ...</td>\n",
       "      <td>The privacy protections implemented in the dat...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           user_input  \\\n",
       "11  How do classifier prompts relate to the traini...   \n",
       "12  How do classifier prompts relate to the traini...   \n",
       "13  What are the classifier prompts used for deter...   \n",
       "14  How does the training of language models influ...   \n",
       "15  How do the privacy protections implemented in ...   \n",
       "\n",
       "                                   reference_contexts  \\\n",
       "11  ['<1-hop>\\n\\nOuyang, Long, Jeff Wu, Xu Jiang, ...   \n",
       "12  ['<1-hop>\\n\\nOuyang, Long, Jeff Wu, Xu Jiang, ...   \n",
       "13  ['<1-hop>\\n\\nOuyang, Long, Jeff Wu, Xu Jiang, ...   \n",
       "14  ['<1-hop>\\n\\nOuyang, Long, Jeff Wu, Xu Jiang, ...   \n",
       "15  ['<1-hop>\\n\\nWe describe the contents of each ...   \n",
       "\n",
       "                                            reference  \\\n",
       "11  Classifier prompts are essential in the traini...   \n",
       "12  Classifier prompts are essential in the traini...   \n",
       "13  The classifier prompts used for determining wo...   \n",
       "14  The training of language models, as discussed ...   \n",
       "15  The privacy protections implemented in the dat...   \n",
       "\n",
       "                        synthesizer_name  \n",
       "11  multi_hop_abstract_query_synthesizer  \n",
       "12  multi_hop_abstract_query_synthesizer  \n",
       "13  multi_hop_abstract_query_synthesizer  \n",
       "14  multi_hop_abstract_query_synthesizer  \n",
       "15  multi_hop_abstract_query_synthesizer  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load and analyze results from the advanced branching pipeline\n",
    "advanced_test_file = \"data_for_eval/synthetic_tests_advanced_branching.csv\"\n",
    "\n",
    "if os.path.exists(advanced_test_file):\n",
    "    advanced_tests_df = pd.read_csv(advanced_test_file)\n",
    "    \n",
    "    display(advanced_tests_df.head())\n",
    "    display(advanced_tests_df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d8b91f",
   "metadata": {},
   "source": [
    "## Summary and Architecture Analysis\n",
    "\n",
    "### What We've Accomplished\n",
    "\n",
    "In this notebook, we've built increasingly sophisticated branching pipelines:\n",
    "\n",
    "1. **Basic Branching Pipeline**: PDF + Web content processing  \n",
    "2. **Production-Ready Pipeline**: Enhanced error handling and monitoring\n",
    "\n",
    "### Key Architectural Benefits\n",
    "\n",
    "1. **Modularity**: Each component has a single responsibility and can be reused\n",
    "2. **Flexibility**: Easy to add new input types (CSV, Word docs, etc.) \n",
    "3. **Scalability**: DocumentJoiner allows processing multiple sources simultaneously\n",
    "4. **Consistency**: Same processing logic regardless of input source\n",
    "5. **Error Isolation**: Problems with one input source don't affect others\n",
    "\n",
    "### Production Considerations\n",
    "\n",
    "**Advantages of Branching Pipelines:**\n",
    "- **Unified Output**: Single knowledge graph and test dataset from multiple sources\n",
    "- **Rich Context**: Cross-referencing information between different document types\n",
    "- **Operational Efficiency**: One pipeline deployment handles multiple scenarios\n",
    "- **Quality Improvement**: More diverse training data leads to better synthetic questions\n",
    "\n",
    "**When to Use Branching Pipelines:**\n",
    "- Processing heterogeneous document collections\n",
    "- Building comprehensive knowledge bases from multiple sources\n",
    "- Creating robust test datasets that cover various content types\n",
    "- Implementing production pipelines that need input flexibility\n",
    "\n",
    "\n",
    "### Extension Patterns\n",
    "\n",
    "To add new input types:\n",
    "1. Add MIME type to `FileTypeRouter`\n",
    "2. Create appropriate converter component\n",
    "3. Connect converter to `DocumentJoiner`\n",
    "4. No changes needed to downstream processing!\n",
    "\n",
    "This modular approach makes the pipeline highly maintainable and extensible for future requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08e838d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-with-haystack-ch5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
