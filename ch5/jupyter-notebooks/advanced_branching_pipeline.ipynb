{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b16f9261",
   "metadata": {},
   "source": [
    "🔧 **Setup Required**: Before running this notebook, please follow the [setup instructions](../README.md#setup-instructions) to configure your environment and API keys."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788b42c4",
   "metadata": {},
   "source": [
    "# Advanced Branching Pipeline - Multi-Source Knowledge Graph Generation\n",
    "\n",
    "This notebook demonstrates how to build sophisticated branching pipelines that can:\n",
    "1. **Process Multiple Input Types**: Handle PDFs, web URLs, and other document formats simultaneously\n",
    "2. **Intelligent Routing**: Automatically route different content types through appropriate processing paths\n",
    "3. **Unified Knowledge Graphs**: Combine information from multiple sources into a single knowledge representation\n",
    "4. **Scalable Architecture**: Design patterns that can be extended to handle additional content types\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "- How to use Haystack's `FileTypeRouter` for automatic input type detection\n",
    "- How to design branching pipelines that process heterogeneous data sources\n",
    "- How to use `DocumentJoiner` to combine processed content from multiple branches\n",
    "- Best practices for production-ready multi-source processing pipelines\n",
    "\n",
    "## Key Architectural Components\n",
    "- **FileTypeRouter**: Automatically detects input types and routes them appropriately\n",
    "- **DocumentJoiner**: Combines documents from different processing branches\n",
    "- **LinkContentFetcher + HTMLToDocument**: Web content processing branch\n",
    "- **PyPDFToDocument**: PDF processing branch\n",
    "- **Shared Processing Components**: Unified cleaning, splitting, and knowledge graph generation\n",
    "\n",
    "## Real-World Applications\n",
    "This approach is essential for:\n",
    "- **Enterprise Knowledge Management**: Processing diverse document collections\n",
    "- **Research Data Integration**: Combining academic papers, web articles, and reports\n",
    "- **Multi-Modal Content Analysis**: Handling various content formats in a single workflow\n",
    "- **Automated Content Pipelines**: Production systems that need to handle varied input types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3afc4fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurafunderburk/Documents/GitHub/Building-Natural-Language-Pipelines/ch5/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from haystack import Pipeline\n",
    "from haystack.components.converters import PyPDFToDocument, HTMLToDocument\n",
    "from haystack.components.fetchers import LinkContentFetcher\n",
    "from haystack.components.routers import FileTypeRouter\n",
    "from haystack.components.joiners import DocumentJoiner\n",
    "from haystack.components.preprocessors import (\n",
    "    DocumentCleaner,\n",
    "    DocumentSplitter)\n",
    "from pathlib import Path\n",
    "from scripts.knowledge_graph_component import KnowledgeGraphGenerator,\\\n",
    "                                                DocumentToLangChainConverter\n",
    "from scripts.synthetic_test_components import SyntheticTestGenerator,\\\n",
    "                                                TestDatasetSaver\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(\"./.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c182d7",
   "metadata": {},
   "source": [
    "## Building the Advanced Branching Pipeline\n",
    "\n",
    "### Pipeline Architecture Overview\n",
    "\n",
    "Our advanced pipeline will follow this architecture:\n",
    "\n",
    "```\n",
    "Input Sources (PDF + Web URL)\n",
    "    ↓                    ↓\n",
    "FileTypeRouter    LinkContentFetcher\n",
    "    ↓                    ↓  \n",
    "PDFConverter      HTMLConverter\n",
    "    ↓                    ↓\n",
    "    └── DocumentJoiner ──┘\n",
    "            ↓\n",
    "    Document Processing Chain\n",
    "    (Cleaner → Splitter → Converter)\n",
    "            ↓\n",
    "    Knowledge Graph Generator\n",
    "            ↓\n",
    "    Synthetic Test Generator  \n",
    "            ↓\n",
    "    Test Dataset Saver\n",
    "```\n",
    "\n",
    "### Key Design Principles\n",
    "\n",
    "1. **Separation of Concerns**: Each component has a single, well-defined responsibility\n",
    "2. **Flexible Input Handling**: Can process multiple input types simultaneously\n",
    "3. **Unified Processing**: Same downstream logic regardless of input source\n",
    "4. **Extensibility**: Easy to add new input types (CSV, Word docs, etc.)\n",
    "5. **Error Isolation**: Problems with one input source don't affect others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5ffcd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x10751cda0>\n",
       "🚅 Components\n",
       "  - file_router: FileTypeRouter\n",
       "  - link_fetcher: LinkContentFetcher\n",
       "  - pdf_converter: PyPDFToDocument\n",
       "  - html_converter: HTMLToDocument\n",
       "  - doc_joiner: DocumentJoiner\n",
       "  - doc_cleaner: DocumentCleaner\n",
       "  - doc_splitter: DocumentSplitter\n",
       "  - doc_converter: DocumentToLangChainConverter\n",
       "  - kg_generator: KnowledgeGraphGenerator\n",
       "  - test_generator: SyntheticTestGenerator\n",
       "  - test_saver: TestDatasetSaver\n",
       "🛤️ Connections\n",
       "  - file_router.application/pdf -> pdf_converter.sources (list[Union[str, Path, ByteStream]])\n",
       "  - link_fetcher.streams -> html_converter.sources (list[ByteStream])\n",
       "  - pdf_converter.documents -> doc_joiner.documents (list[Document])\n",
       "  - html_converter.documents -> doc_joiner.documents (list[Document])\n",
       "  - doc_joiner.documents -> doc_cleaner.documents (list[Document])\n",
       "  - doc_cleaner.documents -> doc_splitter.documents (list[Document])\n",
       "  - doc_splitter.documents -> doc_converter.documents (list[Document])\n",
       "  - doc_converter.langchain_documents -> kg_generator.documents (List[Document])\n",
       "  - doc_converter.langchain_documents -> test_generator.documents (List[Document])\n",
       "  - kg_generator.knowledge_graph -> test_generator.knowledge_graph (KnowledgeGraph)\n",
       "  - test_generator.testset -> test_saver.testset (DataFrame)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize pipeline\n",
    "pipeline = Pipeline()\n",
    "\n",
    "# Core routing and joining components  \n",
    "file_router = FileTypeRouter(mime_types=[\"text/plain\", \"application/pdf\", \"text/html\"])\n",
    "doc_joiner = DocumentJoiner()  # Joins documents from different branches\n",
    "\n",
    "# Input converters for each file type\n",
    "pdf_converter = PyPDFToDocument()\n",
    "html_converter = HTMLToDocument()  \n",
    "link_fetcher = LinkContentFetcher()\n",
    "\n",
    "# Shared processing components\n",
    "doc_cleaner = DocumentCleaner(\n",
    "    remove_empty_lines=True, \n",
    "    remove_extra_whitespaces=True\n",
    ")\n",
    "doc_splitter = DocumentSplitter(split_by=\"sentence\", split_length=50, split_overlap=5)\n",
    "doc_converter = DocumentToLangChainConverter()\n",
    "kg_generator = KnowledgeGraphGenerator(apply_transforms=True)\n",
    "test_generator = SyntheticTestGenerator(\n",
    "    testset_size=15,  # Larger test set for multiple sources\n",
    "    llm_model=\"gpt-4o-mini\",\n",
    "    query_distribution=[\n",
    "        (\"single_hop\", 0.3),\n",
    "        (\"multi_hop_specific\", 0.3), \n",
    "        (\"multi_hop_abstract\", 0.4)\n",
    "    ]\n",
    ")\n",
    "test_saver = TestDatasetSaver(\"data_for_eval/synthetic_tests_advanced_branching.csv\")\n",
    "\n",
    "# Add all components to pipeline\n",
    "pipeline.add_component(\"file_router\", file_router)\n",
    "pipeline.add_component(\"link_fetcher\", link_fetcher)\n",
    "pipeline.add_component(\"pdf_converter\", pdf_converter) \n",
    "pipeline.add_component(\"html_converter\", html_converter)\n",
    "pipeline.add_component(\"doc_joiner\", doc_joiner)\n",
    "pipeline.add_component(\"doc_cleaner\", doc_cleaner)\n",
    "pipeline.add_component(\"doc_splitter\", doc_splitter)\n",
    "pipeline.add_component(\"doc_converter\", doc_converter)\n",
    "pipeline.add_component(\"kg_generator\", kg_generator)\n",
    "pipeline.add_component(\"test_generator\", test_generator)\n",
    "pipeline.add_component(\"test_saver\", test_saver)\n",
    "\n",
    "# Connect file routing branches\n",
    "pipeline.connect(\"file_router.application/pdf\", \"pdf_converter.sources\") \n",
    "pipeline.connect(\"link_fetcher.streams\", \"html_converter.sources\")\n",
    "\n",
    "# Connect converters to joiner\n",
    "pipeline.connect(\"pdf_converter.documents\", \"doc_joiner.documents\")\n",
    "pipeline.connect(\"html_converter.documents\", \"doc_joiner.documents\")\n",
    "\n",
    "# Connect main processing path\n",
    "pipeline.connect(\"doc_joiner.documents\", \"doc_cleaner.documents\")\n",
    "pipeline.connect(\"doc_cleaner.documents\", \"doc_splitter.documents\")\n",
    "pipeline.connect(\"doc_splitter.documents\", \"doc_converter.documents\")\n",
    "pipeline.connect(\"doc_converter.langchain_documents\", \"kg_generator.documents\")\n",
    "pipeline.connect(\"kg_generator.knowledge_graph\", \"test_generator.knowledge_graph\")\n",
    "pipeline.connect(\"doc_converter.langchain_documents\", \"test_generator.documents\")\n",
    "pipeline.connect(\"test_generator.testset\", \"test_saver.testset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee4b0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying HeadlinesExtractor: 100%|██████████| 19/19 [00:10<00:00,  1.86it/s]\n",
      "Applying HeadlineSplitter: 100%|██████████| 20/20 [00:00<00:00, 526.48it/s]\n",
      "Applying SummaryExtractor: 100%|██████████| 19/19 [00:12<00:00,  1.51it/s]\n",
      "Applying CustomNodeFilter:   0%|          | 0/51 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Define inputs\n",
    "pdf_file = Path(\"./data_for_indexing/howpeopleuseai.pdf\")\n",
    "web_url = \"https://www.tableau.com/data-insights/ai/examples\"\n",
    "\n",
    "# Run pipeline with both input types\n",
    "result = pipeline.run({\n",
    "    \"file_router\": {\"sources\": [pdf_file]},  # PDF input through FileTypeRouter\n",
    "    \"link_fetcher\": {\"urls\": [web_url]}      # Web input through LinkContentFetcher\n",
    "})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775608a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📸 Pipeline diagram saved to: ./images/advanced_branching_kg_pipeline.png\n"
     ]
    }
   ],
   "source": [
    "# Visualize the advanced branching pipeline architecture\n",
    "pipeline.draw(path=\"./images/advanced_branching_kg_pipeline.png\")\n",
    "print(\"📸 Pipeline diagram saved to: ./images/advanced_branching_kg_pipeline.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04bb1e0",
   "metadata": {},
   "source": [
    "![Advanced Branching Pipeline](./images/advanced_branching_kg_pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a81ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What AI-powered service does Samsung provide?</td>\n",
       "      <td>['You may or may not be aware of how pervasive...</td>\n",
       "      <td>Samsung provides Bixby, which is a digital ass...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How YouTube use AI for keep people engaged?</td>\n",
       "      <td>['Social media\\nSocial media platforms are ano...</td>\n",
       "      <td>YouTube, like other social media platforms, us...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does the Mars rover Perseverance contribut...</td>\n",
       "      <td>['Some examples of industrial robots include:\\...</td>\n",
       "      <td>The Mars rover Perseverance is programmed to g...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does AI enhance analytics in business deci...</td>\n",
       "      <td>['Fraud prevention\\nIf you have an account wit...</td>\n",
       "      <td>AI enhances analytics in business decision-mak...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is OpenAI's role in the development of Ch...</td>\n",
       "      <td>['NBER WORKING PAPER SERIES\\nHOW PEOPLE USE CH...</td>\n",
       "      <td>OpenAI is involved in the development of ChatG...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0      What AI-powered service does Samsung provide?   \n",
       "1        How YouTube use AI for keep people engaged?   \n",
       "2  How does the Mars rover Perseverance contribut...   \n",
       "3  How does AI enhance analytics in business deci...   \n",
       "4  What is OpenAI's role in the development of Ch...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  ['You may or may not be aware of how pervasive...   \n",
       "1  ['Social media\\nSocial media platforms are ano...   \n",
       "2  ['Some examples of industrial robots include:\\...   \n",
       "3  ['Fraud prevention\\nIf you have an account wit...   \n",
       "4  ['NBER WORKING PAPER SERIES\\nHOW PEOPLE USE CH...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  Samsung provides Bixby, which is a digital ass...   \n",
       "1  YouTube, like other social media platforms, us...   \n",
       "2  The Mars rover Perseverance is programmed to g...   \n",
       "3  AI enhances analytics in business decision-mak...   \n",
       "4  OpenAI is involved in the development of ChatG...   \n",
       "\n",
       "                        synthesizer_name  \n",
       "0  single_hop_specific_query_synthesizer  \n",
       "1  single_hop_specific_query_synthesizer  \n",
       "2  single_hop_specific_query_synthesizer  \n",
       "3  single_hop_specific_query_synthesizer  \n",
       "4  single_hop_specific_query_synthesizer  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>How does ChatGPT usage relate to fraud prevent...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nPanel C2.Technical Help. Panel C3...</td>\n",
       "      <td>ChatGPT usage is broadly focused on seeking in...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>How does the automated classification of messa...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nThe left column\\nshows a standalo...</td>\n",
       "      <td>The automated classification of messages ensur...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>How does ChatGPT usage for decision-making var...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nPanel C2.Technical Help. Panel C3...</td>\n",
       "      <td>ChatGPT usage for decision-making is notably c...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>What are the main writing sub-categories ident...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nFor example, the five sub-categor...</td>\n",
       "      <td>The main writing sub-categories identified in ...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>How does message classification relate to user...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nThe left column\\nshows a standalo...</td>\n",
       "      <td>Message classification is performed using auto...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           user_input  \\\n",
       "11  How does ChatGPT usage relate to fraud prevent...   \n",
       "12  How does the automated classification of messa...   \n",
       "13  How does ChatGPT usage for decision-making var...   \n",
       "14  What are the main writing sub-categories ident...   \n",
       "15  How does message classification relate to user...   \n",
       "\n",
       "                                   reference_contexts  \\\n",
       "11  ['<1-hop>\\n\\nPanel C2.Technical Help. Panel C3...   \n",
       "12  ['<1-hop>\\n\\nThe left column\\nshows a standalo...   \n",
       "13  ['<1-hop>\\n\\nPanel C2.Technical Help. Panel C3...   \n",
       "14  ['<1-hop>\\n\\nFor example, the five sub-categor...   \n",
       "15  ['<1-hop>\\n\\nThe left column\\nshows a standalo...   \n",
       "\n",
       "                                            reference  \\\n",
       "11  ChatGPT usage is broadly focused on seeking in...   \n",
       "12  The automated classification of messages ensur...   \n",
       "13  ChatGPT usage for decision-making is notably c...   \n",
       "14  The main writing sub-categories identified in ...   \n",
       "15  Message classification is performed using auto...   \n",
       "\n",
       "                        synthesizer_name  \n",
       "11  multi_hop_abstract_query_synthesizer  \n",
       "12  multi_hop_abstract_query_synthesizer  \n",
       "13  multi_hop_abstract_query_synthesizer  \n",
       "14  multi_hop_abstract_query_synthesizer  \n",
       "15  multi_hop_abstract_query_synthesizer  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'synthetic_tests_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m display(advanced_tests_df.head())\n\u001b[32m     10\u001b[39m display(advanced_tests_df.tail())\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m display(\u001b[43msynthetic_tests_df\u001b[49m.head())\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLast 5 rows:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m display(synthetic_tests_df.tail())\n",
      "\u001b[31mNameError\u001b[39m: name 'synthetic_tests_df' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load and analyze results from the advanced branching pipeline\n",
    "advanced_test_file = \"data_for_eval/synthetic_tests_advanced_branching.csv\"\n",
    "\n",
    "if os.path.exists(advanced_test_file):\n",
    "    advanced_tests_df = pd.read_csv(advanced_test_file)\n",
    "\n",
    "    display(advanced_tests_df.head())\n",
    "    display(advanced_tests_df.tail())\n",
    "    display(synthetic_tests_df.head())\n",
    "    print(\"Last 5 rows:\")\n",
    "    display(synthetic_tests_df.tail())\n",
    "else:\n",
    "    print(\"❌ Synthetic test file not found\")\n",
    "    print(\"Please run the previous cells to generate the test data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d8b91f",
   "metadata": {},
   "source": [
    "## Summary and Architecture Analysis\n",
    "\n",
    "### What We've Accomplished\n",
    "\n",
    "In this notebook, we've built increasingly sophisticated branching pipelines:\n",
    "\n",
    "1. **Basic Branching Pipeline**: PDF + Web content processing  \n",
    "2. **Production-Ready Pipeline**: Enhanced error handling and monitoring\n",
    "\n",
    "### Key Architectural Benefits\n",
    "\n",
    "1. **Modularity**: Each component has a single responsibility and can be reused\n",
    "2. **Flexibility**: Easy to add new input types (CSV, Word docs, etc.) \n",
    "3. **Scalability**: DocumentJoiner allows processing multiple sources simultaneously\n",
    "4. **Consistency**: Same processing logic regardless of input source\n",
    "5. **Error Isolation**: Problems with one input source don't affect others\n",
    "\n",
    "### Production Considerations\n",
    "\n",
    "**Advantages of Branching Pipelines:**\n",
    "- **Unified Output**: Single knowledge graph and test dataset from multiple sources\n",
    "- **Rich Context**: Cross-referencing information between different document types\n",
    "- **Operational Efficiency**: One pipeline deployment handles multiple scenarios\n",
    "- **Quality Improvement**: More diverse training data leads to better synthetic questions\n",
    "\n",
    "**When to Use Branching Pipelines:**\n",
    "- Processing heterogeneous document collections\n",
    "- Building comprehensive knowledge bases from multiple sources\n",
    "- Creating robust test datasets that cover various content types\n",
    "- Implementing production pipelines that need input flexibility\n",
    "\n",
    "\n",
    "### Extension Patterns\n",
    "\n",
    "To add new input types:\n",
    "1. Add MIME type to `FileTypeRouter`\n",
    "2. Create appropriate converter component\n",
    "3. Connect converter to `DocumentJoiner`\n",
    "4. No changes needed to downstream processing!\n",
    "\n",
    "This modular approach makes the pipeline highly maintainable and extensible for future requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a4e993",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-with-haystack-ch5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
