{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75f744c9",
   "metadata": {},
   "source": [
    "üîß **Setup Required**: Before running this notebook, please follow the [setup instructions](../README.md#setup-instructions) to configure your environment and API keys. **You will need to ensure you've executed the Indexing pipeline before completing this exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ad4fc5",
   "metadata": {},
   "source": [
    "# RAG Evaluation Pipeline: Custom Components Approach\n",
    "\n",
    "## üìã Overview\n",
    "\n",
    "This notebook demonstrates how to create a **reproducible evaluation workflow** for RAG (Retrieval-Augmented Generation) systems using **Haystack custom components**. Instead of manually evaluating RAG systems, we'll build a pipeline that can:\n",
    "\n",
    "1. **Load evaluation datasets** from CSV files\n",
    "2. **Process queries** through different RAG SuperComponents \n",
    "3. **Generate comprehensive metrics** using the RAGAS framework\n",
    "4. **Compare multiple RAG configurations** systematically\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand how to:\n",
    "- Design modular evaluation components for RAG systems\n",
    "- Create reusable pipelines for systematic RAG assessment \n",
    "- Switch between different RAG SuperComponents for comparative evaluation\n",
    "- Interpret RAGAS metrics in the context of pipeline performance\n",
    "\n",
    "## üîß Architecture Overview\n",
    "\n",
    "Our evaluation pipeline consists of three main components:\n",
    "\n",
    "```\n",
    "CSV Data ‚Üí RAGDataAugmenter ‚Üí RagasEvaluation ‚Üí Metrics & Results\n",
    "    ‚Üë              ‚Üë                 ‚Üë\n",
    "CSVReader    SuperComponent    RAGAS Framework\n",
    "```\n",
    "\n",
    "**Key Benefits:**\n",
    "- **Modularity**: Each component has a single responsibility\n",
    "- **Reusability**: Swap RAG systems without changing evaluation logic  \n",
    "- **Scalability**: Process multiple datasets and configurations systematically\n",
    "- **Reproducibility**: Consistent evaluation across different experiments\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4640ae5",
   "metadata": {},
   "source": [
    "## Component 1: CSV Data Loader üìä\n",
    "\n",
    "The **CSVReaderComponent** serves as the entry point for our evaluation pipeline. It handles loading synthetic evaluation datasets and ensures data quality before processing.\n",
    "\n",
    "**Key Features:**\n",
    "- **Robust Error Handling**: Validates file existence and data integrity\n",
    "- **Pandas Integration**: Returns data as DataFrame for easy manipulation\n",
    "- **Pipeline Compatible**: Designed to work seamlessly with Haystack pipelines\n",
    "\n",
    "**Input:** File path to CSV containing evaluation queries and ground truth\n",
    "**Output:** Pandas DataFrame ready for RAG processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08a83513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from haystack import component, Pipeline\n",
    "from typing import List, Optional, Dict, Any, Union\n",
    "\n",
    "@component\n",
    "class CSVReaderComponent:\n",
    "    \"\"\"Reads a CSV file into a Pandas DataFrame.\"\"\"\n",
    "\n",
    "    @component.output_types(data_frame=pd.DataFrame)\n",
    "    def run(self, source: Union[str, Path]):\n",
    "        \"\"\"\n",
    "        Reads the CSV file from the first source in the list.\n",
    "        \n",
    "        Args:\n",
    "            sources: List of file paths to CSV files. Only the first file will be processed.\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary containing the loaded DataFrame under 'data_frame' key.\n",
    "            \n",
    "        Raises:\n",
    "            FileNotFoundError: If the file doesn't exist or can't be read.\n",
    "            ValueError: If the DataFrame is empty after loading.\n",
    "        \"\"\"\n",
    "        if not source:\n",
    "            raise ValueError(\"No sources provided\")\n",
    "            \n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(source)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"File not found at {source}\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error reading CSV file {source}: {str(e)}\")\n",
    "\n",
    "        # Check if DataFrame is empty using proper pandas method\n",
    "        if df.empty:\n",
    "            raise ValueError(f\"DataFrame is empty after loading from {source}\")\n",
    "\n",
    "        print(f\"Loaded DataFrame with {len(df)} rows from {source}.\")\n",
    "        return {\"data_frame\": df}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1861dfb2",
   "metadata": {},
   "source": [
    "## Component 2: RAG Data Augmentation üîÑ\n",
    "\n",
    "The **RAGDataAugmenterComponent** is the core of our evaluation workflow. It takes each query from our evaluation dataset and processes it through a RAG SuperComponent, collecting both the generated responses and retrieved contexts.\n",
    "\n",
    "**üîë Key Design Decisions:**\n",
    "\n",
    "1. **SuperComponent Flexibility**: Accepts any pre-configured RAG SuperComponent (Naive, Hybrid, or custom)\n",
    "2. **Batch Processing**: Efficiently processes entire evaluation datasets\n",
    "3. **Data Augmentation**: Enriches the original dataset with RAG outputs for evaluation\n",
    "4. **Context Extraction**: Captures retrieved documents for context-based metrics\n",
    "\n",
    "**Pipeline Integration:**\n",
    "- **Input**: DataFrame with queries from CSVReaderComponent  \n",
    "- **Process**: Runs each query through the specified RAG SuperComponent\n",
    "- **Output**: Augmented DataFrame with responses and retrieved contexts\n",
    "\n",
    "**üí° Why This Approach?**\n",
    "By separating RAG execution from evaluation, we can:\n",
    "- **Swap RAG systems** without changing evaluation logic\n",
    "- **Cache RAG results** for multiple evaluation runs  \n",
    "- **Debug RAG performance** independently of metrics calculation\n",
    "- **Scale evaluation** across different datasets and configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72bf8b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import SuperComponent\n",
    "\n",
    "@component\n",
    "class RAGDataAugmenterComponent:\n",
    "    \"\"\"\n",
    "    Applies a RAG SuperComponent to each query in a DataFrame and \n",
    "    augments the data with the generated answer and retrieved contexts.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rag_supercomponent: SuperComponent):\n",
    "        # We store the pre-initialized SuperComponent\n",
    "        self.rag_supercomponent = rag_supercomponent\n",
    "        self.output_names = [\"augmented_data_frame\"]\n",
    "\n",
    "    @component.output_types(augmented_data_frame=pd.DataFrame)\n",
    "    def run(self, data_frame: pd.DataFrame):\n",
    "        \n",
    "        # New columns to store RAG results\n",
    "        answers: List[str] = []\n",
    "        contexts: List[List[str]] = []\n",
    "\n",
    "        print(f\"Running RAG SuperComponent on {len(data_frame)} queries...\")\n",
    "\n",
    "        # Iterate through the queries (user_input column)\n",
    "        for _, row in data_frame.iterrows():\n",
    "            query = row[\"user_input\"]\n",
    "            \n",
    "            # 1. Run the RAG SuperComponent\n",
    "            # It expects 'query' as input and returns a dictionary.\n",
    "            rag_output = self.rag_supercomponent.run(query=query)\n",
    "            \n",
    "            # 2. Extract answer and contexts\n",
    "            # Based on the naive_rag_sc/hybrid_rag_sc structure:\n",
    "            answer = rag_output.get('replies', [''])[0]\n",
    "            \n",
    "            # Extract content from the Document objects\n",
    "            retrieved_docs = rag_output.get('documents', [])\n",
    "            retrieved_contexts = [doc.content for doc in retrieved_docs]\n",
    "            \n",
    "            answers.append(answer)\n",
    "            contexts.append(retrieved_contexts)\n",
    "        \n",
    "        # 3. Augment the DataFrame\n",
    "        data_frame['response'] = answers\n",
    "        data_frame['retrieved_contexts'] = contexts\n",
    "        \n",
    "        print(\"RAG processing complete.\")\n",
    "        return {\"augmented_data_frame\": data_frame}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272c9cff",
   "metadata": {},
   "source": [
    "## Component 3: RAGAS Evaluation Engine üìà\n",
    "\n",
    "The **RagasEvaluationComponent** integrates the RAGAS framework into our Haystack pipeline, providing comprehensive evaluation metrics for RAG systems.\n",
    "\n",
    "**üéØ Evaluation Metrics Included:**\n",
    "\n",
    "| Metric | Purpose | What It Measures |\n",
    "|--------|---------|------------------|\n",
    "| **LLMContextRecall** | Retrieval Quality | How well retrieval captures relevant information |\n",
    "| **Faithfulness** | Response Quality | Factual consistency with retrieved context |\n",
    "| **FactualCorrectness** | Accuracy | Correctness of factual claims in responses |\n",
    "| **ResponseRelevancy** | Relevance | How well responses answer the questions |\n",
    "| **ContextEntityRecall** | Entity Coverage | Retrieval of important entities (people, places, dates) |\n",
    "| **NoiseSensitivity** | Robustness | System performance with irrelevant context |\n",
    "\n",
    "**üîß Technical Implementation:**\n",
    "- **Configurable Metrics**: Choose which RAGAS metrics to compute\n",
    "- **LLM Integration**: Uses OpenAI GPT models for evaluation judgments  \n",
    "- **Data Format Handling**: Automatically formats data for RAGAS requirements\n",
    "- **Comprehensive Output**: Returns both aggregated metrics and detailed per-query results\n",
    "\n",
    "**üí° Design Philosophy:**\n",
    "This component abstracts away the complexity of RAGAS integration, allowing you to focus on comparing RAG system performance rather than wrestling with evaluation setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5b842a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurafunderburk/Documents/GitHub/Building-Natural-Language-Pipelines/ch6/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from ragas import EvaluationDataset, evaluate\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, ResponseRelevancy, ContextEntityRecall, NoiseSensitivity\n",
    "\n",
    "from ragas.llms import llm_factory\n",
    "from haystack.utils import Secret\n",
    "import os\n",
    "from ragas.llms import HaystackLLMWrapper\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "\n",
    "# Note: Ensure ragas and its dependencies (like litellm or openai) are installed\n",
    "@component\n",
    "class RagasEvaluationComponent:\n",
    "    \"\"\"\n",
    "    Prepares data for Ragas, runs the evaluation, and returns the metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 metrics: Optional[List[Any]] = None,\n",
    "                 ragas_llm: Optional[Any] = None):\n",
    "        \n",
    "        # Default metrics for RAG evaluation\n",
    "        self.metrics = metrics\n",
    "        \n",
    "        # Ragas requires an LLM for evaluation, often provided through OpenAI or Anthropic.\n",
    "        # It's best practice to use a strong model like gpt-4o-mini or gpt-4.\n",
    "        if ragas_llm is None:\n",
    "            # Assumes OPENAI_API_KEY is set in the environment\n",
    "            self.ragas_llm = HaystackLLMWrapper(OpenAIGenerator(model=\"gpt-4o-mini\",\n",
    "                                                               api_key=Secret.from_env_var(\"OPENAI_API_KEY\")))\n",
    "        else:\n",
    "            self.ragas_llm = ragas_llm\n",
    "\n",
    "    @component.output_types(metrics=Dict[str, float], evaluation_df=pd.DataFrame)\n",
    "    def run(self, augmented_data_frame: pd.DataFrame):\n",
    "        \n",
    "        # 1. Map columns to Ragas requirements - correct column mapping for SingleTurnSample\n",
    "        ragas_data = pd.DataFrame({\n",
    "            'user_input': augmented_data_frame['user_input'],\n",
    "            'response': augmented_data_frame['response'], \n",
    "            'retrieved_contexts': augmented_data_frame['retrieved_contexts'],\n",
    "            'reference': augmented_data_frame['reference'],\n",
    "            'reference_contexts': augmented_data_frame['reference_contexts'].apply(eval)\n",
    "        })\n",
    "\n",
    "        print(\"Creating Ragas EvaluationDataset...\")\n",
    "        # 2. Create EvaluationDataset using from_pandas which handles the format correctly\n",
    "        dataset = EvaluationDataset.from_pandas(ragas_data)\n",
    "\n",
    "        print(\"Starting Ragas evaluation...\")\n",
    "        \n",
    "        # 3. Run Ragas Evaluation\n",
    "        # Pass the configured LLM to Ragas\n",
    "        results = evaluate(\n",
    "            dataset=dataset,\n",
    "            metrics=self.metrics,\n",
    "            llm=self.ragas_llm\n",
    "        )\n",
    "        \n",
    "\n",
    "        results_df = results.to_pandas()\n",
    "        \n",
    "        print(\"Ragas evaluation complete.\")\n",
    "        print(f\"Overall metrics: {results}\")\n",
    "        \n",
    "        return {\"metrics\": results, \"evaluation_df\": results_df}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51104c10",
   "metadata": {},
   "source": [
    "## Experiment 1: Naive RAG Evaluation üî¨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b0aa01",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üß™ Experimental Setup: Systematic RAG Evaluation\n",
    "\n",
    "Now we'll put our custom components together to create a **reproducible evaluation workflow**. This section demonstrates how to systematically evaluate different RAG SuperComponents using the same evaluation pipeline.\n",
    "\n",
    "## üéØ Evaluation Strategy\n",
    "\n",
    "Our approach enables **systematic comparison** of RAG systems:\n",
    "\n",
    "1. **Consistent Evaluation**: Same metrics and datasets across all RAG variants\n",
    "2. **Modular Design**: Easy to swap between Naive RAG, Hybrid RAG, or custom implementations  \n",
    "3. **Reproducible Results**: Pipeline ensures identical evaluation conditions\n",
    "4. **Scalable Assessment**: Process multiple datasets with different complexity levels\n",
    "\n",
    "## üìä Dataset Information\n",
    "\n",
    "We'll use synthetic evaluation datasets with varying complexity:\n",
    "- **`synthetic_tests_advanced_branching_3.csv`**: Focused dataset with 3 test cases\n",
    "- **`synthetic_tests_advanced_branching_10.csv`**: Medium dataset with 10 test cases  \n",
    "- **`synthetic_tests_advanced_branching_50.csv`**: Large dataset with 50 test cases\n",
    "\n",
    "**Dataset Structure:**\n",
    "- `user_input`: Questions to ask the RAG system\n",
    "- `reference`: Ground truth answers for comparison\n",
    "- `reference_contexts`: Expected retrieved contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0384e71c",
   "metadata": {},
   "source": [
    "### Pipeline Configuration for Naive RAG\n",
    "\n",
    "Here we configure our evaluation pipeline for the **Naive RAG SuperComponent**. This demonstrates the **modular design principle**: we can easily swap different RAG implementations while keeping the evaluation logic identical.\n",
    "\n",
    "**üîß Configuration Steps:**\n",
    "1. **Select RAG SuperComponent**: Choose `naive_rag_sc` for this experiment\n",
    "2. **Configure RAGAS Metrics**: Set up comprehensive evaluation criteria\n",
    "3. **Instantiate Components**: Create our three pipeline components\n",
    "4. **Connect Pipeline**: Define the data flow between components\n",
    "\n",
    "**üí° Key Insight:** Notice how the `rag_sc_to_test` variable makes it trivial to switch between different RAG implementations. This is the power of modular pipeline design!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a23faff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x30ee1d520>\n",
       "üöÖ Components\n",
       "  - reader: CSVReaderComponent\n",
       "  - augmenter: RAGDataAugmenterComponent\n",
       "  - evaluator: RagasEvaluationComponent\n",
       "üõ§Ô∏è Connections\n",
       "  - reader.data_frame -> augmenter.data_frame (DataFrame)\n",
       "  - augmenter.augmented_data_frame -> evaluator.augmented_data_frame (DataFrame)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Setup Environment & Dependencies ---\n",
    "# You need to ensure:\n",
    "# 1. Elasticsearch is running (as NaiveRAG/HybridRAG rely on it, see files).\n",
    "# 2. OPENAI_API_KEY is set in your environment.\n",
    "# 3. The document store has been indexed with your data.\n",
    "\n",
    "# --- 1. Import RAG SuperComponents ---\n",
    "# Assuming naiverag.py and hybridrag.py are in your environment\n",
    "from scripts.rag.naiverag import naive_rag_sc\n",
    "from scripts.rag.hybridrag import hybrid_rag_sc\n",
    "from pathlib import Path\n",
    "\n",
    "# --- 2. Define Configurations to Test ---\n",
    "\n",
    "# The RAG SuperComponent to test (change this to swap RAG configurations)\n",
    "rag_sc_to_test = naive_rag_sc # OR hybrid_rag_sc\n",
    "\n",
    "# If you want to test different internal configurations (e.g., chunk size, embedder model), \n",
    "# you should create and index new SuperComponents with those changes \n",
    "# and then choose the appropriate object here.\n",
    "\n",
    "# --- 3. Instantiate Custom Components ---\n",
    "\n",
    "metrics = [LLMContextRecall(), \\\n",
    "                Faithfulness(), \\\n",
    "                FactualCorrectness(), \\\n",
    "                ResponseRelevancy(), \\\n",
    "                ContextEntityRecall(), \\\n",
    "                NoiseSensitivity()]\n",
    "\n",
    "\n",
    "reader = CSVReaderComponent()\n",
    "augmenter = RAGDataAugmenterComponent(rag_supercomponent=rag_sc_to_test)\n",
    "evaluator = RagasEvaluationComponent(metrics=metrics)\n",
    "\n",
    "# --- 4. Build the Evaluation Pipeline ---\n",
    "\n",
    "evaluation_pipeline = Pipeline()\n",
    "\n",
    "evaluation_pipeline.add_component(\"reader\", reader)\n",
    "evaluation_pipeline.add_component(\"augmenter\", augmenter)\n",
    "evaluation_pipeline.add_component(\"evaluator\", evaluator)\n",
    "\n",
    "# Connect the flow: CSV -> Augment -> Evaluate\n",
    "evaluation_pipeline.connect(\"reader.data_frame\", \"augmenter.data_frame\")\n",
    "evaluation_pipeline.connect(\"augmenter.augmented_data_frame\", \"evaluator.augmented_data_frame\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08342b27",
   "metadata": {},
   "source": [
    "### Running the Evaluation Pipeline üöÄ\n",
    "\n",
    "Now we execute our configured pipeline on the evaluation dataset. The pipeline will:\n",
    "\n",
    "1. **Load Data**: Read the CSV file containing evaluation queries\n",
    "2. **Process Queries**: Run each query through the Naive RAG SuperComponent  \n",
    "3. **Generate Metrics**: Calculate RAGAS evaluation scores\n",
    "4. **Return Results**: Provide both detailed and summary metrics\n",
    "\n",
    "**üîç What to Observe:**\n",
    "- Processing time for the dataset\n",
    "- Console output showing pipeline progress  \n",
    "- Any errors or warnings during evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a9d17a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation of SuperComponent...\n",
      "Loaded DataFrame with 10 rows from data_for_eval/synthetic_tests_advanced_branching_10.csv.\n",
      "Running RAG SuperComponent on 10 queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  7.66it/s]\n",
      "\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  7.22it/s]\n",
      "\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.48it/s]\n",
      "\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 18.44it/s]\n",
      "\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  9.99it/s]\n",
      "\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.16it/s]\n",
      "\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 15.68it/s]\n",
      "\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 16.61it/s]\n",
      "\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  9.09it/s]\n",
      "\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.75it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG processing complete.\n",
      "Creating Ragas EvaluationDataset...\n",
      "Starting Ragas evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  10%|‚ñà         | 6/60 [00:08<01:03,  1.17s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  22%|‚ñà‚ñà‚ñè       | 13/60 [00:14<00:36,  1.28it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  32%|‚ñà‚ñà‚ñà‚ñè      | 19/60 [00:20<00:38,  1.08it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  32%|‚ñà‚ñà‚ñà‚ñè      | 19/60 [00:20<00:38,  1.08it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 25/60 [00:24<00:21,  1.63it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 32/60 [00:35<00:31,  1.11s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 36/60 [00:45<00:55,  2.31s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 43/60 [00:54<00:22,  1.31s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 43/60 [00:54<00:22,  1.31s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [03:22<00:00,  3.38s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ragas evaluation complete.\n",
      "Overall metrics: {'context_recall': 0.9667, 'faithfulness': 0.6410, 'factual_correctness(mode=f1)': 0.5170, 'answer_relevancy': 0.5775, 'context_entity_recall': 0.1532, 'noise_sensitivity(mode=relevant)': 0.1972}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 5. Run the Evaluation Pipeline ---\n",
    "csv_file_path = \"data_for_eval/synthetic_tests_advanced_branching_10.csv\"\n",
    "print(f\"Starting evaluation of {rag_sc_to_test.__class__.__name__}...\")\n",
    "\n",
    "results = evaluation_pipeline.run({\"reader\": {\"source\": csv_file_path}})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b965d00",
   "metadata": {},
   "source": [
    "### Analyzing Naive RAG Results üìä\n",
    "\n",
    "Let's examine the detailed evaluation results from our Naive RAG system. The pipeline returns two key outputs:\n",
    "\n",
    "1. **Detailed DataFrame**: Per-query metrics showing individual performance  \n",
    "2. **Summary Metrics**: Aggregated scores across all evaluation queries\n",
    "\n",
    "**üìà Result Interpretation Guide:**\n",
    "- **High scores (>0.8)**: Excellent performance on this metric\n",
    "- **Medium scores (0.5-0.8)**: Good performance with room for improvement  \n",
    "- **Low scores (<0.5)**: Area needing significant attention\n",
    "\n",
    "**üîç What to Look For:**\n",
    "- Which metrics show the strongest performance?\n",
    "- Are there specific queries where the system struggles?  \n",
    "- What patterns emerge in the retrieved contexts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c9ce4eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>factual_correctness(mode=f1)</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_entity_recall</th>\n",
       "      <th>noise_sensitivity(mode=relevant)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the ethical implications and concerns...</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>The ethical implications and concerns surround...</td>\n",
       "      <td>The rise of Meta AI, like other generative AI ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the estimated energy consumption of th...</td>\n",
       "      <td>[NBER WORKING PAPER SERIES\\nHOW PEOPLE USE CHA...</td>\n",
       "      <td>[How does AI effect the environment?\\nIt is no...</td>\n",
       "      <td>According to the information provided, some re...</td>\n",
       "      <td>Some researchers estimate that the AI industry...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wut is the significanse of Artificial Intellig...</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>[This article was published in 2018. To read m...</td>\n",
       "      <td>Artificial intelligence (AI) plays a significa...</td>\n",
       "      <td>Artificial Intelligence (AI) is a technology t...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.946639</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.615385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What does Figure 22 illustrate about the varia...</td>\n",
       "      <td>[‚Ä¢Sampled from all ChatGPT users:a random samp...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n37% of messages are work-related\\n...</td>\n",
       "      <td>I don't have enough information to answer.</td>\n",
       "      <td>Figure 22 illustrates the variation in ChatGPT...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What does Figure 22 show about how ChatGPT is ...</td>\n",
       "      <td>[‚Ä¢Sampled from all ChatGPT users:a random samp...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nPanel A.Work Related\\n Panel B1.As...</td>\n",
       "      <td>I don't have enough information to answer.</td>\n",
       "      <td>Figure 22 illustrates the classification of wo...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How does ChatGPT Business usage vary by occupa...</td>\n",
       "      <td>[‚Ä¢Sampled from all ChatGPT users:a random samp...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nCorporate users may also use ChatG...</td>\n",
       "      <td>I don't have enough information to answer.</td>\n",
       "      <td>ChatGPT Business usage varies significantly by...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How does the environmental impact of artificia...</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nWhat is AI, how does it work and w...</td>\n",
       "      <td>The environmental impact of artificial intelli...</td>\n",
       "      <td>The environmental impact of artificial intelli...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.963276</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How do privacy protections and de-identificati...</td>\n",
       "      <td>[‚Ä¢Sampled from all ChatGPT users:a random samp...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nWe describe the contents of each d...</td>\n",
       "      <td>The analysis of ChatGPT user messages incorpor...</td>\n",
       "      <td>Privacy protections in the analysis of ChatGPT...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.952696</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What trends can be observed in user cohort ana...</td>\n",
       "      <td>[‚Ä¢Sampled from all ChatGPT users:a random samp...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nThe yellow line represents the fir...</td>\n",
       "      <td>In the user cohort analysis regarding ChatGPT ...</td>\n",
       "      <td>User cohort analysis reveals that there has be...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.958646</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What are the environmental concerns related to...</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nWhat is AI, how does it work and w...</td>\n",
       "      <td>The environmental concerns related to artifici...</td>\n",
       "      <td>The environmental concerns related to artifici...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.953338</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  What are the ethical implications and concerns...   \n",
       "1  What is the estimated energy consumption of th...   \n",
       "2  Wut is the significanse of Artificial Intellig...   \n",
       "3  What does Figure 22 illustrate about the varia...   \n",
       "4  What does Figure 22 show about how ChatGPT is ...   \n",
       "5  How does ChatGPT Business usage vary by occupa...   \n",
       "6  How does the environmental impact of artificia...   \n",
       "7  How do privacy protections and de-identificati...   \n",
       "8  What trends can be observed in user cohort ana...   \n",
       "9  What are the environmental concerns related to...   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [What is AI, how does it work and why are some...   \n",
       "1  [NBER WORKING PAPER SERIES\\nHOW PEOPLE USE CHA...   \n",
       "2  [What is AI, how does it work and why are some...   \n",
       "3  [‚Ä¢Sampled from all ChatGPT users:a random samp...   \n",
       "4  [‚Ä¢Sampled from all ChatGPT users:a random samp...   \n",
       "5  [‚Ä¢Sampled from all ChatGPT users:a random samp...   \n",
       "6  [What is AI, how does it work and why are some...   \n",
       "7  [‚Ä¢Sampled from all ChatGPT users:a random samp...   \n",
       "8  [‚Ä¢Sampled from all ChatGPT users:a random samp...   \n",
       "9  [What is AI, how does it work and why are some...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  [What is AI, how does it work and why are some...   \n",
       "1  [How does AI effect the environment?\\nIt is no...   \n",
       "2  [This article was published in 2018. To read m...   \n",
       "3  [<1-hop>\\n\\n37% of messages are work-related\\n...   \n",
       "4  [<1-hop>\\n\\nPanel A.Work Related\\n Panel B1.As...   \n",
       "5  [<1-hop>\\n\\nCorporate users may also use ChatG...   \n",
       "6  [<1-hop>\\n\\nWhat is AI, how does it work and w...   \n",
       "7  [<1-hop>\\n\\nWe describe the contents of each d...   \n",
       "8  [<1-hop>\\n\\nThe yellow line represents the fir...   \n",
       "9  [<1-hop>\\n\\nWhat is AI, how does it work and w...   \n",
       "\n",
       "                                            response  \\\n",
       "0  The ethical implications and concerns surround...   \n",
       "1  According to the information provided, some re...   \n",
       "2  Artificial intelligence (AI) plays a significa...   \n",
       "3         I don't have enough information to answer.   \n",
       "4         I don't have enough information to answer.   \n",
       "5         I don't have enough information to answer.   \n",
       "6  The environmental impact of artificial intelli...   \n",
       "7  The analysis of ChatGPT user messages incorpor...   \n",
       "8  In the user cohort analysis regarding ChatGPT ...   \n",
       "9  The environmental concerns related to artifici...   \n",
       "\n",
       "                                           reference  context_recall  \\\n",
       "0  The rise of Meta AI, like other generative AI ...        1.000000   \n",
       "1  Some researchers estimate that the AI industry...        1.000000   \n",
       "2  Artificial Intelligence (AI) is a technology t...        1.000000   \n",
       "3  Figure 22 illustrates the variation in ChatGPT...        0.666667   \n",
       "4  Figure 22 illustrates the classification of wo...        1.000000   \n",
       "5  ChatGPT Business usage varies significantly by...        1.000000   \n",
       "6  The environmental impact of artificial intelli...        1.000000   \n",
       "7  Privacy protections in the analysis of ChatGPT...        1.000000   \n",
       "8  User cohort analysis reveals that there has be...        1.000000   \n",
       "9  The environmental concerns related to artifici...        1.000000   \n",
       "\n",
       "   faithfulness  factual_correctness(mode=f1)  answer_relevancy  \\\n",
       "0      0.818182                          0.77          0.999999   \n",
       "1      1.000000                          1.00          0.000000   \n",
       "2      1.000000                          0.60          0.946639   \n",
       "3      0.000000                          0.00          0.000000   \n",
       "4      0.000000                          0.00          0.000000   \n",
       "5      0.000000                          0.00          0.000000   \n",
       "6      0.692308                          0.75          0.963276   \n",
       "7      1.000000                          0.52          0.952696   \n",
       "8      1.000000                          0.73          0.958646   \n",
       "9      0.900000                          0.80          0.953338   \n",
       "\n",
       "   context_entity_recall  noise_sensitivity(mode=relevant)  \n",
       "0               0.050000                          0.181818  \n",
       "1               0.000000                          0.333333  \n",
       "2               0.333333                          0.615385  \n",
       "3               0.200000                          0.000000  \n",
       "4               0.000000                          0.000000  \n",
       "5               0.100000                          0.000000  \n",
       "6               0.111111                          0.000000  \n",
       "7               0.333333                          0.541667  \n",
       "8               0.153846                          0.200000  \n",
       "9               0.250000                          0.100000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 6. Access Metrics ---\n",
    "final_metrics = results\n",
    "final_metrics['evaluator']['evaluation_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61bd01d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context_recall': 0.9667, 'faithfulness': 0.6410, 'factual_correctness(mode=f1)': 0.5170, 'answer_relevancy': 0.5775, 'context_entity_recall': 0.1532, 'noise_sensitivity(mode=relevant)': 0.1972}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_metrics['evaluator']['metrics']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8308cf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Experiment 2: Hybrid RAG Evaluation üî¨‚ö°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1db6e1",
   "metadata": {},
   "source": [
    "Now let's evaluate the **Hybrid RAG SuperComponent** using the exact same pipeline. This demonstrates the **power of modular evaluation**: we can systematically compare different RAG approaches with identical evaluation conditions.\n",
    "\n",
    "**üîÑ What Changes:**\n",
    "- **RAG SuperComponent**: Switch from `naive_rag_sc` to `hybrid_rag_sc`  \n",
    "- **Everything Else**: Identical pipeline, metrics, and dataset\n",
    "\n",
    "**üéØ Expected Improvements:**\n",
    "Hybrid RAG typically shows better performance due to:\n",
    "- **Dense + Sparse Retrieval**: Combines semantic and keyword-based search\n",
    "- **Enhanced Context Quality**: Better retrieval often leads to better responses\n",
    "- **Improved Robustness**: Multiple retrieval methods reduce failure modes\n",
    "\n",
    "**üìä Comparative Analysis:**\n",
    "After running both experiments, you'll be able to directly compare:\n",
    "- Which approach handles different query types better\n",
    "- Performance differences across RAGAS metrics  \n",
    "- Trade-offs between complexity and performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6b6eeb",
   "metadata": {},
   "source": [
    "### Configuring Pipeline for Hybrid RAG ‚öôÔ∏è\n",
    "\n",
    "Notice how **minimal** the configuration changes are! This showcases the elegance of our modular design:\n",
    "\n",
    "**üîÑ Single Line Change**: \n",
    "```python\n",
    "rag_sc_to_test = hybrid_rag_sc  # Previously: naive_rag_sc\n",
    "```\n",
    "\n",
    "**üèóÔ∏è Architecture Benefits:**\n",
    "- **Consistency**: Same evaluation methodology across all RAG variants\n",
    "- **Efficiency**: No need to rewrite evaluation logic  \n",
    "- **Reliability**: Eliminates configuration differences that could skew results\n",
    "- **Scalability**: Easy to add new RAG SuperComponents to comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c39dadf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x3d229c200>\n",
       "üöÖ Components\n",
       "  - reader: CSVReaderComponent\n",
       "  - augmenter: RAGDataAugmenterComponent\n",
       "  - evaluator: RagasEvaluationComponent\n",
       "üõ§Ô∏è Connections\n",
       "  - reader.data_frame -> augmenter.data_frame (DataFrame)\n",
       "  - augmenter.augmented_data_frame -> evaluator.augmented_data_frame (DataFrame)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_sc_to_test = hybrid_rag_sc\n",
    "metrics = [LLMContextRecall(), \\\n",
    "                Faithfulness(), \\\n",
    "                FactualCorrectness(), \\\n",
    "                ResponseRelevancy(), \\\n",
    "                ContextEntityRecall(), \\\n",
    "                NoiseSensitivity()]\n",
    "\n",
    "\n",
    "reader = CSVReaderComponent()\n",
    "augmenter = RAGDataAugmenterComponent(rag_supercomponent=rag_sc_to_test)\n",
    "evaluator = RagasEvaluationComponent(metrics=metrics)\n",
    "\n",
    "# --- 4. Build the Evaluation Pipeline ---\n",
    "\n",
    "evaluation_pipeline = Pipeline()\n",
    "\n",
    "evaluation_pipeline.add_component(\"reader\", reader)\n",
    "evaluation_pipeline.add_component(\"augmenter\", augmenter)\n",
    "evaluation_pipeline.add_component(\"evaluator\", evaluator)\n",
    "\n",
    "# Connect the flow: CSV -> Augment -> Evaluate\n",
    "evaluation_pipeline.connect(\"reader.data_frame\", \"augmenter.data_frame\")\n",
    "evaluation_pipeline.connect(\"augmenter.augmented_data_frame\", \"evaluator.augmented_data_frame\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60268ad4",
   "metadata": {},
   "source": [
    "### Executing Hybrid RAG Evaluation üöÄ\n",
    "\n",
    "Running the same evaluation pipeline with the Hybrid RAG SuperComponent. Compare the processing characteristics with the previous Naive RAG run:\n",
    "\n",
    "**üîç Observations to Make:**\n",
    "- **Processing Time**: May be longer due to multiple retrieval methods\n",
    "- **Console Output**: Look for differences in component execution\n",
    "- **Error Patterns**: Note any changes in system robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22c2531e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation of SuperComponent...\n",
      "Loaded DataFrame with 10 rows from data_for_eval/synthetic_tests_advanced_branching_10.csv.\n",
      "Running RAG SuperComponent on 10 queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 25.97it/s]\n",
      "\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 28.60it/s]\n",
      "\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 17.73it/s]\n",
      "\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.85it/s]\n",
      "\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 17.22it/s]\n",
      "\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 17.79it/s]\n",
      "\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 24.57it/s]\n",
      "\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 22.17it/s]\n",
      "\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 17.33it/s]\n",
      "\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  8.26it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG processing complete.\n",
      "Creating Ragas EvaluationDataset...\n",
      "Starting Ragas evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:   2%|‚ñè         | 1/60 [00:01<01:44,  1.77s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  12%|‚ñà‚ñè        | 7/60 [00:09<01:00,  1.14s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  12%|‚ñà‚ñè        | 7/60 [00:09<01:00,  1.14s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  22%|‚ñà‚ñà‚ñè       | 13/60 [00:14<00:49,  1.06s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  22%|‚ñà‚ñà‚ñè       | 13/60 [00:14<00:49,  1.06s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  35%|‚ñà‚ñà‚ñà‚ñå      | 21/60 [00:22<00:31,  1.24it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  40%|‚ñà‚ñà‚ñà‚ñà      | 24/60 [00:30<01:13,  2.05s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 32/60 [00:36<00:20,  1.39it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 37/60 [00:45<00:35,  1.54s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 42/60 [00:56<00:43,  2.42s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [03:11<00:00,  3.18s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ragas evaluation complete.\n",
      "Overall metrics: {'context_recall': 1.0000, 'faithfulness': 0.7900, 'factual_correctness(mode=f1)': 0.5730, 'answer_relevancy': 0.6663, 'context_entity_recall': 0.2503, 'noise_sensitivity(mode=relevant)': 0.2537}\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Run the Evaluation Pipeline ---\n",
    "csv_file_path = \"data_for_eval/synthetic_tests_advanced_branching_10.csv\"\n",
    "print(f\"Starting evaluation of {rag_sc_to_test.__class__.__name__}...\")\n",
    "\n",
    "results = evaluation_pipeline.run({\"reader\": {\"source\": csv_file_path}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a4b1d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "finally_metrics_hybrid = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff5f12ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>factual_correctness(mode=f1)</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_entity_recall</th>\n",
       "      <th>noise_sensitivity(mode=relevant)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the ethical implications and concerns...</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>I don't have enough information to answer.</td>\n",
       "      <td>The rise of Meta AI, like other generative AI ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the estimated energy consumption of th...</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>[How does AI effect the environment?\\nIt is no...</td>\n",
       "      <td>It is estimated that the AI industry as a whol...</td>\n",
       "      <td>Some researchers estimate that the AI industry...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wut is the significanse of Artificial Intellig...</td>\n",
       "      <td>[This article was published in 2018. To read m...</td>\n",
       "      <td>[This article was published in 2018. To read m...</td>\n",
       "      <td>Artificial Intelligence (AI) holds significant...</td>\n",
       "      <td>Artificial Intelligence (AI) is a technology t...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.933296</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.642857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What does Figure 22 illustrate about the varia...</td>\n",
       "      <td>[The prompts for each of these automated class...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n37% of messages are work-related\\n...</td>\n",
       "      <td>Figure 22 illustrates that there is a variatio...</td>\n",
       "      <td>Figure 22 illustrates the variation in ChatGPT...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.964758</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What does Figure 22 show about how ChatGPT is ...</td>\n",
       "      <td>[X‚Äôs indicate that the ranking is\\nunavailable...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nPanel A.Work Related\\n Panel B1.As...</td>\n",
       "      <td>Figure 22 presents data on how ChatGPT is used...</td>\n",
       "      <td>Figure 22 illustrates the classification of wo...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.959466</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How does ChatGPT Business usage vary by occupa...</td>\n",
       "      <td>[The prompts for each of these automated class...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nCorporate users may also use ChatG...</td>\n",
       "      <td>I don't have enough information to answer.</td>\n",
       "      <td>ChatGPT Business usage varies significantly by...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How does the environmental impact of artificia...</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nWhat is AI, how does it work and w...</td>\n",
       "      <td>The environmental impact of artificial intelli...</td>\n",
       "      <td>The environmental impact of artificial intelli...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.949417</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How do privacy protections and de-identificati...</td>\n",
       "      <td>[‚Ä¢Sampled from all ChatGPT users:a random samp...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nWe describe the contents of each d...</td>\n",
       "      <td>The analysis of ChatGPT user messages employs ...</td>\n",
       "      <td>Privacy protections in the analysis of ChatGPT...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.952696</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What trends can be observed in user cohort ana...</td>\n",
       "      <td>[X‚Äôs indicate that the ranking is\\nunavailable...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nThe yellow line represents the fir...</td>\n",
       "      <td>The user cohort analysis regarding ChatGPT que...</td>\n",
       "      <td>User cohort analysis reveals that there has be...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.950398</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What are the environmental concerns related to...</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nWhat is AI, how does it work and w...</td>\n",
       "      <td>The environmental concerns related to artifici...</td>\n",
       "      <td>The environmental concerns related to artifici...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.953324</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.105263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  What are the ethical implications and concerns...   \n",
       "1  What is the estimated energy consumption of th...   \n",
       "2  Wut is the significanse of Artificial Intellig...   \n",
       "3  What does Figure 22 illustrate about the varia...   \n",
       "4  What does Figure 22 show about how ChatGPT is ...   \n",
       "5  How does ChatGPT Business usage vary by occupa...   \n",
       "6  How does the environmental impact of artificia...   \n",
       "7  How do privacy protections and de-identificati...   \n",
       "8  What trends can be observed in user cohort ana...   \n",
       "9  What are the environmental concerns related to...   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [What is AI, how does it work and why are some...   \n",
       "1  [What is AI, how does it work and why are some...   \n",
       "2  [This article was published in 2018. To read m...   \n",
       "3  [The prompts for each of these automated class...   \n",
       "4  [X‚Äôs indicate that the ranking is\\nunavailable...   \n",
       "5  [The prompts for each of these automated class...   \n",
       "6  [What is AI, how does it work and why are some...   \n",
       "7  [‚Ä¢Sampled from all ChatGPT users:a random samp...   \n",
       "8  [X‚Äôs indicate that the ranking is\\nunavailable...   \n",
       "9  [What is AI, how does it work and why are some...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  [What is AI, how does it work and why are some...   \n",
       "1  [How does AI effect the environment?\\nIt is no...   \n",
       "2  [This article was published in 2018. To read m...   \n",
       "3  [<1-hop>\\n\\n37% of messages are work-related\\n...   \n",
       "4  [<1-hop>\\n\\nPanel A.Work Related\\n Panel B1.As...   \n",
       "5  [<1-hop>\\n\\nCorporate users may also use ChatG...   \n",
       "6  [<1-hop>\\n\\nWhat is AI, how does it work and w...   \n",
       "7  [<1-hop>\\n\\nWe describe the contents of each d...   \n",
       "8  [<1-hop>\\n\\nThe yellow line represents the fir...   \n",
       "9  [<1-hop>\\n\\nWhat is AI, how does it work and w...   \n",
       "\n",
       "                                            response  \\\n",
       "0         I don't have enough information to answer.   \n",
       "1  It is estimated that the AI industry as a whol...   \n",
       "2  Artificial Intelligence (AI) holds significant...   \n",
       "3  Figure 22 illustrates that there is a variatio...   \n",
       "4  Figure 22 presents data on how ChatGPT is used...   \n",
       "5         I don't have enough information to answer.   \n",
       "6  The environmental impact of artificial intelli...   \n",
       "7  The analysis of ChatGPT user messages employs ...   \n",
       "8  The user cohort analysis regarding ChatGPT que...   \n",
       "9  The environmental concerns related to artifici...   \n",
       "\n",
       "                                           reference  context_recall  \\\n",
       "0  The rise of Meta AI, like other generative AI ...             1.0   \n",
       "1  Some researchers estimate that the AI industry...             1.0   \n",
       "2  Artificial Intelligence (AI) is a technology t...             1.0   \n",
       "3  Figure 22 illustrates the variation in ChatGPT...             1.0   \n",
       "4  Figure 22 illustrates the classification of wo...             1.0   \n",
       "5  ChatGPT Business usage varies significantly by...             1.0   \n",
       "6  The environmental impact of artificial intelli...             1.0   \n",
       "7  Privacy protections in the analysis of ChatGPT...             1.0   \n",
       "8  User cohort analysis reveals that there has be...             1.0   \n",
       "9  The environmental concerns related to artifici...             1.0   \n",
       "\n",
       "   faithfulness  factual_correctness(mode=f1)  answer_relevancy  \\\n",
       "0           0.0                          0.00          0.000000   \n",
       "1           1.0                          1.00          0.000000   \n",
       "2           1.0                          0.55          0.933296   \n",
       "3           0.9                          0.75          0.964758   \n",
       "4           1.0                          0.00          0.959466   \n",
       "5           0.0                          0.00          0.000000   \n",
       "6           1.0                          0.93          0.949417   \n",
       "7           1.0                          0.77          0.952696   \n",
       "8           1.0                          0.83          0.950398   \n",
       "9           1.0                          0.90          0.953324   \n",
       "\n",
       "   context_entity_recall  noise_sensitivity(mode=relevant)  \n",
       "0               0.100000                          0.000000  \n",
       "1               0.500000                          0.000000  \n",
       "2               0.666667                          0.642857  \n",
       "3               0.100000                          0.400000  \n",
       "4               0.176471                          1.000000  \n",
       "5               0.111111                          0.000000  \n",
       "6               0.105263                          0.000000  \n",
       "7               0.333333                          0.222222  \n",
       "8               0.076923                          0.166667  \n",
       "9               0.333333                          0.105263  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finally_metrics_hybrid['evaluator']['evaluation_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86dfa950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context_recall': 1.0000, 'faithfulness': 0.7900, 'factual_correctness(mode=f1)': 0.5730, 'answer_relevancy': 0.6663, 'context_entity_recall': 0.2503, 'noise_sensitivity(mode=relevant)': 0.2537}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finally_metrics_hybrid['evaluator']['metrics']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ac1367",
   "metadata": {},
   "source": [
    "### Comparative Analysis: Hybrid vs Naive RAG üìäüî¨\n",
    "\n",
    "Naive: \n",
    "\n",
    "```python\n",
    "{'context_recall': 0.9667,\n",
    "'faithfulness': 0.6410, \n",
    "'factual_correctness(mode=f1)': 0.5170, \n",
    "'answer_relevancy': 0.5775, \n",
    "'context_entity_recall': 0.1532, \n",
    "'noise_sensitivity(mode=relevant)': 0.1972}\n",
    "```\n",
    "\n",
    "Hybrid with reranking\n",
    "\n",
    "```python\n",
    "{'context_recall': 1.0000,\n",
    "'faithfulness': 0.7900,\n",
    "'factual_correctness(mode=f1)': 0.5730, \n",
    "'answer_relevancy': 0.6663, \n",
    "'context_entity_recall': 0.2503, \n",
    "'noise_sensitivity(mode=relevant)': 0.2537}\n",
    "```\n",
    "\n",
    "Now you have evaluation results from both RAG systems! Let's compare their performance across all RAGAS metrics.\n",
    "\n",
    "**üîç Comparison Framework:**\n",
    "\n",
    "| Metric | Naive RAG Score | Hybrid RAG Score | Winner | Insights |\n",
    "|--------|----------------|------------------|---------|----------|\n",
    "| **LLMContextRecall** | 0.9667 | 1.0000 | Hybrid with reranking | Retrieval effectiveness |\n",
    "| **Faithfulness** | 0.6410 | 0.7900 | Hybrid with reranking | Response accuracy |\n",
    "| **FactualCorrectness** | 0.5775 | 0.5730 | Naive | Factual reliability |\n",
    "| **ResponseRelevancy** | 0.5775 | 0.6663 | Hybrid with reranking | Answer relevance |\n",
    "| **ContextEntityRecall** | 0.1532 | 0.2503 | Hybrid with reranking | Entity coverage |\n",
    "| **NoiseSensitivity** |  0.1972 | 0.2537 | Naive | Robustness to noise |\n",
    "\n",
    "**üí° Analysis Questions:**\n",
    "1. **Which system performs better overall?**\n",
    "2. **Are there specific metrics where one system significantly outperforms?**  \n",
    "3. **What trade-offs do you observe between the approaches?**\n",
    "4. **How do the retrieved contexts differ between systems?**\n",
    "\n",
    "**üéØ Next Steps:**\n",
    "Based on these results, you can:\n",
    "- **Choose the better performing system** for your use case\n",
    "- **Identify areas for improvement** in both approaches  \n",
    "- **Design hybrid approaches** that combine the best of both\n",
    "- **Scale evaluation** to larger datasets for more robust conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cda6ab3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéì Summary: Reproducible RAG Evaluation Workflows\n",
    "\n",
    "## üéØ What You've Accomplished\n",
    "\n",
    "Congratulations! You've successfully built and executed a **reproducible evaluation pipeline** for RAG systems using Haystack custom components. Here's what you've learned:\n",
    "\n",
    "### ‚úÖ **Technical Skills Developed:**\n",
    "- **Modular Component Design**: Created reusable evaluation components\n",
    "- **Pipeline Architecture**: Built scalable evaluation workflows  \n",
    "- **RAGAS Integration**: Integrated comprehensive RAG metrics\n",
    "- **Systematic Comparison**: Evaluated multiple RAG approaches consistently\n",
    "\n",
    "### ‚úÖ **Methodological Insights:**\n",
    "- **Reproducibility**: Same evaluation conditions across all experiments\n",
    "- **Modularity**: Easy to swap RAG systems and evaluation datasets\n",
    "- **Scalability**: Pipeline handles datasets of varying sizes\n",
    "- **Comprehensive Assessment**: Multiple metrics provide holistic view\n",
    "\n",
    "## üöÄ Next Steps & Extensions\n",
    "\n",
    "### **Immediate Applications:**\n",
    "1. **Scale Up Evaluation**: Test with larger datasets (50+ queries)\n",
    "2. **Add More RAG Variants**: Evaluate custom SuperComponents  \n",
    "3. **Parameter Tuning**: Test different chunk sizes, embedding models\n",
    "4. **Domain Testing**: Use domain-specific evaluation datasets\n",
    "\n",
    "### **Advanced Extensions:**\n",
    "1. **Automated Comparison**: Build comparison dashboards\n",
    "2. **Statistical Significance**: Add significance testing between systems\n",
    "3. **Cost Analysis**: Track API usage and processing time\n",
    "4. **A/B Testing**: Deploy evaluation pipeline for production monitoring\n",
    "\n",
    "## üí° **Key Design Principles Learned:**\n",
    "\n",
    "### **üîß Modularity**\n",
    "- Each component has a single, well-defined responsibility\n",
    "- Easy to swap implementations without changing evaluation logic\n",
    "- Components are reusable across different experiments\n",
    "\n",
    "### **üìä Reproducibility**  \n",
    "- Consistent evaluation conditions eliminate bias\n",
    "- Pipeline ensures identical processing for all RAG variants\n",
    "- Results are comparable and scientifically valid\n",
    "\n",
    "### **‚ö° Scalability**\n",
    "- Architecture handles small experiments and large-scale evaluation\n",
    "- Easy to add new RAG systems or evaluation metrics  \n",
    "- Pipeline can be deployed for automated monitoring\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations!** You now have a production-ready evaluation system for RAG applications. This pipeline will serve as the foundation for systematic RAG system development and optimization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-with-haystack-ch6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
