{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "688e2412",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.rag.hybridrag import hybrid_rag_sc\n",
    "from scripts.rag.naiverag import naive_rag_sc\n",
    "from scripts.rag.pretty_print import pretty_print_rag_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "597dd6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 11.69it/s]\n"
     ]
    }
   ],
   "source": [
    "query = \"Who uses ChatGPT more, men or women, and how does this change by 2025\"\n",
    "hybrid_answer = hybrid_rag_sc.run(query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "711ad52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸ” HYBRID RAG ANSWER\n",
      "================================================================================\n",
      "ðŸ“ Query: Who uses ChatGPT more, men or women, and how does this change by 2025\n",
      "--------------------------------------------------------------------------------\n",
      "ðŸ’¬ Answer:\n",
      "   Initially, a significant share (around 80%) of weekly active users of\n",
      "   ChatGPT in the first few months after its release were individuals with\n",
      "   typically masculine first names. However, by the first half of 2025,\n",
      "   this gap began to close, and the share of active users with typically\n",
      "   feminine names reached near parity with those having typically masculine\n",
      "   names. By June 2025, active users were more likely to have typically\n",
      "   feminine names, indicating that gender gaps in ChatGPT usage had closed\n",
      "   substantially over time.\n",
      "\n",
      "ðŸ“š Source Documents (3 found):\n",
      "--------------------------------------------------\n",
      "1. Source: Unknown source\n",
      "   Preview: The prompts for each of these automated classifiers (with the\n",
      "exception of interaction quality) are available in Appendix A. Values represent the aver...\n",
      "\n",
      "2. Source: Unknown source\n",
      "   Preview: â€¢Sampled from all ChatGPT users:a random sample of approximately one million de-\n",
      "identified messages from logged-in consumer ChatGPT users between May...\n",
      "\n",
      "3. Source: Unknown source\n",
      "   Preview: Xâ€™s indicate that the ranking is\n",
      "unavailable since fewer than 100 users from that occupation group requested that specific GWA within the sample. Seve...\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the pretty print functions\n",
    "pretty_print_rag_answer(hybrid_answer, \"Hybrid RAG\", query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e7b2ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.86it/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "naive_answer = naive_rag_sc.run(query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d1d9ff3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸ” NAIVE RAG ANSWER\n",
      "================================================================================\n",
      "ðŸ“ Query: Who uses ChatGPT more, men or women, and how does this change by 2025\n",
      "--------------------------------------------------------------------------------\n",
      "ðŸ’¬ Answer:\n",
      "   Based on the provided information, initially, around 80% of the weekly\n",
      "   active users (WAU) in the first few months after ChatGPT was released\n",
      "   were by users with typically masculine first names. However, by the\n",
      "   first half of 2025, the share of active users with typically feminine\n",
      "   and typically masculine names reached near-parity. By June 2025, it was\n",
      "   observed that active users were more likely to have typically feminine\n",
      "   names, suggesting that gender gaps in ChatGPT usage have closed\n",
      "   substantially over time.\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pretty_print_rag_answer(naive_answer, \"Naive RAG\", query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8703b16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "9050511f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.generators import OpenAIGenerator\n",
    "\n",
    "from ragas import EvaluationDataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv(\"./data_for_eval/synthetic_tests_advanced_branching_50.csv\")\n",
    "dataset_5 = dataset.head().copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "881684d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Hybrid RAG on all queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.22it/s]\n",
      "\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 22.00it/s]\n",
      "\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 18.38it/s]\n",
      "\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 20.75it/s]\n",
      "\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 75.25it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added columns: response, retrieved_contexts\n",
      "Dataset shape: (5, 5)\n"
     ]
    }
   ],
   "source": [
    "# Create a lambda function to run either RAG system\n",
    "def run_rag_system(query, rag_system):\n",
    "    \"\"\"Helper function to run a RAG system and return formatted response\"\"\"\n",
    "    response = rag_system.run(query=query)\n",
    "    return {\n",
    "        'response': response['replies'][0] if 'replies' in response else '',\n",
    "        'reference': [doc.content for doc in response.get('documents', [])]\n",
    "    }\n",
    "\n",
    "# Create lambda functions for each RAG system\n",
    "run_hybrid_rag = lambda query: run_rag_system(query, hybrid_rag_sc)\n",
    "run_naive_rag = lambda query: run_rag_system(query, naive_rag_sc)\n",
    "\n",
    "# Apply the lambda functions to add RAG responses as columns\n",
    "print(\"Running Hybrid RAG on all queries...\")\n",
    "hybrid_results = dataset_5['user_input'].apply(run_hybrid_rag)\n",
    "\n",
    "# Extract response and retrieved_contexts into separate columns\n",
    "dataset_5['response'] = hybrid_results.apply(lambda x: x['response'])\n",
    "dataset_5['reference'] = hybrid_results.apply(lambda x: x['reference'])\n",
    "\n",
    "print(f\"Added columns: response, retrieved_contexts\")\n",
    "print(f\"Dataset shape: {dataset_5.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c541dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing dataset format for RAGAS SingleTurnSample...\n",
      "Dataset formatted for RAGAS!\n",
      "Shape: (5, 4)\n",
      "Columns: ['user_input', 'response', 'retrieved_contexts', 'reference']\n",
      "\n",
      "Data type verification:\n",
      "user_input type: <class 'str'>\n",
      "response type: <class 'str'>\n",
      "retrieved_contexts type: <class 'list'>\n",
      "reference type: <class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>response</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What OpenAI do?</td>\n",
       "      <td>OpenAI is an artificial intelligence research ...</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>What is AI, how does it work and why are some ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What concerns did Yann LeCun dismiss regarding...</td>\n",
       "      <td>Yann LeCun dismissed the concern expressed by ...</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>Why is AI controversial?\\nWhile acknowledging ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What significant developments regarding AI reg...</td>\n",
       "      <td>I don't have enough information to answer.</td>\n",
       "      <td>[This article was published in 2018. To read m...</td>\n",
       "      <td>Are there laws governing AI?\\nSome governments...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who is Shubhendu and what role does he play in...</td>\n",
       "      <td>I don't have enough information to answer.</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>This article was published in 2018. To read mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Can you explain how Vijay's research contribut...</td>\n",
       "      <td>I don't have enough information to answer.</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>Intelligence, please visit the AI topic page.\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0                                    What OpenAI do?   \n",
       "1  What concerns did Yann LeCun dismiss regarding...   \n",
       "2  What significant developments regarding AI reg...   \n",
       "3  Who is Shubhendu and what role does he play in...   \n",
       "4  Can you explain how Vijay's research contribut...   \n",
       "\n",
       "                                            response  \\\n",
       "0  OpenAI is an artificial intelligence research ...   \n",
       "1  Yann LeCun dismissed the concern expressed by ...   \n",
       "2         I don't have enough information to answer.   \n",
       "3         I don't have enough information to answer.   \n",
       "4         I don't have enough information to answer.   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [What is AI, how does it work and why are some...   \n",
       "1  [What is AI, how does it work and why are some...   \n",
       "2  [This article was published in 2018. To read m...   \n",
       "3  [What is AI, how does it work and why are some...   \n",
       "4  [What is AI, how does it work and why are some...   \n",
       "\n",
       "                                           reference  \n",
       "0  What is AI, how does it work and why are some ...  \n",
       "1  Why is AI controversial?\\nWhile acknowledging ...  \n",
       "2  Are there laws governing AI?\\nSome governments...  \n",
       "3  This article was published in 2018. To read mo...  \n",
       "4  Intelligence, please visit the AI topic page.\\...  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fix dataset format for RAGAS SingleTurnSample requirements\n",
    "import ast\n",
    "\n",
    "def parse_contexts(context_str):\n",
    "    \"\"\"Parse string representation of list to actual list\"\"\"\n",
    "    try:\n",
    "        if isinstance(context_str, str):\n",
    "            return ast.literal_eval(context_str)\n",
    "        elif isinstance(context_str, list):\n",
    "            return context_str\n",
    "        else:\n",
    "            return []\n",
    "    except (ValueError, SyntaxError):\n",
    "        if isinstance(context_str, str):\n",
    "            return [context_str]\n",
    "        return []\n",
    "\n",
    "# Create a clean dataset for RAGAS evaluation\n",
    "eval_dataset = dataset_5.copy()\n",
    "\n",
    "# Parse reference_contexts from string to list\n",
    "eval_dataset['reference_contexts_parsed'] = eval_dataset['reference_contexts'].apply(parse_contexts)\n",
    "\n",
    "# RAGAS expects specific column names for SingleTurnSample:\n",
    "# - user_input (str) \n",
    "# - response (str)\n",
    "# - retrieved_contexts (List[str])\n",
    "# - reference (str) \n",
    "\n",
    "# Ensure user_input is a string, not a list\n",
    "eval_dataset['user_input'] = eval_dataset['user_input'].apply(lambda x: x[0] if isinstance(x, list) else x)\n",
    "\n",
    "# Rename and select columns to match RAGAS SingleTurnSample schema\n",
    "ragas_dataset = pd.DataFrame({\n",
    "    'user_input': eval_dataset['user_input'],           # Question/query as string\n",
    "    'response': eval_dataset['response'],                # RAG response as string  \n",
    "    'retrieved_contexts': eval_dataset['reference'],     # Retrieved contexts as list\n",
    "    'reference': eval_dataset['reference_contexts_parsed'].apply(lambda x: x[0] if x else \"\")  # Reference answer as string\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "2a1274c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluationDataset created successfully!\n",
      "Dataset size: 5\n",
      "Sample type: <class 'ragas.dataset_schema.SingleTurnSample'>\n"
     ]
    }
   ],
   "source": [
    "from ragas import EvaluationDataset\n",
    "\n",
    "# Create evaluation dataset with the properly formatted data\n",
    "evaluation_dataset = EvaluationDataset.from_pandas(ragas_dataset)\n",
    "print(\"EvaluationDataset created successfully!\")\n",
    "print(f\"Dataset size: {len(evaluation_dataset)}\")\n",
    "print(f\"Sample type: {evaluation_dataset.get_sample_type()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "fd20eb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.llms import HaystackLLMWrapper\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "\n",
    "evaluator_llm = HaystackLLMWrapper(OpenAIGenerator(model=\"gpt-4.1-mini\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "b46cc2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:   3%|â–Ž         | 1/30 [00:02<00:59,  2.07s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:   3%|â–Ž         | 1/30 [00:02<00:59,  2.07s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  20%|â–ˆâ–ˆ        | 6/30 [00:13<00:52,  2.19s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 13/30 [00:23<00:19,  1.16s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 24/30 [05:52<08:22, 83.76s/it]Exception raised in Job[5]: TimeoutError()\n",
      "Exception raised in Job[2]: TimeoutError()\n",
      "Evaluating:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 25/30 [06:00<05:08, 61.76s/it]Exception raised in Job[8]: TimeoutError()\n",
      "Exception raised in Job[11]: TimeoutError()\n",
      "Exception raised in Job[5]: TimeoutError()\n",
      "Exception raised in Job[2]: TimeoutError()\n",
      "Evaluating:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 25/30 [06:00<05:08, 61.76s/it]Exception raised in Job[8]: TimeoutError()\n",
      "Exception raised in Job[11]: TimeoutError()\n",
      "Exception raised in Job[23]: TimeoutError()\n",
      "Evaluating:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 29/30 [06:15<00:25, 25.73s/it]Exception raised in Job[23]: TimeoutError()\n",
      "Evaluating:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 29/30 [06:15<00:25, 25.73s/it]Exception raised in Job[29]: TimeoutError()\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [06:25<00:00, 12.87s/it]Exception raised in Job[29]: TimeoutError()\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [06:25<00:00, 12.87s/it]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'context_recall': 0.8444, 'faithfulness': 0.6000, 'factual_correctness(mode=f1)': 0.0000, 'answer_relevancy': 0.1913, 'context_entity_recall': 0.3015, 'noise_sensitivity(mode=relevant)': 1.0000}"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, ResponseRelevancy, ContextEntityRecall, NoiseSensitivity\n",
    "from ragas import evaluate, RunConfig\n",
    "\n",
    "custom_run_config = RunConfig(timeout=360)\n",
    "\n",
    "baseline_result = evaluate(\n",
    "    dataset=evaluation_dataset,\n",
    "    metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness(), ResponseRelevancy(), ContextEntityRecall(), NoiseSensitivity()],\n",
    "    llm=evaluator_llm,\n",
    "    run_config=custom_run_config\n",
    ")\n",
    "baseline_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1857ed1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-with-haystack-ch6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
