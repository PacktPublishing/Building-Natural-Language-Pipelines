{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33c56ea9",
   "metadata": {},
   "source": [
    "🔧 **Setup Required**: Before running this notebook, please follow the [setup instructions](../README.md#setup-instructions) to configure your environment and API keys. **You will need to ensure you've executed the Indexing pipeline before completing this exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2db02a",
   "metadata": {},
   "source": [
    "# RAGAS Evaluation Tutorial: A Complete Guide to RAG System Assessment\n",
    "\n",
    "This notebook provides a comprehensive walkthrough for evaluating Retrieval-Augmented Generation (RAG) systems using the RAGAS (RAG Assessment) framework. \n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "In this tutorial, you will:\n",
    "1. **Set up RAGAS** - Import necessary libraries and understand the evaluation framework\n",
    "2. **Test RAG Systems** - Run queries against different RAG implementations (Hybrid vs Naive)\n",
    "3. **Prepare Evaluation Data** - Format synthetic test data for RAGAS evaluation\n",
    "4. **Run Comprehensive Metrics** - Evaluate your RAG system using multiple assessment criteria\n",
    "5. **Interpret Results** - Understand what the metrics tell us about system performance\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic understanding of RAG (Retrieval-Augmented Generation) systems\n",
    "- Familiarity with Python and Pandas\n",
    "- OpenAI API key configured in your environment\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac590a75",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries and RAG Systems\n",
    "\n",
    "First, let's import the RAG systems we'll be evaluating. We have two implementations:\n",
    "- **Hybrid RAG**: Uses both dense and sparse retrieval methods\n",
    "- **Naive RAG**: Uses a simpler retrieval approach\n",
    "\n",
    "We'll also import utility functions for pretty-printing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "688e2412",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurafunderburk/Documents/GitHub/Building-Natural-Language-Pipelines/ch6/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from scripts.rag.hybridrag import hybrid_rag_sc\n",
    "from scripts.rag.naiverag import naive_rag_sc\n",
    "from scripts.rag.pretty_print import pretty_print_rag_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "597dd6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Running Hybrid RAG system...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.70it/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a sample query to test both RAG systems\n",
    "query = \"Who uses ChatGPT more, men or women, and how does this change by 2025\"\n",
    "\n",
    "# Run the Hybrid RAG system\n",
    "print(\"🔄 Running Hybrid RAG system...\")\n",
    "hybrid_answer = hybrid_rag_sc.run(query=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db4869f",
   "metadata": {},
   "source": [
    "## Step 2: Test Your RAG Systems with a Sample Query\n",
    "\n",
    "Before running evaluation metrics, let's test both RAG systems with a sample query to understand their behavior. This helps us verify everything is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "711ad52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🔍 HYBRID RAG ANSWER\n",
      "================================================================================\n",
      "📝 Query: Who uses ChatGPT more, men or women, and how does this change by 2025\n",
      "--------------------------------------------------------------------------------\n",
      "💬 Answer:\n",
      "   Initially, a significant share (around 80%) of weekly active users of\n",
      "   ChatGPT in the first few months after its release were users with\n",
      "   typically masculine first names. However, by the first half of 2025, the\n",
      "   share of active users with typically feminine names reached near-parity\n",
      "   with those having typically masculine names. By June 2025, active users\n",
      "   were more likely to have typically feminine names, suggesting that\n",
      "   gender gaps in ChatGPT usage have closed substantially over time.\n",
      "\n",
      "📚 Source Documents (3 found):\n",
      "--------------------------------------------------\n",
      "1. Source: Unknown source\n",
      "   Preview: The prompts for each of these automated classifiers (with the\n",
      "exception of interaction quality) are available in Appendix A. Values represent the aver...\n",
      "\n",
      "2. Source: Unknown source\n",
      "   Preview: •Sampled from all ChatGPT users:a random sample of approximately one million de-\n",
      "identified messages from logged-in consumer ChatGPT users between May...\n",
      "\n",
      "3. Source: Unknown source\n",
      "   Preview: X’s indicate that the ranking is\n",
      "unavailable since fewer than 100 users from that occupation group requested that specific GWA within the sample. Seve...\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the Hybrid RAG results in a formatted way\n",
    "pretty_print_rag_answer(hybrid_answer, \"Hybrid RAG\", query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e7b2ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Running Naive RAG system...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.94it/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the Naive RAG system for comparison\n",
    "print(\"🔄 Running Naive RAG system...\")\n",
    "naive_answer = naive_rag_sc.run(query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1d9ff3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🔍 NAIVE RAG ANSWER\n",
      "================================================================================\n",
      "📝 Query: Who uses ChatGPT more, men or women, and how does this change by 2025\n",
      "--------------------------------------------------------------------------------\n",
      "💬 Answer:\n",
      "   Initially, around 80% of the weekly active users (WAU) in the first few\n",
      "   months after ChatGPT was released had typically masculine first names.\n",
      "   However, by the first half of 2025, the share of active users with\n",
      "   typically feminine and typically masculine names reached near-parity. By\n",
      "   June 2025, it was observed that active users were more likely to have\n",
      "   typically feminine names, indicating that gender gaps in ChatGPT usage\n",
      "   have closed significantly over time.\n",
      "\n",
      "📚 Source Documents (3 found):\n",
      "--------------------------------------------------\n",
      "1. Source: Unknown source\n",
      "   Preview: The prompts for each of these automated classifiers (with the\n",
      "exception of interaction quality) are available in Appendix A. Values represent the aver...\n",
      "\n",
      "2. Source: Unknown source\n",
      "   Preview: X’s indicate that the ranking is\n",
      "unavailable since fewer than 100 users from that occupation group requested that specific GWA within the sample. Seve...\n",
      "\n",
      "3. Source: Unknown source\n",
      "   Preview: •Sampled from all ChatGPT users:a random sample of approximately one million de-\n",
      "identified messages from logged-in consumer ChatGPT users between May...\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the Naive RAG results\n",
    "pretty_print_rag_answer(naive_answer, \"Naive RAG\", query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18addff",
   "metadata": {},
   "source": [
    "**💡 Observation**: Compare the responses from both systems. Notice differences in:\n",
    "- Answer quality and completeness\n",
    "- Retrieved context relevance\n",
    "- Response structure and clarity\n",
    "\n",
    "This manual comparison gives us intuition, but RAGAS provides systematic evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5c4db7",
   "metadata": {},
   "source": [
    "## Step 3: Introduction to RAGAS Framework\n",
    "\n",
    "**RAGAS** (RAG Assessment) is a framework designed to evaluate RAG systems comprehensively. It provides several key metrics:\n",
    "\n",
    "### Key RAGAS Metrics We'll Use:\n",
    "\n",
    "1. **Faithfulness** - How factually consistent the answer is with the retrieved context\n",
    "2. **Answer Relevancy** - How relevant the answer is to the question\n",
    "3. **Context Recall** - How well the retrieval captures relevant information\n",
    "4. **Context Precision** - Quality of the retrieved context (signal vs noise)\n",
    "5. **Factual Correctness** - Accuracy of factual claims in the response\n",
    "\n",
    "Let's import RAGAS and prepare our evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8703b16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RAGAS imported successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Import RAGAS core modules\n",
    "from ragas import evaluation\n",
    "print(\"✅ RAGAS imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589d6f28",
   "metadata": {},
   "source": [
    "## Step 4: Load and Examine Evaluation Dataset\n",
    "\n",
    "For systematic evaluation, we need a test dataset with:\n",
    "- **Questions** - Queries to test the RAG system\n",
    "- **Ground Truth Answers** - Expected correct responses  \n",
    "- **Reference Contexts** - Documents that should be retrieved\n",
    "\n",
    "We'll use a synthetic dataset that was pre-generated for evaluation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9050511f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Loading evaluation dataset...\n",
      "✅ Dataset loaded successfully!\n",
      "📈 Dataset contains 10 test cases\n",
      "🔍 Columns: ['user_input', 'reference_contexts', 'reference', 'synthesizer_name']\n"
     ]
    }
   ],
   "source": [
    "# Import additional required libraries\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from ragas import EvaluationDataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the synthetic evaluation dataset\n",
    "print(\"📊 Loading evaluation dataset...\")\n",
    "dataset = pd.read_csv(\"./data_for_eval/synthetic_tests_advanced_branching_10.csv\")\n",
    "dataset_5 = dataset.copy()\n",
    "\n",
    "print(f\"✅ Dataset loaded successfully!\")\n",
    "print(f\"📈 Dataset contains {len(dataset_5)} test cases\")\n",
    "print(f\"🔍 Columns: {list(dataset_5.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1aba46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Examining the evaluation dataset structure:\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the ethical implications and concerns...</td>\n",
       "      <td>['What is AI, how does it work and why are som...</td>\n",
       "      <td>The rise of Meta AI, like other generative AI ...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the estimated energy consumption of th...</td>\n",
       "      <td>['How does AI effect the environment?\\nIt is n...</td>\n",
       "      <td>Some researchers estimate that the AI industry...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wut is the significanse of Artificial Intellig...</td>\n",
       "      <td>['This article was published in 2018. To read ...</td>\n",
       "      <td>Artificial Intelligence (AI) is a technology t...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What does Figure 22 illustrate about the varia...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\n37% of messages are work-related\\...</td>\n",
       "      <td>Figure 22 illustrates the variation in ChatGPT...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What does Figure 22 show about how ChatGPT is ...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nPanel A.Work Related\\n Panel B1.A...</td>\n",
       "      <td>Figure 22 illustrates the classification of wo...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How does ChatGPT Business usage vary by occupa...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nCorporate users may also use Chat...</td>\n",
       "      <td>ChatGPT Business usage varies significantly by...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How does the environmental impact of artificia...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nWhat is AI, how does it work and ...</td>\n",
       "      <td>The environmental impact of artificial intelli...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How do privacy protections and de-identificati...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nWe describe the contents of each ...</td>\n",
       "      <td>Privacy protections in the analysis of ChatGPT...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What trends can be observed in user cohort ana...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nThe yellow line represents the fi...</td>\n",
       "      <td>User cohort analysis reveals that there has be...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What are the environmental concerns related to...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nWhat is AI, how does it work and ...</td>\n",
       "      <td>The environmental concerns related to artifici...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  What are the ethical implications and concerns...   \n",
       "1  What is the estimated energy consumption of th...   \n",
       "2  Wut is the significanse of Artificial Intellig...   \n",
       "3  What does Figure 22 illustrate about the varia...   \n",
       "4  What does Figure 22 show about how ChatGPT is ...   \n",
       "5  How does ChatGPT Business usage vary by occupa...   \n",
       "6  How does the environmental impact of artificia...   \n",
       "7  How do privacy protections and de-identificati...   \n",
       "8  What trends can be observed in user cohort ana...   \n",
       "9  What are the environmental concerns related to...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  ['What is AI, how does it work and why are som...   \n",
       "1  ['How does AI effect the environment?\\nIt is n...   \n",
       "2  ['This article was published in 2018. To read ...   \n",
       "3  ['<1-hop>\\n\\n37% of messages are work-related\\...   \n",
       "4  ['<1-hop>\\n\\nPanel A.Work Related\\n Panel B1.A...   \n",
       "5  ['<1-hop>\\n\\nCorporate users may also use Chat...   \n",
       "6  ['<1-hop>\\n\\nWhat is AI, how does it work and ...   \n",
       "7  ['<1-hop>\\n\\nWe describe the contents of each ...   \n",
       "8  ['<1-hop>\\n\\nThe yellow line represents the fi...   \n",
       "9  ['<1-hop>\\n\\nWhat is AI, how does it work and ...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  The rise of Meta AI, like other generative AI ...   \n",
       "1  Some researchers estimate that the AI industry...   \n",
       "2  Artificial Intelligence (AI) is a technology t...   \n",
       "3  Figure 22 illustrates the variation in ChatGPT...   \n",
       "4  Figure 22 illustrates the classification of wo...   \n",
       "5  ChatGPT Business usage varies significantly by...   \n",
       "6  The environmental impact of artificial intelli...   \n",
       "7  Privacy protections in the analysis of ChatGPT...   \n",
       "8  User cohort analysis reveals that there has be...   \n",
       "9  The environmental concerns related to artifici...   \n",
       "\n",
       "                        synthesizer_name  \n",
       "0  single_hop_specific_query_synthesizer  \n",
       "1  single_hop_specific_query_synthesizer  \n",
       "2  single_hop_specific_query_synthesizer  \n",
       "3   multi_hop_specific_query_synthesizer  \n",
       "4   multi_hop_specific_query_synthesizer  \n",
       "5   multi_hop_specific_query_synthesizer  \n",
       "6   multi_hop_abstract_query_synthesizer  \n",
       "7   multi_hop_abstract_query_synthesizer  \n",
       "8   multi_hop_abstract_query_synthesizer  \n",
       "9   multi_hop_abstract_query_synthesizer  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's examine the structure of our evaluation dataset\n",
    "print(\"🔍 Examining the evaluation dataset structure:\")\n",
    "print(\"=\" * 50)\n",
    "dataset_5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92841178",
   "metadata": {},
   "source": [
    "## Step 5: Generate RAG Responses for Evaluation\n",
    "\n",
    "Now we'll run our RAG system on all test queries to generate responses that RAGAS can evaluate. This is where we collect the data needed for systematic assessment.\n",
    "\n",
    "**What we're doing:**\n",
    "- Run each test query through our Hybrid RAG system\n",
    "- Extract both the generated response and retrieved contexts\n",
    "- Store results in the format RAGAS expects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881684d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Running Hybrid RAG on all evaluation queries...\n",
      "This may take a few minutes depending on dataset size...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.19it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.21it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.46it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.04it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.66it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.97it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.45it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.98it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.40it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.01it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully generated responses for 10 queries!\n",
      "📊 Added columns: 'response', 'reference' (retrieved contexts)\n",
      "📈 Final dataset shape: (10, 5)\n"
     ]
    }
   ],
   "source": [
    "# Create a helper function to run RAG systems and extract needed data\n",
    "def run_rag_system(query, rag_system):\n",
    "    \"\"\"\n",
    "    Helper function to run a RAG system and return formatted response\n",
    "    \n",
    "    Args:\n",
    "        query: The question to ask the RAG system\n",
    "        rag_system: The RAG pipeline to use\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains the response text and retrieved document contexts\n",
    "    \"\"\"\n",
    "    response = rag_system.run(query=query)\n",
    "    return {\n",
    "        'response': response['replies'][0] if 'replies' in response else '',\n",
    "        'reference': [doc.content for doc in response.get('documents', [])]\n",
    "    }\n",
    "\n",
    "# Create lambda functions for each RAG system\n",
    "run_hybrid_rag = lambda query: run_rag_system(query, hybrid_rag_sc)\n",
    "\n",
    "# Apply the hybrid RAG system to all test queries\n",
    "print(\"🔄 Running Hybrid RAG on all evaluation queries...\")\n",
    "print(\"This may take a few minutes depending on dataset size...\")\n",
    "\n",
    "hybrid_results = dataset_5['user_input'].apply(run_hybrid_rag)\n",
    "\n",
    "# Extract response and retrieved contexts into separate columns\n",
    "dataset_5['response'] = hybrid_results.apply(lambda x: x['response'])\n",
    "dataset_5['reference'] = hybrid_results.apply(lambda x: x['reference'])\n",
    "\n",
    "print(f\"✅ Successfully generated responses for {len(dataset_5)} queries!\")\n",
    "print(f\"📊 Added columns: 'response', 'reference' (retrieved contexts)\")\n",
    "print(f\"📈 Final dataset shape: {dataset_5.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cd4f46",
   "metadata": {},
   "source": [
    "## Step 6: Format Data for RAGAS Evaluation\n",
    "\n",
    "RAGAS expects data in a specific format called `SingleTurnSample`. We need to transform our dataset to match these requirements:\n",
    "\n",
    "**RAGAS Required Fields:**\n",
    "- `user_input` (str) - The question/query\n",
    "- `response` (str) - RAG system's answer  \n",
    "- `retrieved_contexts` (List[str]) - Documents retrieved by RAG\n",
    "- `reference` (str) - Ground truth answer\n",
    "\n",
    "Let's transform our data to match this schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c541dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Preparing dataset for RAGAS evaluation...\n",
      "✅ Dataset successfully formatted for RAGAS!\n",
      "📊 Final evaluation dataset shape: (10, 4)\n",
      "🔍 Columns: ['user_input', 'response', 'retrieved_contexts', 'reference']\n"
     ]
    }
   ],
   "source": [
    "# Transform dataset format for RAGAS SingleTurnSample requirements\n",
    "import ast\n",
    "\n",
    "def parse_contexts(context_str):\n",
    "    \"\"\"\n",
    "    Parse string representation of list to actual list\n",
    "    \n",
    "    Args:\n",
    "        context_str: String or list containing context information\n",
    "    \n",
    "    Returns:\n",
    "        list: Parsed context as a list of strings\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if isinstance(context_str, str):\n",
    "            return ast.literal_eval(context_str)\n",
    "        elif isinstance(context_str, list):\n",
    "            return context_str\n",
    "        else:\n",
    "            return []\n",
    "    except (ValueError, SyntaxError):\n",
    "        if isinstance(context_str, str):\n",
    "            return [context_str]\n",
    "        return []\n",
    "\n",
    "print(\"🔄 Preparing dataset for RAGAS evaluation...\")\n",
    "\n",
    "# Create a clean dataset for RAGAS evaluation\n",
    "eval_dataset = dataset_5.copy()\n",
    "\n",
    "# Parse reference_contexts from string to list (ground truth contexts)\n",
    "eval_dataset['reference_contexts_parsed'] = eval_dataset['reference_contexts'].apply(parse_contexts)\n",
    "\n",
    "# Ensure user_input is a string, not a list\n",
    "eval_dataset['user_input'] = eval_dataset['user_input'].apply(\n",
    "    lambda x: x[0] if isinstance(x, list) else x\n",
    ")\n",
    "\n",
    "# Create the final RAGAS-compatible dataset\n",
    "ragas_dataset = pd.DataFrame({\n",
    "    'user_input': eval_dataset['user_input'],           # Question/query as string\n",
    "    'response': eval_dataset['response'],                # RAG response as string  \n",
    "    'retrieved_contexts': eval_dataset['reference'],     # Retrieved contexts as list\n",
    "    'reference': eval_dataset['reference_contexts_parsed'].apply(\n",
    "        lambda x: x[0] if x else \"\"\n",
    "    )  # Reference answer as string\n",
    "})\n",
    "\n",
    "print(f\"✅ Dataset successfully formatted for RAGAS!\")\n",
    "print(f\"📊 Final evaluation dataset shape: {ragas_dataset.shape}\")\n",
    "print(f\"🔍 Columns: {list(ragas_dataset.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a1274c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Creating RAGAS EvaluationDataset...\n",
      "✅ EvaluationDataset created successfully!\n",
      "📊 Dataset size: 10 samples\n",
      "🔧 Sample type: <class 'ragas.dataset_schema.SingleTurnSample'>\n",
      "🎯 Ready for evaluation!\n"
     ]
    }
   ],
   "source": [
    "# Create the RAGAS EvaluationDataset object\n",
    "from ragas import EvaluationDataset\n",
    "\n",
    "print(\"🔄 Creating RAGAS EvaluationDataset...\")\n",
    "\n",
    "# Create evaluation dataset with the properly formatted data\n",
    "evaluation_dataset = EvaluationDataset.from_pandas(ragas_dataset)\n",
    "\n",
    "print(\"✅ EvaluationDataset created successfully!\")\n",
    "print(f\"📊 Dataset size: {len(evaluation_dataset)} samples\")\n",
    "print(f\"🔧 Sample type: {evaluation_dataset.get_sample_type()}\")\n",
    "print(\"🎯 Ready for evaluation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88e3223e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Final RAGAS dataset structure:\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>response</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the ethical implications and concerns...</td>\n",
       "      <td>The ethical implications and concerns surround...</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>What is AI, how does it work and why are some ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the estimated energy consumption of th...</td>\n",
       "      <td>The estimated energy consumption of the AI ind...</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>How does AI effect the environment?\\nIt is not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wut is the significanse of Artificial Intellig...</td>\n",
       "      <td>The significance of artificial intelligence (A...</td>\n",
       "      <td>[This article was published in 2018. To read m...</td>\n",
       "      <td>This article was published in 2018. To read mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What does Figure 22 illustrate about the varia...</td>\n",
       "      <td>Figure 22 illustrates that there is a variatio...</td>\n",
       "      <td>[The prompts for each of these automated class...</td>\n",
       "      <td>&lt;1-hop&gt;\\n\\n37% of messages are work-related\\nf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What does Figure 22 show about how ChatGPT is ...</td>\n",
       "      <td>Figure 22 presents variation in ChatGPT usage ...</td>\n",
       "      <td>[X’s indicate that the ranking is\\nunavailable...</td>\n",
       "      <td>&lt;1-hop&gt;\\n\\nPanel A.Work Related\\n Panel B1.Ask...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How does ChatGPT Business usage vary by occupa...</td>\n",
       "      <td>I don't have enough information to answer.</td>\n",
       "      <td>[The prompts for each of these automated class...</td>\n",
       "      <td>&lt;1-hop&gt;\\n\\nCorporate users may also use ChatGP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How does the environmental impact of artificia...</td>\n",
       "      <td>The environmental impact of artificial intelli...</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>&lt;1-hop&gt;\\n\\nWhat is AI, how does it work and wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How do privacy protections and de-identificati...</td>\n",
       "      <td>Privacy protections and de-identification proc...</td>\n",
       "      <td>[•Sampled from all ChatGPT users:a random samp...</td>\n",
       "      <td>&lt;1-hop&gt;\\n\\nWe describe the contents of each da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What trends can be observed in user cohort ana...</td>\n",
       "      <td>The user cohort analysis regarding ChatGPT que...</td>\n",
       "      <td>[X’s indicate that the ranking is\\nunavailable...</td>\n",
       "      <td>&lt;1-hop&gt;\\n\\nThe yellow line represents the firs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What are the environmental concerns related to...</td>\n",
       "      <td>The environmental concerns related to artifici...</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>&lt;1-hop&gt;\\n\\nWhat is AI, how does it work and wh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  What are the ethical implications and concerns...   \n",
       "1  What is the estimated energy consumption of th...   \n",
       "2  Wut is the significanse of Artificial Intellig...   \n",
       "3  What does Figure 22 illustrate about the varia...   \n",
       "4  What does Figure 22 show about how ChatGPT is ...   \n",
       "5  How does ChatGPT Business usage vary by occupa...   \n",
       "6  How does the environmental impact of artificia...   \n",
       "7  How do privacy protections and de-identificati...   \n",
       "8  What trends can be observed in user cohort ana...   \n",
       "9  What are the environmental concerns related to...   \n",
       "\n",
       "                                            response  \\\n",
       "0  The ethical implications and concerns surround...   \n",
       "1  The estimated energy consumption of the AI ind...   \n",
       "2  The significance of artificial intelligence (A...   \n",
       "3  Figure 22 illustrates that there is a variatio...   \n",
       "4  Figure 22 presents variation in ChatGPT usage ...   \n",
       "5         I don't have enough information to answer.   \n",
       "6  The environmental impact of artificial intelli...   \n",
       "7  Privacy protections and de-identification proc...   \n",
       "8  The user cohort analysis regarding ChatGPT que...   \n",
       "9  The environmental concerns related to artifici...   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [What is AI, how does it work and why are some...   \n",
       "1  [What is AI, how does it work and why are some...   \n",
       "2  [This article was published in 2018. To read m...   \n",
       "3  [The prompts for each of these automated class...   \n",
       "4  [X’s indicate that the ranking is\\nunavailable...   \n",
       "5  [The prompts for each of these automated class...   \n",
       "6  [What is AI, how does it work and why are some...   \n",
       "7  [•Sampled from all ChatGPT users:a random samp...   \n",
       "8  [X’s indicate that the ranking is\\nunavailable...   \n",
       "9  [What is AI, how does it work and why are some...   \n",
       "\n",
       "                                           reference  \n",
       "0  What is AI, how does it work and why are some ...  \n",
       "1  How does AI effect the environment?\\nIt is not...  \n",
       "2  This article was published in 2018. To read mo...  \n",
       "3  <1-hop>\\n\\n37% of messages are work-related\\nf...  \n",
       "4  <1-hop>\\n\\nPanel A.Work Related\\n Panel B1.Ask...  \n",
       "5  <1-hop>\\n\\nCorporate users may also use ChatGP...  \n",
       "6  <1-hop>\\n\\nWhat is AI, how does it work and wh...  \n",
       "7  <1-hop>\\n\\nWe describe the contents of each da...  \n",
       "8  <1-hop>\\n\\nThe yellow line represents the firs...  \n",
       "9  <1-hop>\\n\\nWhat is AI, how does it work and wh...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's examine our final dataset structure\n",
    "print(\"🔍 Final RAGAS dataset structure:\")\n",
    "print(\"=\" * 50)\n",
    "ragas_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577d5ce3",
   "metadata": {},
   "source": [
    "## Step 7: Configure RAGAS Evaluator\n",
    "\n",
    "RAGAS needs an LLM to evaluate responses. We'll use OpenAI's GPT model wrapped in RAGAS's Haystack wrapper. This LLM will act as the \"judge\" that scores our RAG system's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd20eb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Setting up RAGAS evaluator LLM...\n",
      "✅ Evaluator LLM configured successfully!\n",
      "💡 This LLM will act as the 'judge' for evaluating RAG performance\n"
     ]
    }
   ],
   "source": [
    "# Set up the evaluator LLM for RAGAS\n",
    "from ragas import evaluate\n",
    "from ragas.llms import HaystackLLMWrapper\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "\n",
    "print(\"🔧 Setting up RAGAS evaluator LLM...\")\n",
    "\n",
    "# Create evaluator LLM using OpenAI GPT model\n",
    "# Note: Using a smaller model to reduce API costs while maintaining quality\n",
    "evaluator_llm = HaystackLLMWrapper(OpenAIGenerator(model=\"gpt-4o-mini\"))\n",
    "\n",
    "print(\"✅ Evaluator LLM configured successfully!\")\n",
    "print(\"💡 This LLM will act as the 'judge' for evaluating RAG performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ef8817",
   "metadata": {},
   "source": [
    "## Step 8: Run Comprehensive RAGAS Evaluation\n",
    "\n",
    "Now for the main event! We'll run RAGAS evaluation using multiple metrics to get a comprehensive assessment of our RAG system.\n",
    "\n",
    "### Metrics We're Using:\n",
    "\n",
    "1. **LLMContextRecall**: How well did retrieval capture relevant information from the knowledge base?\n",
    "2. **Faithfulness**: Are the responses factually consistent with retrieved contexts?\n",
    "3. **FactualCorrectness**: Are the factual claims in responses accurate?\n",
    "4. **ResponseRelevancy**: How relevant are responses to the input questions?\n",
    "5. **ContextEntityRecall**: Does retrieval capture important entities mentioned in ground truth?\n",
    "6. **NoiseSensitivity**: How robust is the system to irrelevant context?\n",
    "\n",
    "⚠️ **Note**: This process uses API calls and may take several minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b46cc2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting comprehensive RAGAS evaluation...\n",
      "📊 Metrics to be computed:\n",
      "   • LLMContextRecall - Retrieval quality\n",
      "   • Faithfulness - Factual consistency\n",
      "   • FactualCorrectness - Accuracy of claims\n",
      "   • ResponseRelevancy - Answer relevance\n",
      "   • ContextEntityRecall - Entity coverage\n",
      "   • NoiseSensitivity - Robustness to noise\n",
      "\n",
      "⏱️ This may take several minutes... Please wait...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  10%|█         | 6/60 [00:13<01:48,  2.01s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  23%|██▎       | 14/60 [00:31<01:34,  2.05s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  32%|███▏      | 19/60 [00:41<01:24,  2.07s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  42%|████▏     | 25/60 [00:47<00:38,  1.09s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  50%|█████     | 30/60 [01:06<01:58,  3.94s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  60%|██████    | 36/60 [01:31<01:31,  3.80s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  72%|███████▏  | 43/60 [02:05<00:55,  3.26s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  82%|████████▏ | 49/60 [04:05<05:32, 30.25s/it]Exception raised in Job[0]: TimeoutError()\n",
      "Evaluating:  83%|████████▎ | 50/60 [06:00<09:13, 55.32s/it]Exception raised in Job[2]: TimeoutError()\n",
      "Exception raised in Job[5]: TimeoutError()\n",
      "Exception raised in Job[17]: TimeoutError()\n",
      "Evaluating:  88%|████████▊ | 53/60 [06:05<02:56, 25.23s/it]Exception raised in Job[36]: TimeoutError()\n",
      "Evaluating:  90%|█████████ | 54/60 [06:43<02:47, 27.93s/it]Exception raised in Job[38]: TimeoutError()\n",
      "Evaluating:  92%|█████████▏| 55/60 [06:45<01:50, 22.07s/it]Exception raised in Job[41]: TimeoutError()\n",
      "Evaluating:  93%|█████████▎| 56/60 [06:48<01:09, 17.34s/it]Exception raised in Job[47]: TimeoutError()\n",
      "Evaluating:  95%|█████████▌| 57/60 [07:11<00:55, 18.63s/it]Exception raised in Job[54]: TimeoutError()\n",
      "Evaluating:  97%|█████████▋| 58/60 [07:50<00:48, 24.37s/it]Exception raised in Job[56]: TimeoutError()\n",
      "Evaluating:  98%|█████████▊| 59/60 [08:03<00:21, 21.20s/it]Exception raised in Job[59]: TimeoutError()\n",
      "Evaluating: 100%|██████████| 60/60 [08:06<00:00,  8.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 Evaluation completed successfully!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'context_recall': 0.7714, 'faithfulness': 0.8888, 'factual_correctness(mode=f1)': 0.3086, 'answer_relevancy': 0.7632, 'context_entity_recall': 0.4553, 'noise_sensitivity(mode=relevant)': 0.3369}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import all the evaluation metrics we'll use\n",
    "from ragas.metrics import (\n",
    "    LLMContextRecall, \n",
    "    Faithfulness, \n",
    "    FactualCorrectness, \n",
    "    ResponseRelevancy, \n",
    "    ContextEntityRecall, \n",
    "    NoiseSensitivity\n",
    ")\n",
    "from ragas import evaluate, RunConfig\n",
    "\n",
    "print(\"🚀 Starting comprehensive RAGAS evaluation...\")\n",
    "print(\"📊 Metrics to be computed:\")\n",
    "print(\"   • LLMContextRecall - Retrieval quality\")\n",
    "print(\"   • Faithfulness - Factual consistency\")  \n",
    "print(\"   • FactualCorrectness - Accuracy of claims\")\n",
    "print(\"   • ResponseRelevancy - Answer relevance\")\n",
    "print(\"   • ContextEntityRecall - Entity coverage\")\n",
    "print(\"   • NoiseSensitivity - Robustness to noise\")\n",
    "print()\n",
    "print(\"⏱️ This may take several minutes... Please wait...\")\n",
    "\n",
    "# Configure evaluation settings with extended timeout\n",
    "custom_run_config = RunConfig(timeout=360)  # 6-minute timeout per evaluation\n",
    "\n",
    "# Run the comprehensive evaluation\n",
    "baseline_result = evaluate(\n",
    "    dataset=evaluation_dataset,\n",
    "    metrics=[\n",
    "        LLMContextRecall(), \n",
    "        Faithfulness(), \n",
    "        FactualCorrectness(), \n",
    "        ResponseRelevancy(), \n",
    "        ContextEntityRecall(), \n",
    "        NoiseSensitivity()\n",
    "    ],\n",
    "    llm=evaluator_llm,\n",
    "    run_config=custom_run_config\n",
    ")\n",
    "\n",
    "print(\"🎉 Evaluation completed successfully!\")\n",
    "baseline_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f01f0c0",
   "metadata": {},
   "source": [
    "## Step 9: Interpret Your Results\n",
    "\n",
    "Let's examine the evaluation results in detail. Each metric provides insights into different aspects of your RAG system's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1857ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 DETAILED EVALUATION RESULTS\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'context_recall': 0.7714, 'faithfulness': 0.8888, 'factual_correctness(mode=f1)': 0.3086, 'answer_relevancy': 0.7632, 'context_entity_recall': 0.4553, 'noise_sensitivity(mode=relevant)': 0.3369}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display detailed results with interpretation\n",
    "print(\"📊 DETAILED EVALUATION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "\n",
    "# Display the results\n",
    "baseline_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a8ea39",
   "metadata": {},
   "source": [
    "### 📈 Understanding Your RAGAS Scores\n",
    "*Based on official RAGAS documentation*\n",
    "\n",
    "**Score Interpretation Guide:**\n",
    "\n",
    "- **🟢 0.8-1.0**: Excellent performance\n",
    "- **🟡 0.6-0.8**: Good performance  \n",
    "- **🟠 0.4-0.6**: Fair performance - needs improvement\n",
    "- **🔴 0.0-0.4**: Poor performance - significant issues\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "\n",
    "## **Detailed Metric Explanations** 📊\n",
    "\n",
    "### **1. [Context Precision](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/context_precision/)** 🎯\n",
    "*Evaluates the retriever's ability to rank relevant chunks higher than irrelevant ones*\n",
    "\n",
    "**What it measures:** How well your system prioritizes relevant documents at the top of retrieval results.\n",
    "\n",
    "**Formula:** `Precision@K = (Relevant chunks at rank K) / (Total chunks at rank K)`\n",
    "\n",
    "**Score meaning:**\n",
    "- **High scores (>0.8)** = Relevant documents appear at the top of search results\n",
    "- **Low scores (<0.5)** = Important documents buried under irrelevant ones\n",
    "\n",
    "**Real example from RAGAS docs:**\n",
    "- Query: \"Where is the Eiffel Tower located?\"\n",
    "- Good retrieval: [\"Eiffel Tower is in Paris\", \"Brandenburg Gate is in Berlin\"] → Score: 0.99\n",
    "- Poor retrieval: [\"Brandenburg Gate is in Berlin\", \"Eiffel Tower is in Paris\"] → Score: 0.50\n",
    "\n",
    "---\n",
    "\n",
    "### **2. [Context Recall (LLMContextRecall)](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/context_recall/)** 🔍\n",
    "*Measures how many relevant documents were successfully retrieved*\n",
    "\n",
    "**What it measures:** Whether your system finds all the important information available in your knowledge base.\n",
    "\n",
    "**Formula:** `Context Recall = (Claims in reference supported by retrieved context) / (Total claims in reference)`\n",
    "\n",
    "**Score meaning:**\n",
    "- **High scores (>0.8)** = Your system finds most relevant information\n",
    "- **Low scores (<0.5)** = Missing important context from knowledge base\n",
    "\n",
    "**Key insight:** This metric focuses on \"not missing anything important\" - essential for comprehensive answers.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. [Context Entity Recall](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/context_entities_recall/)** 👥\n",
    "*Measures recall of important entities (people, places, dates, etc.)*\n",
    "\n",
    "**What it measures:** How well your retrieval captures key entities mentioned in the ground truth.\n",
    "\n",
    "**Formula:** `Entity Recall = (Common entities between retrieved & reference) / (Total entities in reference)`\n",
    "\n",
    "**Score meaning:**\n",
    "- **High scores (>0.8)** = Important entities (names, dates, places) are retrieved\n",
    "- **Low scores (<0.5)** = Missing key entities from context\n",
    "\n",
    "**Use case:** Especially important for fact-based applications like tourism help desks, historical QA.\n",
    "\n",
    "**Example from RAGAS:**\n",
    "- Reference entities: ['Taj Mahal', 'Yamuna', 'Agra', '1631', 'Shah Jahan', 'Mumtaz Mahal']\n",
    "- Retrieved entities: ['Taj Mahal', 'Agra', 'Shah Jahan', 'Mumtaz Mahal', 'India']\n",
    "- Score: 4/6 = 0.67\n",
    "\n",
    "---\n",
    "\n",
    "### **4. [Faithfulness](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/noise_sensitivity/)** 🤝\n",
    "*Measures how factually consistent responses are with retrieved context*\n",
    "\n",
    "**What it measures:** Whether your RAG system sticks to the facts or \"hallucinates\" information.\n",
    "\n",
    "**Formula:** `Faithfulness = (Claims supported by context) / (Total claims in response)`\n",
    "\n",
    "**Score meaning:**\n",
    "- **High scores (>0.8)** = Responses stick to retrieved facts\n",
    "- **Low scores (<0.5)** = System making unsupported claims or hallucinating\n",
    "\n",
    "**Example from RAGAS:**\n",
    "- Context: \"Einstein born 14 March 1879\"\n",
    "- Good response: \"Einstein was born in Germany on 14th March 1879\" → Score: 1.0\n",
    "- Poor response: \"Einstein was born in Germany on 20th March 1879\" → Score: 0.5\n",
    "\n",
    "---\n",
    "\n",
    "### **5. [Answer Relevance](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/answer_relevance/)y** 📌\n",
    "*Measures how relevant responses are to the input questions*\n",
    "\n",
    "**What it measures:** Whether your system directly answers what was asked.\n",
    "\n",
    "**How it works:** \n",
    "1. Generate artificial questions from the response\n",
    "2. Calculate similarity between original question and generated questions\n",
    "3. Higher similarity = more relevant response\n",
    "\n",
    "**Score meaning:**\n",
    "- **High scores (>0.8)** = Responses directly answer what's asked\n",
    "- **Low scores (<0.5)** = Responses are off-topic or incomplete\n",
    "\n",
    "**Example:**\n",
    "- Question: \"Where is France and what is its capital?\"\n",
    "- Poor answer: \"France is in western Europe\" → Lower relevancy\n",
    "- Good answer: \"France is in western Europe and Paris is its capital\" → Higher relevancy\n",
    "\n",
    "---\n",
    "\n",
    "### **6. [Noise Sensitivity](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/faithfulness/)** 🔇\n",
    "*Measures how often the system makes errors when using irrelevant context*\n",
    "\n",
    "**What it measures:** System robustness to irrelevant or noisy retrieved documents.\n",
    "\n",
    "**Formula:** `Noise Sensitivity = (Incorrect claims in response) / (Total claims in response)`\n",
    "\n",
    "**Score meaning (LOWER is better):**\n",
    "- **Low scores (<0.2)** = Handles irrelevant context well\n",
    "- **High scores (>0.5)** = Gets confused by noise in retrieved documents\n",
    "\n",
    "**Key insight:** Unlike other metrics, **lower scores indicate better performance** for noise sensitivity.\n",
    "\n",
    "---\n",
    "\n",
    "## **Improvement Strategies by Score Pattern** 🚀\n",
    "\n",
    "**If Context Precision/Recall are Low:**\n",
    "- Improve embedding models or add hybrid search\n",
    "- Tune retrieval parameters (similarity thresholds, chunk size)\n",
    "- Expand knowledge base coverage\n",
    "\n",
    "**If Faithfulness is Low:**\n",
    "- Add explicit instructions to stick to retrieved context\n",
    "- Implement fact-checking components  \n",
    "- Use stronger grounding techniques\n",
    "\n",
    "**If Response Relevancy is Low:**\n",
    "- Improve query understanding and processing\n",
    "- Add query classification for better routing\n",
    "- Enhance prompt engineering\n",
    "\n",
    "**If Noise Sensitivity is High:**\n",
    "- Improve context ranking and filtering\n",
    "- Add reranking components after retrieval\n",
    "- Implement context compression techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0516ce37",
   "metadata": {},
   "source": [
    "## 🎓 Congratulations!\n",
    "\n",
    "You've successfully completed a comprehensive RAGAS evaluation of your RAG system! \n",
    "\n",
    "**What you've accomplished:**\n",
    "\n",
    "✅ **Set up RAGAS framework** - Imported and configured the evaluation toolkit  \n",
    "✅ **Tested RAG systems** - Compared Hybrid vs Naive RAG approaches  \n",
    "✅ **Prepared evaluation data** - Formatted synthetic test data for systematic assessment  \n",
    "✅ **Ran comprehensive metrics** - Evaluated 6 key performance dimensions  \n",
    "✅ **Interpreted results** - Learned how to understand and act on RAGAS scores  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a59848d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-with-haystack-ch6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
