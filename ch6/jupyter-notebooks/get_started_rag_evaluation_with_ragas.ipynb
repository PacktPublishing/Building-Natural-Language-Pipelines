{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33c56ea9",
   "metadata": {},
   "source": [
    "üîß **Setup Required**: Before running this notebook, please follow the [setup instructions](../README.md#setup-instructions) to configure your environment and API keys. **You will need to ensure you've executed the Indexing pipeline before completing this exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2db02a",
   "metadata": {},
   "source": [
    "# RAGAS Evaluation Tutorial: A Complete Guide to RAG System Assessment\n",
    "\n",
    "This notebook provides a comprehensive walkthrough for evaluating Retrieval-Augmented Generation (RAG) systems using the RAGAS (RAG Assessment) framework. \n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "In this tutorial, you will:\n",
    "1. **Set up RAGAS** - Import necessary libraries and understand the evaluation framework\n",
    "2. **Test RAG Systems** - Run queries against different RAG implementations (Hybrid vs Naive)\n",
    "3. **Prepare Evaluation Data** - Format synthetic test data for RAGAS evaluation\n",
    "4. **Run Comprehensive Metrics** - Evaluate your RAG system using multiple assessment criteria\n",
    "5. **Interpret Results** - Understand what the metrics tell us about system performance\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic understanding of RAG (Retrieval-Augmented Generation) systems\n",
    "- Familiarity with Python and Pandas\n",
    "- OpenAI API key configured in your environment\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac590a75",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries and RAG System\n",
    "\n",
    "First, let's import the RAG system we'll be evaluating:\n",
    "- **Hybrid RAG**: Uses both dense and sparse retrieval methods for comprehensive document retrieval\n",
    "\n",
    "We'll also import utility functions for pretty-printing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "688e2412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scripts.rag.hybridrag import HybridRAGSuperComponent\n",
    "from scripts.rag.pretty_print import pretty_print_rag_answer\n",
    "from haystack_integrations.document_stores.elasticsearch import ElasticsearchDocumentStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdfad698",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store = ElasticsearchDocumentStore(hosts=\"http://localhost:9200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb3c1e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Hybrid RAG SuperComponent with base parameters\n",
    "hybrid_rag_sc = HybridRAGSuperComponent(\n",
    "    document_store=document_store\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "597dd6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Hybrid RAG system...\n"
     ]
    }
   ],
   "source": [
    "# Define a sample query to test the RAG system\n",
    "query = \"Who uses ChatGPT more, men or women, and how does this change by 2025\"\n",
    "\n",
    "# Run the Hybrid RAG system\n",
    "print(\"Running Hybrid RAG system...\")\n",
    "hybrid_answer = hybrid_rag_sc.run(query=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db4869f",
   "metadata": {},
   "source": [
    "## Step 2: Test Your RAG System with a Sample Query\n",
    "\n",
    "Before running evaluation metrics, let's test the RAG system with a sample query to understand its behavior. This helps us verify everything is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "711ad52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üîç HYBRID RAG ANSWER\n",
      "================================================================================\n",
      "üìù Query: Who uses ChatGPT more, men or women, and how does this change by 2025\n",
      "--------------------------------------------------------------------------------\n",
      "üí¨ Answer:\n",
      "   By the first half of 2025, the share of active users with typically\n",
      "   feminine and typically masculine names reaches near-parity. By June\n",
      "   2025, active users are more likely to have typically feminine names,\n",
      "   indicating that the gender gap in ChatGPT usage has closed substantially\n",
      "   over time. Initially, around 80% of weekly active users were those with\n",
      "   typically masculine names, but this changed as the user base evolved.\n",
      "\n",
      "üìö Source Documents (3 found):\n",
      "--------------------------------------------------\n",
      "1. Source: Unknown source\n",
      "   Preview: However, in the first half of 2025, we see the share of active users with typically feminine and typically\n",
      "masculine names reach near-parity. By June ...\n",
      "\n",
      "2. Source: Unknown source\n",
      "   Preview: Ouyang, Long, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela\n",
      "Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John S...\n",
      "\n",
      "3. Source: Unknown source\n",
      "   Preview: Overall, the majority of ChatGPT usage\n",
      "at work appears to be focused on two broad functions: 1) obtaining, documenting, and interpreting\n",
      "information; ...\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the Hybrid RAG results in a formatted way\n",
    "pretty_print_rag_answer(hybrid_answer, \"Hybrid RAG\", query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18addff",
   "metadata": {},
   "source": [
    "**Observation**: Notice the response quality from the system:\n",
    "- Answer completeness and accuracy\n",
    "- Retrieved context relevance\n",
    "- Response structure and clarity\n",
    "\n",
    "This manual review gives us intuition, but RAGAS provides systematic evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5c4db7",
   "metadata": {},
   "source": [
    "## Step 3: Introduction to RAGAS Framework\n",
    "\n",
    "**RAGAS** (RAG Assessment) is a framework designed to evaluate RAG systems comprehensively. It provides several key metrics:\n",
    "\n",
    "### Basic RAGAS Metrics We'll Use:\n",
    "\n",
    "1. **Faithfulness** - How factually consistent the answer is with the retrieved context\n",
    "2. **Answer Relevancy** - How relevant the answer is to the question\n",
    "\n",
    "These core metrics provide essential insights into RAG system performance while keeping evaluation simple and efficient.\n",
    "\n",
    "Let's import RAGAS and prepare our evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8703b16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAGAS imported successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Import RAGAS core modules\n",
    "from ragas import evaluation\n",
    "print(\"RAGAS imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589d6f28",
   "metadata": {},
   "source": [
    "## Step 4: Load and Examine Evaluation Dataset\n",
    "\n",
    "For systematic evaluation, we need a test dataset with:\n",
    "- **Questions** - Queries to test the RAG system\n",
    "- **Ground Truth Answers** - Expected correct responses  \n",
    "- **Reference Contexts** - Documents that should be retrieved\n",
    "\n",
    "We'll use a synthetic dataset that was pre-generated for evaluation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9050511f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading evaluation dataset...\n",
      "Dataset loaded successfully!\n",
      "Dataset contains 3 test cases\n",
      "Columns: ['user_input', 'reference_contexts', 'reference', 'synthesizer_name']\n"
     ]
    }
   ],
   "source": [
    "# Import additional required libraries\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from ragas import EvaluationDataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the synthetic evaluation dataset\n",
    "print(\"Loading evaluation dataset...\")\n",
    "dataset = pd.read_csv(\"./data_for_eval/synthetic_tests_advanced_branching_2.csv\")\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Dataset contains {len(dataset)} test cases\")\n",
    "print(f\"Columns: {list(dataset.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1aba46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examining the evaluation dataset structure:\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What role does TikTok play in the context of a...</td>\n",
       "      <td>[\"What is AI, how does it work and why are som...</td>\n",
       "      <td>TikTok, along with other social platforms like...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How does the use of automated classifiers ensu...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nPrivacy via Automated Classifiers...</td>\n",
       "      <td>The use of automated classifiers ensures priva...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How do Context-Augmented Message Classificatio...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\n‚Äù (truncated)\\n[user]: ‚Äú10 more‚Äù\\...</td>\n",
       "      <td>Context-Augmented Message Classifications ensu...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  What role does TikTok play in the context of a...   \n",
       "1  How does the use of automated classifiers ensu...   \n",
       "2  How do Context-Augmented Message Classificatio...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  [\"What is AI, how does it work and why are som...   \n",
       "1  ['<1-hop>\\n\\nPrivacy via Automated Classifiers...   \n",
       "2  ['<1-hop>\\n\\n‚Äù (truncated)\\n[user]: ‚Äú10 more‚Äù\\...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  TikTok, along with other social platforms like...   \n",
       "1  The use of automated classifiers ensures priva...   \n",
       "2  Context-Augmented Message Classifications ensu...   \n",
       "\n",
       "                        synthesizer_name  \n",
       "0  single_hop_specific_query_synthesizer  \n",
       "1   multi_hop_specific_query_synthesizer  \n",
       "2   multi_hop_abstract_query_synthesizer  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's examine the structure of our evaluation dataset\n",
    "print(\"Examining the evaluation dataset structure:\")\n",
    "print(\"=\" * 50)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92841178",
   "metadata": {},
   "source": [
    "## Step 5: Generate RAG Responses for Evaluation\n",
    "\n",
    "Now we'll run our RAG system on all test queries to generate responses that RAGAS can evaluate. This is where we collect the data needed for systematic assessment.\n",
    "\n",
    "**What we're doing:**\n",
    "- Run each test query through our Hybrid RAG system\n",
    "- Extract both the generated response and retrieved contexts\n",
    "- Store results in the format RAGAS expects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "881684d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Hybrid RAG on all evaluation queries...\n",
      "This may take a few minutes depending on dataset size...\n",
      "Successfully generated responses for 3 queries!\n",
      "Added columns: 'response', 'reference' (retrieved contexts)\n",
      "Final dataset shape: (3, 5)\n",
      "Successfully generated responses for 3 queries!\n",
      "Added columns: 'response', 'reference' (retrieved contexts)\n",
      "Final dataset shape: (3, 5)\n"
     ]
    }
   ],
   "source": [
    "# Create a helper function to run RAG system and extract needed data\n",
    "def run_rag_system(query, rag_system):\n",
    "    \"\"\"\n",
    "    Helper function to run a RAG system and return formatted response\n",
    "    \n",
    "    Args:\n",
    "        query: The question to ask the RAG system\n",
    "        rag_system: The RAG pipeline to use\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains the response text and retrieved document contexts\n",
    "    \"\"\"\n",
    "    response = rag_system.run(query=query)\n",
    "    return {\n",
    "        'response': response['replies'][0] if 'replies' in response else '',\n",
    "        'reference': [doc.content for doc in response.get('documents', [])]\n",
    "    }\n",
    "\n",
    "# Apply the hybrid RAG system to all test queries\n",
    "print(\"Running Hybrid RAG on all evaluation queries...\")\n",
    "print(\"This may take a few minutes depending on dataset size...\")\n",
    "\n",
    "hybrid_results = dataset['user_input'].apply(lambda query: run_rag_system(query, hybrid_rag_sc))\n",
    "\n",
    "# Extract response and retrieved contexts into separate columns\n",
    "dataset['response'] = hybrid_results.apply(lambda x: x['response'])\n",
    "dataset['reference'] = hybrid_results.apply(lambda x: x['reference'])\n",
    "\n",
    "print(f\"Successfully generated responses for {len(dataset)} queries!\")\n",
    "print(f\"Added columns: 'response', 'reference' (retrieved contexts)\")\n",
    "print(f\"Final dataset shape: {dataset.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cd4f46",
   "metadata": {},
   "source": [
    "## Step 6: Format Data for RAGAS Evaluation\n",
    "\n",
    "RAGAS expects data in a specific format called `SingleTurnSample`. We need to transform our dataset to match these requirements:\n",
    "\n",
    "**RAGAS Required Fields:**\n",
    "- `user_input` (str) - The question/query\n",
    "- `response` (str) - RAG system's answer  \n",
    "- `retrieved_contexts` (List[str]) - Documents retrieved by RAG\n",
    "- `reference` (str) - Ground truth answer\n",
    "\n",
    "Let's transform our data to match this schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c541dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset for RAGAS evaluation...\n",
      "Dataset successfully formatted for RAGAS!\n",
      "Final evaluation dataset shape: (3, 4)\n",
      "Columns: ['user_input', 'response', 'retrieved_contexts', 'reference']\n"
     ]
    }
   ],
   "source": [
    "# Transform dataset format for RAGAS SingleTurnSample requirements\n",
    "import ast\n",
    "\n",
    "def parse_contexts(context_str):\n",
    "    \"\"\"\n",
    "    Parse string representation of list to actual list\n",
    "    \n",
    "    Args:\n",
    "        context_str: String or list containing context information\n",
    "    \n",
    "    Returns:\n",
    "        list: Parsed context as a list of strings\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if isinstance(context_str, str):\n",
    "            return ast.literal_eval(context_str)\n",
    "        elif isinstance(context_str, list):\n",
    "            return context_str\n",
    "        else:\n",
    "            return []\n",
    "    except (ValueError, SyntaxError):\n",
    "        if isinstance(context_str, str):\n",
    "            return [context_str]\n",
    "        return []\n",
    "\n",
    "print(\"Preparing dataset for RAGAS evaluation...\")\n",
    "\n",
    "# Parse reference_contexts from string to list (ground truth contexts)\n",
    "dataset['reference_contexts_parsed'] = dataset['reference_contexts'].apply(parse_contexts)\n",
    "\n",
    "# Ensure user_input is a string, not a list\n",
    "dataset['user_input'] = dataset['user_input'].apply(\n",
    "    lambda x: x[0] if isinstance(x, list) else x\n",
    ")\n",
    "\n",
    "# Create the final RAGAS-compatible dataset\n",
    "ragas_dataset = pd.DataFrame({\n",
    "    'user_input': dataset['user_input'],           # Question/query as string\n",
    "    'response': dataset['response'],                # RAG response as string  \n",
    "    'retrieved_contexts': dataset['reference'],     # Retrieved contexts as list\n",
    "    'reference': dataset['reference_contexts_parsed'].apply(\n",
    "        lambda x: x[0] if x else \"\"\n",
    "    )  # Reference answer as string\n",
    "})\n",
    "\n",
    "print(f\"Dataset successfully formatted for RAGAS!\")\n",
    "print(f\"Final evaluation dataset shape: {ragas_dataset.shape}\")\n",
    "print(f\"Columns: {list(ragas_dataset.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a1274c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating RAGAS EvaluationDataset...\n",
      "EvaluationDataset created successfully!\n",
      "Dataset size: 3 samples\n",
      "Sample type: <class 'ragas.dataset_schema.SingleTurnSample'>\n",
      "Ready for evaluation!\n"
     ]
    }
   ],
   "source": [
    "# Create the RAGAS EvaluationDataset object\n",
    "from ragas import EvaluationDataset\n",
    "\n",
    "print(\"Creating RAGAS EvaluationDataset...\")\n",
    "\n",
    "# Create evaluation dataset with the properly formatted data\n",
    "evaluation_dataset = EvaluationDataset.from_pandas(ragas_dataset)\n",
    "\n",
    "print(\"EvaluationDataset created successfully!\")\n",
    "print(f\"Dataset size: {len(evaluation_dataset)} samples\")\n",
    "print(f\"Sample type: {evaluation_dataset.get_sample_type()}\")\n",
    "print(\"Ready for evaluation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88e3223e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final RAGAS dataset structure:\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>response</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What role does TikTok play in the context of a...</td>\n",
       "      <td>I don't have enough information to answer.</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>What is AI, how does it work and why are some ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How does the use of automated classifiers ensu...</td>\n",
       "      <td>The use of automated classifiers ensures priva...</td>\n",
       "      <td>[We describe the contents of each dataset, the...</td>\n",
       "      <td>&lt;1-hop&gt;\\n\\nPrivacy via Automated Classifiers.N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How do Context-Augmented Message Classificatio...</td>\n",
       "      <td>Context-Augmented Message Classifications ensu...</td>\n",
       "      <td>[‚Äù (truncated)\\n[user]: ‚Äú10 more‚Äù\\nTable 2:Ill...</td>\n",
       "      <td>&lt;1-hop&gt;\\n\\n‚Äù (truncated)\\n[user]: ‚Äú10 more‚Äù\\nT...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  What role does TikTok play in the context of a...   \n",
       "1  How does the use of automated classifiers ensu...   \n",
       "2  How do Context-Augmented Message Classificatio...   \n",
       "\n",
       "                                            response  \\\n",
       "0         I don't have enough information to answer.   \n",
       "1  The use of automated classifiers ensures priva...   \n",
       "2  Context-Augmented Message Classifications ensu...   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [What is AI, how does it work and why are some...   \n",
       "1  [We describe the contents of each dataset, the...   \n",
       "2  [‚Äù (truncated)\\n[user]: ‚Äú10 more‚Äù\\nTable 2:Ill...   \n",
       "\n",
       "                                           reference  \n",
       "0  What is AI, how does it work and why are some ...  \n",
       "1  <1-hop>\\n\\nPrivacy via Automated Classifiers.N...  \n",
       "2  <1-hop>\\n\\n‚Äù (truncated)\\n[user]: ‚Äú10 more‚Äù\\nT...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's examine our final dataset structure\n",
    "print(\"Final RAGAS dataset structure:\")\n",
    "print(\"=\" * 50)\n",
    "ragas_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577d5ce3",
   "metadata": {},
   "source": [
    "## Step 7: Configure RAGAS Evaluator\n",
    "\n",
    "RAGAS needs an LLM to evaluate responses. We'll use OpenAI's GPT 4o-mini model wrapped in RAGAS's Haystack wrapper. This LLM will act as the \"judge\" that scores our RAG system's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd20eb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up RAGAS evaluator LLM...\n",
      "Evaluator LLM configured successfully!\n",
      "This LLM will act as the 'judge' for evaluating RAG performance\n",
      "Evaluator LLM configured successfully!\n",
      "This LLM will act as the 'judge' for evaluating RAG performance\n"
     ]
    }
   ],
   "source": [
    "# Set up the evaluator LLM for RAGAS\n",
    "from ragas import evaluate\n",
    "from ragas.llms import HaystackLLMWrapper\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.utils import Secret\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "print(\"Setting up RAGAS evaluator LLM...\")\n",
    "\n",
    "model = OpenAIGenerator(model=\"gpt-4o-mini\",\n",
    "                        api_key=Secret.from_env_var(\"OPENAI_API_KEY\"))\n",
    "evaluator_llm = HaystackLLMWrapper(haystack_generator=model)\n",
    "\n",
    "print(\"Evaluator LLM configured successfully!\")\n",
    "print(\"This LLM will act as the 'judge' for evaluating RAG performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ef8817",
   "metadata": {},
   "source": [
    "## Step 8: Run Basic RAGAS Evaluation\n",
    "\n",
    "Now for the main event! We'll run RAGAS evaluation using basic metrics to get a focused assessment of our RAG system.\n",
    "\n",
    "### Basic Metrics We're Using:\n",
    "\n",
    "1. **Faithfulness**: Are the responses factually consistent with retrieved contexts?\n",
    "2. **Response Relevancy**: How relevant are responses to the input questions?\n",
    "3. **LLM Context Recall**: How well does the system retrieve relevant contexts that contain the ground truth answer?\n",
    "4. **Factual Correctness**: How factually accurate is the generated response compared to the ground truth?\n",
    "\n",
    "These core metrics provide essential insights while keeping evaluation simple and efficient.\n",
    "\n",
    "**Note**: This process uses API calls and may take several minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b46cc2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting basic RAGAS evaluation...\n",
      "Using core metrics: Faithfulness, Response Relevancy, LLM Context Recall, Factual Correctness\n",
      "\n",
      "Running evaluation with basic metrics...\n",
      "This may take several minutes... Please wait...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca3bfa83b9e54dc0aee767dbdf636cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation completed successfully!\n",
      "\n",
      "RAGAS Basic Evaluation Results:\n",
      "==================================================\n",
      "{'faithfulness': 0.6275, 'answer_relevancy': 0.6473, 'context_recall': 0.9471, 'factual_correctness(mode=f1)': 0.3633}\n"
     ]
    }
   ],
   "source": [
    "# Import basic evaluation metrics\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, ResponseRelevancy\n",
    "from ragas import evaluate, RunConfig\n",
    "\n",
    "print(\"Starting basic RAGAS evaluation...\")\n",
    "print(\"Using core metrics: Faithfulness, Response Relevancy, LLM Context Recall, Factual Correctness\")\n",
    "print()\n",
    "\n",
    "# Use basic metrics only\n",
    "basic_metrics = [\n",
    "    Faithfulness(), \n",
    "    ResponseRelevancy(),\n",
    "    LLMContextRecall(),\n",
    "    FactualCorrectness()\n",
    "]\n",
    "\n",
    "print(\"Running evaluation with basic metrics...\")\n",
    "print(\"This may take several minutes... Please wait...\")\n",
    "\n",
    "try:\n",
    "    # Create evaluation dataset\n",
    "    evaluation_dataset = EvaluationDataset.from_pandas(ragas_dataset)\n",
    "    \n",
    "    # Configure evaluation settings with extended timeout\n",
    "    custom_run_config = RunConfig(timeout=360)\n",
    "    \n",
    "    # Run the evaluation\n",
    "    result = evaluate(\n",
    "        dataset=evaluation_dataset,\n",
    "        metrics=basic_metrics,\n",
    "        llm=evaluator_llm,\n",
    "        run_config=custom_run_config\n",
    "    )\n",
    "    \n",
    "    print(\"Evaluation completed successfully!\")\n",
    "    print(\"\\nRAGAS Basic Evaluation Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(result)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Evaluation failed with error: {e}\")\n",
    "    print(f\"Error type: {type(e).__name__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f01f0c0",
   "metadata": {},
   "source": [
    "## Step 9: Interpret Your Results\n",
    "\n",
    "Let's examine the evaluation results in detail. Each metric provides insights into different aspects of your RAG system's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1857ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed results with interpretation\n",
    "print(\"BASIC RAGAS EVALUATION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a8ea39",
   "metadata": {},
   "source": [
    "### Understanding Your Basic RAGAS Scores\n",
    "*Simplified interpretation of core metrics*\n",
    "\n",
    "## **Basic Metric Explanations**\n",
    "\n",
    "### **1. [Faithfulness](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/faithfulness/)**\n",
    "*Measures how factually consistent responses are with retrieved context*\n",
    "\n",
    "**What it measures:** Whether your RAG system sticks to the facts or \"hallucinates\" information.\n",
    "\n",
    "**Formula:** `Faithfulness = (Claims supported by context) / (Total claims in response)`\n",
    "\n",
    "**Score meaning:**\n",
    "- **High scores (>0.8)** = Responses stick to retrieved facts\n",
    "- **Medium scores (0.5-0.8)** = Generally accurate with some unsupported claims\n",
    "- **Low scores (<0.5)** = System making many unsupported claims or hallucinating\n",
    "\n",
    "**Example:**\n",
    "- Context: \"Einstein born 14 March 1879\"\n",
    "- Good response: \"Einstein was born in Germany on 14th March 1879\" ‚Üí Score: 1.0\n",
    "- Poor response: \"Einstein was born in Germany on 20th March 1879\" ‚Üí Score: 0.5\n",
    "\n",
    "---\n",
    "\n",
    "### **2. [Response Relevancy](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/answer_relevance/)**\n",
    "*Measures how relevant responses are to the input questions*\n",
    "\n",
    "**What it measures:** Whether your system directly answers what was asked.\n",
    "\n",
    "**How it works:** \n",
    "1. Generate artificial questions from the response\n",
    "2. Calculate similarity between original question and generated questions\n",
    "3. Higher similarity = more relevant response\n",
    "\n",
    "**Score meaning:**\n",
    "- **High scores (>0.8)** = Responses directly answer what's asked\n",
    "- **Medium scores (0.5-0.8)** = Generally relevant with some tangential content\n",
    "- **Low scores (<0.5)** = Responses are off-topic or incomplete\n",
    "\n",
    "**Example:**\n",
    "- Question: \"Where is France and what is its capital?\"\n",
    "- Poor answer: \"France is in western Europe\" ‚Üí Lower relevancy (incomplete)\n",
    "- Good answer: \"France is in western Europe and Paris is its capital\" ‚Üí Higher relevancy\n",
    "\n",
    "---\n",
    "\n",
    "### **3. [LLM Context Recall](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/context_recall/)**\n",
    "*Measures how well the retrieval system finds contexts containing the ground truth answer*\n",
    "\n",
    "**What it measures:** The effectiveness of your retrieval component in finding relevant information.\n",
    "\n",
    "**How it works:**\n",
    "1. Compare retrieved contexts against the ground truth reference answer\n",
    "2. Determine what proportion of the ground truth is present in retrieved contexts\n",
    "3. Higher scores = better retrieval coverage\n",
    "\n",
    "**Score meaning:**\n",
    "- **High scores (>0.8)** = Retrieval finds most relevant documents\n",
    "- **Medium scores (0.5-0.8)** = Some relevant documents missed\n",
    "- **Low scores (<0.5)** = Retrieval is missing critical information\n",
    "\n",
    "**Example:**\n",
    "- Ground truth: \"Paris is the capital of France, located on the Seine River\"\n",
    "- Good retrieval: Contains documents about Paris, France, and the Seine ‚Üí High recall\n",
    "- Poor retrieval: Only general France info, nothing about Paris ‚Üí Low recall\n",
    "\n",
    "---\n",
    "\n",
    "### **4. [Factual Correctness](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/factual_correctness/)**\n",
    "*Measures factual accuracy of the generated response compared to ground truth*\n",
    "\n",
    "**What it measures:** How well the generated answer matches the expected correct answer.\n",
    "\n",
    "**How it works:**\n",
    "1. Extract factual claims from both the generated response and ground truth\n",
    "2. Compare claims for correctness and completeness\n",
    "3. Calculate overlap and accuracy\n",
    "\n",
    "**Score meaning:**\n",
    "- **High scores (>0.8)** = Response contains correct facts matching ground truth\n",
    "- **Medium scores (0.5-0.8)** = Some facts correct, some missing or incorrect\n",
    "- **Low scores (<0.5)** = Response has many factual errors or omissions\n",
    "\n",
    "**Example:**\n",
    "- Ground truth: \"The Eiffel Tower is 330 meters tall and was built in 1889\"\n",
    "- Good response: \"The Eiffel Tower stands at 330 meters and was constructed in 1889\" ‚Üí High correctness\n",
    "- Poor response: \"The Eiffel Tower is about 300 meters tall and was built in the 1880s\" ‚Üí Lower correctness\n",
    "\n",
    "---\n",
    "\n",
    "## **Improvement Strategies by Score Pattern**\n",
    "\n",
    "**If Faithfulness is Low:**\n",
    "- Add explicit instructions to stick to retrieved context\n",
    "- Implement fact-checking components  \n",
    "- Use stronger grounding techniques in prompts\n",
    "- Review and improve document quality in knowledge base\n",
    "\n",
    "**If Response Relevancy is Low:**\n",
    "- Improve query understanding and processing\n",
    "- Add query classification for better routing\n",
    "- Enhance prompt engineering to focus on question requirements\n",
    "- Consider query expansion or reformulation techniques\n",
    "\n",
    "**If LLM Context Recall is Low:**\n",
    "- Improve retrieval strategy (adjust embedding model, search parameters)\n",
    "- Increase number of retrieved documents (top_k parameter)\n",
    "- Enhance document chunking strategy\n",
    "- Consider hybrid search (dense + sparse retrieval)\n",
    "- Improve document preprocessing and indexing\n",
    "\n",
    "**If Factual Correctness is Low:**\n",
    "- Improve prompt engineering to emphasize accuracy\n",
    "- Use stronger/larger generation models\n",
    "- Implement retrieval result reranking\n",
    "- Add fact verification steps\n",
    "- Ensure ground truth data quality is high\n",
    "\n",
    "**General Improvements:**\n",
    "- **All scores low**: Review overall RAG architecture and data quality\n",
    "- **All scores high**: System is performing well - consider advanced metrics for fine-tuning\n",
    "- **Mixed scores**: Focus on the lowest-performing area first\n",
    "- **High Recall but Low Faithfulness**: Generation model not using retrieved context properly\n",
    "- **Low Recall but High Faithfulness**: Good generation but poor retrieval - fix retrieval first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0516ce37",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You've successfully completed a basic RAGAS evaluation of your RAG system! \n",
    "\n",
    "**What you've accomplished:**\n",
    "\n",
    "- **Set up RAGAS framework** - Imported and configured the evaluation toolkit  \n",
    "- **Tested RAG system** - Ran queries against the Hybrid RAG implementation  \n",
    "- **Prepared evaluation data** - Formatted synthetic test data for systematic assessment  \n",
    "- **Ran basic metrics** - Evaluated your RAG system using core performance measures  \n",
    "- **Interpreted results** - Learned how to understand and act on RAGAS scores  \n",
    "\n",
    "## **Next Steps**\n",
    "\n",
    "Now that you have basic evaluation working, you can:\n",
    "\n",
    "1. **Expand metrics** - Add more RAGAS metrics like Context Recall and Context Precision\n",
    "2. **Compare systems** - Evaluate different RAG implementations side by side  \n",
    "3. **Iterate and improve** - Use these insights to enhance your RAG system\n",
    "4. **Automate evaluation** - Integrate RAGAS into your development pipeline\n",
    "\n",
    "Great job on completing this essential RAG evaluation workflow!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a59848d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-with-haystack-ch6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
