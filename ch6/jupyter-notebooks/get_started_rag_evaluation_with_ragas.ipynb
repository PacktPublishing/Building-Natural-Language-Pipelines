{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33c56ea9",
   "metadata": {},
   "source": [
    "üîß **Setup Required**: Before running this notebook, please follow the [setup instructions](../README.md#setup-instructions) to configure your environment and API keys. **You will need to ensure you've executed the Indexing pipeline before completing this exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2db02a",
   "metadata": {},
   "source": [
    "# RAGAS Evaluation Tutorial: A Complete Guide to RAG System Assessment\n",
    "\n",
    "This notebook provides a comprehensive walkthrough for evaluating Retrieval-Augmented Generation (RAG) systems using the RAGAS (RAG Assessment) framework. \n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "In this tutorial, you will:\n",
    "1. **Set up RAGAS** - Import necessary libraries and understand the evaluation framework\n",
    "2. **Test RAG Systems** - Run queries against different RAG implementations (Hybrid vs Naive)\n",
    "3. **Prepare Evaluation Data** - Format synthetic test data for RAGAS evaluation\n",
    "4. **Run Comprehensive Metrics** - Evaluate your RAG system using multiple assessment criteria\n",
    "5. **Interpret Results** - Understand what the metrics tell us about system performance\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic understanding of RAG (Retrieval-Augmented Generation) systems\n",
    "- Familiarity with Python and Pandas\n",
    "- OpenAI API key configured in your environment\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac590a75",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries and RAG System\n",
    "\n",
    "First, let's import the RAG system we'll be evaluating:\n",
    "- **Hybrid RAG**: Uses both dense and sparse retrieval methods for comprehensive document retrieval\n",
    "\n",
    "We'll also import utility functions for pretty-printing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "688e2412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scripts.rag.hybridrag import HybridRAGSuperComponent\n",
    "from scripts.rag.naiverag import NaiveRAGSuperComponent\n",
    "from scripts.rag.pretty_print import pretty_print_rag_answer\n",
    "from haystack_integrations.document_stores.elasticsearch import ElasticsearchDocumentStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdfad698",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store = ElasticsearchDocumentStore(hosts=\"http://localhost:9200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3c1e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Hybrid RAG SuperComponent with base parameters\n",
    "hybrid_rag_sc = HybridRAGSuperComponent(\n",
    "    document_store=document_store\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597dd6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Running Hybrid RAG system...\n"
     ]
    }
   ],
   "source": [
    "# Define a sample query to test the RAG system\n",
    "query = \"Who uses ChatGPT more, men or women, and how does this change by 2025\"\n",
    "\n",
    "# Run the Hybrid RAG system\n",
    "print(\"üîÑ Running Hybrid RAG system...\")\n",
    "hybrid_answer = hybrid_rag_sc.run(query=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db4869f",
   "metadata": {},
   "source": [
    "## Step 2: Test Your RAG System with a Sample Query\n",
    "\n",
    "Before running evaluation metrics, let's test the RAG system with a sample query to understand its behavior. This helps us verify everything is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "711ad52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üîç HYBRID RAG ANSWER\n",
      "================================================================================\n",
      "üìù Query: Who uses ChatGPT more, men or women, and how does this change by 2025\n",
      "--------------------------------------------------------------------------------\n",
      "üí¨ Answer:\n",
      "   By 2025, the share of active users with typically masculine and\n",
      "   typically feminine names has reached near-parity in their usage of\n",
      "   ChatGPT. However, by June 2025, it is observed that active users are\n",
      "   more likely to have typically feminine names. This indicates that the\n",
      "   gender gaps in ChatGPT usage have closed substantially over time,\n",
      "   suggesting that women have become more prevalent users of ChatGPT\n",
      "   compared to men.\n",
      "\n",
      "üìö Source Documents (3 found):\n",
      "--------------------------------------------------\n",
      "1. Source: Unknown source\n",
      "   Preview: However, in the first half of 2025, we see the share of active users with typically feminine and typically\n",
      "masculine names reach near-parity. By June ...\n",
      "\n",
      "2. Source: Unknown source\n",
      "   Preview: Ouyang, Long, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela\n",
      "Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John S...\n",
      "\n",
      "3. Source: Unknown source\n",
      "   Preview: Overall, the majority of ChatGPT usage\n",
      "at work appears to be focused on two broad functions: 1) obtaining, documenting, and interpreting\n",
      "information; ...\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the Hybrid RAG results in a formatted way\n",
    "pretty_print_rag_answer(hybrid_answer, \"Hybrid RAG\", query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18addff",
   "metadata": {},
   "source": [
    "**üí° Observation**: Notice the response quality from the system:\n",
    "- Answer completeness and accuracy\n",
    "- Retrieved context relevance\n",
    "- Response structure and clarity\n",
    "\n",
    "This manual review gives us intuition, but RAGAS provides systematic evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5c4db7",
   "metadata": {},
   "source": [
    "## Step 3: Introduction to RAGAS Framework\n",
    "\n",
    "**RAGAS** (RAG Assessment) is a framework designed to evaluate RAG systems comprehensively. It provides several key metrics:\n",
    "\n",
    "### Basic RAGAS Metrics We'll Use:\n",
    "\n",
    "1. **Faithfulness** - How factually consistent the answer is with the retrieved context\n",
    "2. **Answer Relevancy** - How relevant the answer is to the question\n",
    "\n",
    "These core metrics provide essential insights into RAG system performance while keeping evaluation simple and efficient.\n",
    "\n",
    "Let's import RAGAS and prepare our evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8703b16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAGAS imported successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Import RAGAS core modules\n",
    "from ragas import evaluation\n",
    "print(\"‚úÖ RAGAS imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589d6f28",
   "metadata": {},
   "source": [
    "## Step 4: Load and Examine Evaluation Dataset\n",
    "\n",
    "For systematic evaluation, we need a test dataset with:\n",
    "- **Questions** - Queries to test the RAG system\n",
    "- **Ground Truth Answers** - Expected correct responses  \n",
    "- **Reference Contexts** - Documents that should be retrieved\n",
    "\n",
    "We'll use a synthetic dataset that was pre-generated for evaluation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9050511f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loading evaluation dataset...\n",
      "‚úÖ Dataset loaded successfully!\n",
      "üìà Dataset contains 3 test cases\n",
      "üîç Columns: ['user_input', 'reference_contexts', 'reference', 'synthesizer_name']\n"
     ]
    }
   ],
   "source": [
    "# Import additional required libraries\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from ragas import EvaluationDataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the synthetic evaluation dataset\n",
    "print(\"üìä Loading evaluation dataset...\")\n",
    "dataset = pd.read_csv(\"./data_for_eval/synthetic_tests_advanced_branching_2.csv\")\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded successfully!\")\n",
    "print(f\"üìà Dataset contains {len(dataset)} test cases\")\n",
    "print(f\"üîç Columns: {list(dataset.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aba46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Examining the evaluation dataset structure:\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wut is Alexa and how does it use AI?</td>\n",
       "      <td>[\"What is AI, how does it work and why are som...</td>\n",
       "      <td>Alexa is a voice-controlled virtual assistant ...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What trends in user engagement with ChatGPT we...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\n4 The Growth of ChatGPT\\nChatGPT ...</td>\n",
       "      <td>In 2023, the first cohort of ChatGPT users exp...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the implications of artificial intell...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\n27-28.\\n- Christian Davenport, ‚Äú ...</td>\n",
       "      <td>The implications of artificial intelligence on...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0               Wut is Alexa and how does it use AI?   \n",
       "1  What trends in user engagement with ChatGPT we...   \n",
       "2  What are the implications of artificial intell...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  [\"What is AI, how does it work and why are som...   \n",
       "1  ['<1-hop>\\n\\n4 The Growth of ChatGPT\\nChatGPT ...   \n",
       "2  ['<1-hop>\\n\\n27-28.\\n- Christian Davenport, ‚Äú ...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  Alexa is a voice-controlled virtual assistant ...   \n",
       "1  In 2023, the first cohort of ChatGPT users exp...   \n",
       "2  The implications of artificial intelligence on...   \n",
       "\n",
       "                        synthesizer_name  \n",
       "0  single_hop_specific_query_synthesizer  \n",
       "1   multi_hop_specific_query_synthesizer  \n",
       "2   multi_hop_abstract_query_synthesizer  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's examine the structure of our evaluation dataset\n",
    "print(\"üîç Examining the evaluation dataset structure:\")\n",
    "print(\"=\" * 50)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92841178",
   "metadata": {},
   "source": [
    "## Step 5: Generate RAG Responses for Evaluation\n",
    "\n",
    "Now we'll run our RAG system on all test queries to generate responses that RAGAS can evaluate. This is where we collect the data needed for systematic assessment.\n",
    "\n",
    "**What we're doing:**\n",
    "- Run each test query through our Hybrid RAG system\n",
    "- Extract both the generated response and retrieved contexts\n",
    "- Store results in the format RAGAS expects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881684d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Running Hybrid RAG on all evaluation queries...\n",
      "This may take a few minutes depending on dataset size...\n",
      "‚úÖ Successfully generated responses for 3 queries!\n",
      "üìä Added columns: 'response', 'reference' (retrieved contexts)\n",
      "üìà Final dataset shape: (3, 5)\n",
      "‚úÖ Successfully generated responses for 3 queries!\n",
      "üìä Added columns: 'response', 'reference' (retrieved contexts)\n",
      "üìà Final dataset shape: (3, 5)\n"
     ]
    }
   ],
   "source": [
    "# Create a helper function to run RAG system and extract needed data\n",
    "def run_rag_system(query, rag_system):\n",
    "    \"\"\"\n",
    "    Helper function to run a RAG system and return formatted response\n",
    "    \n",
    "    Args:\n",
    "        query: The question to ask the RAG system\n",
    "        rag_system: The RAG pipeline to use\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains the response text and retrieved document contexts\n",
    "    \"\"\"\n",
    "    response = rag_system.run(query=query)\n",
    "    return {\n",
    "        'response': response['replies'][0] if 'replies' in response else '',\n",
    "        'reference': [doc.content for doc in response.get('documents', [])]\n",
    "    }\n",
    "\n",
    "# Apply the hybrid RAG system to all test queries\n",
    "print(\"üîÑ Running Hybrid RAG on all evaluation queries...\")\n",
    "print(\"This may take a few minutes depending on dataset size...\")\n",
    "\n",
    "hybrid_results = dataset['user_input'].apply(lambda query: run_rag_system(query, hybrid_rag_sc))\n",
    "\n",
    "# Extract response and retrieved contexts into separate columns\n",
    "dataset['response'] = hybrid_results.apply(lambda x: x['response'])\n",
    "dataset['reference'] = hybrid_results.apply(lambda x: x['reference'])\n",
    "\n",
    "print(f\"‚úÖ Successfully generated responses for {len(dataset)} queries!\")\n",
    "print(f\"üìä Added columns: 'response', 'reference' (retrieved contexts)\")\n",
    "print(f\"üìà Final dataset shape: {dataset.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cd4f46",
   "metadata": {},
   "source": [
    "## Step 6: Format Data for RAGAS Evaluation\n",
    "\n",
    "RAGAS expects data in a specific format called `SingleTurnSample`. We need to transform our dataset to match these requirements:\n",
    "\n",
    "**RAGAS Required Fields:**\n",
    "- `user_input` (str) - The question/query\n",
    "- `response` (str) - RAG system's answer  \n",
    "- `retrieved_contexts` (List[str]) - Documents retrieved by RAG\n",
    "- `reference` (str) - Ground truth answer\n",
    "\n",
    "Let's transform our data to match this schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c541dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Preparing dataset for RAGAS evaluation...\n",
      "‚úÖ Dataset successfully formatted for RAGAS!\n",
      "üìä Final evaluation dataset shape: (3, 4)\n",
      "üîç Columns: ['user_input', 'response', 'retrieved_contexts', 'reference']\n"
     ]
    }
   ],
   "source": [
    "# Transform dataset format for RAGAS SingleTurnSample requirements\n",
    "import ast\n",
    "\n",
    "def parse_contexts(context_str):\n",
    "    \"\"\"\n",
    "    Parse string representation of list to actual list\n",
    "    \n",
    "    Args:\n",
    "        context_str: String or list containing context information\n",
    "    \n",
    "    Returns:\n",
    "        list: Parsed context as a list of strings\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if isinstance(context_str, str):\n",
    "            return ast.literal_eval(context_str)\n",
    "        elif isinstance(context_str, list):\n",
    "            return context_str\n",
    "        else:\n",
    "            return []\n",
    "    except (ValueError, SyntaxError):\n",
    "        if isinstance(context_str, str):\n",
    "            return [context_str]\n",
    "        return []\n",
    "\n",
    "print(\"üîÑ Preparing dataset for RAGAS evaluation...\")\n",
    "\n",
    "# Parse reference_contexts from string to list (ground truth contexts)\n",
    "dataset['reference_contexts_parsed'] = dataset['reference_contexts'].apply(parse_contexts)\n",
    "\n",
    "# Ensure user_input is a string, not a list\n",
    "dataset['user_input'] = dataset['user_input'].apply(\n",
    "    lambda x: x[0] if isinstance(x, list) else x\n",
    ")\n",
    "\n",
    "# Create the final RAGAS-compatible dataset\n",
    "ragas_dataset = pd.DataFrame({\n",
    "    'user_input': dataset['user_input'],           # Question/query as string\n",
    "    'response': dataset['response'],                # RAG response as string  \n",
    "    'retrieved_contexts': dataset['reference'],     # Retrieved contexts as list\n",
    "    'reference': dataset['reference_contexts_parsed'].apply(\n",
    "        lambda x: x[0] if x else \"\"\n",
    "    )  # Reference answer as string\n",
    "})\n",
    "\n",
    "print(f\"‚úÖ Dataset successfully formatted for RAGAS!\")\n",
    "print(f\"üìä Final evaluation dataset shape: {ragas_dataset.shape}\")\n",
    "print(f\"üîç Columns: {list(ragas_dataset.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a1274c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Creating RAGAS EvaluationDataset...\n",
      "‚úÖ EvaluationDataset created successfully!\n",
      "üìä Dataset size: 3 samples\n",
      "üîß Sample type: <class 'ragas.dataset_schema.SingleTurnSample'>\n",
      "üéØ Ready for evaluation!\n"
     ]
    }
   ],
   "source": [
    "# Create the RAGAS EvaluationDataset object\n",
    "from ragas import EvaluationDataset\n",
    "\n",
    "print(\"üîÑ Creating RAGAS EvaluationDataset...\")\n",
    "\n",
    "# Create evaluation dataset with the properly formatted data\n",
    "evaluation_dataset = EvaluationDataset.from_pandas(ragas_dataset)\n",
    "\n",
    "print(\"‚úÖ EvaluationDataset created successfully!\")\n",
    "print(f\"üìä Dataset size: {len(evaluation_dataset)} samples\")\n",
    "print(f\"üîß Sample type: {evaluation_dataset.get_sample_type()}\")\n",
    "print(\"üéØ Ready for evaluation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88e3223e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Final RAGAS dataset structure:\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>response</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wut is Alexa and how does it use AI?</td>\n",
       "      <td>Alexa is a voice-controlled virtual assistant ...</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>What is AI, how does it work and why are some ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What trends in user engagement with ChatGPT we...</td>\n",
       "      <td>In 2023, the user engagement with ChatGPT, spe...</td>\n",
       "      <td>[The yellow line represents the first cohort o...</td>\n",
       "      <td>&lt;1-hop&gt;\\n\\n4 The Growth of ChatGPT\\nChatGPT wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the implications of artificial intell...</td>\n",
       "      <td>I don't have enough information to answer.</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>&lt;1-hop&gt;\\n\\n27-28.\\n- Christian Davenport, ‚Äú Fu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0               Wut is Alexa and how does it use AI?   \n",
       "1  What trends in user engagement with ChatGPT we...   \n",
       "2  What are the implications of artificial intell...   \n",
       "\n",
       "                                            response  \\\n",
       "0  Alexa is a voice-controlled virtual assistant ...   \n",
       "1  In 2023, the user engagement with ChatGPT, spe...   \n",
       "2         I don't have enough information to answer.   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [What is AI, how does it work and why are some...   \n",
       "1  [The yellow line represents the first cohort o...   \n",
       "2  [What is AI, how does it work and why are some...   \n",
       "\n",
       "                                           reference  \n",
       "0  What is AI, how does it work and why are some ...  \n",
       "1  <1-hop>\\n\\n4 The Growth of ChatGPT\\nChatGPT wa...  \n",
       "2  <1-hop>\\n\\n27-28.\\n- Christian Davenport, ‚Äú Fu...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's examine our final dataset structure\n",
    "print(\"üîç Final RAGAS dataset structure:\")\n",
    "print(\"=\" * 50)\n",
    "ragas_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577d5ce3",
   "metadata": {},
   "source": [
    "## Step 7: Configure RAGAS Evaluator\n",
    "\n",
    "RAGAS needs an LLM to evaluate responses. We'll use OpenAI's GPT model wrapped in RAGAS's Haystack wrapper. This LLM will act as the \"judge\" that scores our RAG system's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd20eb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Setting up RAGAS evaluator LLM...\n",
      "‚úÖ Evaluator LLM configured successfully!\n",
      "üí° This LLM will act as the 'judge' for evaluating RAG performance\n"
     ]
    }
   ],
   "source": [
    "# Set up the evaluator LLM for RAGAS\n",
    "from ragas import evaluate\n",
    "from ragas.llms import HaystackLLMWrapper\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "\n",
    "print(\"üîß Setting up RAGAS evaluator LLM...\")\n",
    "\n",
    "# Create evaluator LLM using OpenAI GPT model\n",
    "# Note: Using a smaller model to reduce API costs while maintaining quality\n",
    "evaluator_llm = HaystackLLMWrapper(OpenAIGenerator(model=\"gpt-4o-mini\"))\n",
    "\n",
    "print(\"‚úÖ Evaluator LLM configured successfully!\")\n",
    "print(\"üí° This LLM will act as the 'judge' for evaluating RAG performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ef8817",
   "metadata": {},
   "source": [
    "## Step 8: Run Basic RAGAS Evaluation\n",
    "\n",
    "Now for the main event! We'll run RAGAS evaluation using basic metrics to get a focused assessment of our RAG system.\n",
    "\n",
    "### Basic Metrics We're Using:\n",
    "\n",
    "1. **Faithfulness**: Are the responses factually consistent with retrieved contexts?\n",
    "2. **Response Relevancy**: How relevant are responses to the input questions?\n",
    "\n",
    "These core metrics provide essential insights while keeping evaluation simple and efficient.\n",
    "\n",
    "‚ö†Ô∏è **Note**: This process uses API calls and may take several minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46cc2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting comprehensive RAGAS evaluation...\n",
      "üìä Using stable metrics that work well with the current RAGAS version\n",
      "\n",
      "‚è±Ô∏è Running evaluation with stable metrics...\n",
      "This may take several minutes... Please wait...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de3f2e7c4ab145d68cb0b1339d6a4b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ Evaluation completed successfully!\n",
      "\n",
      "üìä RAGAS Evaluation Results:\n",
      "==================================================\n",
      "{'faithfulness': 0.6190, 'answer_relevancy': 0.6378, 'context_recall': 0.8333, 'factual_correctness(mode=f1)': 0.3167}\n"
     ]
    }
   ],
   "source": [
    "# Import basic evaluation metrics\n",
    "from ragas.metrics import Faithfulness, ResponseRelevancy\n",
    "from ragas import evaluate, RunConfig\n",
    "\n",
    "print(\"üöÄ Starting basic RAGAS evaluation...\")\n",
    "print(\"üìä Using core metrics: Faithfulness and Response Relevancy\")\n",
    "print()\n",
    "\n",
    "# Use basic metrics only\n",
    "basic_metrics = [\n",
    "    Faithfulness(), \n",
    "    ResponseRelevancy()\n",
    "]\n",
    "\n",
    "print(\"‚è±Ô∏è Running evaluation with basic metrics...\")\n",
    "print(\"This may take several minutes... Please wait...\")\n",
    "\n",
    "try:\n",
    "    # Create evaluation dataset\n",
    "    evaluation_dataset = EvaluationDataset.from_pandas(ragas_dataset)\n",
    "    \n",
    "    # Configure evaluation settings with extended timeout\n",
    "    custom_run_config = RunConfig(timeout=360)\n",
    "    \n",
    "    # Run the evaluation\n",
    "    result = evaluate(\n",
    "        dataset=evaluation_dataset,\n",
    "        metrics=basic_metrics,\n",
    "        llm=evaluator_llm,\n",
    "        run_config=custom_run_config\n",
    "    )\n",
    "    \n",
    "    print(\"üéâ Evaluation completed successfully!\")\n",
    "    print(\"\\nüìä RAGAS Basic Evaluation Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(result)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Evaluation failed with error: {e}\")\n",
    "    print(f\"Error type: {type(e).__name__}\")\n",
    "    \n",
    "    # Create a dummy result for demonstration\n",
    "    print(\"\\n‚ö†Ô∏è Creating dummy results for demonstration purposes...\")\n",
    "    result = {\n",
    "        'faithfulness': 0.75,\n",
    "        'answer_relevancy': 0.68,\n",
    "    }\n",
    "    print(\"Note: These are placeholder values - actual evaluation failed.\")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f01f0c0",
   "metadata": {},
   "source": [
    "## Step 9: Interpret Your Results\n",
    "\n",
    "Let's examine the evaluation results in detail. Each metric provides insights into different aspects of your RAG system's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1857ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä DETAILED EVALUATION RESULTS\n",
      "==================================================\n",
      "\n",
      "{'faithfulness': 0.6190, 'answer_relevancy': 0.6393, 'context_recall': 0.7273, 'factual_correctness(mode=f1)': 0.3167}\n"
     ]
    }
   ],
   "source": [
    "# Display detailed results with interpretation\n",
    "print(\"üìä BASIC RAGAS EVALUATION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a8ea39",
   "metadata": {},
   "source": [
    "### üìà Understanding Your Basic RAGAS Scores\n",
    "*Simplified interpretation of core metrics*\n",
    "\n",
    "## **Basic Metric Explanations** üìä\n",
    "\n",
    "### **1. [Faithfulness](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/faithfulness/)** ü§ù\n",
    "*Measures how factually consistent responses are with retrieved context*\n",
    "\n",
    "**What it measures:** Whether your RAG system sticks to the facts or \"hallucinates\" information.\n",
    "\n",
    "**Formula:** `Faithfulness = (Claims supported by context) / (Total claims in response)`\n",
    "\n",
    "**Score meaning:**\n",
    "- **High scores (>0.8)** = Responses stick to retrieved facts\n",
    "- **Medium scores (0.5-0.8)** = Generally accurate with some unsupported claims\n",
    "- **Low scores (<0.5)** = System making many unsupported claims or hallucinating\n",
    "\n",
    "**Example:**\n",
    "- Context: \"Einstein born 14 March 1879\"\n",
    "- Good response: \"Einstein was born in Germany on 14th March 1879\" ‚Üí Score: 1.0\n",
    "- Poor response: \"Einstein was born in Germany on 20th March 1879\" ‚Üí Score: 0.5\n",
    "\n",
    "---\n",
    "\n",
    "### **2. [Response Relevancy](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/answer_relevance/)** üìå\n",
    "*Measures how relevant responses are to the input questions*\n",
    "\n",
    "**What it measures:** Whether your system directly answers what was asked.\n",
    "\n",
    "**How it works:** \n",
    "1. Generate artificial questions from the response\n",
    "2. Calculate similarity between original question and generated questions\n",
    "3. Higher similarity = more relevant response\n",
    "\n",
    "**Score meaning:**\n",
    "- **High scores (>0.8)** = Responses directly answer what's asked\n",
    "- **Medium scores (0.5-0.8)** = Generally relevant with some tangential content\n",
    "- **Low scores (<0.5)** = Responses are off-topic or incomplete\n",
    "\n",
    "**Example:**\n",
    "- Question: \"Where is France and what is its capital?\"\n",
    "- Poor answer: \"France is in western Europe\" ‚Üí Lower relevancy (incomplete)\n",
    "- Good answer: \"France is in western Europe and Paris is its capital\" ‚Üí Higher relevancy\n",
    "\n",
    "---\n",
    "\n",
    "## **Improvement Strategies by Score Pattern** üöÄ\n",
    "\n",
    "**If Faithfulness is Low:**\n",
    "- Add explicit instructions to stick to retrieved context\n",
    "- Implement fact-checking components  \n",
    "- Use stronger grounding techniques in prompts\n",
    "- Review and improve document quality in knowledge base\n",
    "\n",
    "**If Response Relevancy is Low:**\n",
    "- Improve query understanding and processing\n",
    "- Add query classification for better routing\n",
    "- Enhance prompt engineering to focus on question requirements\n",
    "- Consider query expansion or reformulation techniques\n",
    "\n",
    "**General Improvements:**\n",
    "- **Both scores low**: Review overall RAG architecture and data quality\n",
    "- **Both scores high**: System is performing well - consider advanced metrics for fine-tuning\n",
    "- **Mixed scores**: Focus on the lower-performing area first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0516ce37",
   "metadata": {},
   "source": [
    "## üéì Congratulations!\n",
    "\n",
    "You've successfully completed a basic RAGAS evaluation of your RAG system! \n",
    "\n",
    "**What you've accomplished:**\n",
    "\n",
    "‚úÖ **Set up RAGAS framework** - Imported and configured the evaluation toolkit  \n",
    "‚úÖ **Tested RAG system** - Ran queries against the Hybrid RAG implementation  \n",
    "‚úÖ **Prepared evaluation data** - Formatted synthetic test data for systematic assessment  \n",
    "‚úÖ **Ran basic metrics** - Evaluated your RAG system using core performance measures  \n",
    "‚úÖ **Interpreted results** - Learned how to understand and act on RAGAS scores  \n",
    "\n",
    "## **Next Steps** üöÄ\n",
    "\n",
    "Now that you have basic evaluation working, you can:\n",
    "\n",
    "1. **Expand metrics** - Add more RAGAS metrics like Context Recall and Context Precision\n",
    "2. **Compare systems** - Evaluate different RAG implementations side by side  \n",
    "3. **Iterate and improve** - Use these insights to enhance your RAG system\n",
    "4. **Automate evaluation** - Integrate RAGAS into your development pipeline\n",
    "\n",
    "Great job on completing this essential RAG evaluation workflow!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a59848d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-with-haystack-ch6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
