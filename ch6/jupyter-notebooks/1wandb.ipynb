{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba5f0310",
   "metadata": {},
   "source": [
    "# Hybrid RAG Optimization with Weights & Biases üìäüî¨\n",
    "\n",
    "## üìã Overview\n",
    "\n",
    "This notebook focuses on using **Weights & Biases (W&B)** to systematically evaluate and optimize our **Hybrid RAG** system. We will run two experiments: a baseline and an optimized configuration, with the goal of improving the `faithfulness` metric while monitoring cost.\n",
    "\n",
    "### Hybrid RAG System Configuration (to be optimized):\n",
    "\n",
    "| Parameter | Baseline Value | Optimization Target |\n",
    "| :--- | :--- | :--- |\n",
    "| `embedder_model` | `sentence-transformers/all-MiniLM-L6-v2` | Fixed |\n",
    "| `llm_model` | `gpt-4o-mini` | Fixed |\n",
    "| **`retriever_top_k`** | **3** | **7** |\n",
    "| `rag_type` | `hybrid` | Fixed |\n",
    "| `reranker_model` | `BAAI/bge-reranker-base` | Fixed |\n",
    "| `bm25_enabled` | `True` | Fixed |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup: Imports and Environment variables loaded.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import tiktoken\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# Import Haystack/Ragas components\n",
    "from haystack import Pipeline\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, ResponseRelevancy\n",
    "\n",
    "# Import custom components (assuming these paths exist relative to the notebook)\n",
    "try:\n",
    "    from scripts.ragasevaluation import CSVReaderComponent, RAGDataAugmenterComponent, RagasEvaluationComponent\n",
    "    from scripts.rag.hybridrag import hybrid_rag_sc\n",
    "except ImportError:\n",
    "    print(\"WARNING: Custom components could not be imported. Ensure 'scripts/ragasevaluation.py' and 'scripts/rag/hybridrag.py' are available.\")\n",
    "\n",
    "# Environment setup (reduced logging)\n",
    "os.environ[\"HAYSTACK_CONTENT_TRACING_ENABLED\"] = \"false\"\n",
    "print(\"Setup: Imports and Environment variables loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ragevaluationexperiment_class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGEvaluationExperiment:\n",
    "    \"\"\"Enhanced RAG evaluation workflow with streamlined W&B integration.\"\"\"\n",
    "    \n",
    "    def __init__(self, project_name: str, experiment_name: str):\n",
    "        self.project_name = project_name\n",
    "        self.experiment_name = experiment_name\n",
    "        self.run = None\n",
    "        self.pipeline = None\n",
    "    \n",
    "    def setup_pipeline(self, rag_supercomponent, metrics_list: list, config: dict = None):\n",
    "        \"\"\"Set up the evaluation pipeline with W&B tracking.\"\"\"\n",
    "        self.run = wandb.init(\n",
    "            project=self.project_name,\n",
    "            name=self.experiment_name,\n",
    "            config=config,\n",
    "            reinit=True\n",
    "        )\n",
    "        print(f\"W&B STARTED: {self.experiment_name} | URL: {self.run.url}\")\n",
    "        \n",
    "        # Initialize components\n",
    "        reader = CSVReaderComponent()\n",
    "        # The RAGDataAugmenterComponent needs access to the current config (specifically top_k)\n",
    "        # We assume the underlying implementation of hybrid_rag_sc takes the config or is already correctly set up.\n",
    "        # Since the goal is only to change the parameter in the run config, we leave the RAG SuperComponent initialization simple here.\n",
    "        # The actual RAG configuration modification logic should ideally be inside RAGDataAugmenterComponent/hybrid_rag_sc.\n",
    "        # For this minimal example, we assume the wrapper logic or environment variables handle the parameter change for hybrid_rag_sc based on 'config'.\n",
    "        augmenter = RAGDataAugmenterComponent(rag_supercomponent=rag_supercomponent)\n",
    "        evaluator = RagasEvaluationComponent(metrics=metrics_list)\n",
    "        \n",
    "        self.pipeline = Pipeline()\n",
    "        self.pipeline.add_component(\"reader\", reader)\n",
    "        self.pipeline.add_component(\"augmenter\", augmenter)\n",
    "        self.pipeline.add_component(\"evaluator\", evaluator)\n",
    "        \n",
    "        self.pipeline.connect(\"reader.data_frame\", \"augmenter.data_frame\")\n",
    "        self.pipeline.connect(\"augmenter.augmented_data_frame\", \"evaluator.augmented_data_frame\")\n",
    "        \n",
    "        return self.pipeline\n",
    "    \n",
    "    def run_evaluation(self, csv_file_path: str):\n",
    "        \"\"\"Execute the pipeline, log high-level metrics, and return results.\"\"\"\n",
    "        if not self.pipeline:\n",
    "            raise ValueError(\"Pipeline not set up. Call setup_pipeline() first.\")\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        print(f\"\\nRunning pipeline on {csv_file_path}...\")\n",
    "        results = self.pipeline.run({\"reader\": {\"source\": csv_file_path}})\n",
    "        end_time = datetime.now()\n",
    "        \n",
    "        execution_time = (end_time - start_time).total_seconds()\n",
    "        metrics = results[\"evaluator\"][\"metrics\"]\n",
    "        evaluation_df = results[\"evaluator\"][\"evaluation_df\"].rename(columns={\n",
    "            'factual_correctness(mode=f1)': 'factual_correctness_f1'\n",
    "        })\n",
    "        \n",
    "        # Log dataset artifact\n",
    "        dataset_artifact = wandb.Artifact(name=f\"evaluation-dataset-{Path(csv_file_path).stem}\", type=\"dataset\")\n",
    "        dataset_artifact.add_file(csv_file_path)\n",
    "        self.run.log_artifact(dataset_artifact)\n",
    "        \n",
    "        # Extract and log summary metrics\n",
    "        wandb_metrics = {\n",
    "            \"execution_time_seconds\": execution_time,\n",
    "            \"num_queries_evaluated\": len(evaluation_df),\n",
    "        }\n",
    "        # Simple conversion of Ragas EvaluationResult metrics to flat dictionary\n",
    "        if hasattr(metrics, 'to_dict'):\n",
    "            metrics_dict = metrics.to_dict()\n",
    "            for metric_name, metric_value in metrics_dict.items():\n",
    "                if isinstance(metric_value, (int, float)):\n",
    "                    # Standardize metric names for W&B comparison\n",
    "                    clean_name = metric_name.replace('(mode=f1)', '').replace('ragas_', '').strip()\n",
    "                    wandb_metrics[f\"ragas_{clean_name}\"] = metric_value\n",
    "        \n",
    "        self.run.log(wandb_metrics)\n",
    "        print(f\"Evaluation Complete: Logged {len(evaluation_df)} queries and {len(wandb_metrics)} metrics.\")\n",
    "        \n",
    "        return {\n",
    "            \"metrics\": metrics, # Full EvaluationResult object\n",
    "            \"evaluation_df\": evaluation_df,\n",
    "            \"execution_time\": execution_time,\n",
    "            \"wandb_url\": self.run.url\n",
    "        }\n",
    "    \n",
    "    def finish_experiment(self):\n",
    "        \"\"\"Finish the W&B run.\"\"\"\n",
    "        if self.run:\n",
    "            url = self.run.url\n",
    "            self.run.finish()\n",
    "            print(f\"\\nW&B COMPLETED: {self.experiment_name} | View Results: {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raganalytics_class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGAnalytics:\n",
    "    \"\"\"Simplified analytics class for RAG evaluation results and W&B logging.\"\"\"\n",
    "    \n",
    "    def __init__(self, results: Dict[str, Any], model_name: str = \"gpt-4o-mini\"):\n",
    "        self.results = results\n",
    "        self.model_name = model_name\n",
    "        self.evaluation_df = results['evaluation_df']\n",
    "        \n",
    "        # Token pricing (approximate costs per 1K tokens as of 2024 for demonstration)\n",
    "        self.pricing = {\n",
    "            \"gpt-4o-mini\": {\"input\": 0.00015, \"output\": 0.0006},\n",
    "            \"gpt-4o\": {\"input\": 0.005, \"output\": 0.015},\n",
    "            \"gpt-3.5-turbo\": {\"input\": 0.0005, \"output\": 0.0015}\n",
    "        }\n",
    "        # Initialize tokenizer\n",
    "        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        \n",
    "        self.token_usage = self._calculate_token_usage()\n",
    "        self.costs = self._calculate_costs()\n",
    "    \n",
    "    def _calculate_token_usage(self) -> Dict[str, List[int]]:\n",
    "        \"\"\"Calculate token usage based on context and response lengths.\"\"\"\n",
    "        input_tokens = []\n",
    "        output_tokens = []\n",
    "        \n",
    "        for _, row in self.evaluation_df.iterrows():\n",
    "            input_text = row['user_input']\n",
    "            if 'retrieved_contexts' in row and row['retrieved_contexts']:\n",
    "                context_text = \" \".join(row['retrieved_contexts'])\n",
    "                input_text += \" \" + context_text\n",
    "            \n",
    "            input_tokens.append(len(self.tokenizer.encode(input_text)))\n",
    "            \n",
    "            output_text = row['response'] if 'response' in row else \"\"\n",
    "            output_tokens.append(len(self.tokenizer.encode(output_text)))\n",
    "        \n",
    "        return {\n",
    "            \"input_tokens\": input_tokens,\n",
    "            \"output_tokens\": output_tokens,\n",
    "            \"total_tokens\": [i + o for i, o in zip(input_tokens, output_tokens)]\n",
    "        }\n",
    "    \n",
    "    def _calculate_costs(self) -> Dict[str, List[float]]:\n",
    "        \"\"\"Calculate estimated API costs per query.\"\"\"\n",
    "        model_pricing = self.pricing.get(self.model_name, self.pricing[\"gpt-4o-mini\"])\n",
    "        \n",
    "        input_costs = [(tokens / 1000) * model_pricing[\"input\"] for tokens in self.token_usage[\"input_tokens\"]]\n",
    "        output_costs = [(tokens / 1000) * model_pricing[\"output\"] for tokens in self.token_usage[\"output_tokens\"]]\n",
    "        total_costs = [i + o for i, o in zip(input_costs, output_costs)]\n",
    "        \n",
    "        return {\"input_costs\": input_costs, \"output_costs\": output_costs, \"total_costs\": total_costs}\n",
    "        \n",
    "    def log_to_wandb(self, run: wandb.init) -> Dict[str, Any]:\n",
    "        \"\"\"Calculates summary analytics and logs to W&B with meaningful plots.\"\"\"\n",
    "        total_cost = sum(self.costs['total_costs'])\n",
    "        total_tokens = sum(self.token_usage['total_tokens'])\n",
    "        num_queries = len(self.token_usage['total_tokens'])\n",
    "        \n",
    "        summary_metrics = {\n",
    "            \"total_cost_usd\": total_cost,\n",
    "            \"average_cost_per_query_usd\": np.mean(self.costs['total_costs']),\n",
    "            \"average_tokens_per_query\": np.mean(self.token_usage['total_tokens']),\n",
    "            \"token_efficiency_tps_per_dollar\": total_tokens / total_cost if total_cost > 0 else 0,\n",
    "        }\n",
    "        \n",
    "        # Prepare DataFrame for logging\n",
    "        analysis_df = self.evaluation_df.copy()\n",
    "        analysis_df['total_cost_usd'] = self.costs['total_costs']\n",
    "        analysis_df['total_tokens'] = self.token_usage['total_tokens']\n",
    "        analysis_df['input_tokens'] = self.token_usage['input_tokens']\n",
    "        \n",
    "        # Log detailed table (top 10)\n",
    "        run.log({\"detailed_query_analysis\": wandb.Table(dataframe=analysis_df.head(10))})\n",
    "        \n",
    "    #     # Log meaningful charts\n",
    "    #     run.log({\n",
    "    #         # PLOT 1: Main Tradeoff - Performance (Faithfulness) vs Cost (Scatter Plot)\n",
    "    #         \"faithfulness_vs_cost\": wandb.plot.scatter(\n",
    "    #             wandb.Table(dataframe=analysis_df.rename(columns={'faithfulness': 'faithfulness_score'})), \n",
    "    # # Assuming 'faithfulness' column exists\n",
    "    #             \"total_cost_usd\", \n",
    "    #             \"faithfulness_score\", \n",
    "    #             title=\"Performance (Faithfulness) vs Cost (USD)\"\n",
    "    #         ),\n",
    "            \n",
    "    #         # PLOT 2: Cost Driver Analysis (Scatter Plot)\n",
    "    #         \"cost_vs_input_tokens\": wandb.plot.scatter(\n",
    "    #             wandb.Table(dataframe=analysis_df.rename(columns={'input_tokens': 'context_tokens'})), \n",
    "    #             \"context_tokens\", \n",
    "    #             \"total_cost_usd\", \n",
    "    #             title=\"Cost (USD) vs Input Tokens (Context Size)\"\n",
    "    #         ),\n",
    "            \n",
    "    #         # PLOT 3: Performance Distribution (Histogram)\n",
    "    #         \"faithfulness_distribution\": wandb.plot.histogram(\n",
    "    #             wandb.Table(dataframe=analysis_df.rename(columns={'faithfulness': 'faithfulness_score'})), \n",
    "    #             \"faithfulness_score\", \n",
    "    #             title=\"Faithfulness Score Distribution\"\n",
    "    #         ),\n",
    "            \n",
    "    #         # PLOT 4: Performance Trend (Line Plot) - sequential evaluation\n",
    "    #         \"faithfulness_per_query_id\": wandb.plot.line(\n",
    "    #             wandb.Table(data=[[i, score] for i, score in enumerate(analysis_df['faithfulness'].tolist())], columns=[\"query_id\", \"faithfulness_score\"]),\n",
    "    #             \"query_id\", \n",
    "    #             \"faithfulness_score\", \n",
    "    #             title=\"Faithfulness Score per Query (Sequential)\"\n",
    "    #         )\n",
    "    #     })\n",
    "        \n",
    "        # Log summary metrics to the run\n",
    "        run.log(summary_metrics)\n",
    "        \n",
    "        print(f\"Analytics: Logged comprehensive analysis for {num_queries} queries.\")\n",
    "        return summary_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610afd8a",
   "metadata": {},
   "source": [
    "## üî¨ Experiment 1: Hybrid RAG Baseline (`retriever_top_k=3`)\n",
    "\n",
    "We establish a performance baseline for the Hybrid RAG system using a retrieval limit of `top_k=3` documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "run_1_setup",
   "metadata": {
    "tags": [
     "setup"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlgutierrwr\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/laurafunderburk/Documents/GitHub/Building-Natural-Language-Pipelines/ch6/jupyter-notebooks/wandb/run-20251103_113000-rgdzj2vg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lgutierrwr/hybrid-rag-optimization/runs/rgdzj2vg' target=\"_blank\">hybrid-rag-baseline-k3</a></strong> to <a href='https://wandb.ai/lgutierrwr/hybrid-rag-optimization' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lgutierrwr/hybrid-rag-optimization' target=\"_blank\">https://wandb.ai/lgutierrwr/hybrid-rag-optimization</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lgutierrwr/hybrid-rag-optimization/runs/rgdzj2vg' target=\"_blank\">https://wandb.ai/lgutierrwr/hybrid-rag-optimization/runs/rgdzj2vg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [instructor, openai] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Weave is installed but not imported. Add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B STARTED: hybrid-rag-baseline-k3 | URL: https://wandb.ai/lgutierrwr/hybrid-rag-optimization/runs/rgdzj2vg\n",
      "\n",
      "Running pipeline on data_for_eval/synthetic_tests_advanced_branching_3.csv...\n",
      "Loaded DataFrame with 4 rows from data_for_eval/synthetic_tests_advanced_branching_3.csv.\n",
      "Running RAG SuperComponent on 4 queries...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d387e8b2bbfa4231be9e52d426f9dbdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3bd63db59e1443ba051d3d8da58c09e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "460636d95e7d4f19b35f7087ff176c1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e08d02fe2154ec6b7ab0315fc6e2db1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG processing complete.\n",
      "Creating Ragas EvaluationDataset...\n",
      "Starting Ragas evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10ebfa5329784410999e17d64f0255e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ragas evaluation complete.\n",
      "Overall metrics: {'faithfulness': 0.7500, 'context_recall': 1.0000, 'factual_correctness(mode=f1)': 0.2925, 'answer_relevancy': 0.2323}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Complete: Logged 4 queries and 2 metrics.\n",
      "Analytics: Logged comprehensive analysis for 4 queries.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_cost_per_query_usd</td><td>‚ñÅ</td></tr><tr><td>average_tokens_per_query</td><td>‚ñÅ</td></tr><tr><td>execution_time_seconds</td><td>‚ñÅ</td></tr><tr><td>num_queries_evaluated</td><td>‚ñÅ</td></tr><tr><td>token_efficiency_tps_per_dollar</td><td>‚ñÅ</td></tr><tr><td>total_cost_usd</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_cost_per_query_usd</td><td>0.00194</td></tr><tr><td>average_tokens_per_query</td><td>12783.25</td></tr><tr><td>execution_time_seconds</td><td>40.11649</td></tr><tr><td>num_queries_evaluated</td><td>4</td></tr><tr><td>token_efficiency_tps_per_dollar</td><td>6595083.32044</td></tr><tr><td>total_cost_usd</td><td>0.00775</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">hybrid-rag-baseline-k3</strong> at: <a href='https://wandb.ai/lgutierrwr/hybrid-rag-optimization/runs/rgdzj2vg' target=\"_blank\">https://wandb.ai/lgutierrwr/hybrid-rag-optimization/runs/rgdzj2vg</a><br> View project at: <a href='https://wandb.ai/lgutierrwr/hybrid-rag-optimization' target=\"_blank\">https://wandb.ai/lgutierrwr/hybrid-rag-optimization</a><br>Synced 4 W&B file(s), 5 media file(s), 10 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251103_113000-rgdzj2vg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "W&B COMPLETED: hybrid-rag-baseline-k3 | View Results: https://wandb.ai/lgutierrwr/hybrid-rag-optimization/runs/rgdzj2vg\n"
     ]
    }
   ],
   "source": [
    "# 1. Define evaluation metrics (Focusing on core RAGAS metrics)\n",
    "evaluation_metrics = [Faithfulness(), LLMContextRecall(), FactualCorrectness(), ResponseRelevancy()]\n",
    "csv_file_path = \"data_for_eval/synthetic_tests_advanced_branching_3.csv\"\n",
    "\n",
    "# 2. Configuration for the Baseline Experiment\n",
    "baseline_config = {\n",
    "    \"embedder_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \"llm_model\": \"gpt-4o-mini\",\n",
    "    \"retriever_top_k\": 3,  # Baseline value\n",
    "    \"rag_type\": \"hybrid\",\n",
    "    \"document_store\": \"elasticsearch\",\n",
    "    \"reranker_model\": \"BAAI/bge-reranker-base\",\n",
    "}\n",
    "\n",
    "# 3. Initialize and Setup Baseline Experiment\n",
    "baseline_experiment = RAGEvaluationExperiment(\n",
    "    project_name=\"hybrid-rag-optimization\",\n",
    "    experiment_name=\"hybrid-rag-baseline-k3\"\n",
    ")\n",
    "\n",
    "baseline_pipeline = baseline_experiment.setup_pipeline(\n",
    "    rag_supercomponent=hybrid_rag_sc, \n",
    "    metrics_list=evaluation_metrics,\n",
    "    config=baseline_config\n",
    ")\n",
    "\n",
    "# 4. Run the evaluation and store results\n",
    "baseline_results = baseline_experiment.run_evaluation(\n",
    "    csv_file_path=csv_file_path\n",
    ")\n",
    "\n",
    "# 5. Run Analytics and log to W&B\n",
    "baseline_analytics = RAGAnalytics(baseline_results, model_name=baseline_config['llm_model'])\n",
    "baseline_summary = baseline_analytics.log_to_wandb(baseline_experiment.run)\n",
    "\n",
    "# 6. Finish the experiment run\n",
    "baseline_experiment.finish_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d0130f",
   "metadata": {},
   "source": [
    "## üöÄ Experiment 2: Hybrid RAG Optimization (`retriever_top_k=7`)\n",
    "\n",
    "To potentially improve context recall and faithfulness, we'll increase the number of retrieved documents from 3 to 7. This is a common **hyperparameter tuning** strategy to check the tradeoff between performance and cost/latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "run_2_setup",
   "metadata": {
    "tags": [
     "optimization"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/laurafunderburk/Documents/GitHub/Building-Natural-Language-Pipelines/ch6/jupyter-notebooks/wandb/run-20251103_113142-jqdbpb5v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lgutierrwr/hybrid-rag-optimization/runs/jqdbpb5v' target=\"_blank\">hybrid-rag-optimized-k7</a></strong> to <a href='https://wandb.ai/lgutierrwr/hybrid-rag-optimization' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lgutierrwr/hybrid-rag-optimization' target=\"_blank\">https://wandb.ai/lgutierrwr/hybrid-rag-optimization</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lgutierrwr/hybrid-rag-optimization/runs/jqdbpb5v' target=\"_blank\">https://wandb.ai/lgutierrwr/hybrid-rag-optimization/runs/jqdbpb5v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference, instructor, openai] in use.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B STARTED: hybrid-rag-optimized-k7 | URL: https://wandb.ai/lgutierrwr/hybrid-rag-optimization/runs/jqdbpb5v\n",
      "\n",
      "Running pipeline on data_for_eval/synthetic_tests_advanced_branching_3.csv...\n",
      "Loaded DataFrame with 4 rows from data_for_eval/synthetic_tests_advanced_branching_3.csv.\n",
      "Running RAG SuperComponent on 4 queries...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0f1da6e02e9443db8c7a7ff6e4fcc01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a4e552515f5419a80870350ad12ca85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcf00bd47dc24b898dd6ff85b0456cc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef1d86fd9a0543febabc1f490189b9d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG processing complete.\n",
      "Creating Ragas EvaluationDataset...\n",
      "Starting Ragas evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec25b3e1b02141b49b3bf5baf304479e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ragas evaluation complete.\n",
      "Overall metrics: {'faithfulness': 0.5000, 'context_recall': 0.9167, 'factual_correctness(mode=f1)': 0.2500, 'answer_relevancy': 0.2320}\n",
      "Evaluation Complete: Logged 4 queries and 2 metrics.\n",
      "Analytics: Logged comprehensive analysis for 4 queries.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_cost_per_query_usd</td><td>‚ñÅ</td></tr><tr><td>average_tokens_per_query</td><td>‚ñÅ</td></tr><tr><td>execution_time_seconds</td><td>‚ñÅ</td></tr><tr><td>num_queries_evaluated</td><td>‚ñÅ</td></tr><tr><td>token_efficiency_tps_per_dollar</td><td>‚ñÅ</td></tr><tr><td>total_cost_usd</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_cost_per_query_usd</td><td>0.00193</td></tr><tr><td>average_tokens_per_query</td><td>12771.25</td></tr><tr><td>execution_time_seconds</td><td>29.03698</td></tr><tr><td>num_queries_evaluated</td><td>4</td></tr><tr><td>token_efficiency_tps_per_dollar</td><td>6613458.65051</td></tr><tr><td>total_cost_usd</td><td>0.00772</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">hybrid-rag-optimized-k7</strong> at: <a href='https://wandb.ai/lgutierrwr/hybrid-rag-optimization/runs/jqdbpb5v' target=\"_blank\">https://wandb.ai/lgutierrwr/hybrid-rag-optimization/runs/jqdbpb5v</a><br> View project at: <a href='https://wandb.ai/lgutierrwr/hybrid-rag-optimization' target=\"_blank\">https://wandb.ai/lgutierrwr/hybrid-rag-optimization</a><br>Synced 4 W&B file(s), 5 media file(s), 10 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251103_113142-jqdbpb5v/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "W&B COMPLETED: hybrid-rag-optimized-k7 | View Results: https://wandb.ai/lgutierrwr/hybrid-rag-optimization/runs/jqdbpb5v\n"
     ]
    }
   ],
   "source": [
    "# 1. Configuration for the Optimized Experiment (change top_k)\n",
    "optimized_config = baseline_config.copy()\n",
    "optimized_config['retriever_top_k'] = 7  # Optimized value\n",
    "\n",
    "# 2. Initialize and Setup Optimized Experiment\n",
    "optimized_experiment = RAGEvaluationExperiment(\n",
    "    project_name=\"hybrid-rag-optimization\",\n",
    "    experiment_name=\"hybrid-rag-optimized-k7\"\n",
    ")\n",
    "\n",
    "optimized_pipeline = optimized_experiment.setup_pipeline(\n",
    "    rag_supercomponent=hybrid_rag_sc, \n",
    "    metrics_list=evaluation_metrics,\n",
    "    config=optimized_config\n",
    ")\n",
    "\n",
    "# 3. Run the evaluation and store results\n",
    "optimized_results = optimized_experiment.run_evaluation(\n",
    "    csv_file_path=csv_file_path\n",
    ")\n",
    "\n",
    "# 4. Run Analytics and log to W&B\n",
    "optimized_analytics = RAGAnalytics(optimized_results, model_name=optimized_config['llm_model'])\n",
    "optimized_summary = optimized_analytics.log_to_wandb(optimized_experiment.run)\n",
    "\n",
    "# 5. Finish the experiment run\n",
    "optimized_experiment.finish_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd620687",
   "metadata": {},
   "source": [
    "## üìä Comparative Analysis & Key Insights\n",
    "\n",
    "Now we can programmatically compare the key metrics between the two runs. The full comparison is available in the W&B dashboard, but a quick summary confirms the tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "comparison_analysis",
   "metadata": {
    "tags": [
     "summary"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================\n",
      "Hybrid RAG Optimization: Comparison Summary\n",
      "================================================\n",
      "                 retriever_top_k  Execution Time (s)  Avg Cost (USD)  Avg Tokens/Query\n",
      "System                                                                                \n",
      "Baseline (k=3)                 3             40.1165          0.0019        12783.2500\n",
      "Optimized (k=7)                7             29.0370          0.0019        12771.2500\n",
      "\n",
      "Key Performance Changes:\n",
      "\n",
      "--- Tradeoffs ---\n",
      "  Execution Time (s): -27.62% Change\n",
      "  Avg Cost (USD): -0.37% Change\n",
      "  Avg Tokens/Query: -0.09% Change\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/laurafunderburk/Documents/GitHub/Building-Natural-Language-Pipelines/ch6/jupyter-notebooks/wandb/run-20251103_113247-en7017gg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lgutierrwr/hybrid-rag-optimization/runs/en7017gg' target=\"_blank\">final-comparison</a></strong> to <a href='https://wandb.ai/lgutierrwr/hybrid-rag-optimization' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lgutierrwr/hybrid-rag-optimization' target=\"_blank\">https://wandb.ai/lgutierrwr/hybrid-rag-optimization</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lgutierrwr/hybrid-rag-optimization/runs/en7017gg' target=\"_blank\">https://wandb.ai/lgutierrwr/hybrid-rag-optimization/runs/en7017gg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">final-comparison</strong> at: <a href='https://wandb.ai/lgutierrwr/hybrid-rag-optimization/runs/en7017gg' target=\"_blank\">https://wandb.ai/lgutierrwr/hybrid-rag-optimization/runs/en7017gg</a><br> View project at: <a href='https://wandb.ai/lgutierrwr/hybrid-rag-optimization' target=\"_blank\">https://wandb.ai/lgutierrwr/hybrid-rag-optimization</a><br>Synced 4 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251103_113247-en7017gg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def extract_ragas_metrics(metrics_obj):\n",
    "    \"\"\"Extract and flatten RAGAS metrics from the result object for comparison.\"\"\"\n",
    "    metrics_dict = {} \n",
    "    if hasattr(metrics_obj, 'to_dict'):\n",
    "        raw_metrics = metrics_obj.to_dict()\n",
    "        for k, v in raw_metrics.items():\n",
    "            if isinstance(v, (float, int)):\n",
    "                # Clean metric name for the output table\n",
    "                clean_name = k.replace('(mode=f1)', '').strip()\n",
    "                metrics_dict[clean_name] = v\n",
    "    return metrics_dict\n",
    "\n",
    "# 1. Extract and combine summary data\n",
    "baseline_data = {\n",
    "    'System': 'Baseline (k=3)',\n",
    "    'retriever_top_k': 3,\n",
    "    'Execution Time (s)': baseline_results['execution_time'],\n",
    "    'Avg Cost (USD)': baseline_summary['average_cost_per_query_usd'],\n",
    "    'Avg Tokens/Query': baseline_summary['average_tokens_per_query'],\n",
    "    **extract_ragas_metrics(baseline_results['metrics'])\n",
    "}\n",
    "\n",
    "optimized_data = {\n",
    "    'System': 'Optimized (k=7)',\n",
    "    'retriever_top_k': 7,\n",
    "    'Execution Time (s)': optimized_results['execution_time'],\n",
    "    'Avg Cost (USD)': optimized_summary['average_cost_per_query_usd'],\n",
    "    'Avg Tokens/Query': optimized_summary['average_tokens_per_query'],\n",
    "    **extract_ragas_metrics(optimized_results['metrics'])\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame([baseline_data, optimized_data])\n",
    "comparison_df = comparison_df.set_index('System')\n",
    "\n",
    "# 2. Calculate Percentage Improvement\n",
    "performance_cols = [col for col in comparison_df.columns if col not in ['retriever_top_k', 'Execution Time (s)', 'Avg Cost (USD)', 'Avg Tokens/Query']]\n",
    "tradeoff_cols = ['Execution Time (s)', 'Avg Cost (USD)', 'Avg Tokens/Query']\n",
    "\n",
    "insights = {\"Improvement Summary\": {}}\n",
    "print(\"================================================\")\n",
    "print(\"Hybrid RAG Optimization: Comparison Summary\")\n",
    "print(\"================================================\")\n",
    "print(comparison_df.to_string(float_format='%.4f'))\n",
    "print(\"\\nKey Performance Changes:\")\n",
    "\n",
    "for col in performance_cols:\n",
    "    baseline_val = comparison_df.loc['Baseline (k=3)', col]\n",
    "    optimized_val = comparison_df.loc['Optimized (k=7)', col]\n",
    "    # Calculate change, handle potential division by zero for baseline (rare for performance metrics, but safe)\n",
    "    if baseline_val == 0:\n",
    "        change = float('inf') if optimized_val > 0 else 0.0\n",
    "    else:\n",
    "        change = (optimized_val - baseline_val) / abs(baseline_val) * 100\n",
    "    insights[\"Improvement Summary\"][f\"% Change in {col}\"] = f\"{change:+.2f}%\"\n",
    "    print(f\"  {col}: {change:+.2f}%\")\n",
    "\n",
    "print(\"\\n--- Tradeoffs ---\")\n",
    "for col in tradeoff_cols:\n",
    "    baseline_val = comparison_df.loc['Baseline (k=3)', col]\n",
    "    optimized_val = comparison_df.loc['Optimized (k=7)', col]\n",
    "    change = (optimized_val - baseline_val) / baseline_val * 100\n",
    "    print(f\"  {col}: {change:+.2f}% Change\")\n",
    "\n",
    "# Log final summary table to W&B for easy comparison\n",
    "final_run = wandb.init(project=\"hybrid-rag-optimization\", name=\"final-comparison\", reinit=True)\n",
    "final_run.log({\"final_comparison_table\": wandb.Table(dataframe=comparison_df.reset_index())})\n",
    "final_run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b90b3b",
   "metadata": {},
   "source": [
    "## üéì Summary and Next Steps\n",
    "\n",
    "You have successfully executed a targeted optimization experiment for the Hybrid RAG system and logged all results to a single W&B project: `hybrid-rag-optimization`.\n",
    "\n",
    "### Key Accomplishments:\n",
    "1.  **Baseline Established:** Created a baseline measurement of Hybrid RAG performance (`top_k=3`) across RAGAS metrics, cost, and latency.\n",
    "2.  **Parameter Tuning:** Successfully ran an experiment with an updated hyperparameter (`top_k=7`).\n",
    "3.  **Comprehensive Logging:** Logged both runs, their configurations, raw data samples, and **meaningful visualizations** to W&B.\n",
    "\n",
    "### Next Steps in W&B:\n",
    "1.  **View Comparison:** Navigate to the **`hybrid-rag-optimization`** project dashboard on W&B. Use the **Compare Runs** feature to analyze the **faithfulness vs. cost scatter plot** and the **cost vs. input tokens scatter plot** to understand the true impact of increasing `top_k`.\n",
    "2.  **Deeper Optimization:** Utilize W&B Sweeps to automate the search for the optimal `retriever_top_k` value, or try tuning other parameters like `reranker_model` or document chunking strategies.\n",
    "3.  **Reproduce:** Every run is versioned, ensuring you can reproduce these results precisely later."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-with-haystack-ch6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
