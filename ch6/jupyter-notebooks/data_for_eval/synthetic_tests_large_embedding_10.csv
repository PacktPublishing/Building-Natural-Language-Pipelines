user_input,reference_contexts,reference,synthesizer_name
What is OpenAI and why are people worried about it?,"[""What is AI, how does it work and why are some people concerned about it?\nArtificial intelligence (AI) has increasingly become part of everyday life over the past decade.\nIt is being used to personalise social media feeds, spot friends and family in smartphone photos and pave the way for medical breakthroughs.\nBut the rise of chatbots like OpenAI's ChatGPT and Meta AI has been accompanied by concern about the technology's environmental impact, ethical implications and data use.\n What is AI and what is it used for?\nAI allows computers to process large amounts of data, identify patterns and follow detailed instructions about what to do with that information.\nComputers cannot think, empathise or reason.\nHowever, scientists have developed systems that can perform tasks which usually require human intelligence, trying to replicate how people acquire and use knowledge.\nThis could be trying to anticipate what product an online shopper might buy, based on previous purchases, in order to recommend items.\nThe technology is also behind voice-controlled virtual assistants like Apple's Siri and Amazon's Alexa, and is being used to develop systems for self-driving cars.\nAI also helps social platforms like Facebook, TikTok and X decide what posts to show users. Streaming services Spotify and Deezer use AI to suggest music.\nThere are also a number of applications in medicine, as scientists use AI to help spot cancers, review X-ray results, speed up diagnoses and identify new treatments.\n What is generative AI, and how do apps like ChatGPT and Meta AI work?\nGenerative AI is used to create new content which can seem like it has been made by a human.\nIt does this by learning from vast quantities of existing data such as online text and images.\nChatGPT and Chinese rival DeepSeek's chatbot are popular generative AI tools that can be used to produce text, images, code and more material.\nGoogle's Gemini or Meta AI can similarly hold text conversations with users.\nApps like Midjourney or Veo 3 are dedicated to creating images or video from simple text prompts.\nGenerative AI can also be used to make high-quality music.\nSongs mimicking the style or sound of famous musicians have gone viral, sometimes leaving fans confused about their authenticity.\n""]","OpenAI is associated with the rise of chatbots like ChatGPT, which has raised concerns about the technology's environmental impact, ethical implications, and data use.",single_hop_specific_query_synthesizer
"What concerns did Yann LeCun express regarding the implications of AI, especially in relation to the fears voiced by his fellow AI godfather, Geoffrey Hinton?","['Why is AI controversial?\nWhile acknowledging AI\'s potential, some experts are worried about the implications of its rapid growth.\nThe International Monetary Fund (IMF) has warned AI could affect nearly 40% of jobs, and worsen global financial inequality.\nProf Geoffrey Hinton, a computer scientist regarded as one of the ""godfathers"" of AI development, has expressed concern that powerful AI systems could even make humans extinct - although his fear was dismissed by his fellow ""AI godfather"", Yann LeCun.\nCritics also highlight the tech\'s potential to reproduce biased information, or discriminate against some social groups.\nThis is because much of the data used to train AI comes from public material, including social media posts or comments, which can reflect existing societal biases such as sexism or racism.\nAnd while AI programmes are growing more adept, they are still prone to errors - such as creating images of people with the wrong number of fingers or limbs.\nGenerative AI systems are known for their ability to ""hallucinate"" and assert falsehoods as fact, even sometimes inventing sources for the inaccurate information.\nApple halted a new AI feature in January after it incorrectly summarised news app notifications.\nThe BBC complained about the feature after Apple\'s AI falsely told readers that Luigi Mangione - the man accused of killing UnitedHealthcare CEO Brian Thompson - had shot himself.\nGoogle has also faced criticism over inaccurate answers produced by its AI search overviews.\nThis has added to concerns about the use of AI in schools and workplaces, where it is increasingly used to help summarise texts, write emails or essays and solve bugs in code.\nThere are worries about students using AI technology to ""cheat"" on assignments, or employees ""smuggling"" it into work.\nWriters, musicians and artists have also pushed back against the technology on ethical grounds, accusing AI developers of using their work to train systems without consent or compensation.\nThousands of creators - including Abba singer-songwriter Björn Ulvaeus, writers Ian Rankin and Joanne Harris and actress Julianne Moore - signed a statement in October 2024 calling AI a ""major, unjust threat"" to their livelihoods.\nHow does AI effect the environment?\nIt is not clear how much energy AI systems use, but some researchers estimate the industry as a whole could soon consume as much as the Netherlands.\nCreating the powerful computer chips needed to run AI programmes requires lots of power and water.\nDemand for generative AI services has also meant an increase in the number of data centres which power them.\nThese huge halls - housing thousands of racks of computer servers - use substantial amounts of energy and require large volumes of water to keep them cool.\nSome large tech companies have invested in ways to reduce or reuse the water needed, or have opted for alternative methods such as air-cooling.\nHowever, some experts and activists fear that AI will worsen water supply problems.\nThe BBC was told in February that government plans to make the UK a ""world leader"" in AI could put already stretched supplies of drinking water under strain.\nIn September 2024, Google said it would reconsider proposals for a data centre in Chile, which has struggled with drought.\n']","Yann LeCun dismissed the concerns expressed by Geoffrey Hinton, who warned that powerful AI systems could even make humans extinct. This dismissal highlights LeCun's differing perspective on the potential risks associated with AI technology.",single_hop_specific_query_synthesizer
What are the AI laws that will be in place in 2024?,"['Are there laws governing AI?\nSome governments have already introduced rules governing how AI operates.\nThe EU\'s Artificial Intelligence Act places controls on high risk systems used in areas such as education, healthcare, law enforcement or elections. It bans some AI use altogether.\nGenerative AI developers in China are required to safeguard citizens\' data, and promote transparency and accuracy of information. But they are also bound by the country\'s strict censorship laws.\nIn the UK, Prime Minister Sir Keir Starmer has said the government ""will test and understand AI before we regulate it"".\nBoth the UK and US have AI Safety Institutes that aim to identify risks and evaluate advanced AI models.\nIn 2024 the two countries signed an agreement to collaborate on developing ""robust"" AI testing methods.\nHowever, in February 2025, neither country signed an international AI declaration which pledged an open, inclusive and sustainable approach to the technology.\nSeveral countries including the UK are also clamping down on use of AI systems to create deepfake nude imagery and child sexual abuse material.\nSign up for our Tech Decoded newsletter to follow the world\'s top tech stories and trends. Outside the UK? Sign up here.']","In 2024, the UK and US signed an agreement to collaborate on developing robust AI testing methods. The EU's Artificial Intelligence Act also places controls on high-risk systems in areas such as education, healthcare, law enforcement, or elections, and bans some AI uses altogether. Additionally, there are ongoing efforts in several countries, including the UK, to clamp down on the use of AI systems for creating deepfake nude imagery and child sexual abuse material.",single_hop_specific_query_synthesizer
"What percentage of user messages in ChatGPT are related to education, and how does this compare to the overall message volume for work and non-work usage?","['<1-hop>\n\nFigure 9 disaggregates four of the seven Conversation Topics into smaller groups and sums up\nmessages of each type over a one-year period. For example, the five sub-categories withinWriting\nare (in order of frequency)Editing or Critiquing Provided Text,Personal Writing or Communication,\nTranslation,Argument or Summary Generation, andWriting Fiction. Three of those five categories\n(Editing or Critiquing Provided Text,Translation, andArgument or Summary Generation) are re-\nquests to modify text that has been provided to ChatGPT by the user, whereas the other two are\nrequests to produce novel text. The former constitute two thirds of allWritingconversations, which\n14\x0cFigure 7:Share of consumer ChatGPT messages broken down by high level conversation topic, according\nto the mapping in Table 3. Values are averaged over a 28 day lagging window. Shares are calculated from\na sample of approximately 1.1 million sampled conversations from May 15, 2024 through June 26, 2025.\nObservations are reweighted to reflect total message volumes on a given day. Sampling details available in\nSection 3.\nFigure 8:Share ofwork relatedconsumer ChatGPT messages broken down by high level conversation\ntopic, according to the mapping in Table 3. Values are averaged over a 28 day lagging window. Shares are\ncalculated from a sample of approximately 1.1 million sampled conversations from May 15, 2024 through June\n26, 2025. Observations are reweighted to reflect total message volumes on a given day. Sampling details\navailable in Section 3.\n15\x0csuggests that most userWritingconversations with ChatGPT are requests to modify user inputs\nrather than to create something new. Education is a major use case for ChatGPT. 10.2% of all user\nmessages and 36% ofPractical Guidancemessages are requests forTutoring or Teaching. Another\nlarge share - 8.5% in total and 30% ofPractical Guidance- is general how-to advice on a variety\nof topics.Technical HelpincludesComputer Programming(4.2% of messages),Mathematical Calcu-\nlations(3%), andData Analysis(0.4%). Looking at the topic ofSelf-Expression, only 2.4% of all\nChatGPT messages are aboutRelationships and Personal Reflection(1.9%) orGames and Role Play\n(0.4%).\nWhile users can seek information and advice from traditional web search engines as well as from\nChatGPT, the ability to produce writing, software code, spreadsheets, and other digital products\ndistinguishes generative AI from existing technologies. ChatGPT is also more flexible than web\nsearch even for traditional applications likeSeeking InformationandPractical Guidance, because\nusers receive customized responses (e.g., tailored workout plans, new product ideas, ideas for fantasy\nfootball team names) that represent newly generated content or novel modification of user-provided\ncontent and follow-up requests.\nFigure 9:Breakdown of granular conversation topic shares within the coarse mapping defined in Table 3. The\nunderlying classifier prompt is available in Appendix A. Each bin reports a percentage of the total population.\nShares are calculated from a sample of approximately 1.1 million sampled conversations from May 15, 2024\nthrough June 26, 2025. Observations are reweighted to reflect total message volumes on a given day. Sampling\ndetails available in Section 3.\n5.3 ', '<2-hop>\n\nto classify messages without any human seeing them. We give the text of most prompts in Appendix A along with details about how the prompts were validated in Appendix B.6 The classification pipeline is protected by a series of privacy measures, detailed below, to ensure no leakage of sensitive information during the automated analysis. In a secure data clean room, we relate taxonomies of messages to aggregated employment and education categories. Table 1 shows the growth in total message volume for work and non-work usage. Both types of 1Reuters (2025), Roth (2025) 2Bick et al. (2024) report that 28% of US adults used ChatGPT in late 2024, higher than any other chatbot. 3We use the term LLM loosely here and give more details in the following section. 4Wiggers (2025) reports estimates that in April 2025 ChatGPT was receiving more than 10 times as many visitors as either Claude or Copilot. 5Our sample includes the three consumer plans (Free, Plus, or Pro). OpenAI also offers a variety of other ChatGPT plans (Business fka. Teams, Enterprise, Education), which we do not include in our sample. 6Our classifiers take into account not just the randomly-selected user message, but also a portion of the preceding messages in that conversation. 1 Month Non-Work (M)(%)Work (M)(%)Total Messages (M) Jun 2024 238 53% 213 47% 451 Jun 2025 1,911 73% 716 27% 2,627 Table 1:ChatGPT daily message counts (millions), broken down by likely work-related or non-work-related. Total daily counts are exact measurements of message volume from all consumer plans. Daily counts of work and non-work related messages are estimated by classifying a random sample of conversations from that day.']","Education is a major use case for ChatGPT, with 10.2% of all user messages and 36% of Practical Guidance messages being requests for Tutoring or Teaching. In terms of overall message volume, the data shows that in June 2025, there were 1,911 million non-work messages (73%) and 716 million work-related messages (27%), indicating a significant growth in non-work usage compared to work-related interactions.",multi_hop_specific_query_synthesizer
How Google help researchers with data access?,"['<1-hop>\n\nImproving data access\nThe United States should develop a data strategy that promotes innovation and consumer protection. Right now, there are no uniform standards in terms of data access, data sharing, or data protection. Almost all the data are proprietary in nature and not shared very broadly with the research community, and this limits innovation and system design. AI requires data to test and improve its learning capacity.50 Without structured and unstructured data sets, it will be nearly impossible to gain the full benefits of artificial intelligence.\nIn general, the research community needs better access to government and business data, although with appropriate safeguards to make sure researchers do not misuse data in the way Cambridge Analytica did with Facebook information. There is a variety of ways researchers could gain data access. One is through voluntary agreements with companies holding proprietary data. Facebook, for example, recently announced a partnership with Stanford economist Raj Chetty to use its social media data to explore inequality.51 As part of the arrangement, researchers were required to undergo background checks and could only access data from secured sites in order to protect user privacy and security.\nIn the U.S., there are no uniform standards in terms of data access, data sharing, or data protection. Almost all the data are proprietary in nature and not shared very broadly with the research community, and this limits innovation and system design.\nGoogle long has made available search results in aggregated form for researchers and the general public. Through its “Trends” site, scholars can analyze topics such as interest in Trump, views about democracy, and perspectives on the overall economy.52 That helps people track movements in public interest and identify topics that galvanize the general public.\nTwitter makes much of its tweets available to researchers through application programming interfaces, commonly referred to as APIs. These tools help people outside the company build application software and make use of data from its social media platform. They can study patterns of social media communications and see how people are commenting on or reacting to current events.\nIn some sectors where there is a discernible public benefit, governments can facilitate collaboration by building infrastructure that shares data. For example, the National Cancer Institute has pioneered a data-sharing protocol where certified researchers can query health data it has using de-identified information drawn from clinical data, claims information, and drug therapies. That enables researchers to evaluate efficacy and effectiveness, and make recommendations regarding the best medical approaches, without compromising the privacy of individual patients.\nThere could be public-private data partnerships that combine government and business data sets to improve system performance. For example, cities could integrate information from ride-sharing services with its own material on social service locations, bus lines, mass transit, and highway congestion to improve transportation. That would help metropolitan areas deal with traffic tie-ups and assist in highway and mass transit planning.\nSome combination of these approaches would improve data access for researchers, the government, and the business community, without impinging on personal privacy. As noted by Ian Buck, the vice president of NVIDIA, “Data is the fuel that drives the AI engine. The federal government has access to vast sources of information. Opening access to that data will help us get insights that will transform the U.S. economy.”53 Through its Data.gov portal, the federal government already has put over 230,000 data sets into the public domain, and this has propelled innovation and aided improvements in AI and data analytic technologies.54 The private sector also needs to facilitate research data access so that society can achieve the full benefits of artificial intelligence.\n']","Google long has made available search results in aggregated form for researchers and the general public. Through its 'Trends' site, scholars can analyze topics such as interest in Trump, views about democracy, and perspectives on the overall economy. That helps people track movements in public interest and identify topics that galvanize the general public.",multi_hop_specific_query_synthesizer
What are the primary functions of ChatGPT usage at work as classified by the O*NET Work Activities taxonomy?,"['<1-hop>\n\nOverall, the majority of ChatGPT usage\nat work appears to be focused on two broad functions: 1) obtaining, documenting, and interpreting\ninformation; and 2) making decisions, giving advice, solving problems, and thinking creatively.\n20\x0cFigure 14:GWA Shares of 1.1M ChatGPT Messages. Messages are classified as pertaining to one of 332\nO*NET IWAs, orAmbiguoususing the prompt provided in the Appendix. IWAs were then aggregated to\nGWAs using the O*NET Work Activities taxonomy. Message sample from May 15, 2024 through June 26,\n2025. We do not show the shares for the following GWAs as there were fewer than 100 users sending messages\nfor each category and group them intoSuppressed.\n21\x0cFigure 15:GWA Shares of approximately 366,000 Work-Classified Messages. Messages are classified as\npertaining to one of 332 O*NET IWAs orAmbiguous. IWAs were then aggregated to GWAs using the\nO*NET Work Activities taxonomy. Messages were also additionally classified as pertaining to work or non-\nwork. GWA shares are shown only for work-classified messages. Message sample from May 15, 2024 through\nJune 26, 2025. We do not show the shares for the following GWAs as there were fewer than 100 users sending\nmessages for each category and group them intoSuppressed. Prompts are provided in the Appendix.\n22\x0c', '<2-hop>\n\n5.4 O*NET Work Activities\nWe map message content to work activities using the Occupational Information Network (O*NET)\nDatabase Version 29.0, similar to Tomlinson et al (2025). O*NET was developed in partnership with\nthe U.S. Department of Labor and systematically classifies jobs according to the skills, tasks, and\nwork activities required to perform them. O*NET associates each occupation with a set of tasks that\nare performed at different levels of intensity. Each task is then aggregated up to three levels of detail\n- 2,087 detailed work activities (DWAs), 332 intermediate work activities (IWAs), and 41 generalized\nwork activities (GWAs).\nTo understand the work activities associated with ChatGPT usage, we mapped messages to one\nof the 332 O*NET Intermediate Work Activities (IWA), with an additional option ofAmbiguousto\naccount for situations where the user message lacked sufficient context. 22 We then used the official\n22We drew a sample of approximately 1.1 million conversations from May 2024 to June 2025, selected a random\nmessage within each, and classified it according to the prompt in A.\n19\x0cFigure 13:Shares of Asking, Doing, and Expressing messages split by work vs. non-work. See A to review\nthe prompts used by the automated classifiers. The annotations on the right show the shares of work and\nnon-work for the full sample. Each bin reports a percentage of the total population. Shares are calculated\nfrom a sample of approximately 1.1 million sampled conversations from May 15, 2024 through June 26, 2025.\nObservations are reweighted to reflect total message volumes on a given day. Sampling details available in\nSection 3.\nO*NET taxonomy to map these classified IWAs to one of the Generalized Work Activities (GWA). We\ndo not show the shares for the following GWAs as there were fewer than 100 users sending messages\nfor each category and group them intoSuppressed.\nFigure 14 presents the share of messages that belong to each GWA, in descending order. Nearly\nhalf of all messages (45.2%) fall under just three GWAs related to information use and manipula-\ntion:Getting Information(19.3%),Interpreting the Meaning of Information for Others(13.1%), and\nDocumenting/Recording Information(12.8%). The next most common work activities areProviding\nConsultation and Advice(9.2%),Thinking Creatively(9.1%),Making Decisions and Solving Problems\n(8.5%), andWorking with Computers(4.9%). These seven GWAs collectively account for 76.9% of\nall messages.\nFigure 15 presents the distribution of GWAs for the subsample of messages we classify as work-\nrelated. Among work-related messages, the most common GWAs areDocumenting/Recording In-\nformation(18.4%),Making Decisions and Solving Problems(14.9%),Thinking Creatively(13.0%),\nWorking with Computers(10.8%),Interpreting the Meaning of Information for Others(10.1%),Get-\nting Information(9.3%), andProviding Consultation and Advice to Others(4.4%). These seven GWAs\ncollectively account for nearly 81% of work-related messages. Overall, the majority of ChatGPT usage\nat work appears to be focused on two broad functions: 1) obtaining, documenting, and interpreting\ninformation; and 2) making decisions, giving advice, solving problems, and thinking creatively.\n20\x0cFigure 14:GWA Shares of 1.1M ChatGPT Messages. Messages are classified as pertaining to one of 332\nO*NET IWAs, orAmbiguoususing the prompt provided in the Appendix. IWAs were then aggregated to\nGWAs using the O*NET Work Activities taxonomy. Message sample from May 15, 2024 through June 26,\n2025.']","The primary functions of ChatGPT usage at work, as classified by the O*NET Work Activities taxonomy, focus on two broad areas: 1) obtaining, documenting, and interpreting information; and 2) making decisions, giving advice, solving problems, and thinking creatively. These functions are reflected in the distribution of messages classified under various Generalized Work Activities (GWAs), with significant shares related to information use and manipulation.",multi_hop_specific_query_synthesizer
How do Context-Augmented Message Classifications ensure user privacy while analyzing employment data in a Data Clean Room?,"['<1-hop>\n\n” (truncated)\n[user]: “10 more”\nTable 2:Illustration of Context-Augmented Message Classifications (Synthetic Example). The left column\nshows a standalone message to be classified, and the right column shows the prior context included in the\nclassification of the message on the left.\nWe truncate each message to a maximum of 5,000 characters, because long context windows could\ninduce variability in the quality of the classification (Liu et al., 2023). We classify each message\nwith the “gpt-5-mini” model, with the exception ofInteraction Quality,which uses “gpt-5,” using the\nprompts listed in Appendix A.\n17Internal analyses show that the tool,Privacy Filter, has substantial alignment with human judgment.\n18In the case ofInteraction Quality,we additionally include the next two messages in the conversation as context.\n7\x0cWe validated each of the classification prompts by comparing model classification decisions against\nhuman-judged classifications of a sample of conversations from the publicly available WildChat dataset\n(Zhao et al., 2024), a set of conversations with a third-party chatbot which users affirmatively gave\ntheir assent to share publicly for research purposes. 19 Appendix B provides detail on our validation\napproach and performance relative to human judgment. For additional transparency, we classify\na sample of 100,000 public WildChat messages and provide those data in this paper’s replication\npackage.\n 3.3 Employment Dataset\nWe conduct limited analyses of aggregated employment categories based on publicly available data\nfor a sample of consumer ChatGPT users. This sample included approximately 130,000 Free, Plus,\nand Pro users, and the employment categories were aggregated by a vendor working through a secure\nData Clean Room (DCR). For this analysis, we use the same exclusion criteria as for the message-level\ndatasets: we exclude deactivated users, banned users, users who have opted out of training, and users\nwhose self-reported age is under 18. Because the data was only available for a subset of users the\nresults may not be representative of the full pool of users.\nDescription.The employment data, which is aggregated from publicly available sources, includes\nindustry, occupations coarsened to O*NET categories, seniority level, company size, and education\ninformation that is limited to the degree attained. A vendor working within a DCR procured this\ndataset, restricted us to running only aggregated queries against it through the DCR, and deleted it\nupon the study’s completion.\n', '<2-hop>\n\nPrivacy via a Data Clean Room.We never directly accessed user-level demographic records.\nAll analysis of employment data was executed exclusively within a secure DCR that permits only\npre-approved aggregate computations across independently held datasets; neither party can view or\nexport the other party’s underlying records. We governed the DCR with strict protocols: To execute\nany query that touched the external demographic data, we first obtained explicit sign-off from a\ncommittee of 6 coauthors and then submitted the notebook to our data partner for approval; only\napproved notebooks could run in the DCR (see Figure 2).\nOur partner enforced strict aggregation limits: they only approved code that returned cells meeting\na threshold of 100 users. Consequently, no individual rows or narrowly defined categories were ever\nvisible to researchers. For example, if 99 users had the occupation “anesthesiologist,” any occupation-\nlevel output would place those users into a “suppressed” category, or place these observations in a\ncoarsened category (e.g. “medical professionals”) rather than reporting a separate cell of anesthesiol-\nogists.\n19The dataset was collected from a third party chatbot using OpenAI’s LLMs via their API.\n8\x0cFigure 2:Illustration of Aggregated Employment Category Analysis via a Data Clean Room. All queries run\nin the Data Clean Room must be approved by our data partner, enforcing a strict aggregation threshold (100\nobservations). As a result, researchers cannot access user-level employment data, only aggregated employment\ncategories.\n', '<3-hop>\n\nPrivacy via Automated Classifiers.No one looked at the content of messages while conducting\nanalysis for this paper. All analysis of message content was performed via automated LLM-based\nclassifiers run on de-identified and PII-scrubbed message data (see Figure 1). The messages are first\nscrubbed of PII using an internal LLM-based tool,17 and then classified according to classifiers defined\nover a controlled label space—the most precise classifier we use on the message-level data set is the\nO*NET Intermediate Work Activities taxonomy, which we augment to end up with 333 categories.\nWe introduce technical and procedural frictions that prevent accidental access to the underlying text\n(for example, interfaces that do not render message text to researchers).\nOur classifications aim to discern the intent of a given message, and thus we include the prior 10\nmessages in a conversation as context. 18 For an example, see Table 2.\nStand-Alone Message Message with Prior Context\n[user]: “10 more” [user]: “give me 3 cultural activities to do with teens”\n[assistant]: “1. Visit a museum . . . ” (truncated)\n[user]: “10 more”\nTable 2:Illustration of Context-Augmented Message Classifications (Synthetic Example). The left column\nshows a standalone message to be classified, and the right column shows the prior context included in the\nclassification of the message on the left.\nWe truncate each message to a maximum of 5,000 characters, because long context windows could\ninduce variability in the quality of the classification (Liu et al., 2023). We classify each message\nwith the “gpt-5-mini” model, with the exception ofInteraction Quality,which uses “gpt-5,” using the\nprompts listed in Appendix A.\n17Internal analyses show that the tool,Privacy Filter, has substantial alignment with human judgment.']","Context-Augmented Message Classifications ensure user privacy by utilizing automated LLM-based classifiers that analyze de-identified and PII-scrubbed message data. The analysis is conducted within a secure Data Clean Room (DCR), where no individual user-level demographic records are accessed. All queries executed in the DCR require explicit approval and must meet strict aggregation thresholds, ensuring that no individual data points are visible to researchers. This approach allows for the classification of messages while maintaining privacy, as the classifiers discern intent based on prior context without exposing any identifiable information.",multi_hop_abstract_query_synthesizer
"What are the primary usage patterns of ChatGPT among LLM chatbots, and how has its adoption changed since its launch in November 2022?","['<1-hop>\n\nNBER WORKING PAPER SERIES\nHOW PEOPLE USE CHATGPT\nAaron Chatterji\nThomas Cunningham\nDavid J. Deming\nZoe Hitzig\nChristopher Ong\nCarl Yan Shan\nKevin Wadman\nWorking Paper 34255\nhttp://www.nber.org/papers/w34255\nNATIONAL BUREAU OF ECONOMIC RESEARCH\n1050 Massachusetts Avenue\nCambridge, MA 02138\nSeptember 2025\nWe acknowledge help and comments from Joshua Achiam, Hemanth Asirvatham, Ryan Beiermeister, Rachel Brown, Cassandra Duchan Solis, Jason Kwon, Elliott Mokski, Kevin Rao, Harrison Satcher, Gawesha Weeratunga, Hannah Wong, and Analytics & Insights team. We especially thank Tyna Eloundou and Pamela Mishkin who in several ways laid the foundation for this work. This study was approved by Harvard IRB (IRB25-0983). A repository containing all code run to produce the analyses in this paper is available on request. The views expressed herein are those of the authors and do not necessarily reflect the views of the National Bureau of Economic Research.\nAt least one co-author has disclosed additional relationships of potential relevance for this research. Further information is available online at http://www.nber.org/papers/w34255\nNBER working papers are circulated for discussion and comment purposes. They have not been peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies official NBER publications.\n© 2025 by Aaron Chatterji, Thomas Cunningham, David J. Deming, Zoe Hitzig, Christopher Ong, Carl Yan Shan, and Kevin Wadman. All rights reserved. Short sections of text, not to exceed two paragraphs, may be quoted without explicit permission provided that full credit, including © notice, is given to the source.\x0cHow People Use ChatGPT\nAaron Chatterji, Thomas Cunningham, David J. Deming, Zoe Hitzig, Christopher Ong, Carl\nYan Shan, and Kevin Wadman\nNBER Working Paper No. 34255\nSeptember 2025\nJEL No. J01, O3, O4\nABSTRACT\nDespite the rapid adoption of LLM chatbots, little is known about how they are used. We document the growth of ChatGPT’s consumer product from its launch in November 2022 through July 2025, when it had been adopted by around 10% of the world’s adult population. Early adopters were disproportionately male but the gender gap has narrowed dramatically, and we find higher growth rates in lower-income countries. Using a privacy-preserving automated pipeline, we classify usage patterns within a representative sample of ChatGPT conversations. We find steady growth in work-related messages bu t even faster growth in non-work-related messages, which have grown from 53% to more than 70% of all usage. Work usage is more common for educated users in highly-paid professional occupations. We classify messages by conversation top\nic and find that “Practical Guidance,” “Seeking Information,” and “Writing” are the three most common topics and collectively account for nearly 80% of all conversations. Writing dominates work-related tasks, highlighting chatbots’ unique ability to generate digital outputs compared to traditional sear ch engines. Computer programming and self-expression both represent relatively small shares of use. Overall, we find that ChatGPT provides economic value through decision support, which is especially important in knowledge-intensive jobs.\nAaron Chatterji\nDuke University\nFuqua School of Business and OpenAI\nronnie@duke.edu\nThomas Cunningham OpenAI\ntom.cunningham@gmail.com\nDavid J. Deming\nHarvard University\nHarvard Kennedy School and NBER\ndavid_deming@harvard.edu\nZoe Hitzig\nOpenAI\nand Harvard Society of Fellows\nzhitzig@g.harvard.edu\nChristopher Ong\nHarvard University\nand OpenAI\nchristopherong@hks.harvard.edu\nCarl Yan Shan\nOpenAI\ncshan@openai.com\nKevin Wadman\nOpenAI\nkevin.wadman@c-openai.com\x0c', '<2-hop>\n\nABSTRACT Despite the rapid adoption of LLM chatbots, little is known about how they are used. We document the growth of ChatGPT’s consumer product from its launch in November 2022 through July 2025, when it had been adopted by around 10% of the world’s adult population. Early adopters were disproportionately male but the gender gap has narrowed dramatically, and we find higher growth rates in lower-income countries. Using a privacy-preserving automated pipeline, we classify usage patterns within a representative sample of ChatGPT conversations. We find steady growth in work-related messages bu t even faster growth in non-work-related messages, which have grown from 53% to more than 70% of all usage. Work usage is more common for educated users in highly-paid professional occupations. We classify messages by conversation top ic and find that “Practical Guidance,” “Seeking Information,” and “Writing” are the three most common topics and collectively account for nearly 80% of all conversations. Writing dominates work-related tasks, highlighting chatbots’ unique ability to generate digital outputs compared to traditional sear ch engines. Computer programming and self-expression both represent relatively small shares of use. Overall, we find that ChatGPT provides economic value through decision support, which is especially important in knowledge-intensive jobs. Aaron Chatterji Duke University Fuqua School of Business and OpenAI ronnie@duke.edu Thomas Cunningham OpenAI tom.cunningham@gmail.com David J. Deming Harvard University Harvard Kennedy School and NBER david_deming@harvard.edu Zoe Hitzig OpenAI and Harvard Society of Fellows zhitzig@g.harvard.edu Christopher Ong Harvard University and OpenAI christopherong@hks.harvard.edu Carl Yan Shan OpenAI cshan@openai.com Kevin Wadman OpenAI kevin.wadman@c-openai.com 1 Introduction ChatGPT launched in November 2022. By July 2025, 18 billion messages were being sent each week by 700 million users, representing around 10% of the global adult population. 1 For a new technology, this speed of global diffusion has no precedent (Bick et al., 2024). This paper studies consumer usage of ChatGPT, the first mass-market chatbot and likely the largest.2 ChatGPT is based on a Large Language Model (LLM), a type of Artificial Intelligence (AI) developed over the last decade and generally considered to represent an acceleration in AI capabilities.3 The sudden growth in LLM abilities and adoption has intensified interest in the effects of artificial intelligence on economic growth (Acemoglu, 2024; Korinek and Suh, 2024); employment (Eloundou et al., 2025); and society (Kulveit et al., 2025). However, despite the rapid adoption of LLMs, there is limited public information on how they are used. A number of surveys have measured self-reported adoption of LLMs (Bick et al., 2024; Pew Research Center, 2025); however there are reasons to expect bias in self-reports (Ling and Imas, 2025), and none of these papers have been able to directly track the quantity or nature of chatbot conversations. Two recent papers do report statistics on chatbot conversations, classified in a variety of ways (Handa et al., 2025; Tomlinson et al., 2025). We build on this work in several respects. First, the pool of users on ChatGPT is far larger, meaning we expect our data to be a closer approximation to the average chatbot user.4 Second, we use automated classifiers to report on the types of messages that users send using new classification taxonomies relative to the existing literature. Third, we report the diffusion of chatbot use across populations and the growth of different types of usage within cohorts. Fourth, we use a secure data clean room protocol to analyze aggregated employment and education categories for a sample of our users, lending new insights about differences in the types of messages sent by different groups while protecting user privacy. Our primary sample is a random selection of messages sent to ChatGPT on consumer plans (Free, Plus, Pro) between May 2024 and June 2025. 5 Messages from the user to chatbot are classified automatically using a number of different taxonomies: whether the message is used for paid work, the topic of conversation, and the type of interaction (asking, doing, or expressing), and the O*NET task the user is performing. Each taxonomy is defined in a prompt passed to an LLM, allowing us']","The primary usage patterns of ChatGPT among LLM chatbots include a significant increase in non-work-related messages, which have grown from 53% to more than 70% of all usage. The most common topics of conversation are 'Practical Guidance,' 'Seeking Information,' and 'Writing,' collectively accounting for nearly 80% of all conversations. Since its launch in November 2022, ChatGPT has been adopted by around 10% of the world's adult population by July 2025, with early adopters being disproportionately male, although the gender gap has narrowed. Additionally, higher growth rates have been observed in lower-income countries.",multi_hop_abstract_query_synthesizer
How many people used ChatGPT by July 2025 and what are the main topics of conversation among users?,"['<1-hop>\n\nNBER WORKING PAPER SERIES\nHOW PEOPLE USE CHATGPT\nAaron Chatterji\nThomas Cunningham\nDavid J. Deming\nZoe Hitzig\nChristopher Ong\nCarl Yan Shan\nKevin Wadman\nWorking Paper 34255\nhttp://www.nber.org/papers/w34255\nNATIONAL BUREAU OF ECONOMIC RESEARCH\n1050 Massachusetts Avenue\nCambridge, MA 02138\nSeptember 2025\nWe acknowledge help and comments from Joshua Achiam, Hemanth Asirvatham, Ryan Beiermeister, Rachel Brown, Cassandra Duchan Solis, Jason Kwon, Elliott Mokski, Kevin Rao, Harrison Satcher, Gawesha Weeratunga, Hannah Wong, and Analytics & Insights team. We especially thank Tyna Eloundou and Pamela Mishkin who in several ways laid the foundation for this work. This study was approved by Harvard IRB (IRB25-0983). A repository containing all code run to produce the analyses in this paper is available on request. The views expressed herein are those of the authors and do not necessarily reflect the views of the National Bureau of Economic Research.\nAt least one co-author has disclosed additional relationships of potential relevance for this research. Further information is available online at http://www.nber.org/papers/w34255\nNBER working papers are circulated for discussion and comment purposes. They have not been peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies official NBER publications.\n© 2025 by Aaron Chatterji, Thomas Cunningham, David J. Deming, Zoe Hitzig, Christopher Ong, Carl Yan Shan, and Kevin Wadman. All rights reserved. Short sections of text, not to exceed two paragraphs, may be quoted without explicit permission provided that full credit, including © notice, is given to the source.\x0cHow People Use ChatGPT\nAaron Chatterji, Thomas Cunningham, David J. Deming, Zoe Hitzig, Christopher Ong, Carl\nYan Shan, and Kevin Wadman\nNBER Working Paper No. 34255\nSeptember 2025\nJEL No. J01, O3, O4\nABSTRACT\nDespite the rapid adoption of LLM chatbots, little is known about how they are used. We document the growth of ChatGPT’s consumer product from its launch in November 2022 through July 2025, when it had been adopted by around 10% of the world’s adult population. Early adopters were disproportionately male but the gender gap has narrowed dramatically, and we find higher growth rates in lower-income countries. Using a privacy-preserving automated pipeline, we classify usage patterns within a representative sample of ChatGPT conversations. We find steady growth in work-related messages bu t even faster growth in non-work-related messages, which have grown from 53% to more than 70% of all usage. Work usage is more common for educated users in highly-paid professional occupations. We classify messages by conversation top\nic and find that “Practical Guidance,” “Seeking Information,” and “Writing” are the three most common topics and collectively account for nearly 80% of all conversations. Writing dominates work-related tasks, highlighting chatbots’ unique ability to generate digital outputs compared to traditional sear ch engines. Computer programming and self-expression both represent relatively small shares of use. Overall, we find that ChatGPT provides economic value through decision support, which is especially important in knowledge-intensive jobs.\nAaron Chatterji\nDuke University\nFuqua School of Business and OpenAI\nronnie@duke.edu\nThomas Cunningham OpenAI\ntom.cunningham@gmail.com\nDavid J. Deming\nHarvard University\nHarvard Kennedy School and NBER\ndavid_deming@harvard.edu\nZoe Hitzig\nOpenAI\nand Harvard Society of Fellows\nzhitzig@g.harvard.edu\nChristopher Ong\nHarvard University\nand OpenAI\nchristopherong@hks.harvard.edu\nCarl Yan Shan\nOpenAI\ncshan@openai.com\nKevin Wadman\nOpenAI\nkevin.wadman@c-openai.com\x0c', '<2-hop>\n\nABSTRACT Despite the rapid adoption of LLM chatbots, little is known about how they are used. We document the growth of ChatGPT’s consumer product from its launch in November 2022 through July 2025, when it had been adopted by around 10% of the world’s adult population. Early adopters were disproportionately male but the gender gap has narrowed dramatically, and we find higher growth rates in lower-income countries. Using a privacy-preserving automated pipeline, we classify usage patterns within a representative sample of ChatGPT conversations. We find steady growth in work-related messages bu t even faster growth in non-work-related messages, which have grown from 53% to more than 70% of all usage. Work usage is more common for educated users in highly-paid professional occupations. We classify messages by conversation top ic and find that “Practical Guidance,” “Seeking Information,” and “Writing” are the three most common topics and collectively account for nearly 80% of all conversations. Writing dominates work-related tasks, highlighting chatbots’ unique ability to generate digital outputs compared to traditional sear ch engines. Computer programming and self-expression both represent relatively small shares of use. Overall, we find that ChatGPT provides economic value through decision support, which is especially important in knowledge-intensive jobs. Aaron Chatterji Duke University Fuqua School of Business and OpenAI ronnie@duke.edu Thomas Cunningham OpenAI tom.cunningham@gmail.com David J. Deming Harvard University Harvard Kennedy School and NBER david_deming@harvard.edu Zoe Hitzig OpenAI and Harvard Society of Fellows zhitzig@g.harvard.edu Christopher Ong Harvard University and OpenAI christopherong@hks.harvard.edu Carl Yan Shan OpenAI cshan@openai.com Kevin Wadman OpenAI kevin.wadman@c-openai.com 1 Introduction ChatGPT launched in November 2022. By July 2025, 18 billion messages were being sent each week by 700 million users, representing around 10% of the global adult population. 1 For a new technology, this speed of global diffusion has no precedent (Bick et al., 2024). This paper studies consumer usage of ChatGPT, the first mass-market chatbot and likely the largest.2 ChatGPT is based on a Large Language Model (LLM), a type of Artificial Intelligence (AI) developed over the last decade and generally considered to represent an acceleration in AI capabilities.3 The sudden growth in LLM abilities and adoption has intensified interest in the effects of artificial intelligence on economic growth (Acemoglu, 2024; Korinek and Suh, 2024); employment (Eloundou et al., 2025); and society (Kulveit et al., 2025). However, despite the rapid adoption of LLMs, there is limited public information on how they are used. A number of surveys have measured self-reported adoption of LLMs (Bick et al., 2024; Pew Research Center, 2025); however there are reasons to expect bias in self-reports (Ling and Imas, 2025), and none of these papers have been able to directly track the quantity or nature of chatbot conversations. Two recent papers do report statistics on chatbot conversations, classified in a variety of ways (Handa et al., 2025; Tomlinson et al., 2025). We build on this work in several respects. First, the pool of users on ChatGPT is far larger, meaning we expect our data to be a closer approximation to the average chatbot user.4 Second, we use automated classifiers to report on the types of messages that users send using new classification taxonomies relative to the existing literature. Third, we report the diffusion of chatbot use across populations and the growth of different types of usage within cohorts. Fourth, we use a secure data clean room protocol to analyze aggregated employment and education categories for a sample of our users, lending new insights about differences in the types of messages sent by different groups while protecting user privacy. Our primary sample is a random selection of messages sent to ChatGPT on consumer plans (Free, Plus, Pro) between May 2024 and June 2025. 5 Messages from the user to chatbot are classified automatically using a number of different taxonomies: whether the message is used for paid work, the topic of conversation, and the type of interaction (asking, doing, or expressing), and the O*NET task the user is performing. Each taxonomy is defined in a prompt passed to an LLM, allowing us', '<3-hop>\n\nOuyang, Long, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schul-\nman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter\nWelinder, Paul Christiano, Jan Leike, and Ryan Lowe, “Training Language Models to\nFollow Instructions with Human Feedback,” 2022.\nPew Research Center, “U.S. adults’ use of ChatGPT (June 2025 report),” 2025.\nPhang, Jason, Michael Lampe, Lama Ahmad, Sandhini Agarwal, Cathy Mengying Fang,\nAuren R. Liu, Valdemar Danry, Eunhae Lee, Samantha W. T. Chan, Pat Pataranuta-\nporn, and Pattie Maes, “Investigating Affective Use and Emotional Well-being on ChatGPT,”\n2025.\nReuters, “OpenAI hits$12 billion in annualized revenue, The Information reports,”Reuters, July\n30 2025. Accessed: 2025-09-11.\nRoth, Emma, “OpenAI says ChatGPT users send over 2.5 billion prompts every day,” July 21 2025.\nAccessed: 2025-09-11.\nTomlinson, Kiran, Sonia Jaffe, Will Wang, Scott Counts, and Siddharth Suri, “Working\nwith AI: Measuring the Occupational Implications of Generative AI,” 2025.\n39\x0cVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.\nGomez, Lukasz Kaiser, and Illia Polosukhin, “Attention Is All You Need,” in I. Guyon,\nU. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, eds.,Ad-\nvances in Neural Information Processing Systems, Vol. 30 of31st Conference on Neural Information\nProcessing Systems (NIPS)Curran Associates, Inc. Long Beach, CA, USA 2017.\nWest, Jevin D., Jennifer Jacquet, Molly M. King, Shelley J. Correll, and Carl T.\nBergstrom, “The Role of Gender in Scholarly Authorship,”PLoS ONE, 2013,8(7), e66212.\nWiggers, Kyle, “ChatGPT Isn’t the Only Chatbot That’s Gaining Users,”TechCrunch, 2025. Ac-\ncessed: 2025-09-10.\nZao-Sanders, Marc, “How People Are Really Using Gen AI in 2025,” Harvard Business Review\nApril 2025. https://hbr.org/2025/04/how-people-are-really-using-gen-ai-in-2025.\nZhao, Wenting, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng,\n“WildChat: 1M ChatGPT Interaction Logs in the Wild,” 2024.\n40\x0c']","By July 2025, around 10% of the world’s adult population had adopted ChatGPT, which translates to approximately 700 million users. The main topics of conversation among users include 'Practical Guidance,' 'Seeking Information,' and 'Writing,' which collectively account for nearly 80% of all conversations.",multi_hop_abstract_query_synthesizer
"How does the issue of discrimination in artificial intelligence relate to the claims of discrimination faced by companies like Airbnb, and what implications does this have for the future of AI technologies?","['<1-hop>\n\n27-28.\n- Christian Davenport, “ Future Wars May Depend as Much on Algorithms as on Ammunition, Report Says,” Washington Post, December 3, 2017.\n- Ibid.\n- John R. Allen and Amir Husain, “On Hyperwar,” Naval Institute Proceedings, July 17, 2017, pp. 30-36.\n- Paul Mozur, “ China Sets Goal to Lead in Artificial Intelligence,” New York Times, July 21, 2017, p. B1.\n- Paul Mozur and John Markoff, “Is China Outsmarting American Artificial Intelligence?” New York Times, May 28, 2017.\n- Economist, “America v China: The Battle for Digital Supremacy,” March 15, 2018.\n- Rasmus Rothe, “Applying Deep Learning to Real-World Problems,” Medium, May 23, 2017.\n- Eric Horvitz, “Reflections on the Status and Future of Artificial Intelligence,” Testimony before the U.S. Senate Subcommittee on Space, Science, and Competitiveness, November 30, 2016, p. 5.\n- Jeff Asher and Rob Arthur, “Inside the Algorithm That Tries to Predict Gun Violence in Chicago,” New York Times Upshot, June 13, 2017.\n- Caleb Watney, “It’s Time for our Justice System to Embrace Artificial Intelligence,” TechTank (blog), Brookings Institution, July 20, 2017.\n- Asher and Arthur, “Inside the Algorithm That Tries to Predict Gun Violence in Chicago.”\n- Paul Mozur and Keith Bradsher, “China’s A.I. Advances Help Its Tech Industry, and State Security,” New York Times, December 3, 2017.\n- Simon Denyer, “China’s Watchful Eye,” Washington Post, January 7, 2018.\n- Cameron Kerry and Jack Karsten, “Gauging Investment in Self-Driving Cars,” Brookings Institution, October 16, 2017.\n- Portions of this section are drawn from Darrell M. West, “Driverless Cars in China, Europe, Japan, Korea, and the United States,” Brookings Institution, September 2016.\n- Ibid.\n- Yuming Ge, Xiaoman Liu, Libo Tang, and Darrell M. West, “Smart Transportation in China and the United States,” Center for Technology Innovation, Brookings Institution, December 2017.\n- Peter Holley, “Uber Signs Deal to Buy 24,000 Autonomous Vehicles from Volvo,” Washington Post, November 20, 2017.\n- Daisuke Wakabayashi, “', '<2-hop>\n\nSelf-Driving Uber Car Kills Pedestrian in Arizona, Where Robots Roam,” New York Times, March 19, 2018.\n- Kevin Desouza, Rashmi Krishnamurthy, and Gregory Dawson, “Learning from Public Sector Experimentation with Artificial Intelligence,” TechTank (blog), Brookings Institution, June 23, 2017.\n- Boyd Cohen, “The 10 Smartest Cities in North America,” Fast Company, November 14, 2013.\n- Teena Maddox, “66% of US Cities Are Investing in Smart City Technology,” TechRepublic, November 6, 2017.\n- Osonde Osoba and William Welser IV, “ The Risks of Artificial Intelligence to Security and the Future of Work” (Santa Monica, Calif.: RAND Corp., December 2017) (www.rand.org/pubs/perspectives/PE237.html).\n- Ibid., p. 7.\n- Dominic Barton, Jonathan Woetzel, Jeongmin Seong, and Qinzheng Tian, “Artificial Intelligence: Implications for China” (New York: McKinsey Global Institute, April 2017), p. 7.\n- Executive Office of the President, “', '<3-hop>\n\nPreparing for the Future of Artificial Intelligence,” October 2016, pp. 30-31.\n- Elaine Glusac, “As Airbnb Grows, So Do Claims of Discrimination,” New York Times, June 21, 2016.\n- “Joy Buolamwini,” Bloomberg Businessweek, July 3, 2017, p. 80.\n- Ibid.\n- Mark Purdy and Paul Daugherty, “Why Artificial Intelligence is the Future of Growth,” Accenture, 2016.\n- Jon Valant, “Integrating Charter Schools and Choice-Based Education Systems,” Brown Center Chalkboard blog, Brookings Institution, June 23, 2017.\n- Tucker, “‘A White Mask Worked Better.’”\n- Cliff Kuang, “Can A.I. Be Taught to Explain Itself?” New York Times Magazine, November 21, 2017.\n- Yale Law School Information Society Project, “Governing Machine Learning,” September 2017.\n- Katie Benner, “Airbnb Vows to Fight Racism, But Its Users Can’t Sue to Prompt Fairness,” New York Times, June 19, 2016.\n- Executive Office of the President, “Artificial Intelligence, Automation, and the Economy” and “Preparing for the Future of Artificial Intelligence.”\n- Nancy Scolar, “Facebook’s Next Project: American Inequality,” Politico, February 19, 2018.\n- Darrell M. West, “What Internet Search Data Reveals about Donald Trump’s First Year in Office,” Brookings Institution policy report, January 17, 2018.\n- Ian Buck, “Testimony before the House Committee on Oversight and Government Reform Subcommittee on Information Technology,” February 14, 2018.\n- Keith Nakasone, “Testimony before the House Committee on Oversight and Government Reform Subcommittee on Information Technology,” March 7, 2018.\n- Greg Brockman, “The Dawn of Artificial Intelligence,” Testimony before U.S. Senate Subcommittee on Space, Science, and Competitiveness, November 30, 2016.\n- Amir Khosrowshahi, “Testimony before the House Committee on Oversight and Government Reform Subcommittee on Information Technology,” February 14, 2018.\n- James Kurose, “Testimony before the House Committee on Oversight and Government Reform Subcommittee on Information Technology,” March 7, 2018.\n- Stephen Noonoo, “Teachers Can Now Use IBM’s Watson to Search for Free Lesson Plans,” EdSurge, September 13, 2017.\n- Congress.gov, “H.R. 4625 FUTURE of Artificial Intelligence Act of 2017,” December 12, 2017.\n- Elizabeth Zima, “Could New York City’s AI Transparency Bill Be a Model for the Country?” Government Technology, January 4, 2018.']","The issue of discrimination in artificial intelligence is highlighted by claims faced by companies like Airbnb, which has been reported to have issues with discrimination among its users. This connection emphasizes the ethical considerations surrounding AI technologies, as biases in algorithms can lead to unfair treatment of individuals based on race or other characteristics. The implications for the future of AI technologies include the necessity for regulatory frameworks that address these biases and ensure fairness in AI applications, thereby promoting a more equitable society.",multi_hop_abstract_query_synthesizer
