user_input,reference_contexts,reference,synthesizer_name
What are the ethical implications and concerns surrounding the use of Meta AI in generative applications?,"['What is AI, how does it work and why are some people concerned about it?\nArtificial intelligence (AI) has increasingly become part of everyday life over the past decade.\nIt is being used to personalise social media feeds, spot friends and family in smartphone photos and pave the way for medical breakthroughs.\nBut the rise of chatbots like OpenAI\'s ChatGPT and Meta AI has been accompanied by concern about the technology\'s environmental impact, ethical implications and data use.\n What is AI and what is it used for?\nAI allows computers to process large amounts of data, identify patterns and follow detailed instructions about what to do with that information.\nComputers cannot think, empathise or reason.\nHowever, scientists have developed systems that can perform tasks which usually require human intelligence, trying to replicate how people acquire and use knowledge.\nThis could be trying to anticipate what product an online shopper might buy, based on previous purchases, in order to recommend items.\nThe technology is also behind voice-controlled virtual assistants like Apple\'s Siri and Amazon\'s Alexa, and is being used to develop systems for self-driving cars.\nAI also helps social platforms like Facebook, TikTok and X decide what posts to show users. Streaming services Spotify and Deezer use AI to suggest music.\nThere are also a number of applications in medicine, as scientists use AI to help spot cancers, review X-ray results, speed up diagnoses and identify new treatments.\n What is generative AI, and how do apps like ChatGPT and Meta AI work?\nGenerative AI is used to create new content which can seem like it has been made by a human.\nIt does this by learning from vast quantities of existing data such as online text and images.\nChatGPT and Chinese rival DeepSeek\'s chatbot are popular generative AI tools that can be used to produce text, images, code and more material.\nGoogle\'s Gemini or Meta AI can similarly hold text conversations with users.\nApps like Midjourney or Veo 3 are dedicated to creating images or video from simple text prompts.\nGenerative AI can also be used to make high-quality music.\nSongs mimicking the style or sound of famous musicians have gone viral, sometimes leaving fans confused about their authenticity.\n Why is AI controversial?\nWhile acknowledging AI\'s potential, some experts are worried about the implications of its rapid growth.\nThe International Monetary Fund (IMF) has warned AI could affect nearly 40% of jobs, and worsen global financial inequality.\nProf Geoffrey Hinton, a computer scientist regarded as one of the ""godfathers"" of AI development, has expressed concern that powerful AI systems could even make humans extinct - although his fear was dismissed by his fellow ""AI godfather"", Yann LeCun.\nCritics also highlight the tech\'s potential to reproduce biased information, or discriminate against some social groups.\nThis is because much of the data used to train AI comes from public material, including social media posts or comments, which can reflect existing societal biases such as sexism or racism.\nAnd while AI programmes are growing more adept, they are still prone to errors - such as creating images of people with the wrong number of fingers or limbs.\nGenerative AI systems are known for their ability to ""hallucinate"" and assert falsehoods as fact, even sometimes inventing sources for the inaccurate information.\nApple halted a new AI feature in January after it incorrectly summarised news app notifications.\nThe BBC complained about the feature after Apple\'s AI falsely told readers that Luigi Mangione - the man accused of killing UnitedHealthcare CEO Brian Thompson - had shot himself.\nGoogle has also faced criticism over inaccurate answers produced by its AI search overviews.\nThis has added to concerns about the use of AI in schools and workplaces, where it is increasingly used to help summarise texts, write emails or essays and solve bugs in code.\nThere are worries about students using AI technology to ""cheat"" on assignments, or employees ""smuggling"" it into work.\nWriters, musicians and artists have also pushed back against the technology on ethical grounds, accusing AI developers of using their work to train systems without consent or compensation.\nThousands of creators - including Abba singer-songwriter Björn Ulvaeus, writers Ian Rankin and Joanne Harris and actress Julianne Moore - signed a statement in October 2024 calling AI a ""major, unjust threat"" to their livelihoods.\n']","The rise of Meta AI, like other generative AI tools, has raised several ethical implications and concerns. Critics highlight the potential for AI to reproduce biased information or discriminate against certain social groups, as much of the data used to train AI comes from public material that may reflect existing societal biases such as sexism or racism. Additionally, there are worries about the accuracy of AI outputs, as generative AI systems are known to 'hallucinate' and assert falsehoods as fact, sometimes inventing sources for inaccurate information. This has led to concerns about the use of AI in educational and workplace settings, where it may be used to summarize texts or assist in writing, potentially enabling cheating or unethical practices. Furthermore, creators in various fields have expressed their discontent, arguing that AI developers often use their work to train systems without consent or compensation, which they view as a significant threat to their livelihoods.",single_hop_specific_query_synthesizer
What is the estimated energy consumption of the AI industry compared to the Netherlands?,"['How does AI effect the environment?\nIt is not clear how much energy AI systems use, but some researchers estimate the industry as a whole could soon consume as much as the Netherlands.\nCreating the powerful computer chips needed to run AI programmes requires lots of power and water.\nDemand for generative AI services has also meant an increase in the number of data centres which power them.\nThese huge halls - housing thousands of racks of computer servers - use substantial amounts of energy and require large volumes of water to keep them cool.\nSome large tech companies have invested in ways to reduce or reuse the water needed, or have opted for alternative methods such as air-cooling.\nHowever, some experts and activists fear that AI will worsen water supply problems.\nThe BBC was told in February that government plans to make the UK a ""world leader"" in AI could put already stretched supplies of drinking water under strain.\nIn September 2024, Google said it would reconsider proposals for a data centre in Chile, which has struggled with drought.\n Are there laws governing AI?\nSome governments have already introduced rules governing how AI operates.\nThe EU\'s Artificial Intelligence Act places controls on high risk systems used in areas such as education, healthcare, law enforcement or elections. It bans some AI use altogether.\nGenerative AI developers in China are required to safeguard citizens\' data, and promote transparency and accuracy of information. But they are also bound by the country\'s strict censorship laws.\nIn the UK, Prime Minister Sir Keir Starmer has said the government ""will test and understand AI before we regulate it"".\nBoth the UK and US have AI Safety Institutes that aim to identify risks and evaluate advanced AI models.\nIn 2024 the two countries signed an agreement to collaborate on developing ""robust"" AI testing methods.\nHowever, in February 2025, neither country signed an international AI declaration which pledged an open, inclusive and sustainable approach to the technology.\nSeveral countries including the UK are also clamping down on use of AI systems to create deepfake nude imagery and child sexual abuse material.\nSign up for our Tech Decoded newsletter to follow the world\'s top tech stories and trends. Outside the UK? Sign up here.']",Some researchers estimate that the AI industry as a whole could soon consume as much energy as the Netherlands.,single_hop_specific_query_synthesizer
Wut is the significanse of Artificial Intelligense in modern society and how does it impact business processes?,"['This article was published in 2018. To read more recent content from Brookings on Artificial Intelligence, please visit the AI topic page.\nMost people are not very familiar with the concept of artificial intelligence (AI). As an illustration, when 1,500 senior business leaders in the United States in 2017 were asked about AI, only 17 percent said they were familiar with it.1 A number of them were not sure what it was or how it would affect their particular companies. They understood there was considerable potential for altering business processes, but were not clear how AI could be deployed within their own organizations.\nDespite its widespread lack of familiarity, AI is a technology that is transforming every walk of life. It is a wide-ranging tool that enables people to rethink how we integrate information, analyze data, and use the resulting insights to improve decisionmaking. Our hope through this comprehensive overview is to explain AI to an audience of policymakers, opinion leaders, and interested observers, and demonstrate how AI already is altering the world and raising important questions for society, the economy, and governance.\nIn this paper, we discuss novel applications in finance, national security, health care, criminal justice, transportation, and smart cities, and address issues such as data access problems, algorithmic bias, AI ethics and transparency, and legal liability for AI decisions. We contrast the regulatory approaches of the U.S. and European Union, and close by making a number of recommendations for getting the most out of AI while still protecting important human values.2\nIn order to maximize AI benefits, we recommend nine steps for going forward:\n- Encourage greater data access for researchers without compromising users’ personal privacy,\n- invest more government funding in unclassified AI research,\n- promote new models of digital education and AI workforce development so employees have the skills needed in the 21st-century economy,\n- create a federal AI advisory committee to make policy recommendations,\n- engage with state and local officials so they enact effective policies,\n- regulate broad AI principles rather than specific algorithms,\n- take bias complaints seriously so AI does not replicate historic injustice, unfairness, or discrimination in data or algorithms,\n- maintain mechanisms for human oversight and control, and\n- penalize malicious AI behavior and promote cybersecurity.\n Qualities of artificial intelligence\nAlthough there is no uniformly agreed upon definition, AI generally is thought to refer to “machines that respond to stimulation consistent with traditional responses from humans, given the human capacity for contemplation, judgment and intention.”3 According to researchers Shubhendu and Vijay, these software systems “make decisions which normally require [a] human level of expertise” and help people anticipate problems or deal with issues as they come up.4 As such, they operate in an intentional, intelligent, and adaptive manner.\n']","Artificial Intelligence (AI) is a technology that is transforming every walk of life. It enables people to rethink how we integrate information, analyze data, and use the resulting insights to improve decision-making. Despite a widespread lack of familiarity with AI, it has considerable potential for altering business processes, although many business leaders are unclear on how it can be deployed within their organizations.",single_hop_specific_query_synthesizer
"What does Figure 22 illustrate about the variation in ChatGPT usage by education, particularly in relation to work-related messages?","['<1-hop>\n\n37% of messages are work-related\nfor users with less than a bachelor’s degree, compared to 46% for users with exactly a bachelor’s\ndegree and 48% for those with some graduate education. Those differences are cut roughly in half\nafter adjusting for other characteristics, but they are still statistically significant at the less than 1\npercent level. Educated users are more likely to send work-related messages.\nPanel B explores variation by education in user intent.Askingconstitutes about 49% of messages\nfor users with less than a bachelor’s degree, with little variation for more educated users. After\nregression adjustment, we find that users with a graduate degree are about two percentage points\nmore likely to use ChatGPT forAskingmessages, a difference that is statistically significant at the\n5% level. Prior to regression adjustment, the frequency ofDoingmessages is increasing in education.\nHowever, this pattern reverses after adjusting for other characteristics such as occupation. Users with\na graduate degree are about 1.6 percentage points less likely to sendDoingmessages than users with\nless than a bachelor’s degree, and the difference is statistically significant at the 10% level.\nPanel C studies variation by education in the frequency of four different conversation topics –\nPractical Guidance,Seeking Information,Technical Help, andWriting. We find only modest differ-\nences by education across most of these categories. The one exception is that the share of messages\nrelated toWritingis increasing in relation to education.\n28\x0cPanel A.Work Related\nPanel B1.Asking. Panel B2.Doing.\nPanel B3.Expressing.\nFigure 22:(continued on next page)\n29\x0cPanel C1.Writing. Panel C2.Technical Help.\nPanel C3.Seeking Information. Panel C4.Practical Guidance.\nFigure 22:Variation in ChatGPT usage by education. Each plot shows unadjusted vs. regression-adjusted\nestimates, with 95% confidence intervals. We regress each message share on education and occupation, control-\nling for the following covariates: age, whether the name was typically masculine or feminine, seniority within\nrole, company size, and industry. (To guarantee user privacy, we coarsen all covariates to broad categories and\nprogrammatically enforce that each group has at least 100 members prior to running the regression) We add\nthe coefficients on each education and occupation category to the unadjusted value for the reference category\nand compute 95% confidence intervals using the standard errors from the regression coefficients. The sample\nfor this regression is the approximately 40,000 users of the original 130,000 sample whose publicly available\noccupation was not blank or consisted of strictly special characters (as determined by a classification script).\nShares for each user are calculated by randomly sampling up to six conversations attributed to the user from\nMay 2024 through July 2025.\n30\x0c']","Figure 22 illustrates the variation in ChatGPT usage by education, showing that 37% of messages are work-related for users with less than a bachelor’s degree, compared to 46% for those with a bachelor’s degree and 48% for users with some graduate education. The figure also highlights that educated users are more likely to send work-related messages, and it provides regression-adjusted estimates that account for other characteristics such as occupation.",multi_hop_specific_query_synthesizer
"What does Figure 22 show about how ChatGPT is used at work, especially in relation to work-related tasks?","['<1-hop>\n\nPanel A.Work Related\n Panel B1.Asking. Panel B2.Doing.\nPanel B3.Expressing.\nFigure 22:(continued on next page)\n29\x0c Panel C1.Writing. Panel C2.Technical Help.\nPanel C3.Seeking Information. Panel C4.Practical Guidance.\n are classified asExpressing. 29Appendix D contains a full report of GWA counts broken down by occupation, for both work-related ChatGPT 31 We find remarkable similarity across occupations in how ChatGPT is used at work. For example, Making Decisions and Solving Problemsis one of the two most common GWAs in every single oc- cupation group where at least two GWAs can be reported. 30 Similarly,Documenting and Recording Informationranks in the top four of all occupations.Thinking Creativelyis ranked as the third most common GWA in 10 of the 13 occupation groups where at least three GWAs can be reported.']","Figure 22 illustrates the classification of work-related tasks into various categories such as Asking, Doing, Expressing, Writing, Technical Help, Seeking Information, and Practical Guidance. It highlights that Making Decisions and Solving Problems are among the two most common General Work Activities (GWAs) across all occupation groups where at least two GWAs can be reported. Additionally, Documenting and Recording Information ranks in the top four of all occupations, while Thinking Creatively is the third most common GWA in 10 out of the 13 occupation groups where at least three GWAs can be reported.",multi_hop_specific_query_synthesizer
"How does ChatGPT Business usage vary by occupation and education level, and what are the most common work-related activities associated with it?","['<1-hop>\n\nCorporate users may also use ChatGPT Business (formerly known as Teams) or ChatGPT Enterprise. 28Very few work-related messages are classified asExpressing. 29Appendix D contains a full report of GWA counts broken down by occupation, for both work-related ChatGPT 31 We find remarkable similarity across occupations in how ChatGPT is used at work. For example, Making Decisions and Solving Problemsis one of the two most common GWAs in every single oc- cupation group where at least two GWAs can be reported. 30 Similarly,Documenting and Recording Informationranks in the top four of all occupations.Thinking Creativelyis ranked as the third most common GWA in 10 of the 13 occupation groups where at least three GWAs can be reported. Even though there are 41 GWAs, the seven most common overall are also the most common within each occupation group and are ranked similarly. Not surprisingly,Working with Computersis the most common GWA in computer-related occupations. In the appendix, we report the full distribution of GWA classifications intersected with two-digit SOC codes, as well as the most frequently requested GWAs out of the subset of queries which are work-related. Across all occupations, ChatGPT usage is broadly focused on seeking information and assistance with decision-making. usage and all ChatGPT usage. 30For legal and food service occupations, we are only able to rank one of the GWAs because of user privacy protections - no other GWAs were requested by more than 100 users in that group. 32 Panel A.Work Related Panel B1.Asking. Panel B2.Doing. Figure 23:(continued on next page) 33 Panel C1.Writing. Panel C2.Technical Help. Panel C3.Seeking Information. Panel C4.Practical Guidance. Figure 23:Variation in ChatGPT usage by occupation. Panel A shows the share of messages that are work-related across broad occupation categories. Panel B presents variation in the share of Asking and Doing messages within work-related usage. Panel C presents the distribution of work-related conversation topics by occupation, focusing on Writing and Practical Guidance. The regression for these figures is the same one as the one used in Figure 22. 34 Occupation Group Documenting/ Recording Information Making Decisions And Solving Problems Thinking Creatively Working With Computers Interpreting The Meaning Of Information For Others Getting Information Providing Consultation And Advice To Others Management 2 1 3 6 4 5 8 Business 2 1 3 6 4 5 7 Computer/Math 4 2 5 1 3 6 7 Engineering 3 1 5 2 4 6 7 Science 2 1 4 3 6 5 7 Social Service 2 1 3 X 5 4 X Legal 1 X X X X X X Education 1 2 3 4 6 5 7 Arts/Design/Media 2 1 3 5 4 6 7 Health Professionals 1 2 3 X 5 4 6 Food Service 1 X X X X X X Personal Service 1 2 3 X 4 5 X Sales 2 1 3 6 4 5 7 Administrative 2 1 3 7 4 5 8 Transportation 2 1 3 X X 4 X Military 2 1 X X X X X Figure 24:The seven most commonly requested GWAs for work-related queries. Table reports the frequency ranking of each of these GWAs for each broad occupation groups (two-digit SOC codes). 1 represents the most frequently requested GWA for that occupation. X’s indicate that the ranking is unavailable since fewer than 100 users from that occupation group requested that specific GWA within the sample. Seven occupation groups are omitted because no GWA was requested by more than 100 users from a single occupation group. These omitted occupation groups (with corresponding SOC2 codes) are ”Healthcare Support” (31), ”Protective Service” (33), ”Building and Grounds Cleaning and Maintenance” (37), ”Farming, Fishing, and Forestry” (45), ”Construction and Extraction” (47), ”Installation, Maintenance, and Repair” (49), and ”Production” (51). Not pictured are twelve other GWAs which are less frequently requested and are reported fully in Appendix D. See Appendix for full cross-tabulations between', '<2-hop>\n\nFigure 22:Variation in ChatGPT usage by education. Each plot shows unadjusted vs. regression-adjusted estimates, with 95% confidence intervals. We regress each message share on education and occupation, control- ling for the following covariates: age, whether the name was typically masculine or feminine, seniority within role, company size, and industry. (To guarantee user privacy, we coarsen all covariates to broad categories and programmatically enforce that each group has at least 100 members prior to running the regression) We add the coefficients on each education and occupation category to the unadjusted value for the reference category and compute 95% confidence intervals using the standard errors from the regression coefficients. The sample for this regression is the approximately 40,000 users of the original 130,000 sample whose publicly available occupation was not blank or consisted of strictly special characters (as determined by a classification script). Shares for each user are calculated by randomly sampling up to six conversations attributed to the user from May 2024 through July 2025. 30 6.5 Variation by Occupation Figure 23 presents variation in ChatGPT usage by user occupation. Due to privacy-preserving aggre- gation limits, we report results for the following broad occupation categories – (1) all nonprofessional occupations, including administrative, clerical, service, and blue-collar occupations; (2) computer- related occupations; (3) engineering and science occupations; (4) management and business occupa- tions; and (5) all other professional occupations, including law, education, and health care. 26 As above, the left-hand side of the figure shows unadjusted comparisons and the right-hand side presents the coefficients on each occupation category from a regression of message shares on age, whether the name was typically masculine or feminine, education, occupation categories, job seniority, firm size, and industry. Users in highly paid professional and technical occupations are more likely to use ChatGPT for work.27 Panel A shows that the unadjusted work shares are 57% for computer-related occupations; 50% for management and business; 48% for engineering and science; 44% for other professional oc- cupations; and only 40% for all non-professional occupations. Regression adjustment moves these figures around slightly, but the gaps by occupation remain highly statistically significant. Users in highly-paid professional occupations are more likely to send work-related messages. Because work usage is so different by occupation, we restrict the sample only to work-related messages in Panels B and C. Panel B presents the share of work-related messages that areAsking messages, by occupation. We find that users in highly paid professional occupations are more likely to use ChatGPT forAskingrather thanDoing. 28 This is especially true in scientific and technical occupations. 47% of the work-related messages sent by users employed in computer-related occupa- tions areAskingmessages, compared to only 32% for non-professional occupations. These differences shrink somewhat with regression adjustment, but remain highly statistically significant. Panel C presents results by conversation topic.Writingis especially common for users employed in management and business occupations, accounting for 52% of all work-related messages. Writing is also relatively common in non-professional and other professional occupations like education and health care, accounting for 50% and 49% of work-related messages respectively.Technical Helpconsti- tutes 37% of all work-related messages for users employed in computer-related occupations, compared to 16% in engineering and science and only about 8% for all other categories. Regression adjustment affects gaps by occupation only modestly. Overall there are stark differences in the distribution of conversation topics by user occupation, with work-related messages clearly focused on the core tasks in each job (e.g.Writingfor management and business,Technical Helpfor technical occupations). We also present data on the most common Generalized Work Activities (GWAs) associated with each broad occupation group, as measured by 2-digit Standard Occupation Classification (SOC) codes. Table 24 presents the frequency ranking of work-related messages in each SOC code of the seven most common GWAs.29 26Management and business are SOC2 codes 11 and 13. Computer-related is SOC2 code 15. Engineering and Science are SOC2 codes 17 and 19. Other Professional are SOC2 codes 21 to 29. Nonprofessional occupations are SOC codes 31 to 53. 27As discussed in Section: Data and Privacy, our dataset only includes users on ChatGPT Consumer plans. Corporate users may also use ChatGPT Business (formerly known as Teams) or ChatGPT Enterprise. 28Very few work-related messages']","ChatGPT Business usage varies significantly by occupation and education level. Users in highly paid professional occupations, such as management and business, are more likely to use ChatGPT for work-related tasks. For instance, 50% of work-related messages come from management and business occupations, while only 40% come from non-professional occupations. The most common work-related activities associated with ChatGPT usage include Documenting and Recording Information, Making Decisions and Solving Problems, and Thinking Creatively. These activities rank similarly across various occupation groups, indicating a broad focus on seeking information and assistance with decision-making in the workplace.",multi_hop_specific_query_synthesizer
How does the environmental impact of artificial intelligence relate to the concerns about its ethical implications and energy consumption?,"['<1-hop>\n\nWhat is AI, how does it work and why are some people concerned about it?\nArtificial intelligence (AI) has increasingly become part of everyday life over the past decade.\nIt is being used to personalise social media feeds, spot friends and family in smartphone photos and pave the way for medical breakthroughs.\nBut the rise of chatbots like OpenAI\'s ChatGPT and Meta AI has been accompanied by concern about the technology\'s environmental impact, ethical implications and data use.\n What is AI and what is it used for?\nAI allows computers to process large amounts of data, identify patterns and follow detailed instructions about what to do with that information.\nComputers cannot think, empathise or reason.\nHowever, scientists have developed systems that can perform tasks which usually require human intelligence, trying to replicate how people acquire and use knowledge.\nThis could be trying to anticipate what product an online shopper might buy, based on previous purchases, in order to recommend items.\nThe technology is also behind voice-controlled virtual assistants like Apple\'s Siri and Amazon\'s Alexa, and is being used to develop systems for self-driving cars.\nAI also helps social platforms like Facebook, TikTok and X decide what posts to show users. Streaming services Spotify and Deezer use AI to suggest music.\nThere are also a number of applications in medicine, as scientists use AI to help spot cancers, review X-ray results, speed up diagnoses and identify new treatments.\n What is generative AI, and how do apps like ChatGPT and Meta AI work?\nGenerative AI is used to create new content which can seem like it has been made by a human.\nIt does this by learning from vast quantities of existing data such as online text and images.\nChatGPT and Chinese rival DeepSeek\'s chatbot are popular generative AI tools that can be used to produce text, images, code and more material.\nGoogle\'s Gemini or Meta AI can similarly hold text conversations with users.\nApps like Midjourney or Veo 3 are dedicated to creating images or video from simple text prompts.\nGenerative AI can also be used to make high-quality music.\nSongs mimicking the style or sound of famous musicians have gone viral, sometimes leaving fans confused about their authenticity.\n Why is AI controversial?\nWhile acknowledging AI\'s potential, some experts are worried about the implications of its rapid growth.\nThe International Monetary Fund (IMF) has warned AI could affect nearly 40% of jobs, and worsen global financial inequality.\nProf Geoffrey Hinton, a computer scientist regarded as one of the ""godfathers"" of AI development, has expressed concern that powerful AI systems could even make humans extinct - although his fear was dismissed by his fellow ""AI godfather"", Yann LeCun.\nCritics also highlight the tech\'s potential to reproduce biased information, or discriminate against some social groups.\nThis is because much of the data used to train AI comes from public material, including social media posts or comments, which can reflect existing societal biases such as sexism or racism.\nAnd while AI programmes are growing more adept, they are still prone to errors - such as creating images of people with the wrong number of fingers or limbs.\nGenerative AI systems are known for their ability to ""hallucinate"" and assert falsehoods as fact, even sometimes inventing sources for the inaccurate information.\nApple halted a new AI feature in January after it incorrectly summarised news app notifications.\nThe BBC complained about the feature after Apple\'s AI falsely told readers that Luigi Mangione - the man accused of killing UnitedHealthcare CEO Brian Thompson - had shot himself.\nGoogle has also faced criticism over inaccurate answers produced by its AI search overviews.\nThis has added to concerns about the use of AI in schools and workplaces, where it is increasingly used to help summarise texts, write emails or essays and solve bugs in code.\nThere are worries about students using AI technology to ""cheat"" on assignments, or employees ""smuggling"" it into work.\nWriters, musicians and artists have also pushed back against the technology on ethical grounds, accusing AI developers of using their work to train systems without consent or compensation.\nThousands of creators - including Abba singer-songwriter Björn Ulvaeus, writers Ian Rankin and Joanne Harris and actress Julianne Moore - signed a statement in October 2024 calling AI a ""major, unjust threat"" to their livelihoods.\n', '<2-hop>\n\nHow does AI effect the environment?\nIt is not clear how much energy AI systems use, but some researchers estimate the industry as a whole could soon consume as much as the Netherlands.\nCreating the powerful computer chips needed to run AI programmes requires lots of power and water.\nDemand for generative AI services has also meant an increase in the number of data centres which power them.\nThese huge halls - housing thousands of racks of computer servers - use substantial amounts of energy and require large volumes of water to keep them cool.\nSome large tech companies have invested in ways to reduce or reuse the water needed, or have opted for alternative methods such as air-cooling.\nHowever, some experts and activists fear that AI will worsen water supply problems.\nThe BBC was told in February that government plans to make the UK a ""world leader"" in AI could put already stretched supplies of drinking water under strain.\nIn September 2024, Google said it would reconsider proposals for a data centre in Chile, which has struggled with drought.\n Are there laws governing AI?\nSome governments have already introduced rules governing how AI operates.\nThe EU\'s Artificial Intelligence Act places controls on high risk systems used in areas such as education, healthcare, law enforcement or elections. It bans some AI use altogether.\nGenerative AI developers in China are required to safeguard citizens\' data, and promote transparency and accuracy of information. But they are also bound by the country\'s strict censorship laws.\nIn the UK, Prime Minister Sir Keir Starmer has said the government ""will test and understand AI before we regulate it"".\nBoth the UK and US have AI Safety Institutes that aim to identify risks and evaluate advanced AI models.\nIn 2024 the two countries signed an agreement to collaborate on developing ""robust"" AI testing methods.\nHowever, in February 2025, neither country signed an international AI declaration which pledged an open, inclusive and sustainable approach to the technology.\nSeveral countries including the UK are also clamping down on use of AI systems to create deepfake nude imagery and child sexual abuse material.\nSign up for our Tech Decoded newsletter to follow the world\'s top tech stories and trends. Outside the UK? Sign up here.']","The environmental impact of artificial intelligence is significant, as some researchers estimate that the AI industry could soon consume as much energy as the Netherlands. This high energy demand is partly due to the powerful computer chips required to run AI programs, which consume substantial amounts of power and water. Additionally, the rise of generative AI services has led to an increase in data centers, which house thousands of computer servers and require large volumes of energy and water for cooling. These environmental concerns are intertwined with ethical implications, as the rapid growth of AI raises worries about its potential to exacerbate existing societal biases and inequalities. Critics highlight that the data used to train AI often reflects societal biases, which can lead to discrimination against certain social groups. Furthermore, the environmental strain caused by AI could worsen issues like water supply problems, as seen in regions like Chile, where drought conditions are already a concern. Thus, the ethical governance of AI must consider both its societal implications and its environmental footprint.",multi_hop_abstract_query_synthesizer
How do privacy protections and de-identification processes work in the analysis of ChatGPT user messages?,"['<1-hop>\n\nWe describe the contents of each dataset, the sampling procedures that produced them, and the\nprivacy protections we implemented in constructing and employing them in analysis.\n 3.1 Growth Dataset\nWe compiled a dataset covering all usage on consumer ChatGPT Plans (Free, Plus, Pro) since Chat-\nGPT’s launch in November 2022. We exclude users on non-consumer plans (Business f.k.a. Teams,\n14The exact beginning and end dates of this sample are May 15, 2024 and June 26, 2025.\n15The exact beginning and end dates of this sample are May 15, 2024 and July 31, 2025.\n5\x0cEnterprise, Education).\nFor each user and day, this dataset reports the total number of messages sent by the user on that\nday. It also reports, for each message, de-identified user metadata, including the timestamp of their\nfirst interaction with ChatGPT, the country from which their account is registered, their subscription\nplan on each day, and their self-reported age (reported in coarse 5–7-year buckets to protect user\nprivacy).\n 3.2 Classified Messages\nTo understand usage while preserving user privacy, we construct message-level datasets without any\nhuman ever reading the contents of a message. See Figure 1 for an overview of the privacy-preserving\nclassification pipeline. Messages are categorized according to 5 different LLM-based classifiers. The\nclassifiers are introduced in more detail in Section 5, their exact text is reproduced in Appendix A,\nand our validation procedure is described in Appendix B.\n Sampled From All ChatGPT Users.We uniformly sampled approximately 1.1 million conver-\nsations, and then sampled one message within each conversation, with the following restrictions:\n1. We only include messages from May 2024 to July 2025.\n2. We exclude conversations from users who opted out of sharing their messages for model training.\n3. We exclude users who self-report their age as under 18.\n4. We exclude conversations that users have deleted and from users whose accounts have been\ndeactivated or banned.\n5. We exclude logged-out users, 16 which represented a minority share of ChatGPT users over the\nsample period.\nOur sample is drawn from a table that is itself sampled, where the sampling rate varied over time.\nWe thus adjust our sampling weights to maintain a fixed ratio with aggregate messages sent.\n Sampled From a Subset of ChatGPT Users.We construct two samples of classified messages\nfrom a subset of ChatGPT users (approximately 130,000 users). This sample of users does not include\nany users who opted out of sharing their messages for training, nor does it include users whose self-\nreported age is below 18, nor does it include users who have been banned or deleted their accounts.\nThe first sample contains classifications of 1.58 million messages from this subset of users, sampled\nat the conversation level (a conversation is a series of messages between the user and chatbot). This\nsample is constructed such that the user’s representation in the data is proportional to overall message\nvolume. The second sample contains messages sent from this subset of users, sampled at the user level\nwith up to six messages from each user in the group.\n16ChatGPT became available to logged-out users in April 2024, i.e., users could use ChatGPT without signing up\nfor an account with an email address. However, messages from logged-out users are only available in our dataset from\nMarch 2025, thus for consistency we drop all messages from logged-out users.\n6\x0cFigure 1:Illustration of Privacy-Preserving Automated Classification Pipeline (Synthetic Example). Mes-\nsages are first stripped of PII via an internal LLM-based tool calledPrivacy Filter. Then they are classified by\nLLM-based automated classifiers, described in detail in Appendices A and B. Humans do not see raw messages\nor PII-scrubbed messages, only the final classifications of messages.\n', '<2-hop>\n\nPrivacy via Automated Classifiers.No one looked at the content of messages while conducting\nanalysis for this paper. All analysis of message content was performed via automated LLM-based\nclassifiers run on de-identified and PII-scrubbed message data (see Figure 1). The messages are first\nscrubbed of PII using an internal LLM-based tool,17 and then classified according to classifiers defined\nover a controlled label space—the most precise classifier we use on the message-level data set is the\nO*NET Intermediate Work Activities taxonomy, which we augment to end up with 333 categories.\nWe introduce technical and procedural frictions that prevent accidental access to the underlying text\n(for example, interfaces that do not render message text to researchers).\nOur classifications aim to discern the intent of a given message, and thus we include the prior 10\nmessages in a conversation as context. 18 For an example, see Table 2.\nStand-Alone Message Message with Prior Context\n[user]: “10 more” [user]: “give me 3 cultural activities to do with teens”\n[assistant]: “1. Visit a museum . . . ” (truncated)\n[user]: “10 more”\nTable 2:Illustration of Context-Augmented Message Classifications (Synthetic Example). The left column\nshows a standalone message to be classified, and the right column shows the prior context included in the\nclassification of the message on the left.\nWe truncate each message to a maximum of 5,000 characters, because long context windows could\ninduce variability in the quality of the classification (Liu et al., 2023). We classify each message\nwith the “gpt-5-mini” model, with the exception ofInteraction Quality,which uses “gpt-5,” using the\nprompts listed in Appendix A.\n17Internal analyses show that the tool,Privacy Filter, has substantial alignment with human judgment.\n']","Privacy protections in the analysis of ChatGPT user messages are implemented through a series of automated classifiers that operate on de-identified and PII-scrubbed message data. No human ever looks at the content of the messages; instead, an internal LLM-based tool called Privacy Filter is used to scrub messages of personally identifiable information (PII) before they are classified. The classifiers categorize messages based on a controlled label space, ensuring that the analysis maintains user privacy while still allowing for insights into message intent. This process includes technical and procedural frictions to prevent accidental access to the underlying text, thereby safeguarding user privacy throughout the analysis.",multi_hop_abstract_query_synthesizer
"What trends can be observed in user cohort analysis regarding ChatGPT query classification, particularly in relation to the growth of non-work-related messages from different user cohorts over time?","['<1-hop>\n\nThe yellow line represents the first cohort of ChatGPT users: their usage declined somewhat over\n2023, but started growing again in late 2024 and is now higher than it has ever been. The pink line\nrepresents messages from users who signed up in Q3 of 2023 or earlier, and so thedifferencebetween\n20Note that we expect our counts of distinct accounts to somewhat exceed distinct people when one person has two\naccounts (or, for logged-out users, one person using two devices). For logged-in users, the count is based on distinct\nlogin credentials (email addresses), and one person may have multiple accounts. For logged-out users, the count is based\non distinct browser cookies; this would double-count people if someone returns to ChatGPT after clearing their cookies,\nor if they access ChatGPT with two different devices in the same week.\n10\x0cFigure 4:Daily message volumes from ChatGPT consumer plans (Free, Plus, Pro), split by sign-up date of\nthe requesting user. Reported values are moving averages of the past 90 days. Y-axis is an index normalized\nto the reported value for ”All Cohorts” at the end of Q1 2024 (April 1, 2024).\nthe yellow and pink lines represents the messages sent by users who signed up in Q2 and Q3 of 2023.\nThere has been dramatic growth in message volume both by new cohorts of users, and from growth\nin existing cohorts.\nFigure 5 normalizes each cohort, plotting daily messages per weekly active user. Each line rep-\nresents an individual cohort (instead of a cumulative cohort, as in Figure 4). The figure shows that\nearlier sign-ups have consistently had higher usage, but that usage has also consistently grown within\nevery cohort, which we interpret as due to both (1) improvements in the capabilities of the models,\nand (2) users slowly discovering new uses for existing capabilities.\n 5 How ChatGPT is Used\nWe next report on thecontentof ChatGPT conversations using a variety of different taxonomies. For\neach taxonomy we describe a “prompt” which defines a set of categories, and then apply an LLM\nto map each message to a category. Our categories often apply to the user’sintention, rather than\nthe text of the conversation, and as such we never directly observe the ground truth. Nevertheless\nthe classifier results can be interpreted as the best-guess inferences that a human would make: the\nguesses from the LLM correlate highly with human guesses from the same prompt, and we get similar\nqualitative results when the prompt includes a third category for “uncertain.”\n11\x0cFigure 5:Daily messages sent per weekly active user, split by sign-up cohort. Sample only considers users of\nChatGPT consumer plans (Free, Plus, Pro). Reported values are moving averages of the past 90 days and are\nreported starting 90 days after the cohort is fully formed. Y-axis is an index normalized to the first reported\nvalue for the Q1 2023 cohort.\n', '<2-hop>\n\n5.1 What share of ChatGPT queries are related to paid work?\nWe label each user message in our dataset based on whether it appears to be related to work, using\nan LLM classifier. The critical part of the prompt is as follows: 21\nDoes the last user message of this conversation transcript seem likely to be related to doing\nsome work/employment? Answer with one of the following:\n(1) likely part of work (e.g., “rewrite this HR complaint”)\n(0) likely not part of work (e.g., “does ice reduce pimples?”)\nTable 1 shows that both types of queries grew rapidly between June 2024 and June 2025, however\nnon-work-related messages grew faster: 53% of messages were not related to work in June 2024, which\nclimbed to 73% by June 2025.\nFigure 6 plots the share of non-work messages decomposed by cumulative sign-up cohorts. Succes-\nsive cohorts have had a higher share of non-work messages, but also within each cohort their non-work\nuse has increased. Comparing the share among all users (black line) to the share among the earliest\ncohort of users (yellow line), we can see that they track very closely.\n21See Appendix A for the full prompt, see Appendix B for validation.\n12\x0cFigure 6:The solid black line represents the probability that a messages on a given day is not related to\nwork, as determined by an automated classifier. Values are averaged over a 28-day lagging window. The\ndotted orange line shows the same calculation, but conditioned on messages being from users who first used\nChatGPT during or before Q2 of 2024. The remaining lines are defined similarly for successive quarters, with\ncoloring cooling for more recent cohorts. Counts are calculated from a sample of approximately 1.1 million\nsampled conversations from May 15, 2024 through June 26, 2025. Observations are reweighted to reflect total\nmessage volumes on a given day. Sampling details available in Section 3.\n']","User cohort analysis reveals that there has been dramatic growth in message volume from both new and existing cohorts of ChatGPT users. Specifically, the analysis indicates that earlier sign-ups consistently exhibit higher usage, while all cohorts have shown growth in usage due to improvements in model capabilities and users discovering new applications for existing features. In terms of query classification, a significant trend is observed in the share of messages related to work versus non-work. Between June 2024 and June 2025, non-work-related messages grew faster than work-related ones, with the share of non-work messages increasing from 53% to 73%. This trend is consistent across different cohorts, indicating that as new users join, the proportion of non-work-related queries continues to rise, reflecting a shift in user intent and engagement with ChatGPT.",multi_hop_abstract_query_synthesizer
"What are the environmental concerns related to artificial intelligence, and how do they connect to the energy consumption of AI systems?","['<1-hop>\n\nWhat is AI, how does it work and why are some people concerned about it?\nArtificial intelligence (AI) has increasingly become part of everyday life over the past decade.\nIt is being used to personalise social media feeds, spot friends and family in smartphone photos and pave the way for medical breakthroughs.\nBut the rise of chatbots like OpenAI\'s ChatGPT and Meta AI has been accompanied by concern about the technology\'s environmental impact, ethical implications and data use.\n What is AI and what is it used for?\nAI allows computers to process large amounts of data, identify patterns and follow detailed instructions about what to do with that information.\nComputers cannot think, empathise or reason.\nHowever, scientists have developed systems that can perform tasks which usually require human intelligence, trying to replicate how people acquire and use knowledge.\nThis could be trying to anticipate what product an online shopper might buy, based on previous purchases, in order to recommend items.\nThe technology is also behind voice-controlled virtual assistants like Apple\'s Siri and Amazon\'s Alexa, and is being used to develop systems for self-driving cars.\nAI also helps social platforms like Facebook, TikTok and X decide what posts to show users. Streaming services Spotify and Deezer use AI to suggest music.\nThere are also a number of applications in medicine, as scientists use AI to help spot cancers, review X-ray results, speed up diagnoses and identify new treatments.\n What is generative AI, and how do apps like ChatGPT and Meta AI work?\nGenerative AI is used to create new content which can seem like it has been made by a human.\nIt does this by learning from vast quantities of existing data such as online text and images.\nChatGPT and Chinese rival DeepSeek\'s chatbot are popular generative AI tools that can be used to produce text, images, code and more material.\nGoogle\'s Gemini or Meta AI can similarly hold text conversations with users.\nApps like Midjourney or Veo 3 are dedicated to creating images or video from simple text prompts.\nGenerative AI can also be used to make high-quality music.\nSongs mimicking the style or sound of famous musicians have gone viral, sometimes leaving fans confused about their authenticity.\n Why is AI controversial?\nWhile acknowledging AI\'s potential, some experts are worried about the implications of its rapid growth.\nThe International Monetary Fund (IMF) has warned AI could affect nearly 40% of jobs, and worsen global financial inequality.\nProf Geoffrey Hinton, a computer scientist regarded as one of the ""godfathers"" of AI development, has expressed concern that powerful AI systems could even make humans extinct - although his fear was dismissed by his fellow ""AI godfather"", Yann LeCun.\nCritics also highlight the tech\'s potential to reproduce biased information, or discriminate against some social groups.\nThis is because much of the data used to train AI comes from public material, including social media posts or comments, which can reflect existing societal biases such as sexism or racism.\nAnd while AI programmes are growing more adept, they are still prone to errors - such as creating images of people with the wrong number of fingers or limbs.\nGenerative AI systems are known for their ability to ""hallucinate"" and assert falsehoods as fact, even sometimes inventing sources for the inaccurate information.\nApple halted a new AI feature in January after it incorrectly summarised news app notifications.\nThe BBC complained about the feature after Apple\'s AI falsely told readers that Luigi Mangione - the man accused of killing UnitedHealthcare CEO Brian Thompson - had shot himself.\nGoogle has also faced criticism over inaccurate answers produced by its AI search overviews.\nThis has added to concerns about the use of AI in schools and workplaces, where it is increasingly used to help summarise texts, write emails or essays and solve bugs in code.\nThere are worries about students using AI technology to ""cheat"" on assignments, or employees ""smuggling"" it into work.\nWriters, musicians and artists have also pushed back against the technology on ethical grounds, accusing AI developers of using their work to train systems without consent or compensation.\nThousands of creators - including Abba singer-songwriter Björn Ulvaeus, writers Ian Rankin and Joanne Harris and actress Julianne Moore - signed a statement in October 2024 calling AI a ""major, unjust threat"" to their livelihoods.\n', '<2-hop>\n\nHow does AI effect the environment?\nIt is not clear how much energy AI systems use, but some researchers estimate the industry as a whole could soon consume as much as the Netherlands.\nCreating the powerful computer chips needed to run AI programmes requires lots of power and water.\nDemand for generative AI services has also meant an increase in the number of data centres which power them.\nThese huge halls - housing thousands of racks of computer servers - use substantial amounts of energy and require large volumes of water to keep them cool.\nSome large tech companies have invested in ways to reduce or reuse the water needed, or have opted for alternative methods such as air-cooling.\nHowever, some experts and activists fear that AI will worsen water supply problems.\nThe BBC was told in February that government plans to make the UK a ""world leader"" in AI could put already stretched supplies of drinking water under strain.\nIn September 2024, Google said it would reconsider proposals for a data centre in Chile, which has struggled with drought.\n Are there laws governing AI?\nSome governments have already introduced rules governing how AI operates.\nThe EU\'s Artificial Intelligence Act places controls on high risk systems used in areas such as education, healthcare, law enforcement or elections. It bans some AI use altogether.\nGenerative AI developers in China are required to safeguard citizens\' data, and promote transparency and accuracy of information. But they are also bound by the country\'s strict censorship laws.\nIn the UK, Prime Minister Sir Keir Starmer has said the government ""will test and understand AI before we regulate it"".\nBoth the UK and US have AI Safety Institutes that aim to identify risks and evaluate advanced AI models.\nIn 2024 the two countries signed an agreement to collaborate on developing ""robust"" AI testing methods.\nHowever, in February 2025, neither country signed an international AI declaration which pledged an open, inclusive and sustainable approach to the technology.\nSeveral countries including the UK are also clamping down on use of AI systems to create deepfake nude imagery and child sexual abuse material.\nSign up for our Tech Decoded newsletter to follow the world\'s top tech stories and trends. Outside the UK? Sign up here.']","The environmental concerns related to artificial intelligence (AI) include its significant energy consumption and the impact of data centers that power AI services. Some researchers estimate that the AI industry could soon consume as much energy as the Netherlands. The creation of powerful computer chips necessary for running AI programs requires substantial amounts of power and water. Additionally, the demand for generative AI services has led to an increase in the number of data centers, which house thousands of computer servers and use large volumes of energy and water for cooling. This situation raises fears that AI could exacerbate existing water supply issues, particularly in regions already facing drought, as highlighted by Google's reconsideration of a data center proposal in Chile.",multi_hop_abstract_query_synthesizer
