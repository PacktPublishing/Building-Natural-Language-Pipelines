{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75f744c9",
   "metadata": {},
   "source": [
    "üîß  **Setup Required**: Before running this notebook, please follow the [setup instructions](../README.md#setup-instructions) to configure your environment and API keys. **You will need to ensure you've executed the Indexing pipeline before completing this exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ad4fc5",
   "metadata": {},
   "source": [
    "# Systematic RAG Evaluation: Naive vs Hybrid Comparison\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates a **systematic evaluation workflow** for comparing two RAG (Retrieval-Augmented Generation) approaches using **Haystack custom components**. We'll create a reproducible pipeline to:\n",
    "\n",
    "1. **Load evaluation datasets** from CSV files\n",
    "2. **Process queries** through both Naive and Hybrid RAG SuperComponents \n",
    "3. **Generate comprehensive metrics** using the RAGAS framework\n",
    "4. **Compare performance** systematically\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand how to:\n",
    "- Create reusable evaluation components for RAG systems\n",
    "- Build scalable pipelines for systematic RAG comparison\n",
    "- Interpret RAGAS metrics in comparative context\n",
    "- Make data-driven decisions between RAG approaches\n",
    "\n",
    "## Evaluation Pipeline\n",
    "\n",
    "Our pipeline consists of three main components:\n",
    "\n",
    "```\n",
    "CSV Data ‚Üí RAGDataAugmenter ‚Üí RagasEvaluation ‚Üí Metrics & Results\n",
    "```\n",
    "\n",
    "**Key Benefits:**\n",
    "- **Systematic**: Same evaluation conditions for both RAG systems\n",
    "- **Reproducible**: Consistent evaluation across experiments\n",
    "- **Scalable**: Easy to add new RAG implementations\n",
    "- **Comprehensive**: Multiple metrics provide complete assessment\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4640ae5",
   "metadata": {},
   "source": [
    "## Component 1: CSV Data Loader\n",
    "\n",
    "The **CSVReaderComponent** serves as the entry point for our evaluation pipeline. It handles loading synthetic evaluation datasets and ensures data quality before processing.\n",
    "\n",
    "**Key Features:**\n",
    "- **Robust Error Handling**: Validates file existence and data integrity\n",
    "- **Pandas Integration**: Returns data as DataFrame for easy manipulation\n",
    "- **Pipeline Compatible**: Designed to work seamlessly with Haystack pipelines\n",
    "\n",
    "**Input:** File path to CSV containing evaluation queries and ground truth\n",
    "**Output:** Pandas DataFrame ready for RAG processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08a83513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from haystack import component\n",
    "from typing import Optional, Dict, Any, Union\n",
    "\n",
    "@component\n",
    "class CSVReaderComponent:\n",
    "    \"\"\"Reads a CSV file into a Pandas DataFrame.\"\"\"\n",
    "\n",
    "    @component.output_types(data_frame=pd.DataFrame)\n",
    "    def run(self, source: Union[str, Path]):\n",
    "        \"\"\"\n",
    "        Reads the CSV file from the first source in the list.\n",
    "        \n",
    "        Args:\n",
    "            sources: List of file paths to CSV files. Only the first file will be processed.\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary containing the loaded DataFrame under 'data_frame' key.\n",
    "            \n",
    "        Raises:\n",
    "            FileNotFoundError: If the file doesn't exist or can't be read.\n",
    "            ValueError: If the DataFrame is empty after loading.\n",
    "        \"\"\"\n",
    "        if not source:\n",
    "            raise ValueError(\"No sources provided\")\n",
    "            \n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(source)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"File not found at {source}\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error reading CSV file {source}: {str(e)}\")\n",
    "\n",
    "        # Check if DataFrame is empty using proper pandas method\n",
    "        if df.empty:\n",
    "            raise ValueError(f\"DataFrame is empty after loading from {source}\")\n",
    "\n",
    "        print(f\"Loaded DataFrame with {len(df)} rows from {source}.\")\n",
    "        return {\"data_frame\": df}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1861dfb2",
   "metadata": {},
   "source": [
    "## Component 2: Async RAG Data Augmentation\n",
    "\n",
    "The **AsyncRAGDataAugmenterComponent** is the core of our evaluation workflow. It processes queries through a RAG SuperComponent with concurrent execution for optimal performance.\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "1. **Concurrent Processing**: Processes multiple queries in parallel batches\n",
    "2. **SuperComponent Flexibility**: Accepts any pre-configured RAG SuperComponent (Naive, Hybrid, or custom)\n",
    "3. **Configurable Batch Size**: Control concurrency based on API rate limits\n",
    "4. **Progress Tracking**: Real-time visibility into processing status\n",
    "5. **Error Handling**: Gracefully handles failures without stopping the entire evaluation\n",
    "\n",
    "**Performance Benefits:**\n",
    "- **Speed**: Up to N√ó faster than sequential processing (where N = batch_size)\n",
    "- **Scalability**: Efficiently handles large evaluation datasets\n",
    "- **Resource Optimization**: Maximizes API call efficiency\n",
    "\n",
    "**Pipeline Integration:**\n",
    "- **Input**: DataFrame with queries from CSVReaderComponent  \n",
    "- **Process**: Runs queries through RAG SuperComponent in concurrent batches\n",
    "- **Output**: Augmented DataFrame with responses and retrieved contexts\n",
    "\n",
    "**Why Async?**\n",
    "- **Faster Iteration**: Quickly evaluate large datasets\n",
    "- **Better Resource Utilization**: Maximize throughput without overwhelming APIs\n",
    "- **Production Ready**: Scalable approach for continuous evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3771b673",
   "metadata": {},
   "source": [
    "## Implementation: Concurrent Query Processing\n",
    "\n",
    "Below is the implementation of the async component that enables concurrent query processing.\n",
    "### ‚ö†Ô∏è Important Thread-Safety Considerations\n",
    "\n",
    "**RAG SuperComponent Sharing:**\n",
    "- The `AsyncRAGDataAugmenterComponent` accepts a single `rag_supercomponent` instance\n",
    "- **Haystack SuperComponents are NOT guaranteed to be thread-safe**\n",
    "- When using `batch_size > 1`, multiple queries may access the same component instance concurrently\n",
    "\n",
    "**Current Safety Approach:**\n",
    "- **Default `batch_size=1`**: Processes queries sequentially within each system to avoid conflicts\n",
    "- **Concurrent System Evaluation**: Multiple RAG systems can still be evaluated in parallel\n",
    "- **Trade-off**: Sacrifices within-system concurrency for stability\n",
    "\n",
    "**For Higher Concurrency:**\n",
    "\n",
    "If you need `batch_size > 1`, ensure:\n",
    "1. Each RAG SuperComponent has **separate model instances** (not shared)\n",
    "2. Test thoroughly for race conditions\n",
    "3. Monitor for embedding model conflicts\n",
    "\n",
    "**Alternative Approaches:**\n",
    "```python\n",
    "# Option 1: Create separate RAG instances per query (memory intensive)\n",
    "# Option 2: Use a connection pool pattern with multiple RAG instances\n",
    "# Option 3: Implement proper locking mechanisms (reduces concurrency benefits)\n",
    "```\n",
    "\n",
    "**Current Implementation:**\n",
    "- Safe for concurrent multi-system evaluation\n",
    "- Conservative within-system processing\n",
    "- Prioritizes stability over maximum speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a2b1329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from typing import List\n",
    "from haystack import SuperComponent\n",
    "import traceback\n",
    "\n",
    "\n",
    "@component\n",
    "class AsyncRAGDataAugmenterComponent:\n",
    "    \"\"\"\n",
    "    Async version of RAGDataAugmenterComponent that processes queries concurrently.\n",
    "    \n",
    "    Key Improvements:\n",
    "    - Processes multiple queries in parallel batches\n",
    "    - Configurable concurrency limits to avoid rate limits\n",
    "    - Significant performance improvement for large datasets\n",
    "    - Progress tracking for long-running evaluations\n",
    "    \n",
    "    Thread-Safety Considerations:\n",
    "    - Haystack SuperComponents are NOT guaranteed to be thread-safe\n",
    "    - Default batch_size=1 to avoid concurrent access to shared component state\n",
    "    - Each batch processes sequentially, but multiple systems can run in parallel\n",
    "    - For batch_size > 1: Ensure each RAG system has separate model instances\n",
    "    \n",
    "    Performance Trade-offs:\n",
    "    - batch_size=1: Safe, sequential processing within each system\n",
    "    - batch_size>1: Faster, but requires thread-safe component design\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rag_supercomponent: SuperComponent, batch_size: int = 1):\n",
    "        \"\"\"\n",
    "        Initialize the Async RAG Data Augmenter.\n",
    "        \n",
    "        Args:\n",
    "            rag_supercomponent: Pre-initialized RAG SuperComponent\n",
    "            batch_size (int): Number of queries to process concurrently. Defaults to 1.\n",
    "                             \n",
    "                             batch_size=1: Safe for all configurations (sequential)\n",
    "                             batch_size>1: Only safe if rag_supercomponent is thread-safe\n",
    "                                          and has no shared mutable state\n",
    "        \n",
    "        Warning:\n",
    "            Increasing batch_size beyond 1 may cause race conditions if the\n",
    "            rag_supercomponent shares embedding models or other stateful components\n",
    "            across concurrent calls. Always test thoroughly with batch_size > 1.\n",
    "        \"\"\"\n",
    "        self.rag_supercomponent = rag_supercomponent\n",
    "        self.batch_size = batch_size\n",
    "        self.output_names = [\"augmented_data_frame\"]\n",
    "        \n",
    "        # Warn if batch_size > 1\n",
    "        if batch_size > 1:\n",
    "            print(f\"‚ö†Ô∏è  Warning: batch_size={batch_size} may cause threading issues\")\n",
    "            print(\"   Ensure your RAG SuperComponent is thread-safe\")\n",
    "            print(\"   (separate embedding model instances, no shared state)\")\n",
    "\n",
    "    async def _process_single_query(self, query: str, index: int) -> tuple:\n",
    "        \"\"\"\n",
    "        Process a single query through the RAG SuperComponent.\n",
    "        \n",
    "        Thread Safety:\n",
    "        - Uses asyncio.to_thread() to run synchronous RAG code in thread pool\n",
    "        - Thread pool executor serializes access when batch_size=1\n",
    "        - With batch_size>1, concurrent calls may access shared component state\n",
    "        \n",
    "        Args:\n",
    "            query: The query string to process\n",
    "            index: The query index for tracking\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (index, answer, contexts)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Run the RAG SuperComponent in a thread pool\n",
    "            # asyncio.to_thread automatically handles thread pool execution\n",
    "            rag_output = await asyncio.to_thread(\n",
    "                self.rag_supercomponent.run, \n",
    "                query=query\n",
    "            )\n",
    "            \n",
    "            # Extract answer and contexts\n",
    "            answer = rag_output.get('replies', [''])[0]\n",
    "            retrieved_docs = rag_output.get('documents', [])\n",
    "            retrieved_contexts = [doc.content for doc in retrieved_docs]\n",
    "            \n",
    "            return (index, answer, retrieved_contexts)\n",
    "        except Exception as e:\n",
    "            query_preview = query[:50] + ('...' if len(query) > 50 else '')\n",
    "            print(f\"Error processing query {index} ('{query_preview}'): {str(e)}\")\n",
    "            \n",
    "            traceback.print_exc()\n",
    "            return (index, \"\", [])\n",
    "\n",
    "    async def _process_batch(self, queries_with_indices: List[tuple]) -> List[tuple]:\n",
    "        \"\"\"\n",
    "        Process a batch of queries concurrently.\n",
    "        \n",
    "        Thread Safety Note:\n",
    "        - When batch_size=1, only one query processes at a time (safe)\n",
    "        - When batch_size>1, queries run concurrently (potential race conditions)\n",
    "        \n",
    "        Args:\n",
    "            queries_with_indices: List of (index, query) tuples\n",
    "            \n",
    "        Returns:\n",
    "            List of (index, answer, contexts) tuples\n",
    "        \"\"\"\n",
    "        tasks = [\n",
    "            self._process_single_query(query, idx) \n",
    "            for idx, query in queries_with_indices\n",
    "        ]\n",
    "        return await asyncio.gather(*tasks)\n",
    "\n",
    "    @component.output_types(augmented_data_frame=pd.DataFrame)\n",
    "    def run(self, data_frame: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Process all queries in the DataFrame with concurrent execution.\n",
    "        \n",
    "        Data Safety:\n",
    "        - Creates a copy of input DataFrame to avoid mutation\n",
    "        - Results stored in new columns on the copied DataFrame\n",
    "        - Original data_frame parameter remains unchanged\n",
    "        \n",
    "        Thread Safety:\n",
    "        - Component instance shared across all batch calls\n",
    "        - batch_size=1 ensures sequential processing (safe)\n",
    "        - batch_size>1 requires thread-safe rag_supercomponent\n",
    "        \n",
    "        Args:\n",
    "            data_frame: DataFrame with 'user_input' column containing queries\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary with augmented DataFrame containing responses and contexts\n",
    "        \"\"\"\n",
    "        async def _async_process():\n",
    "            total_queries = len(data_frame)\n",
    "            print(f\"Running Async RAG on {total_queries} queries (batch size: {self.batch_size})...\")\n",
    "            \n",
    "            # Prepare queries with their indices\n",
    "            queries_with_indices = list(enumerate(data_frame[\"user_input\"].tolist()))\n",
    "            \n",
    "            # Initialize results storage\n",
    "            results = [None] * total_queries\n",
    "            \n",
    "            # Process in batches\n",
    "            for batch_start in range(0, total_queries, self.batch_size):\n",
    "                batch_end = min(batch_start + self.batch_size, total_queries)\n",
    "                batch = queries_with_indices[batch_start:batch_end]\n",
    "                \n",
    "                print(f\"Processing batch {batch_start//self.batch_size + 1} \"\n",
    "                      f\"(queries {batch_start+1}-{batch_end} of {total_queries})...\")\n",
    "                \n",
    "                # Process batch concurrently\n",
    "                batch_results = await self._process_batch(batch)\n",
    "                \n",
    "                # Store results in correct order\n",
    "                for idx, answer, contexts in batch_results:\n",
    "                    results[idx] = (answer, contexts)\n",
    "            \n",
    "            # Extract answers and contexts from results\n",
    "            answers = [r[0] for r in results]\n",
    "            contexts = [r[1] for r in results]\n",
    "            \n",
    "            # Create a copy to avoid modifying the original DataFrame\n",
    "            # This ensures the component doesn't have side effects on input data\n",
    "            data_frame_copy = data_frame.copy()\n",
    "            data_frame_copy['response'] = answers\n",
    "            data_frame_copy['retrieved_contexts'] = contexts\n",
    "            \n",
    "            print(f\"‚úì Async RAG processing complete for {total_queries} queries!\")\n",
    "            return {\"augmented_data_frame\": data_frame_copy}\n",
    "        \n",
    "        try:\n",
    "            loop = asyncio.get_running_loop()\n",
    "            # Already in async context\n",
    "            return loop.create_task(_async_process())\n",
    "        except RuntimeError:\n",
    "            # No loop running\n",
    "            return asyncio.run(_async_process())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272c9cff",
   "metadata": {},
   "source": [
    "## Component 3: RAGAS Evaluation Engine\n",
    "\n",
    "The **RagasEvaluationComponent** integrates the RAGAS framework into our Haystack pipeline, providing systematic evaluation metrics for RAG systems.\n",
    "\n",
    "**Core Evaluation Metrics:**\n",
    "\n",
    "| Metric | Purpose | What It Measures |\n",
    "|--------|---------|------------------|\n",
    "| **Faithfulness** | Response Quality | Factual consistency with retrieved context |\n",
    "| **ResponseRelevancy** | Relevance | How well responses answer the questions |\n",
    "| **LLMContextRecall** | Retrieval Quality | How well retrieval captures relevant information |\n",
    "| **FactualCorrectness** | Accuracy | Correctness of factual claims in responses |\n",
    "\n",
    "**Technical Implementation:**\n",
    "- **Focused Metrics**: Core metrics for reliable comparison\n",
    "- **LLM Integration**: Uses OpenAI GPT models for evaluation judgments  \n",
    "- **Data Format Handling**: Automatically formats data for RAGAS requirements\n",
    "- **Comprehensive Output**: Returns both aggregated metrics and detailed per-query results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5b842a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import EvaluationDataset, evaluate\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, ResponseRelevancy\n",
    "from ragas.llms import HaystackLLMWrapper\n",
    "\n",
    "\n",
    "@component\n",
    "class RagasEvaluationComponent:\n",
    "    \"\"\"\n",
    "    Integrates the RAGAS framework into Haystack pipeline for systematic evaluation.\n",
    "    \n",
    "    This component provides systematic evaluation metrics for RAG systems using\n",
    "    the RAGAS framework with focus on core metrics for reliable comparison.\n",
    "    \n",
    "    Core Evaluation Metrics:\n",
    "    - Faithfulness: Factual consistency with retrieved context\n",
    "    - ResponseRelevancy: How well responses answer the questions  \n",
    "    - LLMContextRecall: How well retrieval captures relevant information\n",
    "    - FactualCorrectness: Correctness of factual claims in responses\n",
    "    \n",
    "    Technical Features:\n",
    "    - Focused Metrics: Core metrics for reliable comparison\n",
    "    - LLM Integration: Uses provided generator for evaluation judgments\n",
    "    - Data Format Handling: Automatically formats data for RAGAS requirements\n",
    "    - Comprehensive Output: Returns both aggregated metrics and detailed per-query results\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 generator: Any,\n",
    "                 metrics: Optional[List[Any]] = None):\n",
    "        \"\"\"\n",
    "        Initialize the RAGAS Evaluation Component.\n",
    "        \n",
    "        Args:\n",
    "            generator: LLM generator instance (e.g., OpenAIGenerator or OllamaGenerator).\n",
    "            metrics: List of RAGAS metrics to evaluate (defaults to core metrics)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Default to core metrics for systematic comparison\n",
    "        if metrics is None:\n",
    "            self.metrics = [\n",
    "                Faithfulness(), \n",
    "                ResponseRelevancy(),\n",
    "                LLMContextRecall(),\n",
    "                FactualCorrectness()\n",
    "            ]\n",
    "        else:\n",
    "            self.metrics = metrics\n",
    "        \n",
    "        # Configure RAGAS LLM for evaluation\n",
    "        self.ragas_llm = HaystackLLMWrapper(generator)\n",
    "\n",
    "    @component.output_types(metrics=Dict[str, float], evaluation_df=pd.DataFrame)\n",
    "    def run(self, augmented_data_frame: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Run RAGAS evaluation on augmented dataset.\n",
    "        \n",
    "        Args:\n",
    "            augmented_data_frame: DataFrame with RAG responses and retrieved contexts\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary containing evaluation metrics and detailed results DataFrame\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. Map columns to Ragas requirements\n",
    "        ragas_data = pd.DataFrame({\n",
    "            'user_input': augmented_data_frame['user_input'],\n",
    "            'response': augmented_data_frame['response'], \n",
    "            'retrieved_contexts': augmented_data_frame['retrieved_contexts'],\n",
    "            'reference': augmented_data_frame['reference'],\n",
    "            'reference_contexts': augmented_data_frame['reference_contexts'].apply(eval)\n",
    "        })\n",
    "\n",
    "        print(\"Creating Ragas EvaluationDataset...\")\n",
    "        # 2. Create EvaluationDataset\n",
    "        dataset = EvaluationDataset.from_pandas(ragas_data)\n",
    "\n",
    "        print(\"Starting Ragas evaluation...\")\n",
    "        \n",
    "        # 3. Run Ragas Evaluation\n",
    "        results = evaluate(\n",
    "            dataset=dataset,\n",
    "            metrics=self.metrics,\n",
    "            llm=self.ragas_llm\n",
    "        )\n",
    "        \n",
    "        results_df = results.to_pandas()\n",
    "        \n",
    "        print(\"Ragas evaluation complete.\")\n",
    "        print(f\"Overall metrics: {results}\")\n",
    "        \n",
    "        return {\"metrics\": results, \"evaluation_df\": results_df}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b0aa01",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Systematic RAG Comparison: Naive vs Hybrid\n",
    "\n",
    "Now we'll systematically evaluate both RAG SuperComponents using the same evaluation pipeline. This ensures fair comparison with identical evaluation conditions.\n",
    "\n",
    "## Comparison Strategy\n",
    "\n",
    "Our approach enables **systematic comparison**:\n",
    "\n",
    "1. **Same Dataset**: Both systems evaluated on identical test queries\n",
    "2. **Same Metrics**: Consistent evaluation criteria across both approaches\n",
    "3. **Same Pipeline**: Identical processing workflow eliminates bias\n",
    "4. **Reproducible Results**: Pipeline ensures consistent evaluation conditions\n",
    "\n",
    "## Dataset Information\n",
    "\n",
    "We'll use a synthetic evaluation dataset:\n",
    "- **`synthetic_tests_advanced_branching_10.csv`**: Focused dataset for comparison\n",
    "\n",
    "**Dataset Structure:**\n",
    "- `user_input`: Questions to ask the RAG system\n",
    "- `reference`: Ground truth answers for comparison\n",
    "- `reference_contexts`: Expected retrieved contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0384e71c",
   "metadata": {},
   "source": [
    "### Setup: Initialize Both RAG SuperComponents\n",
    "\n",
    "First, we'll initialize both RAG SuperComponents with consistent parameters for fair comparison.\n",
    "\n",
    "**Configuration:**\n",
    "- **Same base parameters**: Both systems use identical core settings\n",
    "- **Document store**: Shared Elasticsearch document store\n",
    "- **Models**: Consistent LLM and embedding models for both systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a23faff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both RAG SuperComponents initialized successfully!\n",
      "Naive RAG: NaiveRAGSuperComponent\n",
      "Hybrid RAG: HybridRAGSuperComponent\n"
     ]
    }
   ],
   "source": [
    "# --- Setup Environment & Dependencies ---\n",
    "from scripts.rag.hybridrag import HybridRAGSuperComponent\n",
    "from scripts.rag.naiverag import NaiveRAGSuperComponent\n",
    "from haystack_integrations.document_stores.elasticsearch import ElasticsearchDocumentStore\n",
    "import os\n",
    "\n",
    "# Initialize document store\n",
    "document_store = ElasticsearchDocumentStore(hosts=\"http://localhost:9200\")\n",
    "\n",
    "# Create both RAG SuperComponents with base parameters for fair comparison\n",
    "naive_rag_sc = NaiveRAGSuperComponent(\n",
    "    document_store=document_store\n",
    ")\n",
    "\n",
    "hybrid_rag_sc = HybridRAGSuperComponent(\n",
    "    document_store=document_store\n",
    ")\n",
    "\n",
    "print(\"Both RAG SuperComponents initialized successfully!\")\n",
    "print(f\"Naive RAG: {naive_rag_sc.__class__.__name__}\")\n",
    "print(f\"Hybrid RAG: {hybrid_rag_sc.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf21e21",
   "metadata": {},
   "source": [
    "## Evaluation Pipeline: Using Haystack AsyncPipeline\n",
    "\n",
    "Now we'll create an evaluation pipeline using Haystack's `AsyncPipeline` for proper concurrent execution. This provides a cleaner, more maintainable approach than manually coordinating async components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa89ea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import AsyncPipeline\n",
    "import time\n",
    "\n",
    "def create_evaluation_pipeline(\n",
    "    rag_supercomponent,\n",
    "    generator: Any,\n",
    "    batch_size: int = 5\n",
    ") -> AsyncPipeline:\n",
    "    \"\"\"\n",
    "    Create an evaluation pipeline using Haystack's AsyncPipeline.\n",
    "    \n",
    "    AsyncPipeline handles concurrent execution of components automatically.\n",
    "    Components themselves use synchronous run() methods, and AsyncPipeline\n",
    "    orchestrates their async execution.\n",
    "    \n",
    "    Args:\n",
    "        rag_supercomponent: The RAG system to evaluate\n",
    "        generator: LLM generator for RAGAS evaluation\n",
    "        batch_size: Number of queries to process concurrently\n",
    "        \n",
    "    Returns:\n",
    "        AsyncPipeline: Configured evaluation pipeline\n",
    "    \"\"\"\n",
    "    pipeline = AsyncPipeline()\n",
    "    \n",
    "    # Add components to pipeline\n",
    "    pipeline.add_component(\"reader\", CSVReaderComponent())\n",
    "    pipeline.add_component(\n",
    "        \"augmenter\",\n",
    "        AsyncRAGDataAugmenterComponent(\n",
    "            rag_supercomponent=rag_supercomponent,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "    )\n",
    "    pipeline.add_component(\"evaluator\", RagasEvaluationComponent(generator=generator))\n",
    "    \n",
    "    # Connect components\n",
    "    pipeline.connect(\"reader.data_frame\", \"augmenter.data_frame\")\n",
    "    pipeline.connect(\"augmenter.augmented_data_frame\", \"evaluator.augmented_data_frame\")\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "\n",
    "async def evaluate_rag_system_async(\n",
    "    rag_supercomponent,\n",
    "    system_name: str,\n",
    "    csv_file_path: str,\n",
    "    generator: Any,\n",
    "    batch_size: int = 5\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Asynchronously evaluate a single RAG system using AsyncPipeline.\n",
    "    \n",
    "    Args:\n",
    "        rag_supercomponent: The RAG system to evaluate\n",
    "        system_name: Name for logging\n",
    "        csv_file_path: Path to evaluation dataset\n",
    "        generator: LLM generator for RAGAS evaluation\n",
    "        batch_size: Number of queries to process concurrently\n",
    "        \n",
    "    Returns:\n",
    "        dict: Evaluation results with metrics and detailed DataFrame\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Starting async evaluation of {system_name}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Create evaluation pipeline\n",
    "    eval_pipeline = create_evaluation_pipeline(\n",
    "        rag_supercomponent=rag_supercomponent,\n",
    "        generator=generator,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    # Run pipeline asynchronously\n",
    "    results = await eval_pipeline.run_async(data={\"reader\": {\"source\": csv_file_path}})\n",
    "    \n",
    "    print(f\"\\n‚úì {system_name} evaluation complete!\")\n",
    "    return {\n",
    "        \"system_name\": system_name,\n",
    "        \"metrics\": results[\"evaluator\"][\"metrics\"],\n",
    "        \"evaluation_df\": results[\"evaluator\"][\"evaluation_df\"]\n",
    "    }\n",
    "\n",
    "\n",
    "async def evaluate_multiple_rag_systems_async(\n",
    "    rag_systems: List[tuple],\n",
    "    csv_file_path: str,\n",
    "    batch_size: int = 5\n",
    ") -> List[dict]:\n",
    "    \"\"\"\n",
    "    Evaluate multiple RAG systems concurrently using AsyncPipeline.\n",
    "    \n",
    "    Args:\n",
    "        rag_systems: List of (rag_supercomponent, system_name, generator) tuples\n",
    "        csv_file_path: Path to evaluation dataset\n",
    "        batch_size: Number of queries to process concurrently per system\n",
    "        \n",
    "    Returns:\n",
    "        List of evaluation results for each system\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"CONCURRENT EVALUATION OF {len(rag_systems)} RAG SYSTEMS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Dataset: {csv_file_path}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Systems: {[name for _, name, _ in rag_systems]}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Create evaluation tasks for each system\n",
    "    tasks = [\n",
    "        evaluate_rag_system_async(\n",
    "            rag_supercomponent=rag_sc,\n",
    "            system_name=name,\n",
    "            csv_file_path=csv_file_path,\n",
    "            generator=generator,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        for rag_sc, name, generator in rag_systems\n",
    "    ]\n",
    "    \n",
    "    # Run all evaluations concurrently\n",
    "    start_time = time.time()\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"‚úì ALL EVALUATIONS COMPLETE\")\n",
    "    print(f\"‚è±Total time: {elapsed:.2f} seconds\")\n",
    "    print(f\"‚ö° Average time per system: {elapsed/len(rag_systems):.2f} seconds\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7618d9c",
   "metadata": {},
   "source": [
    "## Run Concurrent Evaluation\n",
    "\n",
    "Now let's evaluate both Naive and Hybrid RAG systems **simultaneously** using concurrent evaluation.\n",
    "\n",
    "**Performance Benefits:**\n",
    "- **2√ó Faster**: Both systems evaluated at the same time\n",
    "- **Identical Conditions**: Both evaluations start simultaneously\n",
    "- **Efficient Resource Use**: Maximizes computational efficiency\n",
    "\n",
    "**Configuration:**\n",
    "- `batch_size`: Controls concurrent queries per system (adjust based on API limits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4629f990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üöÄ CONCURRENT EVALUATION OF 2 RAG SYSTEMS\n",
      "================================================================================\n",
      "Dataset: data_for_eval/synthetic_tests_advanced_branching_10.csv\n",
      "Batch size: 1\n",
      "Systems: ['Naive RAG', 'Hybrid RAG']\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Starting async evaluation of Naive RAG\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Starting async evaluation of Hybrid RAG\n",
      "================================================================================\n",
      "\n",
      "Loaded DataFrame with 10 rows from data_for_eval/synthetic_tests_advanced_branching_10.csv.\n",
      "Loaded DataFrame with 10 rows from data_for_eval/synthetic_tests_advanced_branching_10.csv.\n",
      "Running Async RAG on 10 queries (batch size: 1)...\n",
      "Processing batch 1 (queries 1-1 of 10)...\n",
      "Running Async RAG on 10 queries (batch size: 1)...\n",
      "Processing batch 1 (queries 1-1 of 10)...\n",
      "Processing batch 2 (queries 2-2 of 10)...\n",
      "Processing batch 2 (queries 2-2 of 10)...\n",
      "Processing batch 3 (queries 3-3 of 10)...\n",
      "Processing batch 3 (queries 3-3 of 10)...\n",
      "Processing batch 2 (queries 2-2 of 10)...\n",
      "Processing batch 2 (queries 2-2 of 10)...\n",
      "Processing batch 3 (queries 3-3 of 10)...\n",
      "Processing batch 3 (queries 3-3 of 10)...\n",
      "Processing batch 4 (queries 4-4 of 10)...\n",
      "Processing batch 4 (queries 4-4 of 10)...\n",
      "Processing batch 4 (queries 4-4 of 10)...\n",
      "Processing batch 4 (queries 4-4 of 10)...\n",
      "Processing batch 5 (queries 5-5 of 10)...\n",
      "Processing batch 5 (queries 5-5 of 10)...\n",
      "Processing batch 5 (queries 5-5 of 10)...\n",
      "Processing batch 5 (queries 5-5 of 10)...\n",
      "Processing batch 6 (queries 6-6 of 10)...\n",
      "Processing batch 6 (queries 6-6 of 10)...\n",
      "Processing batch 7 (queries 7-7 of 10)...\n",
      "Processing batch 7 (queries 7-7 of 10)...\n",
      "Processing batch 6 (queries 6-6 of 10)...\n",
      "Processing batch 6 (queries 6-6 of 10)...\n",
      "Processing batch 7 (queries 7-7 of 10)...\n",
      "Processing batch 7 (queries 7-7 of 10)...\n",
      "Processing batch 8 (queries 8-8 of 10)...\n",
      "Processing batch 8 (queries 8-8 of 10)...\n",
      "Processing batch 9 (queries 9-9 of 10)...\n",
      "Processing batch 9 (queries 9-9 of 10)...\n",
      "Processing batch 8 (queries 8-8 of 10)...\n",
      "Processing batch 8 (queries 8-8 of 10)...\n",
      "Processing batch 10 (queries 10-10 of 10)...\n",
      "Processing batch 10 (queries 10-10 of 10)...\n",
      "Processing batch 9 (queries 9-9 of 10)...\n",
      "Processing batch 9 (queries 9-9 of 10)...\n",
      "‚úì Async RAG processing complete for 10 queries!\n",
      "Creating Ragas EvaluationDataset...\n",
      "Starting Ragas evaluation...\n",
      "Processing batch 10 (queries 10-10 of 10)...\n",
      "‚úì Async RAG processing complete for 10 queries!\n",
      "Creating Ragas EvaluationDataset...\n",
      "Starting Ragas evaluation...\n",
      "Processing batch 10 (queries 10-10 of 10)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efbffb9c1fb541c388f98db4fbe015b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Async RAG processing complete for 10 queries!\n",
      "Creating Ragas EvaluationDataset...\n",
      "Starting Ragas evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e7b6b3e01b43339783587d9febf30c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ragas evaluation complete.\n",
      "Overall metrics: {'faithfulness': 0.8700, 'answer_relevancy': 0.8655, 'context_recall': 0.9600, 'factual_correctness(mode=f1)': 0.5730}\n",
      "\n",
      "‚úì Hybrid RAG evaluation complete!\n",
      "Ragas evaluation complete.\n",
      "Overall metrics: {'faithfulness': 0.8523, 'answer_relevancy': 0.7670, 'context_recall': 0.9300, 'factual_correctness(mode=f1)': 0.5510}\n",
      "\n",
      "‚úì Naive RAG evaluation complete!\n",
      "\n",
      "================================================================================\n",
      "‚úì ALL EVALUATIONS COMPLETE\n",
      "‚è±Ô∏è  Total time: 132.97 seconds\n",
      "‚ö° Average time per system: 66.49 seconds\n",
      "================================================================================\n",
      "\n",
      "\n",
      "‚úì Concurrent evaluation complete!\n",
      "Naive RAG: 10 queries evaluated\n",
      "Hybrid RAG: 10 queries evaluated\n",
      "Ragas evaluation complete.\n",
      "Overall metrics: {'faithfulness': 0.8523, 'answer_relevancy': 0.7670, 'context_recall': 0.9300, 'factual_correctness(mode=f1)': 0.5510}\n",
      "\n",
      "‚úì Naive RAG evaluation complete!\n",
      "\n",
      "================================================================================\n",
      "‚úì ALL EVALUATIONS COMPLETE\n",
      "‚è±Ô∏è  Total time: 132.97 seconds\n",
      "‚ö° Average time per system: 66.49 seconds\n",
      "================================================================================\n",
      "\n",
      "\n",
      "‚úì Concurrent evaluation complete!\n",
      "Naive RAG: 10 queries evaluated\n",
      "Hybrid RAG: 10 queries evaluated\n"
     ]
    }
   ],
   "source": [
    "# --- Setup for Concurrent Evaluation ---\n",
    "\n",
    "# Create separate generators for each evaluation (components can't be shared)\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.utils import Secret\n",
    "import os\n",
    "\n",
    "eval_generator_naive = OpenAIGenerator(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=Secret.from_token(os.getenv(\"OPENAI_API_KEY\"))\n",
    ")\n",
    "\n",
    "eval_generator_hybrid = OpenAIGenerator(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=Secret.from_token(os.getenv(\"OPENAI_API_KEY\"))\n",
    ")\n",
    "\n",
    "# Define evaluation dataset\n",
    "csv_file_path = \"data_for_eval/synthetic_tests_advanced_branching_10.csv\"\n",
    "\n",
    "# Configure concurrent evaluation\n",
    "# batch_size: Number of queries to process in parallel per RAG system\n",
    "# Set to 1 to avoid embedding model conflicts (still gets 2x speedup from parallel systems)\n",
    "batch_size = 1  # Process 1 query at a time per system (safe for shared models)\n",
    "\n",
    "# Prepare RAG systems for concurrent evaluation\n",
    "rag_systems = [\n",
    "    (naive_rag_sc, \"Naive RAG\", eval_generator_naive),\n",
    "    (hybrid_rag_sc, \"Hybrid RAG\", eval_generator_hybrid)\n",
    "]\n",
    "\n",
    "# Run concurrent evaluation\n",
    "all_results = await evaluate_multiple_rag_systems_async(\n",
    "    rag_systems=rag_systems,\n",
    "    csv_file_path=csv_file_path,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Extract results for each system\n",
    "naive_results_async = all_results[0]\n",
    "hybrid_results_async = all_results[1]\n",
    "\n",
    "print(\"\\n‚úì Concurrent evaluation complete!\")\n",
    "print(f\"Naive RAG: {len(naive_results_async['evaluation_df'])} queries evaluated\")\n",
    "print(f\"Hybrid RAG: {len(hybrid_results_async['evaluation_df'])} queries evaluated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287f9163",
   "metadata": {},
   "source": [
    "### View Evaluation Results\n",
    "\n",
    "Let's examine the results from our concurrent evaluation of both systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e582f25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive RAG - Async Evaluation Results:\n",
      "Metrics: {'faithfulness': 0.8523, 'answer_relevancy': 0.7670, 'context_recall': 0.9300, 'factual_correctness(mode=f1)': 0.5510}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>factual_correctness(mode=f1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What Alexa do in AI?</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>Alexa, developed by Amazon, is an AI-powered v...</td>\n",
       "      <td>Alexa is a voice-controlled virtual assistant ...</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.917891</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What happened to the UnitedHealthcare CEO Bria...</td>\n",
       "      <td>[- James Kurose, ‚ÄúTestimony before the House C...</td>\n",
       "      <td>[Why is AI controversial?\\nWhile acknowledging...</td>\n",
       "      <td>I don't have enough information to answer.</td>\n",
       "      <td>The BBC reported that Apple's AI falsely told ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the current stance of the UK governmen...</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>[Are there laws governing AI?\\nSome government...</td>\n",
       "      <td>The UK government's current stance on the regu...</td>\n",
       "      <td>In the UK, Prime Minister Sir Keir Starmer has...</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does the GDPR impact the transparency and ...</td>\n",
       "      <td>[Smart cities\\nMetropolitan governments are us...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nAI ethics and transparency\\nAlgori...</td>\n",
       "      <td>The General Data Protection Regulation (GDPR) ...</td>\n",
       "      <td>The GDPR impacts the transparency and accounta...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.965147</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the trends in the usage of ChatGPT fo...</td>\n",
       "      <td>[The yellow line represents the first cohort o...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n37% of messages are work-related f...</td>\n",
       "      <td>The usage of ChatGPT for Technical Help has se...</td>\n",
       "      <td>The usage of ChatGPT for Technical Help has sh...</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.946387</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0                               What Alexa do in AI?   \n",
       "1  What happened to the UnitedHealthcare CEO Bria...   \n",
       "2  What is the current stance of the UK governmen...   \n",
       "3  How does the GDPR impact the transparency and ...   \n",
       "4  What are the trends in the usage of ChatGPT fo...   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [What is AI, how does it work and why are some...   \n",
       "1  [- James Kurose, ‚ÄúTestimony before the House C...   \n",
       "2  [What is AI, how does it work and why are some...   \n",
       "3  [Smart cities\\nMetropolitan governments are us...   \n",
       "4  [The yellow line represents the first cohort o...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  [What is AI, how does it work and why are some...   \n",
       "1  [Why is AI controversial?\\nWhile acknowledging...   \n",
       "2  [Are there laws governing AI?\\nSome government...   \n",
       "3  [<1-hop>\\n\\nAI ethics and transparency\\nAlgori...   \n",
       "4  [<1-hop>\\n\\n37% of messages are work-related f...   \n",
       "\n",
       "                                            response  \\\n",
       "0  Alexa, developed by Amazon, is an AI-powered v...   \n",
       "1         I don't have enough information to answer.   \n",
       "2  The UK government's current stance on the regu...   \n",
       "3  The General Data Protection Regulation (GDPR) ...   \n",
       "4  The usage of ChatGPT for Technical Help has se...   \n",
       "\n",
       "                                           reference  faithfulness  \\\n",
       "0  Alexa is a voice-controlled virtual assistant ...      0.230769   \n",
       "1  The BBC reported that Apple's AI falsely told ...      1.000000   \n",
       "2  In the UK, Prime Minister Sir Keir Starmer has...      0.750000   \n",
       "3  The GDPR impacts the transparency and accounta...      1.000000   \n",
       "4  The usage of ChatGPT for Technical Help has sh...      0.923077   \n",
       "\n",
       "   answer_relevancy  context_recall  factual_correctness(mode=f1)  \n",
       "0          0.917891             1.0                          0.53  \n",
       "1          0.000000             1.0                          0.00  \n",
       "2          0.000000             1.0                          0.57  \n",
       "3          0.965147             1.0                          0.56  \n",
       "4          0.946387             1.0                          0.88  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n",
      "Hybrid RAG - Async Evaluation Results:\n",
      "Metrics: {'faithfulness': 0.8700, 'answer_relevancy': 0.8655, 'context_recall': 0.9600, 'factual_correctness(mode=f1)': 0.5730}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>factual_correctness(mode=f1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What Alexa do in AI?</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>Alexa, as a voice-controlled virtual assistant...</td>\n",
       "      <td>Alexa is a voice-controlled virtual assistant ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.909179</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What happened to the UnitedHealthcare CEO Bria...</td>\n",
       "      <td>[AI will reconfigure how society and the econo...</td>\n",
       "      <td>[Why is AI controversial?\\nWhile acknowledging...</td>\n",
       "      <td>I don't have enough information to answer.</td>\n",
       "      <td>The BBC reported that Apple's AI falsely told ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the current stance of the UK governmen...</td>\n",
       "      <td>[AI will reconfigure how society and the econo...</td>\n",
       "      <td>[Are there laws governing AI?\\nSome government...</td>\n",
       "      <td>The UK government has stated that it will \"tes...</td>\n",
       "      <td>In the UK, Prime Minister Sir Keir Starmer has...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.943726</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does the GDPR impact the transparency and ...</td>\n",
       "      <td>[- James Kurose, ‚ÄúTestimony before the House C...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nAI ethics and transparency\\nAlgori...</td>\n",
       "      <td>The GDPR (General Data Protection Regulation) ...</td>\n",
       "      <td>The GDPR impacts the transparency and accounta...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.971650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the trends in the usage of ChatGPT fo...</td>\n",
       "      <td>[37% of messages are work-related\\nfor users w...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n37% of messages are work-related f...</td>\n",
       "      <td>The trends in the usage of ChatGPT for Technic...</td>\n",
       "      <td>The usage of ChatGPT for Technical Help has sh...</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.979072</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0                               What Alexa do in AI?   \n",
       "1  What happened to the UnitedHealthcare CEO Bria...   \n",
       "2  What is the current stance of the UK governmen...   \n",
       "3  How does the GDPR impact the transparency and ...   \n",
       "4  What are the trends in the usage of ChatGPT fo...   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [What is AI, how does it work and why are some...   \n",
       "1  [AI will reconfigure how society and the econo...   \n",
       "2  [AI will reconfigure how society and the econo...   \n",
       "3  [- James Kurose, ‚ÄúTestimony before the House C...   \n",
       "4  [37% of messages are work-related\\nfor users w...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  [What is AI, how does it work and why are some...   \n",
       "1  [Why is AI controversial?\\nWhile acknowledging...   \n",
       "2  [Are there laws governing AI?\\nSome government...   \n",
       "3  [<1-hop>\\n\\nAI ethics and transparency\\nAlgori...   \n",
       "4  [<1-hop>\\n\\n37% of messages are work-related f...   \n",
       "\n",
       "                                            response  \\\n",
       "0  Alexa, as a voice-controlled virtual assistant...   \n",
       "1         I don't have enough information to answer.   \n",
       "2  The UK government has stated that it will \"tes...   \n",
       "3  The GDPR (General Data Protection Regulation) ...   \n",
       "4  The trends in the usage of ChatGPT for Technic...   \n",
       "\n",
       "                                           reference  faithfulness  \\\n",
       "0  Alexa is a voice-controlled virtual assistant ...           1.0   \n",
       "1  The BBC reported that Apple's AI falsely told ...           0.0   \n",
       "2  In the UK, Prime Minister Sir Keir Starmer has...           1.0   \n",
       "3  The GDPR impacts the transparency and accounta...           1.0   \n",
       "4  The usage of ChatGPT for Technical Help has sh...           0.7   \n",
       "\n",
       "   answer_relevancy  context_recall  factual_correctness(mode=f1)  \n",
       "0          0.909179             1.0                          0.46  \n",
       "1          0.000000             1.0                          0.00  \n",
       "2          0.943726             1.0                          0.58  \n",
       "3          0.971650             1.0                          0.61  \n",
       "4          0.979072             0.8                          0.82  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display results from concurrent evaluation\n",
    "print(\"Naive RAG - Async Evaluation Results:\")\n",
    "print(f\"Metrics: {naive_results_async['metrics']}\\n\")\n",
    "display(naive_results_async['evaluation_df'].head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"Hybrid RAG - Async Evaluation Results:\")\n",
    "print(f\"Metrics: {hybrid_results_async['metrics']}\\n\")\n",
    "display(hybrid_results_async['evaluation_df'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06db19ce",
   "metadata": {},
   "source": [
    "## Side-by-Side Performance Comparison\n",
    "\n",
    "Let's create a comprehensive comparison of both RAG systems to understand their relative performance across all metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4f2d91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE RAG SYSTEM COMPARISON (ASYNC EVALUATION)\n",
      "================================================================================\n",
      "                      Metric  Naive RAG  Hybrid RAG  Improvement (%) Better System\n",
      "                faithfulness     0.8523      0.8700             2.08    Hybrid RAG\n",
      "            answer_relevancy     0.7670      0.8655            12.85    Hybrid RAG\n",
      "              context_recall     0.9300      0.9600             3.23    Hybrid RAG\n",
      "factual_correctness(mode=f1)     0.5510      0.5730             3.99    Hybrid RAG\n",
      "\n",
      "================================================================================\n",
      "\n",
      "FINAL SCORECARD:\n",
      "Hybrid RAG wins: 4 metrics\n",
      "Naive RAG wins: 0 metrics\n",
      "Ties: 0 metrics\n",
      "\n",
      "OVERALL WINNER: Hybrid RAG SuperComponent!\n",
      "   Better performance in 4/4 metrics\n",
      "\n",
      "Average improvement by Hybrid RAG: 5.54%\n"
     ]
    }
   ],
   "source": [
    "# Update the comparison to use async results\n",
    "import pandas as pd\n",
    "\n",
    "# Extract average metrics from async evaluations\n",
    "naive_scores_async = naive_results_async['metrics'].scores\n",
    "hybrid_scores_async = hybrid_results_async['metrics'].scores\n",
    "\n",
    "# Compute averages for each metric\n",
    "naive_df_async = pd.DataFrame(naive_scores_async)\n",
    "hybrid_df_async = pd.DataFrame(hybrid_scores_async)\n",
    "\n",
    "naive_averages_async = naive_df_async.mean()\n",
    "hybrid_averages_async = hybrid_df_async.mean()\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_data_async = {\n",
    "    'Metric': list(naive_averages_async.index),\n",
    "    'Naive RAG': list(naive_averages_async.values),\n",
    "    'Hybrid RAG': list(hybrid_averages_async.values)\n",
    "}\n",
    "\n",
    "comparison_df_async = pd.DataFrame(comparison_data_async)\n",
    "comparison_df_async['Improvement (%)'] = (\n",
    "    (comparison_df_async['Hybrid RAG'] - comparison_df_async['Naive RAG']) / \n",
    "    comparison_df_async['Naive RAG'] * 100\n",
    ").round(2)\n",
    "comparison_df_async['Better System'] = comparison_df_async.apply(\n",
    "    lambda row: 'Hybrid RAG' if row['Hybrid RAG'] > row['Naive RAG'] \n",
    "    else 'Naive RAG' if row['Naive RAG'] > row['Hybrid RAG'] \n",
    "    else 'Tie', axis=1\n",
    ")\n",
    "\n",
    "# Round scores\n",
    "comparison_df_async['Naive RAG'] = comparison_df_async['Naive RAG'].round(4)\n",
    "comparison_df_async['Hybrid RAG'] = comparison_df_async['Hybrid RAG'].round(4)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPREHENSIVE RAG SYSTEM COMPARISON (ASYNC EVALUATION)\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df_async.to_string(index=False))\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Calculate overall winner\n",
    "hybrid_wins = sum(comparison_df_async['Hybrid RAG'] > comparison_df_async['Naive RAG'])\n",
    "naive_wins = sum(comparison_df_async['Naive RAG'] > comparison_df_async['Hybrid RAG'])\n",
    "ties = sum(comparison_df_async['Naive RAG'] == comparison_df_async['Hybrid RAG'])\n",
    "\n",
    "print(f\"\\nFINAL SCORECARD:\")\n",
    "print(f\"Hybrid RAG wins: {hybrid_wins} metrics\")\n",
    "print(f\"Naive RAG wins: {naive_wins} metrics\") \n",
    "print(f\"Ties: {ties} metrics\")\n",
    "\n",
    "if hybrid_wins > naive_wins:\n",
    "    print(f\"\\nOVERALL WINNER: Hybrid RAG SuperComponent!\")\n",
    "    print(f\"   Better performance in {hybrid_wins}/{len(comparison_df_async)} metrics\")\n",
    "elif naive_wins > hybrid_wins:\n",
    "    print(f\"\\nOVERALL WINNER: Naive RAG SuperComponent!\")  \n",
    "    print(f\"   Better performance in {naive_wins}/{len(comparison_df_async)} metrics\")\n",
    "else:\n",
    "    print(f\"\\nRESULT: It's a tie between both systems!\")\n",
    "    \n",
    "avg_improvement = comparison_df_async['Improvement (%)'].mean()\n",
    "print(f\"\\nAverage improvement by Hybrid RAG: {avg_improvement:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47be3490",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (haystack-nlp-dev)",
   "language": "python",
   "name": "haystack-nlp-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
