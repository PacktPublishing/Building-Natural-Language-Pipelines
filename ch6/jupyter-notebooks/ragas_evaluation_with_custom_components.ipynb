{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75f744c9",
   "metadata": {},
   "source": [
    "üîß **Setup Required**: Before running this notebook, please follow the [setup instructions](../README.md#setup-instructions) to configure your environment and API keys. **You will need to ensure you've executed the Indexing pipeline before completing this exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ad4fc5",
   "metadata": {},
   "source": [
    "# Systematic RAG Evaluation: Naive vs Hybrid Comparison\n",
    "\n",
    "## üìã Overview\n",
    "\n",
    "This notebook demonstrates a **systematic evaluation workflow** for comparing two RAG (Retrieval-Augmented Generation) approaches using **Haystack custom components**. We'll create a reproducible pipeline to:\n",
    "\n",
    "1. **Load evaluation datasets** from CSV files\n",
    "2. **Process queries** through both Naive and Hybrid RAG SuperComponents \n",
    "3. **Generate comprehensive metrics** using the RAGAS framework\n",
    "4. **Compare performance** systematically\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand how to:\n",
    "- Create reusable evaluation components for RAG systems\n",
    "- Build scalable pipelines for systematic RAG comparison\n",
    "- Interpret RAGAS metrics in comparative context\n",
    "- Make data-driven decisions between RAG approaches\n",
    "\n",
    "## üîß Evaluation Pipeline\n",
    "\n",
    "Our pipeline consists of three main components:\n",
    "\n",
    "```\n",
    "CSV Data ‚Üí RAGDataAugmenter ‚Üí RagasEvaluation ‚Üí Metrics & Results\n",
    "```\n",
    "\n",
    "**Key Benefits:**\n",
    "- **Systematic**: Same evaluation conditions for both RAG systems\n",
    "- **Reproducible**: Consistent evaluation across experiments\n",
    "- **Scalable**: Easy to add new RAG implementations\n",
    "- **Comprehensive**: Multiple metrics provide complete assessment\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4640ae5",
   "metadata": {},
   "source": [
    "## Component 1: CSV Data Loader üìä\n",
    "\n",
    "The **CSVReaderComponent** serves as the entry point for our evaluation pipeline. It handles loading synthetic evaluation datasets and ensures data quality before processing.\n",
    "\n",
    "**Key Features:**\n",
    "- **Robust Error Handling**: Validates file existence and data integrity\n",
    "- **Pandas Integration**: Returns data as DataFrame for easy manipulation\n",
    "- **Pipeline Compatible**: Designed to work seamlessly with Haystack pipelines\n",
    "\n",
    "**Input:** File path to CSV containing evaluation queries and ground truth\n",
    "**Output:** Pandas DataFrame ready for RAG processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08a83513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from haystack import component, Pipeline\n",
    "from typing import List, Optional, Dict, Any, Union\n",
    "\n",
    "@component\n",
    "class CSVReaderComponent:\n",
    "    \"\"\"Reads a CSV file into a Pandas DataFrame.\"\"\"\n",
    "\n",
    "    @component.output_types(data_frame=pd.DataFrame)\n",
    "    def run(self, source: Union[str, Path]):\n",
    "        \"\"\"\n",
    "        Reads the CSV file from the first source in the list.\n",
    "        \n",
    "        Args:\n",
    "            sources: List of file paths to CSV files. Only the first file will be processed.\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary containing the loaded DataFrame under 'data_frame' key.\n",
    "            \n",
    "        Raises:\n",
    "            FileNotFoundError: If the file doesn't exist or can't be read.\n",
    "            ValueError: If the DataFrame is empty after loading.\n",
    "        \"\"\"\n",
    "        if not source:\n",
    "            raise ValueError(\"No sources provided\")\n",
    "            \n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(source)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"File not found at {source}\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error reading CSV file {source}: {str(e)}\")\n",
    "\n",
    "        # Check if DataFrame is empty using proper pandas method\n",
    "        if df.empty:\n",
    "            raise ValueError(f\"DataFrame is empty after loading from {source}\")\n",
    "\n",
    "        print(f\"Loaded DataFrame with {len(df)} rows from {source}.\")\n",
    "        return {\"data_frame\": df}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1861dfb2",
   "metadata": {},
   "source": [
    "## Component 2: RAG Data Augmentation üîÑ\n",
    "\n",
    "The **RAGDataAugmenterComponent** is the core of our evaluation workflow. It takes each query from our evaluation dataset and processes it through a RAG SuperComponent, collecting both the generated responses and retrieved contexts.\n",
    "\n",
    "**üîë Key Design Decisions:**\n",
    "\n",
    "1. **SuperComponent Flexibility**: Accepts any pre-configured RAG SuperComponent (Naive, Hybrid, or custom)\n",
    "2. **Batch Processing**: Efficiently processes entire evaluation datasets\n",
    "3. **Data Augmentation**: Enriches the original dataset with RAG outputs for evaluation\n",
    "4. **Context Extraction**: Captures retrieved documents for context-based metrics\n",
    "\n",
    "**Pipeline Integration:**\n",
    "- **Input**: DataFrame with queries from CSVReaderComponent  \n",
    "- **Process**: Runs each query through the specified RAG SuperComponent\n",
    "- **Output**: Augmented DataFrame with responses and retrieved contexts\n",
    "\n",
    "**üí° Why This Approach?**\n",
    "By separating RAG execution from evaluation, we can:\n",
    "- **Swap RAG systems** without changing evaluation logic\n",
    "- **Cache RAG results** for multiple evaluation runs  \n",
    "- **Debug RAG performance** independently of metrics calculation\n",
    "- **Scale evaluation** across different datasets and configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72bf8b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import SuperComponent\n",
    "\n",
    "@component\n",
    "class RAGDataAugmenterComponent:\n",
    "    \"\"\"\n",
    "    Applies a RAG SuperComponent to each query in a DataFrame and \n",
    "    augments the data with the generated answer and retrieved contexts.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rag_supercomponent: SuperComponent):\n",
    "        # We store the pre-initialized SuperComponent\n",
    "        self.rag_supercomponent = rag_supercomponent\n",
    "        self.output_names = [\"augmented_data_frame\"]\n",
    "\n",
    "    @component.output_types(augmented_data_frame=pd.DataFrame)\n",
    "    def run(self, data_frame: pd.DataFrame):\n",
    "        \n",
    "        # New columns to store RAG results\n",
    "        answers: List[str] = []\n",
    "        contexts: List[List[str]] = []\n",
    "\n",
    "        print(f\"Running RAG SuperComponent on {len(data_frame)} queries...\")\n",
    "\n",
    "        # Iterate through the queries (user_input column)\n",
    "        for _, row in data_frame.iterrows():\n",
    "            query = row[\"user_input\"]\n",
    "            \n",
    "            # 1. Run the RAG SuperComponent\n",
    "            # It expects 'query' as input and returns a dictionary.\n",
    "            rag_output = self.rag_supercomponent.run(query=query)\n",
    "            \n",
    "            # 2. Extract answer and contexts\n",
    "            # Based on the naive_rag_sc/hybrid_rag_sc structure:\n",
    "            answer = rag_output.get('replies', [''])[0]\n",
    "            \n",
    "            # Extract content from the Document objects\n",
    "            retrieved_docs = rag_output.get('documents', [])\n",
    "            retrieved_contexts = [doc.content for doc in retrieved_docs]\n",
    "            \n",
    "            answers.append(answer)\n",
    "            contexts.append(retrieved_contexts)\n",
    "        \n",
    "        # 3. Augment the DataFrame\n",
    "        data_frame['response'] = answers\n",
    "        data_frame['retrieved_contexts'] = contexts\n",
    "        \n",
    "        print(\"RAG processing complete.\")\n",
    "        return {\"augmented_data_frame\": data_frame}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272c9cff",
   "metadata": {},
   "source": [
    "## Component 3: RAGAS Evaluation Engine üìà\n",
    "\n",
    "The **RagasEvaluationComponent** integrates the RAGAS framework into our Haystack pipeline, providing systematic evaluation metrics for RAG systems.\n",
    "\n",
    "**üéØ Core Evaluation Metrics:**\n",
    "\n",
    "| Metric | Purpose | What It Measures |\n",
    "|--------|---------|------------------|\n",
    "| **Faithfulness** | Response Quality | Factual consistency with retrieved context |\n",
    "| **ResponseRelevancy** | Relevance | How well responses answer the questions |\n",
    "| **LLMContextRecall** | Retrieval Quality | How well retrieval captures relevant information |\n",
    "| **FactualCorrectness** | Accuracy | Correctness of factual claims in responses |\n",
    "\n",
    "**üîß Technical Implementation:**\n",
    "- **Focused Metrics**: Core metrics for reliable comparison\n",
    "- **LLM Integration**: Uses OpenAI GPT models for evaluation judgments  \n",
    "- **Data Format Handling**: Automatically formats data for RAGAS requirements\n",
    "- **Comprehensive Output**: Returns both aggregated metrics and detailed per-query results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5b842a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import EvaluationDataset, evaluate\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, ResponseRelevancy\n",
    "\n",
    "from ragas.llms import llm_factory\n",
    "from haystack.utils import Secret\n",
    "import os\n",
    "from ragas.llms import HaystackLLMWrapper\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "\n",
    "@component\n",
    "class RagasEvaluationComponent:\n",
    "    \"\"\"\n",
    "    Prepares data for Ragas, runs the evaluation, and returns the metrics.\n",
    "    Simplified for core metrics comparison.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 metrics: Optional[List[Any]] = None,\n",
    "                 llm_model: str = \"gpt-4o-mini\",\n",
    "                 openai_api_key: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Initialize the RagasEvaluationComponent.\n",
    "        \n",
    "        Args:\n",
    "            metrics: List of RAGAS metrics to evaluate (defaults to core metrics)\n",
    "            llm_model (str): OpenAI model for evaluation. Defaults to \"gpt-4o-mini\".\n",
    "            openai_api_key (Optional[str]): OpenAI API key. If None, will use environment variable.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Default to core metrics for systematic comparison\n",
    "        if metrics is None:\n",
    "            self.metrics = [\n",
    "                Faithfulness(), \n",
    "                ResponseRelevancy(),\n",
    "                LLMContextRecall(),\n",
    "                FactualCorrectness()\n",
    "            ]\n",
    "        else:\n",
    "            self.metrics = metrics\n",
    "            \n",
    "        self.llm_model = llm_model\n",
    "        self.openai_api_key = openai_api_key or os.getenv('OPENAI_API_KEY')\n",
    "        \n",
    "        if not self.openai_api_key:\n",
    "            raise ValueError(\"OpenAI API key not found. Please set OPENAI_API_KEY environment variable or pass openai_api_key parameter.\")\n",
    "        \n",
    "        # Configure RAGAS LLM for evaluation\n",
    "        self.ragas_llm = HaystackLLMWrapper(\n",
    "            OpenAIGenerator(\n",
    "                model=self.llm_model,\n",
    "                api_key=Secret.from_token(self.openai_api_key)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    @component.output_types(metrics=Dict[str, float], evaluation_df=pd.DataFrame)\n",
    "    def run(self, augmented_data_frame: pd.DataFrame):\n",
    "        \n",
    "        # 1. Map columns to Ragas requirements\n",
    "        ragas_data = pd.DataFrame({\n",
    "            'user_input': augmented_data_frame['user_input'],\n",
    "            'response': augmented_data_frame['response'], \n",
    "            'retrieved_contexts': augmented_data_frame['retrieved_contexts'],\n",
    "            'reference': augmented_data_frame['reference'],\n",
    "            'reference_contexts': augmented_data_frame['reference_contexts'].apply(eval)\n",
    "        })\n",
    "\n",
    "        print(\"Creating Ragas EvaluationDataset...\")\n",
    "        # 2. Create EvaluationDataset\n",
    "        dataset = EvaluationDataset.from_pandas(ragas_data)\n",
    "\n",
    "        print(\"Starting Ragas evaluation...\")\n",
    "        \n",
    "        # 3. Run Ragas Evaluation\n",
    "        results = evaluate(\n",
    "            dataset=dataset,\n",
    "            metrics=self.metrics,\n",
    "            llm=self.ragas_llm\n",
    "        )\n",
    "        \n",
    "        results_df = results.to_pandas()\n",
    "        \n",
    "        print(\"Ragas evaluation complete.\")\n",
    "        print(f\"Overall metrics: {results}\")\n",
    "        \n",
    "        return {\"metrics\": results, \"evaluation_df\": results_df}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b0aa01",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üß™ Systematic RAG Comparison: Naive vs Hybrid\n",
    "\n",
    "Now we'll systematically evaluate both RAG SuperComponents using the same evaluation pipeline. This ensures fair comparison with identical evaluation conditions.\n",
    "\n",
    "## üéØ Comparison Strategy\n",
    "\n",
    "Our approach enables **systematic comparison**:\n",
    "\n",
    "1. **Same Dataset**: Both systems evaluated on identical test queries\n",
    "2. **Same Metrics**: Consistent evaluation criteria across both approaches\n",
    "3. **Same Pipeline**: Identical processing workflow eliminates bias\n",
    "4. **Reproducible Results**: Pipeline ensures consistent evaluation conditions\n",
    "\n",
    "## üìä Dataset Information\n",
    "\n",
    "We'll use a synthetic evaluation dataset:\n",
    "- **`synthetic_tests_advanced_branching_2.csv`**: Focused dataset for comparison\n",
    "\n",
    "**Dataset Structure:**\n",
    "- `user_input`: Questions to ask the RAG system\n",
    "- `reference`: Ground truth answers for comparison\n",
    "- `reference_contexts`: Expected retrieved contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0384e71c",
   "metadata": {},
   "source": [
    "### Setup: Initialize Both RAG SuperComponents\n",
    "\n",
    "First, we'll initialize both RAG SuperComponents with consistent parameters for fair comparison.\n",
    "\n",
    "**üîß Configuration:**\n",
    "- **Same base parameters**: Both systems use identical core settings\n",
    "- **Document store**: Shared Elasticsearch document store\n",
    "- **Models**: Consistent LLM and embedding models for both systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a23faff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Both RAG SuperComponents initialized successfully!\n",
      "üìä Naive RAG: NaiveRAGSuperComponent\n",
      "üìä Hybrid RAG: HybridRAGSuperComponent\n"
     ]
    }
   ],
   "source": [
    "# --- Setup Environment & Dependencies ---\n",
    "from scripts.rag.hybridrag import HybridRAGSuperComponent\n",
    "from scripts.rag.naiverag import NaiveRAGSuperComponent\n",
    "from haystack_integrations.document_stores.elasticsearch import ElasticsearchDocumentStore\n",
    "import os\n",
    "\n",
    "# Initialize document store\n",
    "document_store = ElasticsearchDocumentStore(hosts=\"http://localhost:9200\")\n",
    "\n",
    "# Create both RAG SuperComponents with base parameters for fair comparison\n",
    "naive_rag_sc = NaiveRAGSuperComponent(\n",
    "    document_store=document_store\n",
    ")\n",
    "\n",
    "hybrid_rag_sc = HybridRAGSuperComponent(\n",
    "    document_store=document_store\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Both RAG SuperComponents initialized successfully!\")\n",
    "print(f\"üìä Naive RAG: {naive_rag_sc.__class__.__name__}\")\n",
    "print(f\"üìä Hybrid RAG: {hybrid_rag_sc.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd5bfc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Evaluation function ready for systematic comparison\n"
     ]
    }
   ],
   "source": [
    "# --- Create Evaluation Function for Systematic Comparison ---\n",
    "\n",
    "def evaluate_rag_system(rag_supercomponent, system_name):\n",
    "    \"\"\"\n",
    "    Evaluate a RAG SuperComponent using the evaluation pipeline\n",
    "    \n",
    "    Args:\n",
    "        rag_supercomponent: The RAG system to evaluate\n",
    "        system_name: Name for logging and identification\n",
    "    \n",
    "    Returns:\n",
    "        dict: Evaluation results containing metrics and detailed dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nüîÑ Evaluating {system_name}...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Initialize pipeline components\n",
    "    reader = CSVReaderComponent()\n",
    "    augmenter = RAGDataAugmenterComponent(rag_supercomponent=rag_supercomponent)\n",
    "    evaluator = RagasEvaluationComponent()  # Uses default core metrics\n",
    "    \n",
    "    # Build evaluation pipeline\n",
    "    evaluation_pipeline = Pipeline()\n",
    "    evaluation_pipeline.add_component(\"reader\", reader)\n",
    "    evaluation_pipeline.add_component(\"augmenter\", augmenter)\n",
    "    evaluation_pipeline.add_component(\"evaluator\", evaluator)\n",
    "    \n",
    "    # Connect pipeline components\n",
    "    evaluation_pipeline.connect(\"reader.data_frame\", \"augmenter.data_frame\")\n",
    "    evaluation_pipeline.connect(\"augmenter.augmented_data_frame\", \"evaluator.augmented_data_frame\")\n",
    "    \n",
    "    # Run evaluation\n",
    "    csv_file_path = \"data_for_eval/synthetic_tests_advanced_branching_2.csv\"\n",
    "    results = evaluation_pipeline.run({\"reader\": {\"source\": csv_file_path}})\n",
    "    \n",
    "    print(f\"‚úÖ {system_name} evaluation complete!\")\n",
    "    return results\n",
    "\n",
    "print(\"üîß Evaluation function ready for systematic comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08342b27",
   "metadata": {},
   "source": [
    "## Experiment 1: Naive RAG Evaluation üî¨\n",
    "\n",
    "Let's evaluate the Naive RAG SuperComponent first. This will establish our baseline performance metrics.\n",
    "\n",
    "**üéØ What to Observe:**\n",
    "- Processing time and efficiency\n",
    "- Core metric scores (Faithfulness, Relevancy, Context Recall, Factual Correctness)\n",
    "- Any errors or warnings during evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a9d17a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Evaluating Naive RAG SuperComponent...\n",
      "==================================================\n",
      "Loaded DataFrame with 3 rows from data_for_eval/synthetic_tests_advanced_branching_2.csv.\n",
      "Running RAG SuperComponent on 3 queries...\n",
      "RAG processing complete.\n",
      "Creating Ragas EvaluationDataset...\n",
      "Starting Ragas evaluation...\n",
      "RAG processing complete.\n",
      "Creating Ragas EvaluationDataset...\n",
      "Starting Ragas evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f97d145bde314e9388f9ee4108b82331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ragas evaluation complete.\n",
      "Overall metrics: {'faithfulness': 0.5238, 'answer_relevancy': 0.6277, 'context_recall': 0.8333, 'factual_correctness(mode=f1)': 0.3900}\n",
      "‚úÖ Naive RAG SuperComponent evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Naive RAG SuperComponent\n",
    "naive_results = evaluate_rag_system(naive_rag_sc, \"Naive RAG SuperComponent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b965d00",
   "metadata": {},
   "source": [
    "### Naive RAG Results Analysis üìä\n",
    "\n",
    "Let's examine the detailed evaluation results from our Naive RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c9ce4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Naive RAG - Detailed Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>factual_correctness(mode=f1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wut is Alexa and how does it use AI?</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>Alexa is a voice-controlled virtual assistant ...</td>\n",
       "      <td>Alexa is a voice-controlled virtual assistant ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.93069</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What trends in user engagement with ChatGPT we...</td>\n",
       "      <td>[‚Äù (truncated)\\n[user]: ‚Äú10 more‚Äù\\nTable 2:Ill...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n4 The Growth of ChatGPT\\nChatGPT w...</td>\n",
       "      <td>In 2023, the first cohort of ChatGPT users exh...</td>\n",
       "      <td>In 2023, the first cohort of ChatGPT users exp...</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.95237</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the implications of artificial intell...</td>\n",
       "      <td>[Corporate\\nusers may also use ChatGPT Busines...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n27-28.\\n- Christian Davenport, ‚Äú F...</td>\n",
       "      <td>I don't have enough information to answer.</td>\n",
       "      <td>The implications of artificial intelligence on...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0               Wut is Alexa and how does it use AI?   \n",
       "1  What trends in user engagement with ChatGPT we...   \n",
       "2  What are the implications of artificial intell...   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [What is AI, how does it work and why are some...   \n",
       "1  [‚Äù (truncated)\\n[user]: ‚Äú10 more‚Äù\\nTable 2:Ill...   \n",
       "2  [Corporate\\nusers may also use ChatGPT Busines...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  [What is AI, how does it work and why are some...   \n",
       "1  [<1-hop>\\n\\n4 The Growth of ChatGPT\\nChatGPT w...   \n",
       "2  [<1-hop>\\n\\n27-28.\\n- Christian Davenport, ‚Äú F...   \n",
       "\n",
       "                                            response  \\\n",
       "0  Alexa is a voice-controlled virtual assistant ...   \n",
       "1  In 2023, the first cohort of ChatGPT users exh...   \n",
       "2         I don't have enough information to answer.   \n",
       "\n",
       "                                           reference  faithfulness  \\\n",
       "0  Alexa is a voice-controlled virtual assistant ...      1.000000   \n",
       "1  In 2023, the first cohort of ChatGPT users exp...      0.571429   \n",
       "2  The implications of artificial intelligence on...      0.000000   \n",
       "\n",
       "   answer_relevancy  context_recall  factual_correctness(mode=f1)  \n",
       "0           0.93069             1.0                          0.57  \n",
       "1           0.95237             1.0                          0.60  \n",
       "2           0.00000             0.5                          0.00  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display Naive RAG detailed results\n",
    "print(\"üìä Naive RAG - Detailed Results:\")\n",
    "naive_results['evaluator']['evaluation_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61bd01d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Naive RAG - Summary Metrics:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'faithfulness': 0.5238, 'answer_relevancy': 0.6277, 'context_recall': 0.8333, 'factual_correctness(mode=f1)': 0.3900}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display Naive RAG summary metrics\n",
    "print(\"üìà Naive RAG - Summary Metrics:\")\n",
    "naive_metrics = naive_results['evaluator']['metrics']\n",
    "naive_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8308cf",
   "metadata": {},
   "source": [
    "## Experiment 2: Hybrid RAG Evaluation üî¨‚ö°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1db6e1",
   "metadata": {},
   "source": [
    "Now let's evaluate the **Hybrid RAG SuperComponent** using the identical evaluation pipeline. This systematic approach ensures fair comparison.\n",
    "\n",
    "**üîÑ Key Benefits of This Approach:**\n",
    "- **Identical Conditions**: Same pipeline, metrics, and dataset\n",
    "- **Systematic Comparison**: Eliminates evaluation bias\n",
    "- **Reproducible Results**: Consistent methodology across both systems\n",
    "\n",
    "**üéØ Expected Improvements:**\n",
    "Hybrid RAG typically shows better performance due to:\n",
    "- **Dense + Sparse Retrieval**: Combines semantic and keyword-based search\n",
    "- **Enhanced Context Quality**: Better retrieval often leads to better responses\n",
    "- **Improved Robustness**: Multiple retrieval methods reduce failure modes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60268ad4",
   "metadata": {},
   "source": [
    "# Evaluate Hybrid RAG SuperComponent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff5f12ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Evaluating Hybrid RAG SuperComponent...\n",
      "==================================================\n",
      "Loaded DataFrame with 3 rows from data_for_eval/synthetic_tests_advanced_branching_2.csv.\n",
      "Running RAG SuperComponent on 3 queries...\n",
      "RAG processing complete.\n",
      "Creating Ragas EvaluationDataset...\n",
      "Starting Ragas evaluation...\n",
      "RAG processing complete.\n",
      "Creating Ragas EvaluationDataset...\n",
      "Starting Ragas evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d82bf3ad24784c1ea3c342be3260004b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ragas evaluation complete.\n",
      "Overall metrics: {'faithfulness': 1.0000, 'answer_relevancy': 0.6342, 'context_recall': 0.8333, 'factual_correctness(mode=f1)': 0.6667}\n",
      "‚úÖ Hybrid RAG SuperComponent evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "hybrid_results = evaluate_rag_system(hybrid_rag_sc, \"Hybrid RAG SuperComponent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fb55e6",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f152688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Hybrid RAG - Detailed Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>factual_correctness(mode=f1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wut is Alexa and how does it use AI?</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>Alexa is a voice-controlled virtual assistant ...</td>\n",
       "      <td>Alexa is a voice-controlled virtual assistant ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.917761</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What trends in user engagement with ChatGPT we...</td>\n",
       "      <td>[The yellow line represents the first cohort o...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n4 The Growth of ChatGPT\\nChatGPT w...</td>\n",
       "      <td>In 2023, user engagement with ChatGPT, particu...</td>\n",
       "      <td>In 2023, the first cohort of ChatGPT users exp...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.984896</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the implications of artificial intell...</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n27-28.\\n- Christian Davenport, ‚Äú F...</td>\n",
       "      <td>The provided information does not specifically...</td>\n",
       "      <td>The implications of artificial intelligence on...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0               Wut is Alexa and how does it use AI?   \n",
       "1  What trends in user engagement with ChatGPT we...   \n",
       "2  What are the implications of artificial intell...   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [What is AI, how does it work and why are some...   \n",
       "1  [The yellow line represents the first cohort o...   \n",
       "2  [What is AI, how does it work and why are some...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  [What is AI, how does it work and why are some...   \n",
       "1  [<1-hop>\\n\\n4 The Growth of ChatGPT\\nChatGPT w...   \n",
       "2  [<1-hop>\\n\\n27-28.\\n- Christian Davenport, ‚Äú F...   \n",
       "\n",
       "                                            response  \\\n",
       "0  Alexa is a voice-controlled virtual assistant ...   \n",
       "1  In 2023, user engagement with ChatGPT, particu...   \n",
       "2  The provided information does not specifically...   \n",
       "\n",
       "                                           reference  faithfulness  \\\n",
       "0  Alexa is a voice-controlled virtual assistant ...           1.0   \n",
       "1  In 2023, the first cohort of ChatGPT users exp...           1.0   \n",
       "2  The implications of artificial intelligence on...           1.0   \n",
       "\n",
       "   answer_relevancy  context_recall  factual_correctness(mode=f1)  \n",
       "0          0.917761             1.0                           1.0  \n",
       "1          0.984896             1.0                           1.0  \n",
       "2          0.000000             0.5                           0.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"üìä Hybrid RAG - Detailed Results:\")\n",
    "hybrid_results['evaluator']['evaluation_df']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06413754",
   "metadata": {},
   "source": [
    "Naive\n",
    "```python\n",
    "{'faithfulness': 0.5238, \n",
    "'answer_relevancy': 0.6277, \n",
    "'context_recall': 0.8333, \n",
    "'factual_correctness(mode=f1)': 0.3900}\n",
    "```\n",
    "\n",
    "\n",
    "Hybrid\n",
    "\n",
    "```python\n",
    "{'faithfulness': 1.0000, \n",
    "'answer_relevancy': 0.6342, \n",
    "'context_recall': 0.8333, \n",
    "'factual_correctness(mode=f1)': 0.6667}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef3093a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-with-haystack-ch6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
