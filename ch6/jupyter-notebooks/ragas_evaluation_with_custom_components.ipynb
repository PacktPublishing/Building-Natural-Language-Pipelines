{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75f744c9",
   "metadata": {},
   "source": [
    "üîß  **Setup Required**: Before running this notebook, please follow the [setup instructions](../README.md#setup-instructions) to configure your environment and API keys. **You will need to ensure you've executed the Indexing pipeline before completing this exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ad4fc5",
   "metadata": {},
   "source": [
    "# Systematic RAG Evaluation: Naive vs Hybrid Comparison\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates a **systematic evaluation workflow** for comparing two RAG (Retrieval-Augmented Generation) approaches using **Haystack custom components**. We'll create a reproducible pipeline to:\n",
    "\n",
    "1. **Load evaluation datasets** from CSV files\n",
    "2. **Process queries** through both Naive and Hybrid RAG SuperComponents \n",
    "3. **Generate comprehensive metrics** using the RAGAS framework\n",
    "4. **Compare performance** systematically\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand how to:\n",
    "- Create reusable evaluation components for RAG systems\n",
    "- Build scalable pipelines for systematic RAG comparison\n",
    "- Interpret RAGAS metrics in comparative context\n",
    "- Make data-driven decisions between RAG approaches\n",
    "\n",
    "## Evaluation Pipeline\n",
    "\n",
    "Our pipeline consists of three main components:\n",
    "\n",
    "```\n",
    "CSV Data ‚Üí RAGDataAugmenter ‚Üí RagasEvaluation ‚Üí Metrics & Results\n",
    "```\n",
    "\n",
    "**Key Benefits:**\n",
    "- **Systematic**: Same evaluation conditions for both RAG systems\n",
    "- **Reproducible**: Consistent evaluation across experiments\n",
    "- **Scalable**: Easy to add new RAG implementations\n",
    "- **Comprehensive**: Multiple metrics provide complete assessment\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4640ae5",
   "metadata": {},
   "source": [
    "## Component 1: CSV Data Loader\n",
    "\n",
    "The **CSVReaderComponent** serves as the entry point for our evaluation pipeline. It handles loading synthetic evaluation datasets and ensures data quality before processing.\n",
    "\n",
    "**Key Features:**\n",
    "- **Robust Error Handling**: Validates file existence and data integrity\n",
    "- **Pandas Integration**: Returns data as DataFrame for easy manipulation\n",
    "- **Pipeline Compatible**: Designed to work seamlessly with Haystack pipelines\n",
    "\n",
    "**Input:** File path to CSV containing evaluation queries and ground truth\n",
    "**Output:** Pandas DataFrame ready for RAG processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a83513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from haystack import component, Pipeline\n",
    "from typing import List, Optional, Dict, Any, Union\n",
    "\n",
    "@component\n",
    "class CSVReaderComponent:\n",
    "    \"\"\"Reads a CSV file into a Pandas DataFrame.\"\"\"\n",
    "\n",
    "    @component.output_types(data_frame=pd.DataFrame)\n",
    "    def run(self, source: Union[str, Path]):\n",
    "        \"\"\"\n",
    "        Reads the CSV file from the first source in the list.\n",
    "        \n",
    "        Args:\n",
    "            sources: List of file paths to CSV files. Only the first file will be processed.\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary containing the loaded DataFrame under 'data_frame' key.\n",
    "            \n",
    "        Raises:\n",
    "            FileNotFoundError: If the file doesn't exist or can't be read.\n",
    "            ValueError: If the DataFrame is empty after loading.\n",
    "        \"\"\"\n",
    "        if not source:\n",
    "            raise ValueError(\"No sources provided\")\n",
    "            \n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(source)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"File not found at {source}\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error reading CSV file {source}: {str(e)}\")\n",
    "\n",
    "        # Check if DataFrame is empty using proper pandas method\n",
    "        if df.empty:\n",
    "            raise ValueError(f\"DataFrame is empty after loading from {source}\")\n",
    "\n",
    "        print(f\"Loaded DataFrame with {len(df)} rows from {source}.\")\n",
    "        return {\"data_frame\": df}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1861dfb2",
   "metadata": {},
   "source": [
    "## Component 2: Async RAG Data Augmentation\n",
    "\n",
    "The **AsyncRAGDataAugmenterComponent** is the core of our evaluation workflow. It processes queries through a RAG SuperComponent with concurrent execution for optimal performance.\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "1. **Concurrent Processing**: Processes multiple queries in parallel batches\n",
    "2. **SuperComponent Flexibility**: Accepts any pre-configured RAG SuperComponent (Naive, Hybrid, or custom)\n",
    "3. **Configurable Batch Size**: Control concurrency based on API rate limits\n",
    "4. **Progress Tracking**: Real-time visibility into processing status\n",
    "5. **Error Handling**: Gracefully handles failures without stopping the entire evaluation\n",
    "\n",
    "**Performance Benefits:**\n",
    "- **Speed**: Up to N√ó faster than sequential processing (where N = batch_size)\n",
    "- **Scalability**: Efficiently handles large evaluation datasets\n",
    "- **Resource Optimization**: Maximizes API call efficiency\n",
    "\n",
    "**Pipeline Integration:**\n",
    "- **Input**: DataFrame with queries from CSVReaderComponent  \n",
    "- **Process**: Runs queries through RAG SuperComponent in concurrent batches\n",
    "- **Output**: Augmented DataFrame with responses and retrieved contexts\n",
    "\n",
    "**Why Async?**\n",
    "- **Faster Iteration**: Quickly evaluate large datasets\n",
    "- **Better Resource Utilization**: Maximize throughput without overwhelming APIs\n",
    "- **Production Ready**: Scalable approach for continuous evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3771b673",
   "metadata": {},
   "source": [
    "## Implementation: Concurrent Query Processing\n",
    "\n",
    "Below is the implementation of the async component that enables concurrent query processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2b1329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from typing import List\n",
    "from haystack import SuperComponent\n",
    "\n",
    "@component\n",
    "class AsyncRAGDataAugmenterComponent:\n",
    "    \"\"\"\n",
    "    Async version of RAGDataAugmenterComponent that processes queries concurrently.\n",
    "    \n",
    "    Key Improvements:\n",
    "    - Processes multiple queries in parallel batches\n",
    "    - Configurable concurrency limits to avoid rate limits\n",
    "    - Significant performance improvement for large datasets\n",
    "    - Progress tracking for long-running evaluations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rag_supercomponent: SuperComponent, batch_size: int = 5):\n",
    "        \"\"\"\n",
    "        Initialize the Async RAG Data Augmenter.\n",
    "        \n",
    "        Args:\n",
    "            rag_supercomponent: Pre-initialized RAG SuperComponent\n",
    "            batch_size (int): Number of queries to process concurrently. Defaults to 5.\n",
    "                             Adjust based on API rate limits and system resources.\n",
    "        \"\"\"\n",
    "        self.rag_supercomponent = rag_supercomponent\n",
    "        self.batch_size = batch_size\n",
    "        self.output_names = [\"augmented_data_frame\"]\n",
    "\n",
    "    async def _process_single_query(self, query: str, index: int) -> tuple:\n",
    "        \"\"\"\n",
    "        Process a single query through the RAG SuperComponent.\n",
    "        \n",
    "        Args:\n",
    "            query: The query string to process\n",
    "            index: The query index for tracking\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (index, answer, contexts)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Run the RAG SuperComponent\n",
    "            rag_output = self.rag_supercomponent.run(query=query)\n",
    "            \n",
    "            # Extract answer and contexts\n",
    "            answer = rag_output.get('replies', [''])[0]\n",
    "            retrieved_docs = rag_output.get('documents', [])\n",
    "            retrieved_contexts = [doc.content for doc in retrieved_docs]\n",
    "            \n",
    "            return (index, answer, retrieved_contexts)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query {index}: {str(e)}\")\n",
    "            return (index, \"\", [])\n",
    "\n",
    "    async def _process_batch(self, queries_with_indices: List[tuple]) -> List[tuple]:\n",
    "        \"\"\"\n",
    "        Process a batch of queries concurrently.\n",
    "        \n",
    "        Args:\n",
    "            queries_with_indices: List of (index, query) tuples\n",
    "            \n",
    "        Returns:\n",
    "            List of (index, answer, contexts) tuples\n",
    "        \"\"\"\n",
    "        tasks = [\n",
    "            self._process_single_query(query, idx) \n",
    "            for idx, query in queries_with_indices\n",
    "        ]\n",
    "        return await asyncio.gather(*tasks)\n",
    "\n",
    "    @component.output_types(augmented_data_frame=pd.DataFrame)\n",
    "    async def run(self, data_frame: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Process all queries in the DataFrame with concurrent execution.\n",
    "        \n",
    "        Args:\n",
    "            data_frame: DataFrame with 'user_input' column containing queries\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary with augmented DataFrame containing responses and contexts\n",
    "        \"\"\"\n",
    "        total_queries = len(data_frame)\n",
    "        print(f\"Running Async RAG on {total_queries} queries (batch size: {self.batch_size})...\")\n",
    "        \n",
    "        # Prepare queries with their indices\n",
    "        queries_with_indices = list(enumerate(data_frame[\"user_input\"].tolist()))\n",
    "        \n",
    "        # Initialize results storage\n",
    "        results = [None] * total_queries\n",
    "        \n",
    "        # Process in batches\n",
    "        for batch_start in range(0, total_queries, self.batch_size):\n",
    "            batch_end = min(batch_start + self.batch_size, total_queries)\n",
    "            batch = queries_with_indices[batch_start:batch_end]\n",
    "            \n",
    "            print(f\"Processing batch {batch_start//self.batch_size + 1} \"\n",
    "                  f\"(queries {batch_start+1}-{batch_end} of {total_queries})...\")\n",
    "            \n",
    "            # Process batch concurrently\n",
    "            batch_results = await self._process_batch(batch)\n",
    "            \n",
    "            # Store results in correct order\n",
    "            for idx, answer, contexts in batch_results:\n",
    "                results[idx] = (answer, contexts)\n",
    "        \n",
    "        # Extract answers and contexts from results\n",
    "        answers = [r[0] for r in results]\n",
    "        contexts = [r[1] for r in results]\n",
    "        \n",
    "        # Augment the DataFrame\n",
    "        data_frame = data_frame.copy()\n",
    "        data_frame['response'] = answers\n",
    "        data_frame['retrieved_contexts'] = contexts\n",
    "        \n",
    "        print(f\"‚úì Async RAG processing complete for {total_queries} queries!\")\n",
    "        return {\"augmented_data_frame\": data_frame}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272c9cff",
   "metadata": {},
   "source": [
    "## Component 3: RAGAS Evaluation Engine\n",
    "\n",
    "The **RagasEvaluationComponent** integrates the RAGAS framework into our Haystack pipeline, providing systematic evaluation metrics for RAG systems.\n",
    "\n",
    "**Core Evaluation Metrics:**\n",
    "\n",
    "| Metric | Purpose | What It Measures |\n",
    "|--------|---------|------------------|\n",
    "| **Faithfulness** | Response Quality | Factual consistency with retrieved context |\n",
    "| **ResponseRelevancy** | Relevance | How well responses answer the questions |\n",
    "| **LLMContextRecall** | Retrieval Quality | How well retrieval captures relevant information |\n",
    "| **FactualCorrectness** | Accuracy | Correctness of factual claims in responses |\n",
    "\n",
    "**Technical Implementation:**\n",
    "- **Focused Metrics**: Core metrics for reliable comparison\n",
    "- **LLM Integration**: Uses OpenAI GPT models for evaluation judgments  \n",
    "- **Data Format Handling**: Automatically formats data for RAGAS requirements\n",
    "- **Comprehensive Output**: Returns both aggregated metrics and detailed per-query results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b842a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import EvaluationDataset, evaluate\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, ResponseRelevancy\n",
    "\n",
    "from ragas.llms import llm_factory\n",
    "from haystack.utils import Secret\n",
    "import os\n",
    "from ragas.llms import HaystackLLMWrapper\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "\n",
    "\n",
    "@component\n",
    "class RagasEvaluationComponent:\n",
    "    \"\"\"\n",
    "    Integrates the RAGAS framework into Haystack pipeline for systematic evaluation.\n",
    "    \n",
    "    This component provides systematic evaluation metrics for RAG systems using\n",
    "    the RAGAS framework with focus on core metrics for reliable comparison.\n",
    "    \n",
    "    Core Evaluation Metrics:\n",
    "    - Faithfulness: Factual consistency with retrieved context\n",
    "    - ResponseRelevancy: How well responses answer the questions  \n",
    "    - LLMContextRecall: How well retrieval captures relevant information\n",
    "    - FactualCorrectness: Correctness of factual claims in responses\n",
    "    \n",
    "    Technical Features:\n",
    "    - Focused Metrics: Core metrics for reliable comparison\n",
    "    - LLM Integration: Uses provided generator for evaluation judgments\n",
    "    - Data Format Handling: Automatically formats data for RAGAS requirements\n",
    "    - Comprehensive Output: Returns both aggregated metrics and detailed per-query results\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 generator: Any,\n",
    "                 metrics: Optional[List[Any]] = None):\n",
    "        \"\"\"\n",
    "        Initialize the RAGAS Evaluation Component.\n",
    "        \n",
    "        Args:\n",
    "            generator: LLM generator instance (e.g., OpenAIGenerator or OllamaGenerator).\n",
    "            metrics: List of RAGAS metrics to evaluate (defaults to core metrics)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Default to core metrics for systematic comparison\n",
    "        if metrics is None:\n",
    "            self.metrics = [\n",
    "                Faithfulness(), \n",
    "                ResponseRelevancy(),\n",
    "                LLMContextRecall(),\n",
    "                FactualCorrectness()\n",
    "            ]\n",
    "        else:\n",
    "            self.metrics = metrics\n",
    "        \n",
    "        # Configure RAGAS LLM for evaluation\n",
    "        self.ragas_llm = HaystackLLMWrapper(generator)\n",
    "\n",
    "    @component.output_types(metrics=Dict[str, float], evaluation_df=pd.DataFrame)\n",
    "    def run(self, augmented_data_frame: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Run RAGAS evaluation on augmented dataset.\n",
    "        \n",
    "        Args:\n",
    "            augmented_data_frame: DataFrame with RAG responses and retrieved contexts\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary containing evaluation metrics and detailed results DataFrame\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. Map columns to Ragas requirements\n",
    "        ragas_data = pd.DataFrame({\n",
    "            'user_input': augmented_data_frame['user_input'],\n",
    "            'response': augmented_data_frame['response'], \n",
    "            'retrieved_contexts': augmented_data_frame['retrieved_contexts'],\n",
    "            'reference': augmented_data_frame['reference'],\n",
    "            'reference_contexts': augmented_data_frame['reference_contexts'].apply(eval)\n",
    "        })\n",
    "\n",
    "        print(\"Creating Ragas EvaluationDataset...\")\n",
    "        # 2. Create EvaluationDataset\n",
    "        dataset = EvaluationDataset.from_pandas(ragas_data)\n",
    "\n",
    "        print(\"Starting Ragas evaluation...\")\n",
    "        \n",
    "        # 3. Run Ragas Evaluation\n",
    "        results = evaluate(\n",
    "            dataset=dataset,\n",
    "            metrics=self.metrics,\n",
    "            llm=self.ragas_llm\n",
    "        )\n",
    "        \n",
    "        results_df = results.to_pandas()\n",
    "        \n",
    "        print(\"Ragas evaluation complete.\")\n",
    "        print(f\"Overall metrics: {results}\")\n",
    "        \n",
    "        return {\"metrics\": results, \"evaluation_df\": results_df}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b0aa01",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Systematic RAG Comparison: Naive vs Hybrid\n",
    "\n",
    "Now we'll systematically evaluate both RAG SuperComponents using the same evaluation pipeline. This ensures fair comparison with identical evaluation conditions.\n",
    "\n",
    "## Comparison Strategy\n",
    "\n",
    "Our approach enables **systematic comparison**:\n",
    "\n",
    "1. **Same Dataset**: Both systems evaluated on identical test queries\n",
    "2. **Same Metrics**: Consistent evaluation criteria across both approaches\n",
    "3. **Same Pipeline**: Identical processing workflow eliminates bias\n",
    "4. **Reproducible Results**: Pipeline ensures consistent evaluation conditions\n",
    "\n",
    "## Dataset Information\n",
    "\n",
    "We'll use a synthetic evaluation dataset:\n",
    "- **`synthetic_tests_advanced_branching_2.csv`**: Focused dataset for comparison\n",
    "\n",
    "**Dataset Structure:**\n",
    "- `user_input`: Questions to ask the RAG system\n",
    "- `reference`: Ground truth answers for comparison\n",
    "- `reference_contexts`: Expected retrieved contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0384e71c",
   "metadata": {},
   "source": [
    "### Setup: Initialize Both RAG SuperComponents\n",
    "\n",
    "First, we'll initialize both RAG SuperComponents with consistent parameters for fair comparison.\n",
    "\n",
    "**Configuration:**\n",
    "- **Same base parameters**: Both systems use identical core settings\n",
    "- **Document store**: Shared Elasticsearch document store\n",
    "- **Models**: Consistent LLM and embedding models for both systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a23faff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup Environment & Dependencies ---\n",
    "from scripts.rag.hybridrag import HybridRAGSuperComponent\n",
    "from scripts.rag.naiverag import NaiveRAGSuperComponent\n",
    "from haystack_integrations.document_stores.elasticsearch import ElasticsearchDocumentStore\n",
    "import os\n",
    "\n",
    "# Initialize document store\n",
    "document_store = ElasticsearchDocumentStore(hosts=\"http://localhost:9200\")\n",
    "\n",
    "# Create both RAG SuperComponents with base parameters for fair comparison\n",
    "naive_rag_sc = NaiveRAGSuperComponent(\n",
    "    document_store=document_store\n",
    ")\n",
    "\n",
    "hybrid_rag_sc = HybridRAGSuperComponent(\n",
    "    document_store=document_store\n",
    ")\n",
    "\n",
    "print(\"Both RAG SuperComponents initialized successfully!\")\n",
    "print(f\"Naive RAG: {naive_rag_sc.__class__.__name__}\")\n",
    "print(f\"Hybrid RAG: {hybrid_rag_sc.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf21e21",
   "metadata": {},
   "source": [
    "## Evaluation Pipeline: Using Haystack AsyncPipeline\n",
    "\n",
    "Now we'll create an evaluation pipeline using Haystack's `AsyncPipeline` for proper concurrent execution. This provides a cleaner, more maintainable approach than manually coordinating async components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa89ea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import AsyncPipeline\n",
    "\n",
    "def create_evaluation_pipeline(\n",
    "    rag_supercomponent,\n",
    "    generator: Any,\n",
    "    batch_size: int = 5\n",
    ") -> AsyncPipeline:\n",
    "    \"\"\"\n",
    "    Create an evaluation pipeline using Haystack's AsyncPipeline.\n",
    "    \n",
    "    Args:\n",
    "        rag_supercomponent: The RAG system to evaluate\n",
    "        generator: LLM generator for RAGAS evaluation\n",
    "        batch_size: Number of queries to process concurrently\n",
    "        \n",
    "    Returns:\n",
    "        AsyncPipeline: Configured evaluation pipeline\n",
    "    \"\"\"\n",
    "    pipeline = AsyncPipeline()\n",
    "    \n",
    "    # Add components\n",
    "    pipeline.add_component(\"reader\", CSVReaderComponent())\n",
    "    pipeline.add_component(\n",
    "        \"augmenter\",\n",
    "        AsyncRAGDataAugmenterComponent(\n",
    "            rag_supercomponent=rag_supercomponent,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "    )\n",
    "    pipeline.add_component(\"evaluator\", RagasEvaluationComponent(generator=generator))\n",
    "    \n",
    "    # Connect components\n",
    "    pipeline.connect(\"reader.data_frame\", \"augmenter.data_frame\")\n",
    "    pipeline.connect(\"augmenter.augmented_data_frame\", \"evaluator.augmented_data_frame\")\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "\n",
    "async def evaluate_rag_system_async(\n",
    "    rag_supercomponent,\n",
    "    system_name: str,\n",
    "    csv_file_path: str,\n",
    "    generator: Any,\n",
    "    batch_size: int = 5\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Asynchronously evaluate a single RAG system using AsyncPipeline.\n",
    "    \n",
    "    Args:\n",
    "        rag_supercomponent: The RAG system to evaluate\n",
    "        system_name: Name for logging\n",
    "        csv_file_path: Path to evaluation dataset\n",
    "        generator: LLM generator for RAGAS evaluation\n",
    "        batch_size: Number of queries to process concurrently\n",
    "        \n",
    "    Returns:\n",
    "        dict: Evaluation results with metrics and detailed DataFrame\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Starting async evaluation of {system_name}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Create evaluation pipeline\n",
    "    eval_pipeline = create_evaluation_pipeline(\n",
    "        rag_supercomponent=rag_supercomponent,\n",
    "        generator=generator,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    # Run pipeline asynchronously\n",
    "    results = await eval_pipeline.run_async(data={\"reader\": {\"source\": csv_file_path}})\n",
    "    \n",
    "    print(f\"\\n‚úì {system_name} evaluation complete!\")\n",
    "    return {\n",
    "        \"system_name\": system_name,\n",
    "        \"metrics\": results[\"evaluator\"][\"metrics\"],\n",
    "        \"evaluation_df\": results[\"evaluator\"][\"evaluation_df\"]\n",
    "    }\n",
    "\n",
    "\n",
    "async def evaluate_multiple_rag_systems_async(\n",
    "    rag_systems: List[tuple],\n",
    "    csv_file_path: str,\n",
    "    batch_size: int = 5\n",
    ") -> List[dict]:\n",
    "    \"\"\"\n",
    "    Evaluate multiple RAG systems concurrently using AsyncPipeline.\n",
    "    \n",
    "    Args:\n",
    "        rag_systems: List of (rag_supercomponent, system_name, generator) tuples\n",
    "        csv_file_path: Path to evaluation dataset\n",
    "        batch_size: Number of queries to process concurrently per system\n",
    "        \n",
    "    Returns:\n",
    "        List of evaluation results for each system\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üöÄ CONCURRENT EVALUATION OF {len(rag_systems)} RAG SYSTEMS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Dataset: {csv_file_path}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Systems: {[name for _, name, _ in rag_systems]}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Create evaluation tasks for each system\n",
    "    tasks = [\n",
    "        evaluate_rag_system_async(\n",
    "            rag_supercomponent=rag_sc,\n",
    "            system_name=name,\n",
    "            csv_file_path=csv_file_path,\n",
    "            generator=generator,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        for rag_sc, name, generator in rag_systems\n",
    "    ]\n",
    "    \n",
    "    # Run all evaluations concurrently\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"‚úì ALL EVALUATIONS COMPLETE\")\n",
    "    print(f\"‚è±Ô∏è  Total time: {elapsed:.2f} seconds\")\n",
    "    print(f\"‚ö° Average time per system: {elapsed/len(rag_systems):.2f} seconds\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7618d9c",
   "metadata": {},
   "source": [
    "## Run Concurrent Evaluation\n",
    "\n",
    "Now let's evaluate both Naive and Hybrid RAG systems **simultaneously** using concurrent evaluation.\n",
    "\n",
    "**Performance Benefits:**\n",
    "- **2√ó Faster**: Both systems evaluated at the same time\n",
    "- **Identical Conditions**: Both evaluations start simultaneously\n",
    "- **Efficient Resource Use**: Maximizes computational efficiency\n",
    "\n",
    "**Configuration:**\n",
    "- `batch_size`: Controls concurrent queries per system (adjust based on API limits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4629f990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup for Concurrent Evaluation ---\n",
    "\n",
    "# Create separate generators for each evaluation (components can't be shared)\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.utils import Secret\n",
    "import os\n",
    "\n",
    "eval_generator_naive = OpenAIGenerator(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=Secret.from_token(os.getenv(\"OPENAI_API_KEY\"))\n",
    ")\n",
    "\n",
    "eval_generator_hybrid = OpenAIGenerator(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=Secret.from_token(os.getenv(\"OPENAI_API_KEY\"))\n",
    ")\n",
    "\n",
    "# Define evaluation dataset\n",
    "csv_file_path = \"data_for_eval/synthetic_tests_advanced_branching_50.csv\"\n",
    "\n",
    "# Configure concurrent evaluation\n",
    "# batch_size: Number of queries to process in parallel (adjust based on API limits)\n",
    "batch_size = 5  # Process 5 queries at a time\n",
    "\n",
    "# Prepare RAG systems for concurrent evaluation\n",
    "rag_systems = [\n",
    "    (naive_rag_sc, \"Naive RAG\", eval_generator_naive),\n",
    "    (hybrid_rag_sc, \"Hybrid RAG\", eval_generator_hybrid)\n",
    "]\n",
    "\n",
    "# Run concurrent evaluation\n",
    "all_results = await evaluate_multiple_rag_systems_async(\n",
    "    rag_systems=rag_systems,\n",
    "    csv_file_path=csv_file_path,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Extract results for each system\n",
    "naive_results_async = all_results[0]\n",
    "hybrid_results_async = all_results[1]\n",
    "\n",
    "print(\"\\n‚úì Concurrent evaluation complete!\")\n",
    "print(f\"Naive RAG: {len(naive_results_async['evaluation_df'])} queries evaluated\")\n",
    "print(f\"Hybrid RAG: {len(hybrid_results_async['evaluation_df'])} queries evaluated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287f9163",
   "metadata": {},
   "source": [
    "### View Evaluation Results\n",
    "\n",
    "Let's examine the results from our concurrent evaluation of both systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e582f25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results from concurrent evaluation\n",
    "print(\"Naive RAG - Async Evaluation Results:\")\n",
    "print(f\"Metrics: {naive_results_async['metrics']}\\n\")\n",
    "display(naive_results_async['evaluation_df'].head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"Hybrid RAG - Async Evaluation Results:\")\n",
    "print(f\"Metrics: {hybrid_results_async['metrics']}\\n\")\n",
    "display(hybrid_results_async['evaluation_df'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06db19ce",
   "metadata": {},
   "source": [
    "## Side-by-Side Performance Comparison\n",
    "\n",
    "Let's create a comprehensive comparison of both RAG systems to understand their relative performance across all metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f2d91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the comparison to use async results\n",
    "import pandas as pd\n",
    "\n",
    "# Extract average metrics from async evaluations\n",
    "naive_scores_async = naive_results_async['metrics'].scores\n",
    "hybrid_scores_async = hybrid_results_async['metrics'].scores\n",
    "\n",
    "# Compute averages for each metric\n",
    "naive_df_async = pd.DataFrame(naive_scores_async)\n",
    "hybrid_df_async = pd.DataFrame(hybrid_scores_async)\n",
    "\n",
    "naive_averages_async = naive_df_async.mean()\n",
    "hybrid_averages_async = hybrid_df_async.mean()\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_data_async = {\n",
    "    'Metric': list(naive_averages_async.index),\n",
    "    'Naive RAG': list(naive_averages_async.values),\n",
    "    'Hybrid RAG': list(hybrid_averages_async.values)\n",
    "}\n",
    "\n",
    "comparison_df_async = pd.DataFrame(comparison_data_async)\n",
    "comparison_df_async['Improvement (%)'] = (\n",
    "    (comparison_df_async['Hybrid RAG'] - comparison_df_async['Naive RAG']) / \n",
    "    comparison_df_async['Naive RAG'] * 100\n",
    ").round(2)\n",
    "comparison_df_async['Better System'] = comparison_df_async.apply(\n",
    "    lambda row: 'Hybrid RAG' if row['Hybrid RAG'] > row['Naive RAG'] \n",
    "    else 'Naive RAG' if row['Naive RAG'] > row['Hybrid RAG'] \n",
    "    else 'Tie', axis=1\n",
    ")\n",
    "\n",
    "# Round scores\n",
    "comparison_df_async['Naive RAG'] = comparison_df_async['Naive RAG'].round(4)\n",
    "comparison_df_async['Hybrid RAG'] = comparison_df_async['Hybrid RAG'].round(4)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPREHENSIVE RAG SYSTEM COMPARISON (ASYNC EVALUATION)\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df_async.to_string(index=False))\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Calculate overall winner\n",
    "hybrid_wins = sum(comparison_df_async['Hybrid RAG'] > comparison_df_async['Naive RAG'])\n",
    "naive_wins = sum(comparison_df_async['Naive RAG'] > comparison_df_async['Hybrid RAG'])\n",
    "ties = sum(comparison_df_async['Naive RAG'] == comparison_df_async['Hybrid RAG'])\n",
    "\n",
    "print(f\"\\nFINAL SCORECARD:\")\n",
    "print(f\"Hybrid RAG wins: {hybrid_wins} metrics\")\n",
    "print(f\"Naive RAG wins: {naive_wins} metrics\") \n",
    "print(f\"Ties: {ties} metrics\")\n",
    "\n",
    "if hybrid_wins > naive_wins:\n",
    "    print(f\"\\nOVERALL WINNER: Hybrid RAG SuperComponent!\")\n",
    "    print(f\"   Better performance in {hybrid_wins}/{len(comparison_df_async)} metrics\")\n",
    "elif naive_wins > hybrid_wins:\n",
    "    print(f\"\\nOVERALL WINNER: Naive RAG SuperComponent!\")  \n",
    "    print(f\"   Better performance in {naive_wins}/{len(comparison_df_async)} metrics\")\n",
    "else:\n",
    "    print(f\"\\nRESULT: It's a tie between both systems!\")\n",
    "    \n",
    "avg_improvement = comparison_df_async['Improvement (%)'].mean()\n",
    "print(f\"\\nAverage improvement by Hybrid RAG: {avg_improvement:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47be3490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c832f9f0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Summary: Async Optimization Benefits\n",
    "\n",
    "### Key Performance Improvements\n",
    "\n",
    "We've optimized the RAGAS evaluation pipeline with async capabilities in three key areas:\n",
    "\n",
    "#### 1. **Concurrent Query Processing** (`AsyncRAGDataAugmenterComponent`)\n",
    "- **Sequential**: Processes queries one at a time\n",
    "- **Async**: Processes multiple queries in parallel batches\n",
    "- **Speed Improvement**: Up to N√ó faster (where N = batch_size)\n",
    "- **Configurable**: Adjust `batch_size` based on API rate limits\n",
    "\n",
    "#### 2. **Parallel System Evaluation** (`evaluate_multiple_rag_systems_async`)\n",
    "- **Sequential**: Evaluate Naive RAG ‚Üí wait ‚Üí Evaluate Hybrid RAG\n",
    "- **Async**: Both systems evaluated simultaneously\n",
    "- **Speed Improvement**: ~2√ó faster total evaluation time\n",
    "- **Scalable**: Easy to add more RAG systems for comparison\n",
    "\n",
    "#### 3. **Progress Monitoring**\n",
    "- Real-time progress tracking during batch processing\n",
    "- Visibility into which queries are being processed\n",
    "- Better debugging and monitoring capabilities\n",
    "\n",
    "### When to Use Each Approach\n",
    "\n",
    "| Scenario | Sequential | Async |\n",
    "|----------|-----------|-------|\n",
    "| Small datasets (<10 queries) | ‚úÖ Simpler | ‚ö†Ô∏è Overkill |\n",
    "| Large datasets (>50 queries) | ‚ö†Ô∏è Slow | ‚úÖ Much faster |\n",
    "| Single RAG system | ‚úÖ Adequate | ‚ö†Ô∏è Minor benefit |\n",
    "| Multiple RAG systems | ‚ö†Ô∏è Slow | ‚úÖ 2-N√ó faster |\n",
    "| API rate limits | ‚úÖ Safe | ‚ö†Ô∏è Need batch tuning |\n",
    "| Production evaluations | ‚ö†Ô∏è Time-consuming | ‚úÖ Recommended |\n",
    "\n",
    "### Configuration Tips\n",
    "\n",
    "**Batch Size Selection:**\n",
    "```python\n",
    "# Conservative (API rate limit safe)\n",
    "batch_size = 3\n",
    "\n",
    "# Balanced (good for most cases)\n",
    "batch_size = 5\n",
    "\n",
    "# Aggressive (requires high API limits)\n",
    "batch_size = 10\n",
    "```\n",
    "\n",
    "**For OpenAI API:**\n",
    "- Free tier: `batch_size = 2-3`\n",
    "- Pay-as-you-go: `batch_size = 5-10`\n",
    "- Enterprise: `batch_size = 10-20`\n",
    "\n",
    "### Code Structure Comparison\n",
    "\n",
    "**Original Sequential Approach:**\n",
    "```python\n",
    "# Process queries one by one\n",
    "for query in queries:\n",
    "    result = rag_sc.run(query=query)\n",
    "```\n",
    "\n",
    "**Optimized Async Approach:**\n",
    "```python\n",
    "# Process queries in batches concurrently\n",
    "async def process_batch(queries):\n",
    "    tasks = [rag_sc.run(query=q) for q in queries]\n",
    "    return await asyncio.gather(*tasks)\n",
    "```\n",
    "\n",
    "### Performance Metrics Example\n",
    "\n",
    "For a typical evaluation with 50 queries:\n",
    "\n",
    "| Approach | Time per Query | Total Time | Speedup |\n",
    "|----------|---------------|------------|---------|\n",
    "| Sequential | 5s | 250s (~4min) | 1√ó |\n",
    "| Async (batch=5) | 5s | 50s (~50s) | **5√ó** |\n",
    "| Async (2 systems) | 5s | 50s (~50s) | **5√ó** |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To further optimize your evaluation pipeline:\n",
    "\n",
    "1. **Increase batch size** if you have higher API rate limits\n",
    "2. **Use local models** (Ollama) to eliminate rate limit concerns\n",
    "3. **Cache RAG results** to avoid re-processing identical queries\n",
    "4. **Implement retry logic** for handling transient API failures\n",
    "5. **Add monitoring** to track API usage and costs\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (haystack-nlp-dev)",
   "language": "python",
   "name": "haystack-nlp-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
