{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75f744c9",
   "metadata": {},
   "source": [
    "ðŸ”§ **Setup Required**: Before running this notebook, please follow the [setup instructions](../README.md#setup-instructions) to configure your environment and API keys. **You will need to ensure you've executed the Indexing pipeline before completing this exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ad4fc5",
   "metadata": {},
   "source": [
    "# Systematic RAG Evaluation: Naive vs Hybrid Comparison\n",
    "\n",
    "## ðŸ“‹ Overview\n",
    "\n",
    "This notebook demonstrates a **systematic evaluation workflow** for comparing two RAG (Retrieval-Augmented Generation) approaches using **Haystack custom components**. We'll create a reproducible pipeline to:\n",
    "\n",
    "1. **Load evaluation datasets** from CSV files\n",
    "2. **Process queries** through both Naive and Hybrid RAG SuperComponents \n",
    "3. **Generate comprehensive metrics** using the RAGAS framework\n",
    "4. **Compare performance** systematically\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand how to:\n",
    "- Create reusable evaluation components for RAG systems\n",
    "- Build scalable pipelines for systematic RAG comparison\n",
    "- Interpret RAGAS metrics in comparative context\n",
    "- Make data-driven decisions between RAG approaches\n",
    "\n",
    "## ðŸ”§ Evaluation Pipeline\n",
    "\n",
    "Our pipeline consists of three main components:\n",
    "\n",
    "```\n",
    "CSV Data â†’ RAGDataAugmenter â†’ RagasEvaluation â†’ Metrics & Results\n",
    "```\n",
    "\n",
    "**Key Benefits:**\n",
    "- **Systematic**: Same evaluation conditions for both RAG systems\n",
    "- **Reproducible**: Consistent evaluation across experiments\n",
    "- **Scalable**: Easy to add new RAG implementations\n",
    "- **Comprehensive**: Multiple metrics provide complete assessment\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4640ae5",
   "metadata": {},
   "source": [
    "## Component 1: CSV Data Loader ðŸ“Š\n",
    "\n",
    "The **CSVReaderComponent** serves as the entry point for our evaluation pipeline. It handles loading synthetic evaluation datasets and ensures data quality before processing.\n",
    "\n",
    "**Key Features:**\n",
    "- **Robust Error Handling**: Validates file existence and data integrity\n",
    "- **Pandas Integration**: Returns data as DataFrame for easy manipulation\n",
    "- **Pipeline Compatible**: Designed to work seamlessly with Haystack pipelines\n",
    "\n",
    "**Input:** File path to CSV containing evaluation queries and ground truth\n",
    "**Output:** Pandas DataFrame ready for RAG processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08a83513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from haystack import component, Pipeline\n",
    "from typing import List, Optional, Dict, Any, Union\n",
    "\n",
    "@component\n",
    "class CSVReaderComponent:\n",
    "    \"\"\"Reads a CSV file into a Pandas DataFrame.\"\"\"\n",
    "\n",
    "    @component.output_types(data_frame=pd.DataFrame)\n",
    "    def run(self, source: Union[str, Path]):\n",
    "        \"\"\"\n",
    "        Reads the CSV file from the first source in the list.\n",
    "        \n",
    "        Args:\n",
    "            sources: List of file paths to CSV files. Only the first file will be processed.\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary containing the loaded DataFrame under 'data_frame' key.\n",
    "            \n",
    "        Raises:\n",
    "            FileNotFoundError: If the file doesn't exist or can't be read.\n",
    "            ValueError: If the DataFrame is empty after loading.\n",
    "        \"\"\"\n",
    "        if not source:\n",
    "            raise ValueError(\"No sources provided\")\n",
    "            \n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(source)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"File not found at {source}\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error reading CSV file {source}: {str(e)}\")\n",
    "\n",
    "        # Check if DataFrame is empty using proper pandas method\n",
    "        if df.empty:\n",
    "            raise ValueError(f\"DataFrame is empty after loading from {source}\")\n",
    "\n",
    "        print(f\"Loaded DataFrame with {len(df)} rows from {source}.\")\n",
    "        return {\"data_frame\": df}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1861dfb2",
   "metadata": {},
   "source": [
    "## Component 2: RAG Data Augmentation ðŸ”„\n",
    "\n",
    "The **RAGDataAugmenterComponent** is the core of our evaluation workflow. It takes each query from our evaluation dataset and processes it through a RAG SuperComponent, collecting both the generated responses and retrieved contexts.\n",
    "\n",
    "**ðŸ”‘ Key Design Decisions:**\n",
    "\n",
    "1. **SuperComponent Flexibility**: Accepts any pre-configured RAG SuperComponent (Naive, Hybrid, or custom)\n",
    "2. **Batch Processing**: Efficiently processes entire evaluation datasets\n",
    "3. **Data Augmentation**: Enriches the original dataset with RAG outputs for evaluation\n",
    "4. **Context Extraction**: Captures retrieved documents for context-based metrics\n",
    "\n",
    "**Pipeline Integration:**\n",
    "- **Input**: DataFrame with queries from CSVReaderComponent  \n",
    "- **Process**: Runs each query through the specified RAG SuperComponent\n",
    "- **Output**: Augmented DataFrame with responses and retrieved contexts\n",
    "\n",
    "**ðŸ’¡ Why This Approach?**\n",
    "By separating RAG execution from evaluation, we can:\n",
    "- **Swap RAG systems** without changing evaluation logic\n",
    "- **Cache RAG results** for multiple evaluation runs  \n",
    "- **Debug RAG performance** independently of metrics calculation\n",
    "- **Scale evaluation** across different datasets and configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72bf8b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import SuperComponent, super_component\n",
    "\n",
    "@component\n",
    "class RAGDataAugmenterComponent:\n",
    "    \"\"\"\n",
    "    Applies a RAG SuperComponent to each query in a DataFrame and \n",
    "    augments the data with the generated answer and retrieved contexts.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rag_supercomponent: SuperComponent):\n",
    "        # We store the pre-initialized SuperComponent\n",
    "        self.rag_supercomponent = rag_supercomponent\n",
    "        self.output_names = [\"augmented_data_frame\"]\n",
    "\n",
    "    @component.output_types(augmented_data_frame=pd.DataFrame)\n",
    "    def run(self, data_frame: pd.DataFrame):\n",
    "        \n",
    "        # New columns to store RAG results\n",
    "        answers: List[str] = []\n",
    "        contexts: List[List[str]] = []\n",
    "\n",
    "        print(f\"Running RAG SuperComponent on {len(data_frame)} queries...\")\n",
    "\n",
    "        # Iterate through the queries (user_input column)\n",
    "        for _, row in data_frame.iterrows():\n",
    "            query = row[\"user_input\"]\n",
    "            \n",
    "            # 1. Run the RAG SuperComponent\n",
    "            # It expects 'query' as input and returns a dictionary.\n",
    "            rag_output = self.rag_supercomponent.run(query=query)\n",
    "            \n",
    "            # 2. Extract answer and contexts\n",
    "            # Based on the naive_rag_sc/hybrid_rag_sc structure:\n",
    "            answer = rag_output.get('replies', [''])[0]\n",
    "            \n",
    "            # Extract content from the Document objects\n",
    "            retrieved_docs = rag_output.get('documents', [])\n",
    "            retrieved_contexts = [doc.content for doc in retrieved_docs]\n",
    "            \n",
    "            answers.append(answer)\n",
    "            contexts.append(retrieved_contexts)\n",
    "        \n",
    "        # 3. Augment the DataFrame\n",
    "        data_frame['response'] = answers\n",
    "        data_frame['retrieved_contexts'] = contexts\n",
    "        \n",
    "        print(\"RAG processing complete.\")\n",
    "        return {\"augmented_data_frame\": data_frame}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272c9cff",
   "metadata": {},
   "source": [
    "## Component 3: RAGAS Evaluation Engine ðŸ“ˆ\n",
    "\n",
    "The **RagasEvaluationComponent** integrates the RAGAS framework into our Haystack pipeline, providing systematic evaluation metrics for RAG systems.\n",
    "\n",
    "**ðŸŽ¯ Core Evaluation Metrics:**\n",
    "\n",
    "| Metric | Purpose | What It Measures |\n",
    "|--------|---------|------------------|\n",
    "| **Faithfulness** | Response Quality | Factual consistency with retrieved context |\n",
    "| **ResponseRelevancy** | Relevance | How well responses answer the questions |\n",
    "| **LLMContextRecall** | Retrieval Quality | How well retrieval captures relevant information |\n",
    "| **FactualCorrectness** | Accuracy | Correctness of factual claims in responses |\n",
    "\n",
    "**ðŸ”§ Technical Implementation:**\n",
    "- **Focused Metrics**: Core metrics for reliable comparison\n",
    "- **LLM Integration**: Uses OpenAI GPT models for evaluation judgments  \n",
    "- **Data Format Handling**: Automatically formats data for RAGAS requirements\n",
    "- **Comprehensive Output**: Returns both aggregated metrics and detailed per-query results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5b842a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import EvaluationDataset, evaluate\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, ResponseRelevancy\n",
    "\n",
    "from ragas.llms import llm_factory\n",
    "from haystack.utils import Secret\n",
    "import os\n",
    "from ragas.llms import HaystackLLMWrapper\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "\n",
    "\n",
    "@component\n",
    "class RagasEvaluationComponent:\n",
    "    \"\"\"\n",
    "    Integrates the RAGAS framework into Haystack pipeline for systematic evaluation.\n",
    "    \n",
    "    This component provides systematic evaluation metrics for RAG systems using\n",
    "    the RAGAS framework with focus on core metrics for reliable comparison.\n",
    "    \n",
    "    Core Evaluation Metrics:\n",
    "    - Faithfulness: Factual consistency with retrieved context\n",
    "    - ResponseRelevancy: How well responses answer the questions  \n",
    "    - LLMContextRecall: How well retrieval captures relevant information\n",
    "    - FactualCorrectness: Correctness of factual claims in responses\n",
    "    \n",
    "    Technical Features:\n",
    "    - Focused Metrics: Core metrics for reliable comparison\n",
    "    - LLM Integration: Uses provided generator for evaluation judgments\n",
    "    - Data Format Handling: Automatically formats data for RAGAS requirements\n",
    "    - Comprehensive Output: Returns both aggregated metrics and detailed per-query results\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 generator: Any,\n",
    "                 metrics: Optional[List[Any]] = None):\n",
    "        \"\"\"\n",
    "        Initialize the RAGAS Evaluation Component.\n",
    "        \n",
    "        Args:\n",
    "            generator: LLM generator instance (e.g., OpenAIGenerator or OllamaGenerator).\n",
    "            metrics: List of RAGAS metrics to evaluate (defaults to core metrics)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Default to core metrics for systematic comparison\n",
    "        if metrics is None:\n",
    "            self.metrics = [\n",
    "                Faithfulness(), \n",
    "                ResponseRelevancy(),\n",
    "                LLMContextRecall(),\n",
    "                FactualCorrectness()\n",
    "            ]\n",
    "        else:\n",
    "            self.metrics = metrics\n",
    "        \n",
    "        # Configure RAGAS LLM for evaluation\n",
    "        self.ragas_llm = HaystackLLMWrapper(generator)\n",
    "\n",
    "    @component.output_types(metrics=Dict[str, float], evaluation_df=pd.DataFrame)\n",
    "    def run(self, augmented_data_frame: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Run RAGAS evaluation on augmented dataset.\n",
    "        \n",
    "        Args:\n",
    "            augmented_data_frame: DataFrame with RAG responses and retrieved contexts\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary containing evaluation metrics and detailed results DataFrame\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. Map columns to Ragas requirements\n",
    "        ragas_data = pd.DataFrame({\n",
    "            'user_input': augmented_data_frame['user_input'],\n",
    "            'response': augmented_data_frame['response'], \n",
    "            'retrieved_contexts': augmented_data_frame['retrieved_contexts'],\n",
    "            'reference': augmented_data_frame['reference'],\n",
    "            'reference_contexts': augmented_data_frame['reference_contexts'].apply(eval)\n",
    "        })\n",
    "\n",
    "        print(\"Creating Ragas EvaluationDataset...\")\n",
    "        # 2. Create EvaluationDataset\n",
    "        dataset = EvaluationDataset.from_pandas(ragas_data)\n",
    "\n",
    "        print(\"Starting Ragas evaluation...\")\n",
    "        \n",
    "        # 3. Run Ragas Evaluation\n",
    "        results = evaluate(\n",
    "            dataset=dataset,\n",
    "            metrics=self.metrics,\n",
    "            llm=self.ragas_llm\n",
    "        )\n",
    "        \n",
    "        results_df = results.to_pandas()\n",
    "        \n",
    "        print(\"Ragas evaluation complete.\")\n",
    "        print(f\"Overall metrics: {results}\")\n",
    "        \n",
    "        return {\"metrics\": results, \"evaluation_df\": results_df}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b0aa01",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ§ª Systematic RAG Comparison: Naive vs Hybrid\n",
    "\n",
    "Now we'll systematically evaluate both RAG SuperComponents using the same evaluation pipeline. This ensures fair comparison with identical evaluation conditions.\n",
    "\n",
    "## ðŸŽ¯ Comparison Strategy\n",
    "\n",
    "Our approach enables **systematic comparison**:\n",
    "\n",
    "1. **Same Dataset**: Both systems evaluated on identical test queries\n",
    "2. **Same Metrics**: Consistent evaluation criteria across both approaches\n",
    "3. **Same Pipeline**: Identical processing workflow eliminates bias\n",
    "4. **Reproducible Results**: Pipeline ensures consistent evaluation conditions\n",
    "\n",
    "## ðŸ“Š Dataset Information\n",
    "\n",
    "We'll use a synthetic evaluation dataset:\n",
    "- **`synthetic_tests_advanced_branching_2.csv`**: Focused dataset for comparison\n",
    "\n",
    "**Dataset Structure:**\n",
    "- `user_input`: Questions to ask the RAG system\n",
    "- `reference`: Ground truth answers for comparison\n",
    "- `reference_contexts`: Expected retrieved contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0384e71c",
   "metadata": {},
   "source": [
    "### Setup: Initialize Both RAG SuperComponents\n",
    "\n",
    "First, we'll initialize both RAG SuperComponents with consistent parameters for fair comparison.\n",
    "\n",
    "**ðŸ”§ Configuration:**\n",
    "- **Same base parameters**: Both systems use identical core settings\n",
    "- **Document store**: Shared Elasticsearch document store\n",
    "- **Models**: Consistent LLM and embedding models for both systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a23faff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Both RAG SuperComponents initialized successfully!\n",
      "ðŸ“Š Naive RAG: NaiveRAGSuperComponent\n",
      "ðŸ“Š Hybrid RAG: HybridRAGSuperComponent\n"
     ]
    }
   ],
   "source": [
    "# --- Setup Environment & Dependencies ---\n",
    "from scripts.rag.hybridrag import HybridRAGSuperComponent\n",
    "from scripts.rag.naiverag import NaiveRAGSuperComponent\n",
    "from haystack_integrations.document_stores.elasticsearch import ElasticsearchDocumentStore\n",
    "import os\n",
    "\n",
    "# Initialize document store\n",
    "document_store = ElasticsearchDocumentStore(hosts=\"http://localhost:9200\")\n",
    "\n",
    "# Create both RAG SuperComponents with base parameters for fair comparison\n",
    "naive_rag_sc = NaiveRAGSuperComponent(\n",
    "    document_store=document_store\n",
    ")\n",
    "\n",
    "hybrid_rag_sc = HybridRAGSuperComponent(\n",
    "    document_store=document_store\n",
    ")\n",
    "\n",
    "print(\"âœ… Both RAG SuperComponents initialized successfully!\")\n",
    "print(f\"ðŸ“Š Naive RAG: {naive_rag_sc.__class__.__name__}\")\n",
    "print(f\"ðŸ“Š Hybrid RAG: {hybrid_rag_sc.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd5bfc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create RAG Evaluation SuperComponent for Systematic Comparison ---\n",
    "\n",
    "@super_component\n",
    "class RAGEvaluationSuperComponent:\n",
    "    \"\"\"\n",
    "    Complete RAG evaluation pipeline for systematic comparison of RAG systems.\n",
    "    \n",
    "    This SuperComponent provides a systematic evaluation workflow for comparing\n",
    "    RAG approaches with consistent evaluation conditions and comprehensive metrics.\n",
    "    \n",
    "    Pipeline Flow:\n",
    "    CSV Data â†’ RAGDataAugmenter â†’ RagasEvaluation â†’ Metrics & Results\n",
    "    \n",
    "    Key Benefits:\n",
    "    - Systematic: Same evaluation conditions for all RAG systems\n",
    "    - Reproducible: Consistent evaluation across experiments\n",
    "    - Scalable: Easy to add new RAG implementations\n",
    "    - Comprehensive: Multiple metrics provide complete assessment\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 rag_supercomponent, \n",
    "                 system_name: str,\n",
    "                 generator: Any,\n",
    "                 openai_api_key: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Initialize the RAG Evaluation SuperComponent.\n",
    "        \n",
    "        Args:\n",
    "            rag_supercomponent: The RAG system to evaluate\n",
    "            system_name (str): Name for logging and identification\n",
    "            generator: LLM generator instance (e.g., OpenAIGenerator or OllamaGenerator).\n",
    "            openai_api_key (Optional[str]): OpenAI API key. If None, will use environment variable.\n",
    "        \"\"\"\n",
    "        self.rag_supercomponent = rag_supercomponent\n",
    "        self.system_name = system_name\n",
    "        self.generator = generator\n",
    "        self.openai_api_key = openai_api_key or os.getenv('OPENAI_API_KEY')\n",
    "        \n",
    "        if not self.openai_api_key:\n",
    "            raise ValueError(\"OpenAI API key not found. Please set OPENAI_API_KEY environment variable or pass openai_api_key parameter.\")\n",
    "        \n",
    "        self._build_pipeline()\n",
    "    \n",
    "    def _build_pipeline(self):\n",
    "        \"\"\"Build the RAG evaluation pipeline with initialized components.\"\"\"\n",
    "        \n",
    "        print(f\"\\nðŸ”„ Building evaluation pipeline for {self.system_name}...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # --- 1. Initialize Evaluation Pipeline Components ---\n",
    "        \n",
    "        # CSV Reader: Loads evaluation dataset\n",
    "        reader = CSVReaderComponent()\n",
    "        \n",
    "        # RAG Data Augmenter: Processes queries through the RAG system\n",
    "        augmenter = RAGDataAugmenterComponent(rag_supercomponent=self.rag_supercomponent)\n",
    "        \n",
    "        # RAGAS Evaluator: Computes evaluation metrics\n",
    "        evaluator = RagasEvaluationComponent(\n",
    "            generator=self.generator\n",
    "        )\n",
    "        \n",
    "        # --- 2. Build the Evaluation Pipeline ---\n",
    "        self.pipeline = Pipeline()\n",
    "        \n",
    "        # Add all components to the pipeline\n",
    "        self.pipeline.add_component(\"reader\", reader)\n",
    "        self.pipeline.add_component(\"augmenter\", augmenter)\n",
    "        self.pipeline.add_component(\"evaluator\", evaluator)\n",
    "        \n",
    "        # --- 3. Connect the Components in a Graph ---\n",
    "        \n",
    "        # CSV Data -> RAG Augmentation -> RAGAS Evaluation\n",
    "        self.pipeline.connect(\"reader.data_frame\", \"augmenter.data_frame\")\n",
    "        self.pipeline.connect(\"augmenter.augmented_data_frame\", \"evaluator.augmented_data_frame\")\n",
    "        \n",
    "        # --- 4. Define Input and Output Mappings ---\n",
    "        self.input_mapping = {\n",
    "            \"csv_source\": [\"reader.source\"]\n",
    "        }\n",
    "\n",
    "        self.output_mapping = {\n",
    "            \"evaluator.metrics\": \"metrics\",\n",
    "            \"evaluator.evaluation_df\": \"evaluation_df\"\n",
    "        }\n",
    "        \n",
    "        print(f\"âœ… Evaluation pipeline for {self.system_name} built successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08342b27",
   "metadata": {},
   "source": [
    "## Experiment 1: Naive RAG Evaluation ðŸ”¬\n",
    "\n",
    "Let's evaluate the Naive RAG SuperComponent first. This will establish our baseline performance metrics.\n",
    "\n",
    "**ðŸŽ¯ What to Observe:**\n",
    "- Processing time and efficiency\n",
    "- Core metric scores (Faithfulness, Relevancy, Context Recall, Factual Correctness)\n",
    "- Any errors or warnings during evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a9d17a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”„ Building evaluation pipeline for Naive RAG...\n",
      "==================================================\n",
      "âœ… Evaluation pipeline for Naive RAG built successfully!\n",
      "Loaded DataFrame with 10 rows from data_for_eval/synthetic_tests_advanced_branching_10.csv.\n",
      "Running RAG SuperComponent on 10 queries...\n",
      "RAG processing complete.\n",
      "Creating Ragas EvaluationDataset...\n",
      "Starting Ragas evaluation...\n",
      "RAG processing complete.\n",
      "Creating Ragas EvaluationDataset...\n",
      "Starting Ragas evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a74e35b54764c7e8c133609b290ddb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ragas evaluation complete.\n",
      "Overall metrics: {'faithfulness': 0.9167, 'answer_relevancy': 0.7699, 'context_recall': 0.8967, 'factual_correctness(mode=f1)': 0.5260}\n",
      "âœ… Naive RAG evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "# Create generator for evaluation\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.utils import Secret\n",
    "import os\n",
    "\n",
    "eval_generator = OpenAIGenerator(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=Secret.from_token(os.getenv(\"OPENAI_API_KEY\"))\n",
    ")\n",
    "\n",
    "# Evaluate Naive RAG SuperComponent\n",
    "csv_file_path = \"data_for_eval/synthetic_tests_advanced_branching_10.csv\"\n",
    "\n",
    "# Create evaluation SuperComponent (csv_source now passed in run method)\n",
    "evaluation_sc = RAGEvaluationSuperComponent(\n",
    "    rag_supercomponent=naive_rag_sc,\n",
    "    system_name=\"Naive RAG\",\n",
    "    generator=eval_generator\n",
    ")\n",
    "\n",
    "# Run evaluation with CSV source passed to run method\n",
    "naive_results = evaluation_sc.run(csv_source=csv_file_path)\n",
    "\n",
    "print(f\"âœ… Naive RAG evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b965d00",
   "metadata": {},
   "source": [
    "### Naive RAG Results Analysis ðŸ“Š\n",
    "\n",
    "Let's examine the detailed evaluation results from our Naive RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c9ce4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Naive RAG - Detailed Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>factual_correctness(mode=f1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What Alexa do in AI?</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>Alexa, as a voice-controlled virtual assistant...</td>\n",
       "      <td>Alexa is a voice-controlled virtual assistant ...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.912168</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What happened to the UnitedHealthcare CEO Bria...</td>\n",
       "      <td>[- James Kurose, â€œTestimony before the House C...</td>\n",
       "      <td>[Why is AI controversial?\\nWhile acknowledging...</td>\n",
       "      <td>I don't have enough information to answer.</td>\n",
       "      <td>The BBC reported that Apple's AI falsely told ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the current stance of the UK governmen...</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>[Are there laws governing AI?\\nSome government...</td>\n",
       "      <td>The UK government has stated that it will \"tes...</td>\n",
       "      <td>In the UK, Prime Minister Sir Keir Starmer has...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does the GDPR impact the transparency and ...</td>\n",
       "      <td>[Smart cities\\nMetropolitan governments are us...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nAI ethics and transparency\\nAlgori...</td>\n",
       "      <td>The GDPR (General Data Protection Regulation) ...</td>\n",
       "      <td>The GDPR impacts the transparency and accounta...</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.971650</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the trends in the usage of ChatGPT fo...</td>\n",
       "      <td>[The yellow line represents the first cohort o...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n37% of messages are work-related f...</td>\n",
       "      <td>The usage of ChatGPT for Technical Help has sh...</td>\n",
       "      <td>The usage of ChatGPT for Technical Help has sh...</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.972123</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What trends in user interactions and gender re...</td>\n",
       "      <td>[Overall, the majority of ChatGPT usage\\nat wo...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nHowever, in the first half of 2025...</td>\n",
       "      <td>By June 2025, trends in user interactions with...</td>\n",
       "      <td>By June 2025, the share of active users with t...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.974209</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What are the main conversation topics users en...</td>\n",
       "      <td>[The yellow line represents the first cohort o...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nFigure 9 disaggregates four of the...</td>\n",
       "      <td>The main conversation topics users engage with...</td>\n",
       "      <td>Users engage with ChatGPT primarily through co...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.954396</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How does the quality of interactions with Chat...</td>\n",
       "      <td>[Corporate\\nusers may also use ChatGPT Busines...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nOverall, the majority of ChatGPT u...</td>\n",
       "      <td>The quality of interactions with ChatGPT at wo...</td>\n",
       "      <td>The quality of interactions with ChatGPT at wo...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985840</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What are the trends in ChatGPT message counts ...</td>\n",
       "      <td>[Overall, the majority of ChatGPT usage\\nat wo...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nTeams, Enterprise, Education), whi...</td>\n",
       "      <td>The trends in ChatGPT message counts highlight...</td>\n",
       "      <td>The trends in ChatGPT message counts indicate ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966834</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What are the implications of artificial intell...</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nWhat is AI, how does it work and w...</td>\n",
       "      <td>Artificial intelligence (AI) has significant i...</td>\n",
       "      <td>The implications of artificial intelligence (A...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961463</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0                               What Alexa do in AI?   \n",
       "1  What happened to the UnitedHealthcare CEO Bria...   \n",
       "2  What is the current stance of the UK governmen...   \n",
       "3  How does the GDPR impact the transparency and ...   \n",
       "4  What are the trends in the usage of ChatGPT fo...   \n",
       "5  What trends in user interactions and gender re...   \n",
       "6  What are the main conversation topics users en...   \n",
       "7  How does the quality of interactions with Chat...   \n",
       "8  What are the trends in ChatGPT message counts ...   \n",
       "9  What are the implications of artificial intell...   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [What is AI, how does it work and why are some...   \n",
       "1  [- James Kurose, â€œTestimony before the House C...   \n",
       "2  [What is AI, how does it work and why are some...   \n",
       "3  [Smart cities\\nMetropolitan governments are us...   \n",
       "4  [The yellow line represents the first cohort o...   \n",
       "5  [Overall, the majority of ChatGPT usage\\nat wo...   \n",
       "6  [The yellow line represents the first cohort o...   \n",
       "7  [Corporate\\nusers may also use ChatGPT Busines...   \n",
       "8  [Overall, the majority of ChatGPT usage\\nat wo...   \n",
       "9  [What is AI, how does it work and why are some...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  [What is AI, how does it work and why are some...   \n",
       "1  [Why is AI controversial?\\nWhile acknowledging...   \n",
       "2  [Are there laws governing AI?\\nSome government...   \n",
       "3  [<1-hop>\\n\\nAI ethics and transparency\\nAlgori...   \n",
       "4  [<1-hop>\\n\\n37% of messages are work-related f...   \n",
       "5  [<1-hop>\\n\\nHowever, in the first half of 2025...   \n",
       "6  [<1-hop>\\n\\nFigure 9 disaggregates four of the...   \n",
       "7  [<1-hop>\\n\\nOverall, the majority of ChatGPT u...   \n",
       "8  [<1-hop>\\n\\nTeams, Enterprise, Education), whi...   \n",
       "9  [<1-hop>\\n\\nWhat is AI, how does it work and w...   \n",
       "\n",
       "                                            response  \\\n",
       "0  Alexa, as a voice-controlled virtual assistant...   \n",
       "1         I don't have enough information to answer.   \n",
       "2  The UK government has stated that it will \"tes...   \n",
       "3  The GDPR (General Data Protection Regulation) ...   \n",
       "4  The usage of ChatGPT for Technical Help has sh...   \n",
       "5  By June 2025, trends in user interactions with...   \n",
       "6  The main conversation topics users engage with...   \n",
       "7  The quality of interactions with ChatGPT at wo...   \n",
       "8  The trends in ChatGPT message counts highlight...   \n",
       "9  Artificial intelligence (AI) has significant i...   \n",
       "\n",
       "                                           reference  faithfulness  \\\n",
       "0  Alexa is a voice-controlled virtual assistant ...      0.500000   \n",
       "1  The BBC reported that Apple's AI falsely told ...      1.000000   \n",
       "2  In the UK, Prime Minister Sir Keir Starmer has...      1.000000   \n",
       "3  The GDPR impacts the transparency and accounta...      0.833333   \n",
       "4  The usage of ChatGPT for Technical Help has sh...      0.833333   \n",
       "5  By June 2025, the share of active users with t...      1.000000   \n",
       "6  Users engage with ChatGPT primarily through co...      1.000000   \n",
       "7  The quality of interactions with ChatGPT at wo...      1.000000   \n",
       "8  The trends in ChatGPT message counts indicate ...      1.000000   \n",
       "9  The implications of artificial intelligence (A...      1.000000   \n",
       "\n",
       "   answer_relevancy  context_recall  factual_correctness(mode=f1)  \n",
       "0          0.912168        1.000000                          0.33  \n",
       "1          0.000000        1.000000                          0.00  \n",
       "2          0.000000        1.000000                          0.71  \n",
       "3          0.971650        1.000000                          0.37  \n",
       "4          0.972123        1.000000                          0.87  \n",
       "5          0.974209        0.666667                          0.56  \n",
       "6          0.954396        1.000000                          0.50  \n",
       "7          0.985840        0.500000                          0.56  \n",
       "8          0.966834        0.800000                          0.46  \n",
       "9          0.961463        1.000000                          0.90  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display Naive RAG detailed results\n",
    "print(\"ðŸ“Š Naive RAG - Detailed Results:\")\n",
    "naive_results['evaluation_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61bd01d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Naive RAG - Summary Metrics:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'faithfulness': 0.9167, 'answer_relevancy': 0.7699, 'context_recall': 0.8967, 'factual_correctness(mode=f1)': 0.5260}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display Naive RAG summary metrics\n",
    "print(\"ðŸ“ˆ Naive RAG - Summary Metrics:\")\n",
    "naive_metrics = naive_results['metrics']\n",
    "naive_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8308cf",
   "metadata": {},
   "source": [
    "## Experiment 2: Hybrid RAG Evaluation ðŸ”¬âš¡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1db6e1",
   "metadata": {},
   "source": [
    "Now let's evaluate the **Hybrid RAG SuperComponent** using the identical evaluation pipeline. This systematic approach ensures fair comparison.\n",
    "\n",
    "**ðŸ”„ Key Benefits of This Approach:**\n",
    "- **Identical Conditions**: Same pipeline, metrics, and dataset\n",
    "- **Systematic Comparison**: Eliminates evaluation bias\n",
    "- **Reproducible Results**: Consistent methodology across both systems\n",
    "\n",
    "**ðŸŽ¯ Expected Improvements:**\n",
    "Hybrid RAG typically shows better performance due to:\n",
    "- **Dense + Sparse Retrieval**: Combines semantic and keyword-based search\n",
    "- **Enhanced Context Quality**: Better retrieval often leads to better responses\n",
    "- **Improved Robustness**: Multiple retrieval methods reduce failure modes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60268ad4",
   "metadata": {},
   "source": [
    "# Evaluate Hybrid RAG SuperComponent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff5f12ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”„ Building evaluation pipeline for Hybrid RAG...\n",
      "==================================================\n",
      "âœ… Evaluation pipeline for Hybrid RAG built successfully!\n",
      "Loaded DataFrame with 10 rows from data_for_eval/synthetic_tests_advanced_branching_10.csv.\n",
      "Running RAG SuperComponent on 10 queries...\n",
      "RAG processing complete.\n",
      "Creating Ragas EvaluationDataset...\n",
      "Starting Ragas evaluation...\n",
      "RAG processing complete.\n",
      "Creating Ragas EvaluationDataset...\n",
      "Starting Ragas evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05f8d71782ae4517aae3e9bc053a5ca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ragas evaluation complete.\n",
      "Overall metrics: {'faithfulness': 0.8080, 'answer_relevancy': 0.8716, 'context_recall': 0.9467, 'factual_correctness(mode=f1)': 0.5720}\n",
      "âœ… Hybrid RAG evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Hybrid RAG SuperComponent\n",
    "\n",
    "# Create a new generator for this evaluation (components can't be shared between pipelines)\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.utils import Secret\n",
    "\n",
    "eval_generator_hybrid = OpenAIGenerator(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=Secret.from_token(os.getenv(\"OPENAI_API_KEY\"))\n",
    ")\n",
    "\n",
    "# Create evaluation SuperComponent (csv_source now passed in run method)\n",
    "hybrid_evaluation_sc = RAGEvaluationSuperComponent(\n",
    "    rag_supercomponent=hybrid_rag_sc,\n",
    "    system_name=\"Hybrid RAG\",\n",
    "    generator=eval_generator_hybrid\n",
    ")\n",
    "\n",
    "# Run evaluation with CSV source passed to run method\n",
    "hybrid_results = hybrid_evaluation_sc.run(csv_source=csv_file_path)\n",
    "\n",
    "print(f\"âœ… Hybrid RAG evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fb55e6",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f152688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Hybrid RAG - Detailed Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>factual_correctness(mode=f1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What Alexa do in AI?</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>Amazon's Alexa is a voice-controlled virtual a...</td>\n",
       "      <td>Alexa is a voice-controlled virtual assistant ...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.908875</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What happened to the UnitedHealthcare CEO Bria...</td>\n",
       "      <td>[AI will reconfigure how society and the econo...</td>\n",
       "      <td>[Why is AI controversial?\\nWhile acknowledging...</td>\n",
       "      <td>I don't have enough information to answer.</td>\n",
       "      <td>The BBC reported that Apple's AI falsely told ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the current stance of the UK governmen...</td>\n",
       "      <td>[AI will reconfigure how society and the econo...</td>\n",
       "      <td>[Are there laws governing AI?\\nSome government...</td>\n",
       "      <td>The current stance of the UK government regard...</td>\n",
       "      <td>In the UK, Prime Minister Sir Keir Starmer has...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.989034</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does the GDPR impact the transparency and ...</td>\n",
       "      <td>[- James Kurose, â€œTestimony before the House C...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nAI ethics and transparency\\nAlgori...</td>\n",
       "      <td>The GDPR (General Data Protection Regulation) ...</td>\n",
       "      <td>The GDPR impacts the transparency and accounta...</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.971650</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the trends in the usage of ChatGPT fo...</td>\n",
       "      <td>[37% of messages are work-related\\nfor users w...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n37% of messages are work-related f...</td>\n",
       "      <td>The usage of ChatGPT for Technical Help has sh...</td>\n",
       "      <td>The usage of ChatGPT for Technical Help has sh...</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.973908</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What trends in user interactions and gender re...</td>\n",
       "      <td>[However, in the first half of 2025, we see th...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nHowever, in the first half of 2025...</td>\n",
       "      <td>By June 2025, trends in user interactions and ...</td>\n",
       "      <td>By June 2025, the share of active users with t...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996117</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What are the main conversation topics users en...</td>\n",
       "      <td>[Figure 9 disaggregates four of the seven Conv...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nFigure 9 disaggregates four of the...</td>\n",
       "      <td>The main conversation topics users engage with...</td>\n",
       "      <td>Users engage with ChatGPT primarily through co...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.954396</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How does the quality of interactions with Chat...</td>\n",
       "      <td>[Overall, the majority of ChatGPT usage\\nat wo...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nOverall, the majority of ChatGPT u...</td>\n",
       "      <td>The quality of interactions with ChatGPT at wo...</td>\n",
       "      <td>The quality of interactions with ChatGPT at wo...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993349</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What are the trends in ChatGPT message counts ...</td>\n",
       "      <td>[Teams, Enterprise, Education), which we do no...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nTeams, Enterprise, Education), whi...</td>\n",
       "      <td>ChatGPT message counts have shown a significan...</td>\n",
       "      <td>The trends in ChatGPT message counts indicate ...</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.966806</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What are the implications of artificial intell...</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nWhat is AI, how does it work and w...</td>\n",
       "      <td>The implications of artificial intelligence (A...</td>\n",
       "      <td>The implications of artificial intelligence (A...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0                               What Alexa do in AI?   \n",
       "1  What happened to the UnitedHealthcare CEO Bria...   \n",
       "2  What is the current stance of the UK governmen...   \n",
       "3  How does the GDPR impact the transparency and ...   \n",
       "4  What are the trends in the usage of ChatGPT fo...   \n",
       "5  What trends in user interactions and gender re...   \n",
       "6  What are the main conversation topics users en...   \n",
       "7  How does the quality of interactions with Chat...   \n",
       "8  What are the trends in ChatGPT message counts ...   \n",
       "9  What are the implications of artificial intell...   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [What is AI, how does it work and why are some...   \n",
       "1  [AI will reconfigure how society and the econo...   \n",
       "2  [AI will reconfigure how society and the econo...   \n",
       "3  [- James Kurose, â€œTestimony before the House C...   \n",
       "4  [37% of messages are work-related\\nfor users w...   \n",
       "5  [However, in the first half of 2025, we see th...   \n",
       "6  [Figure 9 disaggregates four of the seven Conv...   \n",
       "7  [Overall, the majority of ChatGPT usage\\nat wo...   \n",
       "8  [Teams, Enterprise, Education), which we do no...   \n",
       "9  [What is AI, how does it work and why are some...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  [What is AI, how does it work and why are some...   \n",
       "1  [Why is AI controversial?\\nWhile acknowledging...   \n",
       "2  [Are there laws governing AI?\\nSome government...   \n",
       "3  [<1-hop>\\n\\nAI ethics and transparency\\nAlgori...   \n",
       "4  [<1-hop>\\n\\n37% of messages are work-related f...   \n",
       "5  [<1-hop>\\n\\nHowever, in the first half of 2025...   \n",
       "6  [<1-hop>\\n\\nFigure 9 disaggregates four of the...   \n",
       "7  [<1-hop>\\n\\nOverall, the majority of ChatGPT u...   \n",
       "8  [<1-hop>\\n\\nTeams, Enterprise, Education), whi...   \n",
       "9  [<1-hop>\\n\\nWhat is AI, how does it work and w...   \n",
       "\n",
       "                                            response  \\\n",
       "0  Amazon's Alexa is a voice-controlled virtual a...   \n",
       "1         I don't have enough information to answer.   \n",
       "2  The current stance of the UK government regard...   \n",
       "3  The GDPR (General Data Protection Regulation) ...   \n",
       "4  The usage of ChatGPT for Technical Help has sh...   \n",
       "5  By June 2025, trends in user interactions and ...   \n",
       "6  The main conversation topics users engage with...   \n",
       "7  The quality of interactions with ChatGPT at wo...   \n",
       "8  ChatGPT message counts have shown a significan...   \n",
       "9  The implications of artificial intelligence (A...   \n",
       "\n",
       "                                           reference  faithfulness  \\\n",
       "0  Alexa is a voice-controlled virtual assistant ...      0.500000   \n",
       "1  The BBC reported that Apple's AI falsely told ...      0.000000   \n",
       "2  In the UK, Prime Minister Sir Keir Starmer has...      1.000000   \n",
       "3  The GDPR impacts the transparency and accounta...      0.714286   \n",
       "4  The usage of ChatGPT for Technical Help has sh...      0.928571   \n",
       "5  By June 2025, the share of active users with t...      1.000000   \n",
       "6  Users engage with ChatGPT primarily through co...      1.000000   \n",
       "7  The quality of interactions with ChatGPT at wo...      1.000000   \n",
       "8  The trends in ChatGPT message counts indicate ...      0.937500   \n",
       "9  The implications of artificial intelligence (A...      1.000000   \n",
       "\n",
       "   answer_relevancy  context_recall  factual_correctness(mode=f1)  \n",
       "0          0.908875        1.000000                          0.37  \n",
       "1          0.000000        1.000000                          0.00  \n",
       "2          0.989034        1.000000                          0.50  \n",
       "3          0.971650        1.000000                          0.48  \n",
       "4          0.973908        1.000000                          0.96  \n",
       "5          0.996117        0.666667                          0.77  \n",
       "6          0.954396        1.000000                          0.80  \n",
       "7          0.993349        1.000000                          0.50  \n",
       "8          0.966806        0.800000                          0.57  \n",
       "9          0.961500        1.000000                          0.77  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display Hybrid RAG detailed results\n",
    "print(\"ðŸ“Š Hybrid RAG - Detailed Results:\")\n",
    "hybrid_results['evaluation_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd0607d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Hybrid RAG - Summary Metrics:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'faithfulness': 0.8080, 'answer_relevancy': 0.8716, 'context_recall': 0.9467, 'factual_correctness(mode=f1)': 0.5720}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display Hybrid RAG summary metrics\n",
    "print(\"ðŸ“ˆ Hybrid RAG - Summary Metrics:\")\n",
    "hybrid_metrics = hybrid_results['metrics']\n",
    "hybrid_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f64eeb",
   "metadata": {},
   "source": [
    "## ðŸ“Š Side-by-Side Performance Comparison\n",
    "\n",
    "Let's create a comprehensive comparison of both RAG systems to understand their relative performance across all metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "787c31f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Processing individual query scores...\n",
      "Naive RAG: 10 queries evaluated\n",
      "Hybrid RAG: 10 queries evaluated\n",
      "\n",
      "ðŸ“Š Average Scores Computed:\n",
      "Naive RAG averages: {'faithfulness': np.float64(0.9166666666666667), 'answer_relevancy': np.float64(0.769868266906707), 'context_recall': np.float64(0.8966666666666665), 'factual_correctness(mode=f1)': np.float64(0.526)}\n",
      "Hybrid RAG averages: {'faithfulness': np.float64(0.8080357142857142), 'answer_relevancy': np.float64(0.8715633776901648), 'context_recall': np.float64(0.9466666666666667), 'factual_correctness(mode=f1)': np.float64(0.5720000000000001)}\n",
      "\n",
      "================================================================================\n",
      "ðŸ” COMPREHENSIVE RAG SYSTEM COMPARISON\n",
      "================================================================================\n",
      "                      Metric  Naive RAG  Hybrid RAG  Improvement (%) Better System\n",
      "                faithfulness     0.9167      0.8080           -11.85   ðŸ† Naive RAG\n",
      "            answer_relevancy     0.7699      0.8716            13.21  ðŸ† Hybrid RAG\n",
      "              context_recall     0.8967      0.9467             5.58  ðŸ† Hybrid RAG\n",
      "factual_correctness(mode=f1)     0.5260      0.5720             8.75  ðŸ† Hybrid RAG\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ðŸ FINAL SCORECARD:\n",
      "ðŸ† Hybrid RAG wins: 3 metrics\n",
      "ðŸ† Naive RAG wins: 1 metrics\n",
      "ðŸ¤ Ties: 0 metrics\n",
      "\n",
      "ðŸŽ‰ OVERALL WINNER: Hybrid RAG SuperComponent!\n",
      "   Better performance in 3/4 metrics\n",
      "\n",
      "ðŸ“ˆ Average improvement by Hybrid RAG: 3.92%\n",
      "\n",
      "ðŸ“Š Score Consistency (Standard Deviation):\n",
      "   faithfulness:\n",
      "     Naive RAG: 0.1620\n",
      "     Hybrid RAG: 0.3287\n",
      "     More consistent: Naive RAG\n",
      "   answer_relevancy:\n",
      "     Naive RAG: 0.4062\n",
      "     Hybrid RAG: 0.3073\n",
      "     More consistent: Hybrid RAG\n",
      "   context_recall:\n",
      "     Naive RAG: 0.1808\n",
      "     Hybrid RAG: 0.1167\n",
      "     More consistent: Hybrid RAG\n",
      "   factual_correctness(mode=f1):\n",
      "     Naive RAG: 0.2664\n",
      "     Hybrid RAG: 0.2724\n",
      "     More consistent: Naive RAG\n"
     ]
    }
   ],
   "source": [
    "# --- Create Side-by-Side Performance Comparison ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Extract average metrics from both evaluations using proper RAGAS data structure methods\n",
    "# The EvaluationResult object contains individual scores for each query\n",
    "naive_scores = naive_metrics.scores\n",
    "hybrid_scores = hybrid_metrics.scores\n",
    "\n",
    "print(\"ðŸ” Processing individual query scores...\")\n",
    "print(f\"Naive RAG: {len(naive_scores)} queries evaluated\")\n",
    "print(f\"Hybrid RAG: {len(hybrid_scores)} queries evaluated\")\n",
    "\n",
    "# Compute averages for each metric using pandas for easier calculation\n",
    "naive_df = pd.DataFrame(naive_scores)\n",
    "hybrid_df = pd.DataFrame(hybrid_scores)\n",
    "\n",
    "# Calculate mean values for each metric\n",
    "naive_averages = naive_df.mean()\n",
    "hybrid_averages = hybrid_df.mean()\n",
    "\n",
    "print(\"\\nðŸ“Š Average Scores Computed:\")\n",
    "print(f\"Naive RAG averages: {dict(naive_averages)}\")\n",
    "print(f\"Hybrid RAG averages: {dict(hybrid_averages)}\")\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_data = {\n",
    "    'Metric': list(naive_averages.index),\n",
    "    'Naive RAG': list(naive_averages.values),\n",
    "    'Hybrid RAG': list(hybrid_averages.values)\n",
    "}\n",
    "\n",
    "# Calculate improvement percentages\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df['Improvement (%)'] = ((comparison_df['Hybrid RAG'] - comparison_df['Naive RAG']) / comparison_df['Naive RAG'] * 100).round(2)\n",
    "comparison_df['Better System'] = comparison_df.apply(\n",
    "    lambda row: 'ðŸ† Hybrid RAG' if row['Hybrid RAG'] > row['Naive RAG'] \n",
    "    else 'ðŸ† Naive RAG' if row['Naive RAG'] > row['Hybrid RAG'] \n",
    "    else 'ðŸ¤ Tie', axis=1\n",
    ")\n",
    "\n",
    "# Round scores for better display\n",
    "comparison_df['Naive RAG'] = comparison_df['Naive RAG'].round(4)\n",
    "comparison_df['Hybrid RAG'] = comparison_df['Hybrid RAG'].round(4)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸ” COMPREHENSIVE RAG SYSTEM COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Calculate overall winner\n",
    "hybrid_wins = sum(comparison_df['Hybrid RAG'] > comparison_df['Naive RAG'])\n",
    "naive_wins = sum(comparison_df['Naive RAG'] > comparison_df['Hybrid RAG'])\n",
    "ties = sum(comparison_df['Naive RAG'] == comparison_df['Hybrid RAG'])\n",
    "\n",
    "print(f\"\\nðŸ FINAL SCORECARD:\")\n",
    "print(f\"ðŸ† Hybrid RAG wins: {hybrid_wins} metrics\")\n",
    "print(f\"ðŸ† Naive RAG wins: {naive_wins} metrics\") \n",
    "print(f\"ðŸ¤ Ties: {ties} metrics\")\n",
    "\n",
    "if hybrid_wins > naive_wins:\n",
    "    print(f\"\\nðŸŽ‰ OVERALL WINNER: Hybrid RAG SuperComponent!\")\n",
    "    print(f\"   Better performance in {hybrid_wins}/{len(comparison_df)} metrics\")\n",
    "elif naive_wins > hybrid_wins:\n",
    "    print(f\"\\nðŸŽ‰ OVERALL WINNER: Naive RAG SuperComponent!\")  \n",
    "    print(f\"   Better performance in {naive_wins}/{len(comparison_df)} metrics\")\n",
    "else:\n",
    "    print(f\"\\nðŸ¤ RESULT: It's a tie between both systems!\")\n",
    "    \n",
    "avg_improvement = comparison_df['Improvement (%)'].mean()\n",
    "print(f\"\\nðŸ“ˆ Average improvement by Hybrid RAG: {avg_improvement:.2f}%\")\n",
    "\n",
    "# Show standard deviations to understand score consistency\n",
    "naive_stds = naive_df.std()\n",
    "hybrid_stds = hybrid_df.std()\n",
    "\n",
    "print(f\"\\nðŸ“Š Score Consistency (Standard Deviation):\")\n",
    "for metric in naive_averages.index:\n",
    "    print(f\"   {metric}:\")\n",
    "    print(f\"     Naive RAG: {naive_stds[metric]:.4f}\")\n",
    "    print(f\"     Hybrid RAG: {hybrid_stds[metric]:.4f}\")\n",
    "    stability_winner = \"Hybrid RAG\" if hybrid_stds[metric] < naive_stds[metric] else \"Naive RAG\"\n",
    "    print(f\"     More consistent: {stability_winner}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47be3490",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-with-haystack-ch6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
