{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75f744c9",
   "metadata": {},
   "source": [
    "ðŸ”§  **Setup Required**: Before running this notebook, please follow the [setup instructions](../README.md#setup-instructions) to configure your environment and API keys. **You will need to ensure you've executed the Indexing pipeline before completing this exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ad4fc5",
   "metadata": {},
   "source": [
    "# Systematic RAG Evaluation: Naive vs Hybrid Comparison\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates a **systematic evaluation workflow** for comparing two RAG (Retrieval-Augmented Generation) approaches using **Haystack custom components**. We'll create a reproducible pipeline to:\n",
    "\n",
    "1. **Load evaluation datasets** from CSV files\n",
    "2. **Process queries** through both Naive and Hybrid RAG SuperComponents \n",
    "3. **Generate comprehensive metrics** using the RAGAS framework\n",
    "4. **Compare performance** systematically\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand how to:\n",
    "- Create reusable evaluation components for RAG systems\n",
    "- Build scalable pipelines for systematic RAG comparison\n",
    "- Interpret RAGAS metrics in comparative context\n",
    "- Make data-driven decisions between RAG approaches\n",
    "\n",
    "## Evaluation Pipeline\n",
    "\n",
    "Our pipeline consists of three main components:\n",
    "\n",
    "```\n",
    "CSV Data â†’ RAGDataAugmenter â†’ RagasEvaluation â†’ Metrics & Results\n",
    "```\n",
    "\n",
    "**Key Benefits:**\n",
    "- **Systematic**: Same evaluation conditions for both RAG systems\n",
    "- **Reproducible**: Consistent evaluation across experiments\n",
    "- **Scalable**: Easy to add new RAG implementations\n",
    "- **Comprehensive**: Multiple metrics provide complete assessment\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4640ae5",
   "metadata": {},
   "source": [
    "## Component 1: CSV Data Loader\n",
    "\n",
    "The **CSVReaderComponent** serves as the entry point for our evaluation pipeline. It handles loading synthetic evaluation datasets and ensures data quality before processing.\n",
    "\n",
    "**Key Features:**\n",
    "- **Robust Error Handling**: Validates file existence and data integrity\n",
    "- **Pandas Integration**: Returns data as DataFrame for easy manipulation\n",
    "- **Pipeline Compatible**: Designed to work seamlessly with Haystack pipelines\n",
    "\n",
    "**Input:** File path to CSV containing evaluation queries and ground truth\n",
    "**Output:** Pandas DataFrame ready for RAG processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a83513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from haystack import component\n",
    "from typing import Optional, Dict, Any, Union\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@component\n",
    "class CSVReaderComponent:\n",
    "    \"\"\"Reads a CSV file into a Pandas DataFrame.\"\"\"\n",
    "\n",
    "    @component.output_types(data_frame=pd.DataFrame)\n",
    "    def run(self, source: Union[str, Path]):\n",
    "        \"\"\"\n",
    "        Reads the CSV file from the first source in the list.\n",
    "        \n",
    "        Args:\n",
    "            sources: List of file paths to CSV files. Only the first file will be processed.\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary containing the loaded DataFrame under 'data_frame' key.\n",
    "            \n",
    "        Raises:\n",
    "            FileNotFoundError: If the file doesn't exist or can't be read.\n",
    "            ValueError: If the DataFrame is empty after loading.\n",
    "        \"\"\"\n",
    "        if not source:\n",
    "            raise ValueError(\"No sources provided\")\n",
    "            \n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(source)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"File not found at {source}\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error reading CSV file {source}: {str(e)}\")\n",
    "\n",
    "        # Check if DataFrame is empty using proper pandas method\n",
    "        if df.empty:\n",
    "            raise ValueError(f\"DataFrame is empty after loading from {source}\")\n",
    "\n",
    "        return {\"data_frame\": df}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1861dfb2",
   "metadata": {},
   "source": [
    "## Component 2: Async RAG Data Augmentation\n",
    "\n",
    "The **AsyncRAGDataAugmenterComponent** is the core of our evaluation workflow. It processes queries through a RAG SuperComponent with concurrent execution for optimal performance.\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "1. **Concurrent Processing**: Processes multiple queries in parallel batches\n",
    "2. **SuperComponent Flexibility**: Accepts any pre-configured RAG SuperComponent (Naive, Hybrid, or custom)\n",
    "3. **Configurable Batch Size**: Control concurrency based on API rate limits\n",
    "4. **Progress Tracking**: Real-time visibility into processing status\n",
    "5. **Error Handling**: Gracefully handles failures without stopping the entire evaluation\n",
    "\n",
    "**Performance Benefits:**\n",
    "- **Speed**: Up to NÃ— faster than sequential processing (where N = batch_size)\n",
    "- **Scalability**: Efficiently handles large evaluation datasets\n",
    "- **Resource Optimization**: Maximizes API call efficiency\n",
    "\n",
    "**Pipeline Integration:**\n",
    "- **Input**: DataFrame with queries from CSVReaderComponent  \n",
    "- **Process**: Runs queries through RAG SuperComponent in concurrent batches\n",
    "- **Output**: Augmented DataFrame with responses and retrieved contexts\n",
    "\n",
    "**Why Async?**\n",
    "- **Faster Iteration**: Quickly evaluate large datasets\n",
    "- **Better Resource Utilization**: Maximize throughput without overwhelming APIs\n",
    "- **Production Ready**: Scalable approach for continuous evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3771b673",
   "metadata": {},
   "source": [
    "## Implementation: Concurrent Query Processing\n",
    "\n",
    "Below is the implementation of the async component that enables concurrent query processing.\n",
    "\n",
    "### âš ï¸ Important Thread-Safety Considerations\n",
    "\n",
    "**RAG SuperComponent Sharing:**\n",
    "- The `AsyncRAGDataAugmenterComponent` processes queries sequentially within each RAG system\n",
    "- **Haystack SuperComponents are NOT guaranteed to be thread-safe**\n",
    "- `batch_size` is fixed at 1 to ensure safe operation with shared component state\n",
    "\n",
    "**Current Safety Approach:**\n",
    "- **Fixed `batch_size=1`**: Processes queries sequentially within each system to avoid conflicts\n",
    "- **Concurrent System Evaluation**: Multiple RAG systems can still be evaluated in parallel\n",
    "- **Trade-off**: Sacrifices within-system concurrency for stability and reliability\n",
    "\n",
    "**Why Sequential Processing:**\n",
    "- Embedding models in RAG SuperComponents may share state\n",
    "- Concurrent access to the same SuperComponent instance can cause race conditions\n",
    "- Sequential processing within each system ensures consistent, reliable results\n",
    "\n",
    "**Performance:**\n",
    "- **2Ã— speedup** from evaluating multiple RAG systems concurrently\n",
    "- Safe and stable for production use\n",
    "- No risk of embedding model conflicts or race conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2b1329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from typing import List\n",
    "from haystack import SuperComponent\n",
    "import traceback\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@component\n",
    "class AsyncRAGDataAugmenterComponent:\n",
    "    \"\"\"\n",
    "    Async version of RAGDataAugmenterComponent for concurrent system evaluation.\n",
    "    \n",
    "    Key Features:\n",
    "    - Processes queries sequentially within each RAG system (thread-safe)\n",
    "    - Enables concurrent evaluation of multiple RAG systems\n",
    "    - Progress tracking for long-running evaluations\n",
    "    - Robust error handling\n",
    "    \n",
    "    Thread-Safety Design:\n",
    "    - Fixed batch_size=1 to avoid concurrent access to shared component state\n",
    "    - Haystack SuperComponents are NOT guaranteed to be thread-safe\n",
    "    - Sequential processing ensures no race conditions or model conflicts\n",
    "    - Multiple systems can still be evaluated concurrently (2Ã— speedup)\n",
    "    \n",
    "    Component Design:\n",
    "    - run() method is synchronous (required by Haystack component interface)\n",
    "    - Uses asyncio.run() internally to execute async processing\n",
    "    - AsyncPipeline orchestrates concurrent execution across components\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rag_supercomponent: SuperComponent):\n",
    "        \"\"\"\n",
    "        Initialize the Async RAG Data Augmenter.\n",
    "        \n",
    "        Args:\n",
    "            rag_supercomponent: Pre-initialized RAG SuperComponent\n",
    "        \n",
    "        Note:\n",
    "            batch_size is fixed at 1 to ensure thread-safe operation.\n",
    "            This prevents race conditions when accessing the RAG SuperComponent,\n",
    "            which may contain shared embedding models or other stateful components.\n",
    "        \"\"\"\n",
    "        self.rag_supercomponent = rag_supercomponent\n",
    "        self.batch_size = 1  # Fixed at 1 for thread safety\n",
    "        self.output_names = [\"augmented_data_frame\"]\n",
    "\n",
    "    async def _process_single_query(self, query: str, index: int) -> tuple:\n",
    "        \"\"\"\n",
    "        Process a single query through the RAG SuperComponent.\n",
    "        \n",
    "        Thread Safety:\n",
    "        - Uses asyncio.to_thread() to run synchronous RAG code in thread pool\n",
    "        - Sequential processing (batch_size=1) prevents concurrent access\n",
    "        \n",
    "        Args:\n",
    "            query: The query string to process\n",
    "            index: The query index for tracking\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (index, answer, contexts)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Run the RAG SuperComponent in a thread pool\n",
    "            # asyncio.to_thread automatically handles thread pool execution\n",
    "            rag_output = await asyncio.to_thread(\n",
    "                self.rag_supercomponent.run, \n",
    "                query=query\n",
    "            )\n",
    "            \n",
    "            # Extract answer and contexts\n",
    "            answer = rag_output.get('replies', [''])[0]\n",
    "            retrieved_docs = rag_output.get('documents', [])\n",
    "            retrieved_contexts = [doc.content for doc in retrieved_docs]\n",
    "            \n",
    "            return (index, answer, retrieved_contexts)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing query {index} ('{query[:50]}...'): {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "            return (index, \"\", [])\n",
    "\n",
    "    async def _process_batch(self, queries_with_indices: List[tuple]) -> List[tuple]:\n",
    "        \"\"\"\n",
    "        Process a batch of queries.\n",
    "        \n",
    "        With batch_size=1, this processes one query at a time (thread-safe).\n",
    "        \n",
    "        Args:\n",
    "            queries_with_indices: List of (index, query) tuples\n",
    "            \n",
    "        Returns:\n",
    "            List of (index, answer, contexts) tuples\n",
    "        \"\"\"\n",
    "        tasks = [\n",
    "            self._process_single_query(query, idx) \n",
    "            for idx, query in queries_with_indices\n",
    "        ]\n",
    "        return await asyncio.gather(*tasks)\n",
    "\n",
    "    async def _async_process(self, data_frame: pd.DataFrame) -> dict:\n",
    "        \"\"\"\n",
    "        Internal async processing logic.\n",
    "        \n",
    "        This method contains the actual async processing logic and is called\n",
    "        by the synchronous run() method via asyncio.run().\n",
    "        \n",
    "        Args:\n",
    "            data_frame: DataFrame with 'user_input' column containing queries\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary with augmented DataFrame containing responses and contexts\n",
    "        \"\"\"\n",
    "        total_queries = len(data_frame)\n",
    "        \n",
    "        # Prepare queries with their indices\n",
    "        queries_with_indices = list(enumerate(data_frame[\"user_input\"].tolist()))\n",
    "        \n",
    "        # Initialize results storage\n",
    "        results = [None] * total_queries\n",
    "        \n",
    "        # Process in batches (batch_size=1, so one at a time)\n",
    "        for batch_start in range(0, total_queries, self.batch_size):\n",
    "            batch_end = min(batch_start + self.batch_size, total_queries)\n",
    "            batch = queries_with_indices[batch_start:batch_end]\n",
    "            \n",
    "            # Process batch (single query with batch_size=1)\n",
    "            batch_results = await self._process_batch(batch)\n",
    "            \n",
    "            # Store results in correct order\n",
    "            for idx, answer, contexts in batch_results:\n",
    "                results[idx] = (answer, contexts)\n",
    "        \n",
    "        # Extract answers and contexts from results\n",
    "        answers = [r[0] for r in results]\n",
    "        contexts = [r[1] for r in results]\n",
    "        \n",
    "        # Create a copy to avoid modifying the original DataFrame\n",
    "        # This ensures the component doesn't have side effects on input data\n",
    "        data_frame_copy = data_frame.copy()\n",
    "        data_frame_copy['response'] = answers\n",
    "        data_frame_copy['retrieved_contexts'] = contexts\n",
    "        \n",
    "        logger.info(f\"âœ“ Async RAG processing complete for {total_queries} queries!\")\n",
    "        return {\"augmented_data_frame\": data_frame_copy}\n",
    "\n",
    "        return {\"augmented_data_frame\": data_frame_copy}\n",
    "\n",
    "    @component.output_types(augmented_data_frame=pd.DataFrame)\n",
    "    def run(self, data_frame: pd.DataFrame) -> dict:\n",
    "        \"\"\"\n",
    "        Synchronous run method required by Haystack component interface.\n",
    "        \n",
    "        This method is synchronous and always returns a dictionary directly,\n",
    "        maintaining consistency with the Haystack component contract.\n",
    "        AsyncPipeline handles the orchestration of concurrent execution.\n",
    "        \n",
    "        Data Safety:\n",
    "        - Creates a copy of input DataFrame to avoid mutation\n",
    "        - Results stored in new columns on the copied DataFrame\n",
    "        - Original data_frame parameter remains unchanged\n",
    "        \n",
    "        Thread Safety:\n",
    "        - Sequential processing (batch_size=1) ensures thread safety\n",
    "        - No concurrent access to the RAG SuperComponent instance\n",
    "        - Safe for use with shared embedding models\n",
    "        \n",
    "        Args:\n",
    "            data_frame: DataFrame with 'user_input' column containing queries\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary with augmented DataFrame containing responses and contexts\n",
    "        \"\"\"\n",
    "        # Use asyncio.run() to execute async processing within synchronous method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272c9cff",
   "metadata": {},
   "source": [
    "## Component 3: RAGAS Evaluation Engine\n",
    "\n",
    "The **RagasEvaluationComponent** integrates the RAGAS framework into our Haystack pipeline, providing systematic evaluation metrics for RAG systems.\n",
    "\n",
    "**Core Evaluation Metrics:**\n",
    "\n",
    "| Metric | Purpose | What It Measures |\n",
    "|--------|---------|------------------|\n",
    "| **Faithfulness** | Response Quality | Factual consistency with retrieved context |\n",
    "| **ResponseRelevancy** | Relevance | How well responses answer the questions |\n",
    "| **LLMContextRecall** | Retrieval Quality | How well retrieval captures relevant information |\n",
    "| **FactualCorrectness** | Accuracy | Correctness of factual claims in responses |\n",
    "\n",
    "**Technical Implementation:**\n",
    "- **Focused Metrics**: Core metrics for reliable comparison\n",
    "- **LLM Integration**: Uses OpenAI GPT models for evaluation judgments  \n",
    "- **Data Format Handling**: Automatically formats data for RAGAS requirements\n",
    "- **Comprehensive Output**: Returns both aggregated metrics and detailed per-query results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b842a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import EvaluationDataset, evaluate\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, ResponseRelevancy\n",
    "from ragas.llms import HaystackLLMWrapper\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@component\n",
    "class RagasEvaluationComponent:\n",
    "    \"\"\"\n",
    "    Integrates the RAGAS framework into Haystack pipeline for systematic evaluation.\n",
    "    \n",
    "    This component provides systematic evaluation metrics for RAG systems using\n",
    "    the RAGAS framework with focus on core metrics for reliable comparison.\n",
    "    \n",
    "    Core Evaluation Metrics:\n",
    "    - Faithfulness: Factual consistency with retrieved context\n",
    "    - ResponseRelevancy: How well responses answer the questions  \n",
    "    - LLMContextRecall: How well retrieval captures relevant information\n",
    "    - FactualCorrectness: Correctness of factual claims in responses\n",
    "    \n",
    "    Technical Features:\n",
    "    - Focused Metrics: Core metrics for reliable comparison\n",
    "    - LLM Integration: Uses provided generator for evaluation judgments\n",
    "    - Data Format Handling: Automatically formats data for RAGAS requirements\n",
    "    - Comprehensive Output: Returns both aggregated metrics and detailed per-query results\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 generator: Any,\n",
    "                 metrics: Optional[List[Any]] = None):\n",
    "        \"\"\"\n",
    "        Initialize the RAGAS Evaluation Component.\n",
    "        \n",
    "        Args:\n",
    "            generator: LLM generator instance (e.g., OpenAIGenerator or OllamaGenerator).\n",
    "            metrics: List of RAGAS metrics to evaluate (defaults to core metrics)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Default to core metrics for systematic comparison\n",
    "        if metrics is None:\n",
    "            self.metrics = [\n",
    "                Faithfulness(), \n",
    "                ResponseRelevancy(),\n",
    "                LLMContextRecall(),\n",
    "                FactualCorrectness()\n",
    "            ]\n",
    "        else:\n",
    "            self.metrics = metrics\n",
    "        \n",
    "        # Configure RAGAS LLM for evaluation\n",
    "        self.ragas_llm = HaystackLLMWrapper(generator)\n",
    "\n",
    "    @component.output_types(metrics=Dict[str, float], evaluation_df=pd.DataFrame)\n",
    "    def run(self, augmented_data_frame: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Run RAGAS evaluation on augmented dataset.\n",
    "        \n",
    "        Args:\n",
    "            augmented_data_frame: DataFrame with RAG responses and retrieved contexts\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary containing evaluation metrics and detailed results DataFrame\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. Map columns to Ragas requirements\n",
    "        ragas_data = pd.DataFrame({\n",
    "            'user_input': augmented_data_frame['user_input'],\n",
    "            'response': augmented_data_frame['response'], \n",
    "            'retrieved_contexts': augmented_data_frame['retrieved_contexts'],\n",
    "            'reference': augmented_data_frame['reference'],\n",
    "            'reference_contexts': augmented_data_frame['reference_contexts'].apply(eval)\n",
    "        })\n",
    "\n",
    "        # 2. Create EvaluationDataset\n",
    "        dataset = EvaluationDataset.from_pandas(ragas_data)\n",
    "        \n",
    "        # 3. Run Ragas Evaluation\n",
    "        results = evaluate(\n",
    "            dataset=dataset,\n",
    "            metrics=self.metrics,\n",
    "            llm=self.ragas_llm\n",
    "        )\n",
    "        \n",
    "        results_df = results.to_pandas()\n",
    "        \n",
    "        return {\"metrics\": results, \"evaluation_df\": results_df}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b0aa01",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Systematic RAG Comparison: Naive vs Hybrid\n",
    "\n",
    "Now we'll systematically evaluate both RAG SuperComponents using the same evaluation pipeline. This ensures fair comparison with identical evaluation conditions.\n",
    "\n",
    "## Comparison Strategy\n",
    "\n",
    "Our approach enables **systematic comparison**:\n",
    "\n",
    "1. **Same Dataset**: Both systems evaluated on identical test queries\n",
    "2. **Same Metrics**: Consistent evaluation criteria across both approaches\n",
    "3. **Same Pipeline**: Identical processing workflow eliminates bias\n",
    "4. **Reproducible Results**: Pipeline ensures consistent evaluation conditions\n",
    "\n",
    "## Dataset Information\n",
    "\n",
    "We'll use a synthetic evaluation dataset:\n",
    "- **`synthetic_tests_advanced_branching_10.csv`**: Focused dataset for comparison\n",
    "\n",
    "**Dataset Structure:**\n",
    "- `user_input`: Questions to ask the RAG system\n",
    "- `reference`: Ground truth answers for comparison\n",
    "- `reference_contexts`: Expected retrieved contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0384e71c",
   "metadata": {},
   "source": [
    "### Setup: Initialize Both RAG SuperComponents\n",
    "\n",
    "First, we'll initialize both RAG SuperComponents with consistent parameters for fair comparison.\n",
    "\n",
    "**Configuration:**\n",
    "- **Same base parameters**: Both systems use identical core settings\n",
    "- **Document store**: Shared Elasticsearch document store\n",
    "- **Models**: Consistent LLM and embedding models for both systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a23faff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both RAG SuperComponents initialized successfully!\n",
      "Naive RAG: NaiveRAGSuperComponent\n",
      "Hybrid RAG: HybridRAGSuperComponent\n"
     ]
    }
   ],
   "source": [
    "# --- Setup Environment & Dependencies ---\n",
    "from scripts.rag.hybridrag import HybridRAGSuperComponent\n",
    "from scripts.rag.naiverag import NaiveRAGSuperComponent\n",
    "from haystack_integrations.document_stores.elasticsearch import ElasticsearchDocumentStore\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Initialize document store\n",
    "document_store = ElasticsearchDocumentStore(hosts=\"http://localhost:9200\")\n",
    "\n",
    "# Create both RAG SuperComponents with base parameters for fair comparison\n",
    "naive_rag_sc = NaiveRAGSuperComponent(\n",
    "    document_store=document_store\n",
    ")\n",
    "\n",
    "hybrid_rag_sc = HybridRAGSuperComponent(\n",
    "    document_store=document_store\n",
    ")\n",
    "\n",
    "print(\"Both RAG SuperComponents initialized successfully!\")\n",
    "print(f\"Naive RAG: {naive_rag_sc.__class__.__name__}\")\n",
    "print(f\"Hybrid RAG: {hybrid_rag_sc.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf21e21",
   "metadata": {},
   "source": [
    "## Evaluation Pipeline: Using Haystack AsyncPipeline\n",
    "\n",
    "Now we'll create an evaluation pipeline using Haystack's `AsyncPipeline` for proper concurrent execution. This provides a cleaner, more maintainable approach than manually coordinating async components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa89ea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import AsyncPipeline\n",
    "import time\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def create_evaluation_pipeline(\n",
    "    rag_supercomponent,\n",
    "    generator: Any\n",
    ") -> AsyncPipeline:\n",
    "    \"\"\"\n",
    "    Create an evaluation pipeline using Haystack's AsyncPipeline.\n",
    "    \n",
    "    AsyncPipeline handles concurrent execution of components automatically.\n",
    "    Components themselves use synchronous run() methods, and AsyncPipeline\n",
    "    orchestrates their async execution.\n",
    "    \n",
    "    Args:\n",
    "        rag_supercomponent: The RAG system to evaluate\n",
    "        generator: LLM generator for RAGAS evaluation\n",
    "        \n",
    "    Returns:\n",
    "        AsyncPipeline: Configured evaluation pipeline\n",
    "    \n",
    "    Note:\n",
    "        The AsyncRAGDataAugmenterComponent uses batch_size=1 internally\n",
    "        for thread-safe operation. Concurrent evaluation happens at the\n",
    "        system level (multiple RAG systems evaluated simultaneously).\n",
    "    \"\"\"\n",
    "    pipeline = AsyncPipeline()\n",
    "    \n",
    "    # Add components to pipeline\n",
    "    pipeline.add_component(\"reader\", CSVReaderComponent())\n",
    "    pipeline.add_component(\n",
    "        \"augmenter\",\n",
    "        AsyncRAGDataAugmenterComponent(\n",
    "            rag_supercomponent=rag_supercomponent\n",
    "        )\n",
    "    )\n",
    "    pipeline.add_component(\"evaluator\", RagasEvaluationComponent(generator=generator))\n",
    "    \n",
    "    # Connect components\n",
    "    pipeline.connect(\"reader.data_frame\", \"augmenter.data_frame\")\n",
    "    pipeline.connect(\"augmenter.augmented_data_frame\", \"evaluator.augmented_data_frame\")\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "\n",
    "async def evaluate_rag_system_async(\n",
    "    rag_supercomponent,\n",
    "    system_name: str,\n",
    "    csv_file_path: str,\n",
    "    generator: Any\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Asynchronously evaluate a single RAG system using AsyncPipeline.\n",
    "    \n",
    "    Args:\n",
    "        rag_supercomponent: The RAG system to evaluate\n",
    "        system_name: Name for logging\n",
    "        csv_file_path: Path to evaluation dataset\n",
    "        generator: LLM generator for RAGAS evaluation\n",
    "        \n",
    "    Returns:\n",
    "        dict: Evaluation results with metrics and detailed DataFrame\n",
    "    \n",
    "    Note:\n",
    "        Queries are processed sequentially within each system (batch_size=1)\n",
    "        for thread safety. Multiple systems can be evaluated concurrently.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Evaluating {system_name}...\")\n",
    "    \n",
    "    # Create evaluation pipeline\n",
    "    eval_pipeline = create_evaluation_pipeline(\n",
    "        rag_supercomponent=rag_supercomponent,\n",
    "        generator=generator\n",
    "    )\n",
    "    \n",
    "    # Run pipeline asynchronously\n",
    "    results = await eval_pipeline.run_async(data={\"reader\": {\"source\": csv_file_path}})\n",
    "    \n",
    "    return {\n",
    "        \"system_name\": system_name,\n",
    "        \"metrics\": results[\"evaluator\"][\"metrics\"],\n",
    "        \"evaluation_df\": results[\"evaluator\"][\"evaluation_df\"]\n",
    "    }\n",
    "\n",
    "\n",
    "async def evaluate_multiple_rag_systems_async(\n",
    "    rag_systems: List[tuple],\n",
    "    csv_file_path: str\n",
    ") -> List[dict]:\n",
    "    \"\"\"\n",
    "    Evaluate multiple RAG systems concurrently using AsyncPipeline.\n",
    "    \n",
    "    This function enables concurrent evaluation of multiple RAG systems,\n",
    "    providing a ~2Ã— speedup compared to sequential evaluation.\n",
    "    \n",
    "    Args:\n",
    "        rag_systems: List of (rag_supercomponent, system_name, generator) tuples\n",
    "        csv_file_path: Path to evaluation dataset\n",
    "        \n",
    "    Returns:\n",
    "        List of evaluation results for each system\n",
    "    \n",
    "    Performance:\n",
    "        - Queries within each system: Sequential (batch_size=1, thread-safe)\n",
    "        - Multiple systems: Concurrent (2Ã— speedup for 2 systems)\n",
    "        - Total speedup: ~2Ã— for evaluating 2 systems simultaneously\n",
    "    \"\"\"\n",
    "    logger.info(f\"\\n{'='*80}\")\n",
    "    logger.info(f\"ðŸš€ CONCURRENT EVALUATION OF {len(rag_systems)} RAG SYSTEMS\")\n",
    "    logger.info(f\"{'='*80}\")\n",
    "    logger.info(f\"Starting concurrent evaluation of {len(rag_systems)} RAG systems...\")\n",
    "    \n",
    "    # Create evaluation tasks for each system\n",
    "    tasks = [\n",
    "        evaluate_rag_system_async(\n",
    "            rag_supercomponent=rag_sc,\n",
    "            system_name=name,\n",
    "            csv_file_path=csv_file_path,\n",
    "            generator=generator\n",
    "        )\n",
    "        for rag_sc, name, generator in rag_systems\n",
    "    ]\n",
    "    \n",
    "    # Run all evaluations concurrently\n",
    "    start_time = time.time()\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    logger.info(f\"Evaluation complete in {elapsed:.1f}s\")\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7618d9c",
   "metadata": {},
   "source": [
    "## Run Concurrent Evaluation\n",
    "\n",
    "Now let's evaluate both Naive and Hybrid RAG systems **simultaneously** using concurrent evaluation.\n",
    "\n",
    "**Performance Benefits:**\n",
    "- **2Ã— Faster**: Both systems evaluated at the same time\n",
    "- **Identical Conditions**: Both evaluations start simultaneously\n",
    "- **Thread-Safe**: Sequential processing within each system prevents race conditions\n",
    "- **Efficient Resource Use**: Maximizes computational efficiency\n",
    "\n",
    "**Architecture:**\n",
    "- **Within System**: Queries processed sequentially (batch_size=1)\n",
    "- **Across Systems**: Multiple RAG systems evaluated concurrently\n",
    "- **Result**: ~2Ã— speedup with guaranteed thread safety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4629f990",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 13:10:43,263 - __main__ - INFO - \n",
      "================================================================================\n",
      "2025-12-08 13:10:43,264 - __main__ - INFO - ðŸš€ CONCURRENT EVALUATION OF 2 RAG SYSTEMS\n",
      "2025-12-08 13:10:43,264 - __main__ - INFO - ================================================================================\n",
      "2025-12-08 13:10:43,264 - __main__ - INFO - Dataset: data_for_eval/synthetic_tests_advanced_branching_10.csv\n",
      "2025-12-08 13:10:43,264 - __main__ - INFO - Processing: Sequential within each system (thread-safe)\n",
      "2025-12-08 13:10:43,264 - __main__ - INFO - Concurrency: 2 systems evaluated in parallel\n",
      "2025-12-08 13:10:43,265 - __main__ - INFO - Systems: ['Naive RAG', 'Hybrid RAG']\n",
      "2025-12-08 13:10:43,265 - __main__ - INFO - ================================================================================\n",
      "\n",
      "2025-12-08 13:10:43,265 - __main__ - INFO - \n",
      "================================================================================\n",
      "2025-12-08 13:10:43,265 - __main__ - INFO - Starting async evaluation of Naive RAG\n",
      "2025-12-08 13:10:43,265 - __main__ - INFO - ================================================================================\n",
      "\n",
      "2025-12-08 13:10:43,264 - __main__ - INFO - ðŸš€ CONCURRENT EVALUATION OF 2 RAG SYSTEMS\n",
      "2025-12-08 13:10:43,264 - __main__ - INFO - ================================================================================\n",
      "2025-12-08 13:10:43,264 - __main__ - INFO - Dataset: data_for_eval/synthetic_tests_advanced_branching_10.csv\n",
      "2025-12-08 13:10:43,264 - __main__ - INFO - Processing: Sequential within each system (thread-safe)\n",
      "2025-12-08 13:10:43,264 - __main__ - INFO - Concurrency: 2 systems evaluated in parallel\n",
      "2025-12-08 13:10:43,265 - __main__ - INFO - Systems: ['Naive RAG', 'Hybrid RAG']\n",
      "2025-12-08 13:10:43,265 - __main__ - INFO - ================================================================================\n",
      "\n",
      "2025-12-08 13:10:43,265 - __main__ - INFO - \n",
      "================================================================================\n",
      "2025-12-08 13:10:43,265 - __main__ - INFO - Starting async evaluation of Naive RAG\n",
      "2025-12-08 13:10:43,265 - __main__ - INFO - ================================================================================\n",
      "\n",
      "2025-12-08 13:10:43,351 - __main__ - INFO - \n",
      "================================================================================\n",
      "2025-12-08 13:10:43,351 - __main__ - INFO - Starting async evaluation of Hybrid RAG\n",
      "2025-12-08 13:10:43,352 - __main__ - INFO - ================================================================================\n",
      "\n",
      "2025-12-08 13:10:43,352 - haystack.core.pipeline.async_pipeline - INFO - Running component reader\n",
      "2025-12-08 13:10:43,353 - haystack.core.pipeline.async_pipeline - INFO - Running component reader\n",
      "2025-12-08 13:10:43,357 - __main__ - INFO - Loaded DataFrame with 10 rows from data_for_eval/synthetic_tests_advanced_branching_10.csv.\n",
      "2025-12-08 13:10:43,360 - __main__ - INFO - Loaded DataFrame with 10 rows from data_for_eval/synthetic_tests_advanced_branching_10.csv.\n",
      "2025-12-08 13:10:43,351 - __main__ - INFO - \n",
      "================================================================================\n",
      "2025-12-08 13:10:43,351 - __main__ - INFO - Starting async evaluation of Hybrid RAG\n",
      "2025-12-08 13:10:43,352 - __main__ - INFO - ================================================================================\n",
      "\n",
      "2025-12-08 13:10:43,352 - haystack.core.pipeline.async_pipeline - INFO - Running component reader\n",
      "2025-12-08 13:10:43,353 - haystack.core.pipeline.async_pipeline - INFO - Running component reader\n",
      "2025-12-08 13:10:43,357 - __main__ - INFO - Loaded DataFrame with 10 rows from data_for_eval/synthetic_tests_advanced_branching_10.csv.\n",
      "2025-12-08 13:10:43,360 - __main__ - INFO - Loaded DataFrame with 10 rows from data_for_eval/synthetic_tests_advanced_branching_10.csv.\n",
      "2025-12-08 13:10:43,363 - haystack.core.pipeline.async_pipeline - INFO - Running component augmenter\n",
      "2025-12-08 13:10:43,366 - haystack.core.pipeline.async_pipeline - INFO - Running component augmenter\n",
      "2025-12-08 13:10:43,368 - __main__ - INFO - Running Async RAG on 10 queries (sequential processing)...\n",
      "2025-12-08 13:10:43,368 - __main__ - INFO - Running Async RAG on 10 queries (sequential processing)...\n",
      "2025-12-08 13:10:43,369 - __main__ - INFO - Processing query 1 of 10...\n",
      "2025-12-08 13:10:43,369 - __main__ - INFO - Processing query 1 of 10...\n",
      "2025-12-08 13:10:43,371 - haystack.core.pipeline.pipeline - INFO - Running component text_embedder\n",
      "2025-12-08 13:10:43,372 - haystack.core.pipeline.base - INFO - Warming up component ranker...\n",
      "2025-12-08 13:10:43,363 - haystack.core.pipeline.async_pipeline - INFO - Running component augmenter\n",
      "2025-12-08 13:10:43,366 - haystack.core.pipeline.async_pipeline - INFO - Running component augmenter\n",
      "2025-12-08 13:10:43,368 - __main__ - INFO - Running Async RAG on 10 queries (sequential processing)...\n",
      "2025-12-08 13:10:43,368 - __main__ - INFO - Running Async RAG on 10 queries (sequential processing)...\n",
      "2025-12-08 13:10:43,369 - __main__ - INFO - Processing query 1 of 10...\n",
      "2025-12-08 13:10:43,369 - __main__ - INFO - Processing query 1 of 10...\n",
      "2025-12-08 13:10:43,371 - haystack.core.pipeline.pipeline - INFO - Running component text_embedder\n",
      "2025-12-08 13:10:43,372 - haystack.core.pipeline.base - INFO - Warming up component ranker...\n",
      "2025-12-08 13:10:44,228 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:10:44,236 - haystack.core.pipeline.pipeline - INFO - Running component retriever\n",
      "2025-12-08 13:10:44,228 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:10:44,236 - haystack.core.pipeline.pipeline - INFO - Running component retriever\n",
      "2025-12-08 13:10:44,244 - elastic_transport.transport - INFO - GET http://localhost:9200/ [status:200 duration:0.007s]\n",
      "2025-12-08 13:10:44,249 - elastic_transport.transport - INFO - HEAD http://localhost:9200/default [status:200 duration:0.005s]\n",
      "2025-12-08 13:10:44,244 - elastic_transport.transport - INFO - GET http://localhost:9200/ [status:200 duration:0.007s]\n",
      "2025-12-08 13:10:44,249 - elastic_transport.transport - INFO - HEAD http://localhost:9200/default [status:200 duration:0.005s]\n",
      "2025-12-08 13:10:44,268 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.018s]\n",
      "2025-12-08 13:10:44,270 - haystack.core.pipeline.pipeline - INFO - Running component prompt_builder\n",
      "2025-12-08 13:10:44,270 - haystack.core.pipeline.pipeline - INFO - Running component llm\n",
      "2025-12-08 13:10:44,268 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.018s]\n",
      "2025-12-08 13:10:44,270 - haystack.core.pipeline.pipeline - INFO - Running component prompt_builder\n",
      "2025-12-08 13:10:44,270 - haystack.core.pipeline.pipeline - INFO - Running component llm\n",
      "2025-12-08 13:10:45,739 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:10:45,747 - __main__ - INFO - Processing query 2 of 10...\n",
      "2025-12-08 13:10:45,747 - haystack.core.pipeline.pipeline - INFO - Running component text_embedder\n",
      "2025-12-08 13:10:45,739 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:10:45,747 - __main__ - INFO - Processing query 2 of 10...\n",
      "2025-12-08 13:10:45,747 - haystack.core.pipeline.pipeline - INFO - Running component text_embedder\n",
      "2025-12-08 13:10:45,976 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:10:45,982 - haystack.core.pipeline.pipeline - INFO - Running component retriever\n",
      "2025-12-08 13:10:45,976 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:10:45,982 - haystack.core.pipeline.pipeline - INFO - Running component retriever\n",
      "2025-12-08 13:10:45,993 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.010s]\n",
      "2025-12-08 13:10:45,995 - haystack.core.pipeline.pipeline - INFO - Running component prompt_builder\n",
      "2025-12-08 13:10:45,995 - haystack.core.pipeline.pipeline - INFO - Running component llm\n",
      "2025-12-08 13:10:45,993 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.010s]\n",
      "2025-12-08 13:10:45,995 - haystack.core.pipeline.pipeline - INFO - Running component prompt_builder\n",
      "2025-12-08 13:10:45,995 - haystack.core.pipeline.pipeline - INFO - Running component llm\n",
      "2025-12-08 13:10:46,457 - haystack.core.pipeline.pipeline - INFO - Running component bm25_retriever\n",
      "2025-12-08 13:10:46,457 - haystack.core.pipeline.pipeline - INFO - Running component bm25_retriever\n",
      "2025-12-08 13:10:46,534 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.077s]\n",
      "2025-12-08 13:10:46,536 - haystack.core.pipeline.pipeline - INFO - Running component text_embedder\n",
      "2025-12-08 13:10:46,534 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.077s]\n",
      "2025-12-08 13:10:46,536 - haystack.core.pipeline.pipeline - INFO - Running component text_embedder\n",
      "2025-12-08 13:10:46,622 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:10:46,628 - __main__ - INFO - Processing query 3 of 10...\n",
      "2025-12-08 13:10:46,630 - haystack.core.pipeline.pipeline - INFO - Running component text_embedder\n",
      "2025-12-08 13:10:46,622 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:10:46,628 - __main__ - INFO - Processing query 3 of 10...\n",
      "2025-12-08 13:10:46,630 - haystack.core.pipeline.pipeline - INFO - Running component text_embedder\n",
      "2025-12-08 13:10:46,751 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:10:46,751 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:10:46,766 - haystack.core.pipeline.pipeline - INFO - Running component embedding_retriever\n",
      "2025-12-08 13:10:46,776 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.009s]\n",
      "2025-12-08 13:10:46,766 - haystack.core.pipeline.pipeline - INFO - Running component embedding_retriever\n",
      "2025-12-08 13:10:46,776 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.009s]\n",
      "2025-12-08 13:10:46,779 - haystack.core.pipeline.pipeline - INFO - Running component document_joiner\n",
      "2025-12-08 13:10:46,781 - haystack.core.pipeline.pipeline - INFO - Running component ranker\n",
      "2025-12-08 13:10:46,779 - haystack.core.pipeline.pipeline - INFO - Running component document_joiner\n",
      "2025-12-08 13:10:46,781 - haystack.core.pipeline.pipeline - INFO - Running component ranker\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92359a1b4446405a8c959a0f513336e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 13:10:46,918 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:10:46,926 - haystack.core.pipeline.pipeline - INFO - Running component retriever\n",
      "2025-12-08 13:10:46,926 - haystack.core.pipeline.pipeline - INFO - Running component retriever\n",
      "2025-12-08 13:10:46,941 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.014s]\n",
      "2025-12-08 13:10:46,943 - haystack.core.pipeline.pipeline - INFO - Running component prompt_builder\n",
      "2025-12-08 13:10:46,944 - haystack.core.pipeline.pipeline - INFO - Running component llm\n",
      "2025-12-08 13:10:46,941 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.014s]\n",
      "2025-12-08 13:10:46,943 - haystack.core.pipeline.pipeline - INFO - Running component prompt_builder\n",
      "2025-12-08 13:10:46,944 - haystack.core.pipeline.pipeline - INFO - Running component llm\n",
      "2025-12-08 13:10:47,498 - haystack.core.pipeline.pipeline - INFO - Running component prompt_builder\n",
      "2025-12-08 13:10:47,499 - haystack.core.pipeline.pipeline - INFO - Running component llm\n",
      "2025-12-08 13:10:47,498 - haystack.core.pipeline.pipeline - INFO - Running component prompt_builder\n",
      "2025-12-08 13:10:47,499 - haystack.core.pipeline.pipeline - INFO - Running component llm\n",
      "2025-12-08 13:10:49,704 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:10:49,707 - __main__ - INFO - Processing query 2 of 10...\n",
      "2025-12-08 13:10:49,708 - haystack.core.pipeline.base - INFO - Warming up component ranker...\n",
      "2025-12-08 13:10:49,709 - haystack.core.pipeline.pipeline - INFO - Running component bm25_retriever\n",
      "2025-12-08 13:10:49,704 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:10:49,707 - __main__ - INFO - Processing query 2 of 10...\n",
      "2025-12-08 13:10:49,708 - haystack.core.pipeline.base - INFO - Warming up component ranker...\n",
      "2025-12-08 13:10:49,709 - haystack.core.pipeline.pipeline - INFO - Running component bm25_retriever\n",
      "2025-12-08 13:10:49,790 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.081s]\n",
      "2025-12-08 13:10:49,791 - haystack.core.pipeline.pipeline - INFO - Running component text_embedder\n",
      "2025-12-08 13:10:49,790 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.081s]\n",
      "2025-12-08 13:10:49,791 - haystack.core.pipeline.pipeline - INFO - Running component text_embedder\n",
      "2025-12-08 13:10:50,037 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:10:50,037 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:10:50,102 - haystack.core.pipeline.pipeline - INFO - Running component embedding_retriever\n",
      "2025-12-08 13:10:50,102 - haystack.core.pipeline.pipeline - INFO - Running component embedding_retriever\n",
      "2025-12-08 13:10:50,119 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.015s]\n",
      "2025-12-08 13:10:50,122 - haystack.core.pipeline.pipeline - INFO - Running component document_joiner\n",
      "2025-12-08 13:10:50,125 - haystack.core.pipeline.pipeline - INFO - Running component ranker\n",
      "2025-12-08 13:10:50,119 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.015s]\n",
      "2025-12-08 13:10:50,122 - haystack.core.pipeline.pipeline - INFO - Running component document_joiner\n",
      "2025-12-08 13:10:50,125 - haystack.core.pipeline.pipeline - INFO - Running component ranker\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f2dac1c01664aed95e05dce244038c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 13:10:50,520 - haystack.core.pipeline.pipeline - INFO - Running component prompt_builder\n",
      "2025-12-08 13:10:50,521 - haystack.core.pipeline.pipeline - INFO - Running component llm\n",
      "2025-12-08 13:10:50,521 - haystack.core.pipeline.pipeline - INFO - Running component llm\n",
      "2025-12-08 13:10:50,798 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:10:50,807 - __main__ - INFO - Processing query 4 of 10...\n",
      "2025-12-08 13:10:50,798 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:10:50,807 - __main__ - INFO - Processing query 4 of 10...\n",
      "2025-12-08 13:10:50,808 - haystack.core.pipeline.pipeline - INFO - Running component text_embedder\n",
      "2025-12-08 13:10:50,808 - haystack.core.pipeline.pipeline - INFO - Running component text_embedder\n",
      "2025-12-08 13:10:50,984 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:10:50,985 - __main__ - INFO - Processing query 3 of 10...\n",
      "2025-12-08 13:10:50,986 - haystack.core.pipeline.base - INFO - Warming up component ranker...\n",
      "2025-12-08 13:10:50,987 - haystack.core.pipeline.pipeline - INFO - Running component bm25_retriever\n",
      "2025-12-08 13:10:50,984 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:10:50,985 - __main__ - INFO - Processing query 3 of 10...\n",
      "2025-12-08 13:10:50,986 - haystack.core.pipeline.base - INFO - Warming up component ranker...\n",
      "2025-12-08 13:10:50,987 - haystack.core.pipeline.pipeline - INFO - Running component bm25_retriever\n",
      "2025-12-08 13:10:51,023 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:10:51,023 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:10:51,036 - haystack.core.pipeline.pipeline - INFO - Running component retriever\n",
      "2025-12-08 13:10:51,036 - haystack.core.pipeline.pipeline - INFO - Running component retriever\n",
      "2025-12-08 13:10:51,046 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.009s]\n",
      "2025-12-08 13:10:51,050 - haystack.core.pipeline.pipeline - INFO - Running component prompt_builder\n",
      "2025-12-08 13:10:51,050 - haystack.core.pipeline.pipeline - INFO - Running component llm\n",
      "2025-12-08 13:10:51,046 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.009s]\n",
      "2025-12-08 13:10:51,050 - haystack.core.pipeline.pipeline - INFO - Running component prompt_builder\n",
      "2025-12-08 13:10:51,050 - haystack.core.pipeline.pipeline - INFO - Running component llm\n",
      "2025-12-08 13:10:51,192 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.205s]\n",
      "2025-12-08 13:10:51,193 - haystack.core.pipeline.pipeline - INFO - Running component text_embedder\n",
      "2025-12-08 13:10:51,192 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.205s]\n",
      "2025-12-08 13:10:51,193 - haystack.core.pipeline.pipeline - INFO - Running component text_embedder\n",
      "2025-12-08 13:10:51,358 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:10:51,365 - haystack.core.pipeline.pipeline - INFO - Running component embedding_retriever\n",
      "2025-12-08 13:10:51,358 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:10:51,365 - haystack.core.pipeline.pipeline - INFO - Running component embedding_retriever\n",
      "2025-12-08 13:10:51,375 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.009s]\n",
      "2025-12-08 13:10:51,377 - haystack.core.pipeline.pipeline - INFO - Running component document_joiner\n",
      "2025-12-08 13:10:51,379 - haystack.core.pipeline.pipeline - INFO - Running component ranker\n",
      "2025-12-08 13:10:51,375 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.009s]\n",
      "2025-12-08 13:10:51,377 - haystack.core.pipeline.pipeline - INFO - Running component document_joiner\n",
      "2025-12-08 13:10:51,379 - haystack.core.pipeline.pipeline - INFO - Running component ranker\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b2a4a2fffd946f9bc5b7a2949396781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 13:10:51,640 - haystack.core.pipeline.pipeline - INFO - Running component prompt_builder\n",
      "2025-12-08 13:10:51,641 - haystack.core.pipeline.pipeline - INFO - Running component llm\n",
      "2025-12-08 13:10:51,641 - haystack.core.pipeline.pipeline - INFO - Running component llm\n",
      "2025-12-08 13:10:54,477 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:10:54,484 - __main__ - INFO - Processing query 4 of 10...\n",
      "2025-12-08 13:10:54,477 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:10:54,484 - __main__ - INFO - Processing query 4 of 10...\n",
      "2025-12-08 13:10:54,485 - haystack.core.pipeline.base - INFO - Warming up component ranker...\n",
      "2025-12-08 13:10:54,487 - haystack.core.pipeline.pipeline - INFO - Running component bm25_retriever\n",
      "2025-12-08 13:10:54,485 - haystack.core.pipeline.base - INFO - Warming up component ranker...\n",
      "2025-12-08 13:10:54,487 - haystack.core.pipeline.pipeline - INFO - Running component bm25_retriever\n",
      "2025-12-08 13:10:54,708 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.218s]\n",
      "2025-12-08 13:10:54,709 - haystack.core.pipeline.pipeline - INFO - Running component text_embedder\n",
      "2025-12-08 13:10:54,708 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.218s]\n",
      "2025-12-08 13:10:54,709 - haystack.core.pipeline.pipeline - INFO - Running component text_embedder\n",
      "2025-12-08 13:10:54,748 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:10:54,753 - __main__ - INFO - Processing query 5 of 10...\n",
      "2025-12-08 13:10:54,754 - haystack.core.pipeline.pipeline - INFO - Running component text_embedder\n",
      "2025-12-08 13:10:54,748 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:10:54,753 - __main__ - INFO - Processing query 5 of 10...\n",
      "2025-12-08 13:10:54,754 - haystack.core.pipeline.pipeline - INFO - Running component text_embedder\n",
      "2025-12-08 13:10:54,977 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:10:54,977 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:10:55,045 - haystack.core.pipeline.pipeline - INFO - Running component retriever\n",
      "2025-12-08 13:10:55,053 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.008s]\n",
      "2025-12-08 13:10:55,045 - haystack.core.pipeline.pipeline - INFO - Running component retriever\n",
      "2025-12-08 13:10:55,053 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.008s]\n",
      "2025-12-08 13:10:55,056 - haystack.core.pipeline.pipeline - INFO - Running component prompt_builder\n",
      "2025-12-08 13:10:55,057 - haystack.core.pipeline.pipeline - INFO - Running component llm\n",
      "2025-12-08 13:10:55,056 - haystack.core.pipeline.pipeline - INFO - Running component prompt_builder\n",
      "2025-12-08 13:10:55,057 - haystack.core.pipeline.pipeline - INFO - Running component llm\n",
      "2025-12-08 13:10:56,084 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:10:56,091 - haystack.core.pipeline.pipeline - INFO - Running component embedding_retriever\n",
      "2025-12-08 13:10:56,084 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:10:56,091 - haystack.core.pipeline.pipeline - INFO - Running component embedding_retriever\n",
      "2025-12-08 13:10:56,103 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.011s]\n",
      "2025-12-08 13:10:56,106 - haystack.core.pipeline.pipeline - INFO - Running component document_joiner\n",
      "2025-12-08 13:10:56,107 - haystack.core.pipeline.pipeline - INFO - Running component ranker\n",
      "2025-12-08 13:10:56,103 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.011s]\n",
      "2025-12-08 13:10:56,106 - haystack.core.pipeline.pipeline - INFO - Running component document_joiner\n",
      "2025-12-08 13:10:56,107 - haystack.core.pipeline.pipeline - INFO - Running component ranker\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d475262460848f496e6935e0265ae93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 13:10:56,536 - haystack.core.pipeline.pipeline - INFO - Running component prompt_builder\n",
      "2025-12-08 13:10:56,536 - haystack.core.pipeline.pipeline - INFO - Running component llm\n",
      "2025-12-08 13:10:56,536 - haystack.core.pipeline.pipeline - INFO - Running component llm\n",
      "2025-12-08 13:10:59,579 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:10:59,582 - __main__ - INFO - Processing query 6 of 10...\n",
      "2025-12-08 13:10:59,583 - haystack.core.pipeline.pipeline - INFO - Running component text_embedder\n",
      "2025-12-08 13:10:59,579 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:10:59,582 - __main__ - INFO - Processing query 6 of 10...\n",
      "2025-12-08 13:10:59,583 - haystack.core.pipeline.pipeline - INFO - Running component text_embedder\n",
      "2025-12-08 13:10:59,798 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:10:59,798 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:10:59,859 - haystack.core.pipeline.pipeline - INFO - Running component retriever\n",
      "2025-12-08 13:10:59,859 - haystack.core.pipeline.pipeline - INFO - Running component retriever\n",
      "2025-12-08 13:10:59,872 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.011s]\n",
      "2025-12-08 13:10:59,874 - haystack.core.pipeline.pipeline - INFO - Running component prompt_builder\n",
      "2025-12-08 13:10:59,875 - haystack.core.pipeline.pipeline - INFO - Running component llm\n",
      "2025-12-08 13:10:59,872 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.011s]\n",
      "2025-12-08 13:10:59,874 - haystack.core.pipeline.pipeline - INFO - Running component prompt_builder\n",
      "2025-12-08 13:10:59,875 - haystack.core.pipeline.pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:01,235 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:01,243 - __main__ - INFO - Processing query 5 of 10...\n",
      "2025-12-08 13:11:01,244 - haystack.core.pipeline.base - INFO - Warming up component ranker...\n",
      "2025-12-08 13:11:01,245 - haystack.core.pipeline.pipeline - INFO - Running component bm25_retriever\n",
      "2025-12-08 13:11:01,235 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:01,243 - __main__ - INFO - Processing query 5 of 10...\n",
      "2025-12-08 13:11:01,244 - haystack.core.pipeline.base - INFO - Warming up component ranker...\n",
      "2025-12-08 13:11:01,245 - haystack.core.pipeline.pipeline - INFO - Running component bm25_retriever\n",
      "2025-12-08 13:11:01,540 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.294s]\n",
      "2025-12-08 13:11:01,541 - haystack.core.pipeline.pipeline - INFO - Running component text_embedder\n",
      "2025-12-08 13:11:01,540 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.294s]\n",
      "2025-12-08 13:11:01,541 - haystack.core.pipeline.pipeline - INFO - Running component text_embedder\n",
      "2025-12-08 13:11:01,767 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:01,767 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:01,833 - haystack.core.pipeline.pipeline - INFO - Running component embedding_retriever\n",
      "2025-12-08 13:11:01,833 - haystack.core.pipeline.pipeline - INFO - Running component embedding_retriever\n",
      "2025-12-08 13:11:01,848 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.012s]\n",
      "2025-12-08 13:11:01,852 - haystack.core.pipeline.pipeline - INFO - Running component document_joiner\n",
      "2025-12-08 13:11:01,857 - haystack.core.pipeline.pipeline - INFO - Running component ranker\n",
      "2025-12-08 13:11:01,848 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.012s]\n",
      "2025-12-08 13:11:01,852 - haystack.core.pipeline.pipeline - INFO - Running component document_joiner\n",
      "2025-12-08 13:11:01,857 - haystack.core.pipeline.pipeline - INFO - Running component ranker\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b31f4420742f45a8ad0e5d0c6fbab5d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 13:11:02,222 - haystack.core.pipeline.pipeline - INFO - Running component prompt_builder\n",
      "2025-12-08 13:11:02,222 - haystack.core.pipeline.pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:02,222 - haystack.core.pipeline.pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:05,331 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:05,337 - __main__ - INFO - Processing query 7 of 10...\n",
      "2025-12-08 13:11:05,339 - haystack.core.pipeline.pipeline - INFO - Running component text_embedder\n",
      "2025-12-08 13:11:05,331 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:05,337 - __main__ - INFO - Processing query 7 of 10...\n",
      "2025-12-08 13:11:05,339 - haystack.core.pipeline.pipeline - INFO - Running component text_embedder\n",
      "2025-12-08 13:11:05,643 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:05,643 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:05,707 - haystack.core.pipeline.pipeline - INFO - Running component retriever\n",
      "2025-12-08 13:11:05,707 - haystack.core.pipeline.pipeline - INFO - Running component retriever\n",
      "2025-12-08 13:11:05,721 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.012s]\n",
      "2025-12-08 13:11:05,725 - haystack.core.pipeline.pipeline - INFO - Running component prompt_builder\n",
      "2025-12-08 13:11:05,726 - haystack.core.pipeline.pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:05,721 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.012s]\n",
      "2025-12-08 13:11:05,725 - haystack.core.pipeline.pipeline - INFO - Running component prompt_builder\n",
      "2025-12-08 13:11:05,726 - haystack.core.pipeline.pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:06,394 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:06,394 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:06,411 - __main__ - INFO - Processing query 6 of 10...\n",
      "2025-12-08 13:11:06,413 - haystack.core.pipeline.base - INFO - Warming up component ranker...\n",
      "2025-12-08 13:11:06,413 - haystack.core.pipeline.pipeline - INFO - Running component bm25_retriever\n",
      "2025-12-08 13:11:06,411 - __main__ - INFO - Processing query 6 of 10...\n",
      "2025-12-08 13:11:06,413 - haystack.core.pipeline.base - INFO - Warming up component ranker...\n",
      "2025-12-08 13:11:06,413 - haystack.core.pipeline.pipeline - INFO - Running component bm25_retriever\n",
      "2025-12-08 13:11:06,553 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.139s]\n",
      "2025-12-08 13:11:06,554 - haystack.core.pipeline.pipeline - INFO - Running component text_embedder\n",
      "2025-12-08 13:11:06,553 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.139s]\n",
      "2025-12-08 13:11:06,554 - haystack.core.pipeline.pipeline - INFO - Running component text_embedder\n",
      "2025-12-08 13:11:06,792 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:06,792 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:06,806 - haystack.core.pipeline.pipeline - INFO - Running component embedding_retriever\n",
      "2025-12-08 13:11:06,806 - haystack.core.pipeline.pipeline - INFO - Running component embedding_retriever\n",
      "2025-12-08 13:11:06,826 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.019s]\n",
      "2025-12-08 13:11:06,830 - haystack.core.pipeline.pipeline - INFO - Running component document_joiner\n",
      "2025-12-08 13:11:06,833 - haystack.core.pipeline.pipeline - INFO - Running component ranker\n",
      "2025-12-08 13:11:06,826 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.019s]\n",
      "2025-12-08 13:11:06,830 - haystack.core.pipeline.pipeline - INFO - Running component document_joiner\n",
      "2025-12-08 13:11:06,833 - haystack.core.pipeline.pipeline - INFO - Running component ranker\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17d9703aef4f448db9a738cc91d19cf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 13:11:07,394 - haystack.core.pipeline.pipeline - INFO - Running component prompt_builder\n",
      "2025-12-08 13:11:07,395 - haystack.core.pipeline.pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:07,395 - haystack.core.pipeline.pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:12,155 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:12,161 - __main__ - INFO - Processing query 7 of 10...\n",
      "2025-12-08 13:11:12,161 - haystack.core.pipeline.base - INFO - Warming up component ranker...\n",
      "2025-12-08 13:11:12,162 - haystack.core.pipeline.pipeline - INFO - Running component bm25_retriever\n",
      "2025-12-08 13:11:12,155 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:12,161 - __main__ - INFO - Processing query 7 of 10...\n",
      "2025-12-08 13:11:12,161 - haystack.core.pipeline.base - INFO - Warming up component ranker...\n",
      "2025-12-08 13:11:12,162 - haystack.core.pipeline.pipeline - INFO - Running component bm25_retriever\n",
      "2025-12-08 13:11:12,332 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.169s]\n",
      "2025-12-08 13:11:12,334 - haystack.core.pipeline.pipeline - INFO - Running component text_embedder\n",
      "2025-12-08 13:11:12,332 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.169s]\n",
      "2025-12-08 13:11:12,334 - haystack.core.pipeline.pipeline - INFO - Running component text_embedder\n",
      "2025-12-08 13:11:12,633 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:12,644 - haystack.core.pipeline.pipeline - INFO - Running component embedding_retriever\n",
      "2025-12-08 13:11:12,633 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:12,644 - haystack.core.pipeline.pipeline - INFO - Running component embedding_retriever\n",
      "2025-12-08 13:11:12,658 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.013s]\n",
      "2025-12-08 13:11:12,661 - haystack.core.pipeline.pipeline - INFO - Running component document_joiner\n",
      "2025-12-08 13:11:12,665 - haystack.core.pipeline.pipeline - INFO - Running component ranker\n",
      "2025-12-08 13:11:12,658 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.013s]\n",
      "2025-12-08 13:11:12,661 - haystack.core.pipeline.pipeline - INFO - Running component document_joiner\n",
      "2025-12-08 13:11:12,665 - haystack.core.pipeline.pipeline - INFO - Running component ranker\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e0a909ae7bb4dbbab738100ebe02abb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 13:11:12,710 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:12,712 - __main__ - INFO - Processing query 8 of 10...\n",
      "2025-12-08 13:11:12,712 - haystack.core.pipeline.pipeline - INFO - Running component text_embedder\n",
      "2025-12-08 13:11:12,712 - __main__ - INFO - Processing query 8 of 10...\n",
      "2025-12-08 13:11:12,712 - haystack.core.pipeline.pipeline - INFO - Running component text_embedder\n",
      "2025-12-08 13:11:12,947 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:12,955 - haystack.core.pipeline.pipeline - INFO - Running component retriever\n",
      "2025-12-08 13:11:12,947 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:12,955 - haystack.core.pipeline.pipeline - INFO - Running component retriever\n",
      "2025-12-08 13:11:12,967 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.011s]\n",
      "2025-12-08 13:11:12,970 - haystack.core.pipeline.pipeline - INFO - Running component prompt_builder\n",
      "2025-12-08 13:11:12,970 - haystack.core.pipeline.pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:12,967 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.011s]\n",
      "2025-12-08 13:11:12,970 - haystack.core.pipeline.pipeline - INFO - Running component prompt_builder\n",
      "2025-12-08 13:11:12,970 - haystack.core.pipeline.pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:13,067 - haystack.core.pipeline.pipeline - INFO - Running component prompt_builder\n",
      "2025-12-08 13:11:13,067 - haystack.core.pipeline.pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:13,067 - haystack.core.pipeline.pipeline - INFO - Running component prompt_builder\n",
      "2025-12-08 13:11:13,067 - haystack.core.pipeline.pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:16,137 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:16,174 - __main__ - INFO - Processing query 9 of 10...\n",
      "2025-12-08 13:11:16,177 - haystack.core.pipeline.pipeline - INFO - Running component text_embedder\n",
      "2025-12-08 13:11:16,137 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:16,174 - __main__ - INFO - Processing query 9 of 10...\n",
      "2025-12-08 13:11:16,177 - haystack.core.pipeline.pipeline - INFO - Running component text_embedder\n",
      "2025-12-08 13:11:16,435 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:16,442 - haystack.core.pipeline.pipeline - INFO - Running component retriever\n",
      "2025-12-08 13:11:16,435 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:16,442 - haystack.core.pipeline.pipeline - INFO - Running component retriever\n",
      "2025-12-08 13:11:16,456 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.013s]\n",
      "2025-12-08 13:11:16,459 - haystack.core.pipeline.pipeline - INFO - Running component prompt_builder\n",
      "2025-12-08 13:11:16,459 - haystack.core.pipeline.pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:16,456 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.013s]\n",
      "2025-12-08 13:11:16,459 - haystack.core.pipeline.pipeline - INFO - Running component prompt_builder\n",
      "2025-12-08 13:11:16,459 - haystack.core.pipeline.pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:19,325 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:19,327 - __main__ - INFO - Processing query 8 of 10...\n",
      "2025-12-08 13:11:19,328 - haystack.core.pipeline.base - INFO - Warming up component ranker...\n",
      "2025-12-08 13:11:19,329 - haystack.core.pipeline.pipeline - INFO - Running component bm25_retriever\n",
      "2025-12-08 13:11:19,325 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:19,327 - __main__ - INFO - Processing query 8 of 10...\n",
      "2025-12-08 13:11:19,328 - haystack.core.pipeline.base - INFO - Warming up component ranker...\n",
      "2025-12-08 13:11:19,329 - haystack.core.pipeline.pipeline - INFO - Running component bm25_retriever\n",
      "2025-12-08 13:11:19,489 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.158s]\n",
      "2025-12-08 13:11:19,490 - haystack.core.pipeline.pipeline - INFO - Running component text_embedder\n",
      "2025-12-08 13:11:19,489 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.158s]\n",
      "2025-12-08 13:11:19,490 - haystack.core.pipeline.pipeline - INFO - Running component text_embedder\n",
      "2025-12-08 13:11:19,811 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:19,821 - haystack.core.pipeline.pipeline - INFO - Running component embedding_retriever\n",
      "2025-12-08 13:11:19,811 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:19,821 - haystack.core.pipeline.pipeline - INFO - Running component embedding_retriever\n",
      "2025-12-08 13:11:19,836 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.013s]\n",
      "2025-12-08 13:11:19,839 - haystack.core.pipeline.pipeline - INFO - Running component document_joiner\n",
      "2025-12-08 13:11:19,842 - haystack.core.pipeline.pipeline - INFO - Running component ranker\n",
      "2025-12-08 13:11:19,836 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.013s]\n",
      "2025-12-08 13:11:19,839 - haystack.core.pipeline.pipeline - INFO - Running component document_joiner\n",
      "2025-12-08 13:11:19,842 - haystack.core.pipeline.pipeline - INFO - Running component ranker\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56a0a413e1fa40d69f6c4f16935f21eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 13:11:20,351 - haystack.core.pipeline.pipeline - INFO - Running component prompt_builder\n",
      "2025-12-08 13:11:20,352 - haystack.core.pipeline.pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:20,352 - haystack.core.pipeline.pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:21,813 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:21,818 - __main__ - INFO - Processing query 10 of 10...\n",
      "2025-12-08 13:11:21,819 - haystack.core.pipeline.pipeline - INFO - Running component text_embedder\n",
      "2025-12-08 13:11:21,813 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:21,818 - __main__ - INFO - Processing query 10 of 10...\n",
      "2025-12-08 13:11:21,819 - haystack.core.pipeline.pipeline - INFO - Running component text_embedder\n",
      "2025-12-08 13:11:22,119 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:22,119 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:22,131 - haystack.core.pipeline.pipeline - INFO - Running component retriever\n",
      "2025-12-08 13:11:22,131 - haystack.core.pipeline.pipeline - INFO - Running component retriever\n",
      "2025-12-08 13:11:22,154 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.021s]\n",
      "2025-12-08 13:11:22,158 - haystack.core.pipeline.pipeline - INFO - Running component prompt_builder\n",
      "2025-12-08 13:11:22,159 - haystack.core.pipeline.pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:22,154 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.021s]\n",
      "2025-12-08 13:11:22,158 - haystack.core.pipeline.pipeline - INFO - Running component prompt_builder\n",
      "2025-12-08 13:11:22,159 - haystack.core.pipeline.pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:23,348 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:23,355 - __main__ - INFO - Processing query 9 of 10...\n",
      "2025-12-08 13:11:23,356 - haystack.core.pipeline.base - INFO - Warming up component ranker...\n",
      "2025-12-08 13:11:23,356 - haystack.core.pipeline.pipeline - INFO - Running component bm25_retriever\n",
      "2025-12-08 13:11:23,348 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:23,355 - __main__ - INFO - Processing query 9 of 10...\n",
      "2025-12-08 13:11:23,356 - haystack.core.pipeline.base - INFO - Warming up component ranker...\n",
      "2025-12-08 13:11:23,356 - haystack.core.pipeline.pipeline - INFO - Running component bm25_retriever\n",
      "2025-12-08 13:11:23,551 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.194s]\n",
      "2025-12-08 13:11:23,552 - haystack.core.pipeline.pipeline - INFO - Running component text_embedder\n",
      "2025-12-08 13:11:23,551 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.194s]\n",
      "2025-12-08 13:11:23,552 - haystack.core.pipeline.pipeline - INFO - Running component text_embedder\n",
      "2025-12-08 13:11:24,273 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:24,281 - haystack.core.pipeline.pipeline - INFO - Running component embedding_retriever\n",
      "2025-12-08 13:11:24,273 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:24,281 - haystack.core.pipeline.pipeline - INFO - Running component embedding_retriever\n",
      "2025-12-08 13:11:24,294 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.012s]\n",
      "2025-12-08 13:11:24,296 - haystack.core.pipeline.pipeline - INFO - Running component document_joiner\n",
      "2025-12-08 13:11:24,299 - haystack.core.pipeline.pipeline - INFO - Running component ranker\n",
      "2025-12-08 13:11:24,294 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.012s]\n",
      "2025-12-08 13:11:24,296 - haystack.core.pipeline.pipeline - INFO - Running component document_joiner\n",
      "2025-12-08 13:11:24,299 - haystack.core.pipeline.pipeline - INFO - Running component ranker\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2224ad227d99456eb7b0968979b30014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 13:11:24,665 - haystack.core.pipeline.pipeline - INFO - Running component prompt_builder\n",
      "2025-12-08 13:11:24,666 - haystack.core.pipeline.pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:24,666 - haystack.core.pipeline.pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:28,919 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:28,931 - __main__ - INFO - âœ“ Async RAG processing complete for 10 queries!\n",
      "2025-12-08 13:11:28,919 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:28,931 - __main__ - INFO - âœ“ Async RAG processing complete for 10 queries!\n",
      "2025-12-08 13:11:28,935 - haystack.core.pipeline.async_pipeline - INFO - Running component evaluator\n",
      "2025-12-08 13:11:28,938 - __main__ - INFO - Creating Ragas EvaluationDataset...\n",
      "2025-12-08 13:11:28,945 - __main__ - INFO - Starting Ragas evaluation...\n",
      "2025-12-08 13:11:28,935 - haystack.core.pipeline.async_pipeline - INFO - Running component evaluator\n",
      "2025-12-08 13:11:28,938 - __main__ - INFO - Creating Ragas EvaluationDataset...\n",
      "2025-12-08 13:11:28,945 - __main__ - INFO - Starting Ragas evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e0058596aff46eea6a593b926dad002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 13:11:29,477 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:29,480 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:29,481 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:29,487 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:29,490 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:29,480 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:29,481 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:29,487 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:29,490 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:29,492 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:29,496 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:29,498 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:29,499 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:29,501 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:29,492 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:29,496 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:29,498 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:29,499 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:29,501 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:29,504 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:29,506 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:29,508 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:29,511 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:29,513 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:29,514 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:29,504 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:29,506 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:29,508 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:29,511 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:29,513 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:29,514 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:29,516 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:29,519 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:29,520 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:29,521 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:29,516 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:29,519 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:29,520 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:29,521 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:29,645 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:29,649 - __main__ - INFO - Processing query 10 of 10...\n",
      "2025-12-08 13:11:29,649 - haystack.core.pipeline.base - INFO - Warming up component ranker...\n",
      "2025-12-08 13:11:29,650 - haystack.core.pipeline.pipeline - INFO - Running component bm25_retriever\n",
      "2025-12-08 13:11:29,645 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:29,649 - __main__ - INFO - Processing query 10 of 10...\n",
      "2025-12-08 13:11:29,649 - haystack.core.pipeline.base - INFO - Warming up component ranker...\n",
      "2025-12-08 13:11:29,650 - haystack.core.pipeline.pipeline - INFO - Running component bm25_retriever\n",
      "2025-12-08 13:11:29,798 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.148s]\n",
      "2025-12-08 13:11:29,799 - haystack.core.pipeline.pipeline - INFO - Running component text_embedder\n",
      "2025-12-08 13:11:29,798 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.148s]\n",
      "2025-12-08 13:11:29,799 - haystack.core.pipeline.pipeline - INFO - Running component text_embedder\n",
      "2025-12-08 13:11:30,104 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:30,112 - haystack.core.pipeline.pipeline - INFO - Running component embedding_retriever\n",
      "2025-12-08 13:11:30,104 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:30,112 - haystack.core.pipeline.pipeline - INFO - Running component embedding_retriever\n",
      "2025-12-08 13:11:30,120 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.007s]\n",
      "2025-12-08 13:11:30,122 - haystack.core.pipeline.pipeline - INFO - Running component document_joiner\n",
      "2025-12-08 13:11:30,124 - haystack.core.pipeline.pipeline - INFO - Running component ranker\n",
      "2025-12-08 13:11:30,120 - elastic_transport.transport - INFO - POST http://localhost:9200/default/_search [status:200 duration:0.007s]\n",
      "2025-12-08 13:11:30,122 - haystack.core.pipeline.pipeline - INFO - Running component document_joiner\n",
      "2025-12-08 13:11:30,124 - haystack.core.pipeline.pipeline - INFO - Running component ranker\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a46c2fd2ed54db3aab1bd36d619cd8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 13:11:30,239 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:30,240 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:30,240 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:30,275 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:30,279 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:30,275 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:30,279 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:30,325 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:30,325 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:30,403 - haystack.core.pipeline.pipeline - INFO - Running component prompt_builder\n",
      "2025-12-08 13:11:30,404 - haystack.core.pipeline.pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:30,403 - haystack.core.pipeline.pipeline - INFO - Running component prompt_builder\n",
      "2025-12-08 13:11:30,404 - haystack.core.pipeline.pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:30,485 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:30,485 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:30,499 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:30,500 - ragas.prompt.pydantic_prompt - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-12-08 13:11:30,499 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:30,500 - ragas.prompt.pydantic_prompt - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-12-08 13:11:31,137 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:31,137 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:31,286 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:31,286 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:31,528 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:31,528 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:31,543 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:31,543 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:31,657 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:31,657 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:31,737 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:31,745 - ragas.prompt.pydantic_prompt - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-12-08 13:11:31,737 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:31,745 - ragas.prompt.pydantic_prompt - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-12-08 13:11:32,022 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:32,022 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:32,135 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:32,135 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:32,573 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:32,573 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:32,807 - ragas.prompt.pydantic_prompt - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-12-08 13:11:32,807 - ragas.prompt.pydantic_prompt - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-12-08 13:11:32,832 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:32,832 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:33,014 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:33,020 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:33,014 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:33,020 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:33,216 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:33,218 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:33,216 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:33,218 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:33,244 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:33,256 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:33,244 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:33,256 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:33,573 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:33,573 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:33,605 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:33,605 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:33,846 - ragas.prompt.pydantic_prompt - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-12-08 13:11:33,846 - ragas.prompt.pydantic_prompt - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-12-08 13:11:34,233 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:34,233 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:34,602 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:34,602 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:34,764 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:34,764 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:34,896 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:34,896 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:35,044 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:35,044 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:35,047 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:35,049 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:35,052 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:35,044 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:35,044 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:35,047 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:35,049 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:35,052 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:35,053 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:35,053 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:36,012 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:36,012 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:36,183 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:36,183 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:37,766 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:37,766 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:37,955 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:37,958 - __main__ - INFO - âœ“ Async RAG processing complete for 10 queries!\n",
      "2025-12-08 13:11:37,963 - haystack.core.pipeline.async_pipeline - INFO - Running component evaluator\n",
      "2025-12-08 13:11:37,965 - __main__ - INFO - Creating Ragas EvaluationDataset...\n",
      "2025-12-08 13:11:37,965 - __main__ - INFO - Starting Ragas evaluation...\n",
      "2025-12-08 13:11:37,955 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:37,958 - __main__ - INFO - âœ“ Async RAG processing complete for 10 queries!\n",
      "2025-12-08 13:11:37,963 - haystack.core.pipeline.async_pipeline - INFO - Running component evaluator\n",
      "2025-12-08 13:11:37,965 - __main__ - INFO - Creating Ragas EvaluationDataset...\n",
      "2025-12-08 13:11:37,965 - __main__ - INFO - Starting Ragas evaluation...\n",
      "2025-12-08 13:11:38,425 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,425 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,426 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,428 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,431 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,432 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,433 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,425 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,425 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,426 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,428 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,431 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,432 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,433 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,435 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,437 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,440 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,441 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,444 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,435 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,437 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,440 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,441 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,444 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,445 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,492 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,509 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,445 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,492 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,509 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "292fe49e5c03446f83c09bc958cc1695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 13:11:38,536 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,537 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,538 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,542 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,543 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,544 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,537 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,538 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,542 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,543 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,544 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,550 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,551 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,552 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,554 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,557 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,559 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,550 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,551 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,552 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,554 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,557 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,559 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,562 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,567 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,568 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,569 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,570 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,571 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,571 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,572 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,562 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,567 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,568 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,569 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,570 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,571 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,571 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,572 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:38,753 - ragas.prompt.pydantic_prompt - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-12-08 13:11:38,753 - ragas.prompt.pydantic_prompt - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-12-08 13:11:39,119 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:39,119 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:39,131 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:39,131 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:39,241 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:39,241 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:39,281 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:39,281 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:39,369 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:39,370 - ragas.prompt.pydantic_prompt - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-12-08 13:11:39,369 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:39,370 - ragas.prompt.pydantic_prompt - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-12-08 13:11:39,416 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:39,424 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:39,416 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:39,424 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:39,432 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:39,432 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:39,496 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:39,496 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:39,563 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:39,563 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:39,745 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:39,745 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:39,849 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:39,849 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:40,011 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:40,012 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:40,013 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:40,011 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:40,012 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:40,013 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:40,167 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:40,167 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:40,212 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:40,212 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:40,242 - ragas.prompt.pydantic_prompt - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-12-08 13:11:40,242 - ragas.prompt.pydantic_prompt - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-12-08 13:11:40,291 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:40,291 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:40,398 - ragas.prompt.pydantic_prompt - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-12-08 13:11:40,398 - ragas.prompt.pydantic_prompt - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-12-08 13:11:40,444 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:40,444 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:40,538 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:40,538 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:40,627 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:40,627 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:40,669 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:40,669 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:40,682 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:40,682 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:40,741 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:40,750 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:40,741 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:40,750 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:40,765 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:40,765 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:40,948 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:40,948 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:40,984 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:40,984 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:41,049 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:41,049 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:41,141 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:41,141 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:41,374 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:41,375 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:41,378 - ragas.prompt.pydantic_prompt - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-12-08 13:11:41,374 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:41,375 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:41,378 - ragas.prompt.pydantic_prompt - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-12-08 13:11:41,487 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:41,490 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:41,491 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:41,487 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:41,490 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:41,491 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:41,823 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:41,823 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:42,102 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:42,102 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:42,221 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:42,221 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:42,450 - ragas.prompt.pydantic_prompt - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-12-08 13:11:42,461 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:42,450 - ragas.prompt.pydantic_prompt - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-12-08 13:11:42,461 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:42,495 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:42,495 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:42,538 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:42,538 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:42,574 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:42,574 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:42,630 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:42,630 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:42,704 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:42,708 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:42,704 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:42,708 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:42,872 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:42,872 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:42,886 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:42,886 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:42,921 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:42,921 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:42,950 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:42,950 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:42,950 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:42,950 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:43,237 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:43,237 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:43,260 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:43,270 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:43,260 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:43,270 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:43,396 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:43,398 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:43,396 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:43,398 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:43,410 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:43,410 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:43,481 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:43,481 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:43,628 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:43,629 - ragas.prompt.pydantic_prompt - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-12-08 13:11:43,628 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:43,629 - ragas.prompt.pydantic_prompt - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-12-08 13:11:43,881 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:43,881 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:43,952 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:43,952 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:44,023 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:44,023 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:44,457 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:44,457 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:44,570 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:44,570 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:44,698 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:44,698 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:44,928 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:44,928 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:45,288 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:45,288 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:45,294 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:45,294 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:45,296 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:45,299 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:45,288 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:45,288 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:45,294 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:45,294 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:45,296 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:45,299 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:45,301 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:45,302 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:45,304 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:45,307 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:45,309 - ragas.prompt.pydantic_prompt - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-12-08 13:11:45,301 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:45,302 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:45,304 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:45,307 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:45,309 - ragas.prompt.pydantic_prompt - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-12-08 13:11:45,648 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:45,648 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:45,682 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:45,682 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:45,884 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:45,884 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:46,074 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:46,074 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:46,145 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:46,145 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:46,672 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:46,672 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:46,801 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:46,801 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:47,031 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:47,031 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:47,197 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:47,197 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:47,237 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:47,237 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:47,421 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:47,421 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:47,421 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:47,421 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:47,553 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:47,553 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:47,795 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:47,795 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:47,945 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:47,955 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:47,945 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:47,955 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:47,979 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:47,979 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:48,217 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:48,218 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:48,220 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:48,222 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:48,225 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:48,226 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:48,217 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:48,218 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:48,220 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:48,222 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:48,225 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:48,226 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:48,232 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:48,236 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:48,236 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:48,239 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:48,232 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:48,236 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:48,236 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:48,239 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:48,245 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:48,249 - ragas.prompt.pydantic_prompt - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-12-08 13:11:48,245 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:48,249 - ragas.prompt.pydantic_prompt - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-12-08 13:11:48,337 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:48,337 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:48,444 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:48,444 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:48,632 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:48,632 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:48,670 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:48,672 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:48,670 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:48,672 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:48,903 - ragas.prompt.pydantic_prompt - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-12-08 13:11:48,903 - ragas.prompt.pydantic_prompt - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-12-08 13:11:48,941 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:48,941 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:49,008 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:49,008 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:49,036 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:49,036 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:49,252 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:49,252 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:49,360 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:49,360 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:49,587 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:49,587 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:49,753 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:49,756 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:49,757 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:49,753 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:49,756 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:49,757 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:49,772 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:49,772 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:49,999 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:50,001 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:49,999 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:50,001 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:50,029 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:50,029 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:50,494 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:50,494 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:50,494 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:50,494 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:50,708 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:50,708 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:50,709 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:50,708 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:50,708 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:50,709 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:50,887 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:50,887 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:50,941 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:50,942 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:50,943 - ragas.prompt.pydantic_prompt - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-12-08 13:11:50,941 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:50,942 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:50,943 - ragas.prompt.pydantic_prompt - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-12-08 13:11:51,024 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:51,024 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:51,083 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:51,083 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:51,155 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:51,160 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:51,155 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:51,160 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:51,242 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:51,242 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:51,306 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:51,306 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:51,468 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:51,468 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:51,591 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:51,591 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:51,641 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:51,641 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:51,722 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:51,722 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:51,759 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:51,759 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:51,813 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:51,815 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:51,813 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:51,815 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:52,042 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:52,051 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:52,042 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:52,051 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:52,155 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:52,155 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:52,216 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:52,216 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:52,287 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:52,288 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:52,289 - ragas.prompt.pydantic_prompt - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-12-08 13:11:52,287 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:52,288 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:52,289 - ragas.prompt.pydantic_prompt - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-12-08 13:11:52,609 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:52,609 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:52,691 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:52,691 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:52,747 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:52,748 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:52,747 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:52,748 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:52,978 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:52,979 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:52,978 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:52,979 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:53,114 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:53,114 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:53,194 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:53,194 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:53,449 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:53,449 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:53,501 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:53,501 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:53,568 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:53,568 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:53,643 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:53,643 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:53,682 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:53,682 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:53,682 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:53,682 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:53,789 - ragas.prompt.pydantic_prompt - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-12-08 13:11:53,789 - ragas.prompt.pydantic_prompt - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-12-08 13:11:54,210 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:54,210 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:54,326 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:54,326 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:54,562 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:54,562 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:54,593 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:54,593 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:55,052 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:55,052 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:55,052 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:55,052 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:55,212 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:55,212 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:55,433 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:55,433 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:55,513 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:55,513 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:55,704 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:55,704 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:55,928 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:55,928 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:56,023 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:56,023 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:56,094 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:56,094 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:56,172 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:56,172 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:56,269 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:56,269 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:56,418 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:56,418 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:56,539 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:56,549 - ragas.prompt.pydantic_prompt - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-12-08 13:11:56,539 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:56,549 - ragas.prompt.pydantic_prompt - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-12-08 13:11:56,564 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:56,564 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:56,716 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:56,716 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:56,792 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:56,792 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:56,938 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:56,938 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:56,994 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:56,994 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:57,343 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:57,343 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:57,474 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:57,474 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:57,586 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:57,586 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:57,711 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:57,711 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:58,088 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:58,088 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:58,209 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:58,209 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:58,282 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:58,282 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:58,328 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:58,328 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:58,431 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:58,432 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:58,432 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:58,431 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:58,432 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:58,432 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:58,501 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:58,501 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:58,667 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:58,667 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:58,810 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:58,810 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:58,911 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:58,913 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:58,913 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:58,911 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:58,913 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:58,913 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:11:59,194 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:59,194 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:59,487 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:59,490 - ragas.prompt.pydantic_prompt - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-12-08 13:11:59,487 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:59,490 - ragas.prompt.pydantic_prompt - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-12-08 13:11:59,669 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:59,669 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:59,936 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:11:59,936 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:00,084 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:00,084 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:00,159 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:00,159 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:00,332 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:00,332 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:00,380 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:12:00,380 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:12:00,569 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:12:00,570 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:12:00,569 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:12:00,570 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:12:01,031 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:12:01,034 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:12:01,031 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:12:01,034 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:12:01,261 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:01,261 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:01,477 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:01,484 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:01,477 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:01,484 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:01,517 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:12:01,517 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:12:01,760 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:12:01,760 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:12:02,542 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:02,542 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:03,239 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:03,239 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:03,479 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:12:03,480 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:12:03,479 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:12:03,480 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:12:03,949 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:03,949 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:04,058 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:04,058 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:04,377 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:04,377 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:04,684 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:12:04,684 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:12:04,710 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:04,753 - ragas.prompt.pydantic_prompt - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-12-08 13:12:04,710 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:04,753 - ragas.prompt.pydantic_prompt - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-12-08 13:12:04,774 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:04,774 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:05,149 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:05,149 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:05,209 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:05,212 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:05,209 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:05,212 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:05,303 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:05,303 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:05,519 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:05,528 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:05,519 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:05,528 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:05,829 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:05,829 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:06,226 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:06,226 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:06,326 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:06,326 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:06,451 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:12:06,454 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:06,451 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:12:06,454 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:06,494 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:06,494 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:06,826 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:06,826 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:06,954 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:12:06,954 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:12:08,084 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:08,084 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:08,745 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:08,745 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:08,941 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:08,941 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:08,989 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:12:08,989 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:12:09,577 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:09,577 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:09,721 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:09,721 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:10,043 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:12:10,043 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:12:10,645 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:10,645 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:10,927 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:10,938 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:10,927 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:10,938 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:12,788 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:12,788 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:13,023 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:12:13,023 - haystack.core.pipeline.async_pipeline - INFO - Running component llm\n",
      "2025-12-08 13:12:14,061 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:14,061 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:16,355 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:16,355 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:16,705 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:16,705 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:18,935 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:18,935 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:20,425 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:20,425 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:21,342 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:21,342 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:22,058 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:22,058 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:23,832 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:23,832 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:23,876 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:23,876 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:24,804 - __main__ - INFO - Ragas evaluation complete.\n",
      "2025-12-08 13:12:24,805 - __main__ - INFO - Overall metrics: {'faithfulness': 0.8972, 'answer_relevancy': 0.8696, 'context_recall': 0.9100, 'factual_correctness(mode=f1)': 0.5140}\n",
      "2025-12-08 13:12:24,804 - __main__ - INFO - Ragas evaluation complete.\n",
      "2025-12-08 13:12:24,805 - __main__ - INFO - Overall metrics: {'faithfulness': 0.8972, 'answer_relevancy': 0.8696, 'context_recall': 0.9100, 'factual_correctness(mode=f1)': 0.5140}\n",
      "2025-12-08 13:12:24,825 - __main__ - INFO - \n",
      "âœ“ Naive RAG evaluation complete!\n",
      "2025-12-08 13:12:24,825 - __main__ - INFO - \n",
      "âœ“ Naive RAG evaluation complete!\n",
      "2025-12-08 13:12:27,410 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:27,410 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:28,634 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:28,634 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:33,912 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:33,912 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:40,114 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:40,114 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-08 13:12:40,855 - __main__ - INFO - Ragas evaluation complete.\n",
      "2025-12-08 13:12:40,856 - __main__ - INFO - Overall metrics: {'faithfulness': 0.8636, 'answer_relevancy': 0.8679, 'context_recall': 0.9267, 'factual_correctness(mode=f1)': 0.5940}\n",
      "2025-12-08 13:12:40,884 - __main__ - INFO - \n",
      "âœ“ Hybrid RAG evaluation complete!\n",
      "2025-12-08 13:12:40,855 - __main__ - INFO - Ragas evaluation complete.\n",
      "2025-12-08 13:12:40,856 - __main__ - INFO - Overall metrics: {'faithfulness': 0.8636, 'answer_relevancy': 0.8679, 'context_recall': 0.9267, 'factual_correctness(mode=f1)': 0.5940}\n",
      "2025-12-08 13:12:40,884 - __main__ - INFO - \n",
      "âœ“ Hybrid RAG evaluation complete!\n",
      "2025-12-08 13:12:40,886 - __main__ - INFO - \n",
      "================================================================================\n",
      "2025-12-08 13:12:40,886 - __main__ - INFO - âœ“ ALL EVALUATIONS COMPLETE\n",
      "2025-12-08 13:12:40,887 - __main__ - INFO - â±ï¸  Total time: 117.62 seconds\n",
      "2025-12-08 13:12:40,888 - __main__ - INFO - âš¡ Average time per system: 58.81 seconds\n",
      "2025-12-08 13:12:40,889 - __main__ - INFO - ðŸš€ Speedup: ~2Ã— (concurrent system evaluation)\n",
      "2025-12-08 13:12:40,890 - __main__ - INFO - ================================================================================\n",
      "\n",
      "2025-12-08 13:12:40,886 - __main__ - INFO - \n",
      "================================================================================\n",
      "2025-12-08 13:12:40,886 - __main__ - INFO - âœ“ ALL EVALUATIONS COMPLETE\n",
      "2025-12-08 13:12:40,887 - __main__ - INFO - â±ï¸  Total time: 117.62 seconds\n",
      "2025-12-08 13:12:40,888 - __main__ - INFO - âš¡ Average time per system: 58.81 seconds\n",
      "2025-12-08 13:12:40,889 - __main__ - INFO - ðŸš€ Speedup: ~2Ã— (concurrent system evaluation)\n",
      "2025-12-08 13:12:40,890 - __main__ - INFO - ================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Concurrent evaluation complete!\n",
      "Naive RAG: 10 queries evaluated\n",
      "Hybrid RAG: 10 queries evaluated\n"
     ]
    }
   ],
   "source": [
    "# --- Setup for Concurrent Evaluation ---\n",
    "\n",
    "# Create separate generators for each evaluation (components can't be shared)\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.utils import Secret\n",
    "import os\n",
    "\n",
    "eval_generator_naive = OpenAIGenerator(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=Secret.from_token(os.getenv(\"OPENAI_API_KEY\"))\n",
    ")\n",
    "\n",
    "eval_generator_hybrid = OpenAIGenerator(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=Secret.from_token(os.getenv(\"OPENAI_API_KEY\"))\n",
    ")\n",
    "\n",
    "# Define evaluation dataset\n",
    "csv_file_path = \"data_for_eval/synthetic_tests_advanced_branching_10.csv\"\n",
    "\n",
    "# Prepare RAG systems for concurrent evaluation\n",
    "# Note: Each system processes queries sequentially (batch_size=1 internally)\n",
    "# but both systems run concurrently for ~2Ã— speedup\n",
    "rag_systems = [\n",
    "    (naive_rag_sc, \"Naive RAG\", eval_generator_naive),\n",
    "    (hybrid_rag_sc, \"Hybrid RAG\", eval_generator_hybrid)\n",
    "]\n",
    "\n",
    "# Run concurrent evaluation\n",
    "all_results = await evaluate_multiple_rag_systems_async(\n",
    "    rag_systems=rag_systems,\n",
    "    csv_file_path=csv_file_path\n",
    ")\n",
    "\n",
    "# Extract results for each system\n",
    "naive_results_async = all_results[0]\n",
    "hybrid_results_async = all_results[1]\n",
    "\n",
    "print(\"\\nâœ“ Concurrent evaluation complete!\")\n",
    "print(f\"Naive RAG: {len(naive_results_async['evaluation_df'])} queries evaluated\")\n",
    "print(f\"Hybrid RAG: {len(hybrid_results_async['evaluation_df'])} queries evaluated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287f9163",
   "metadata": {},
   "source": [
    "### View Evaluation Results\n",
    "\n",
    "Let's examine the results from our concurrent evaluation of both systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e582f25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive RAG - Async Evaluation Results:\n",
      "Metrics: {'faithfulness': 0.8972, 'answer_relevancy': 0.8696, 'context_recall': 0.9100, 'factual_correctness(mode=f1)': 0.5140}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>factual_correctness(mode=f1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What Alexa do in AI?</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>Alexa, as a voice-controlled virtual assistant...</td>\n",
       "      <td>Alexa is a voice-controlled virtual assistant ...</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.913990</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What happened to the UnitedHealthcare CEO Bria...</td>\n",
       "      <td>[- James Kurose, â€œTestimony before the House C...</td>\n",
       "      <td>[Why is AI controversial?\\nWhile acknowledging...</td>\n",
       "      <td>I don't have enough information to answer.</td>\n",
       "      <td>The BBC reported that Apple's AI falsely told ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the current stance of the UK governmen...</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>[Are there laws governing AI?\\nSome government...</td>\n",
       "      <td>The current stance of the UK government regard...</td>\n",
       "      <td>In the UK, Prime Minister Sir Keir Starmer has...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.976753</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does the GDPR impact the transparency and ...</td>\n",
       "      <td>[Smart cities\\nMetropolitan governments are us...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nAI ethics and transparency\\nAlgori...</td>\n",
       "      <td>The General Data Protection Regulation (GDPR) ...</td>\n",
       "      <td>The GDPR impacts the transparency and accounta...</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.968926</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the trends in the usage of ChatGPT fo...</td>\n",
       "      <td>[The yellow line represents the first cohort o...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n37% of messages are work-related f...</td>\n",
       "      <td>The usage of ChatGPT for Technical Help has sh...</td>\n",
       "      <td>The usage of ChatGPT for Technical Help has sh...</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.981060</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0                               What Alexa do in AI?   \n",
       "1  What happened to the UnitedHealthcare CEO Bria...   \n",
       "2  What is the current stance of the UK governmen...   \n",
       "3  How does the GDPR impact the transparency and ...   \n",
       "4  What are the trends in the usage of ChatGPT fo...   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [What is AI, how does it work and why are some...   \n",
       "1  [- James Kurose, â€œTestimony before the House C...   \n",
       "2  [What is AI, how does it work and why are some...   \n",
       "3  [Smart cities\\nMetropolitan governments are us...   \n",
       "4  [The yellow line represents the first cohort o...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  [What is AI, how does it work and why are some...   \n",
       "1  [Why is AI controversial?\\nWhile acknowledging...   \n",
       "2  [Are there laws governing AI?\\nSome government...   \n",
       "3  [<1-hop>\\n\\nAI ethics and transparency\\nAlgori...   \n",
       "4  [<1-hop>\\n\\n37% of messages are work-related f...   \n",
       "\n",
       "                                            response  \\\n",
       "0  Alexa, as a voice-controlled virtual assistant...   \n",
       "1         I don't have enough information to answer.   \n",
       "2  The current stance of the UK government regard...   \n",
       "3  The General Data Protection Regulation (GDPR) ...   \n",
       "4  The usage of ChatGPT for Technical Help has sh...   \n",
       "\n",
       "                                           reference  faithfulness  \\\n",
       "0  Alexa is a voice-controlled virtual assistant ...      0.700000   \n",
       "1  The BBC reported that Apple's AI falsely told ...      1.000000   \n",
       "2  In the UK, Prime Minister Sir Keir Starmer has...      1.000000   \n",
       "3  The GDPR impacts the transparency and accounta...      0.615385   \n",
       "4  The usage of ChatGPT for Technical Help has sh...      0.923077   \n",
       "\n",
       "   answer_relevancy  context_recall  factual_correctness(mode=f1)  \n",
       "0          0.913990             1.0                          0.63  \n",
       "1          0.000000             1.0                          0.00  \n",
       "2          0.976753             1.0                          0.42  \n",
       "3          0.968926             1.0                          0.55  \n",
       "4          0.981060             0.6                          0.92  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n",
      "Hybrid RAG - Async Evaluation Results:\n",
      "Metrics: {'faithfulness': 0.8636, 'answer_relevancy': 0.8679, 'context_recall': 0.9267, 'factual_correctness(mode=f1)': 0.5940}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>factual_correctness(mode=f1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What Alexa do in AI?</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>Alexa is a voice-controlled virtual assistant ...</td>\n",
       "      <td>Alexa is a voice-controlled virtual assistant ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.917891</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What happened to the UnitedHealthcare CEO Bria...</td>\n",
       "      <td>[AI will reconfigure how society and the econo...</td>\n",
       "      <td>[Why is AI controversial?\\nWhile acknowledging...</td>\n",
       "      <td>I don't have enough information to answer.</td>\n",
       "      <td>The BBC reported that Apple's AI falsely told ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the current stance of the UK governmen...</td>\n",
       "      <td>[AI will reconfigure how society and the econo...</td>\n",
       "      <td>[Are there laws governing AI?\\nSome government...</td>\n",
       "      <td>The current stance of the UK government regard...</td>\n",
       "      <td>In the UK, Prime Minister Sir Keir Starmer has...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.979209</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does the GDPR impact the transparency and ...</td>\n",
       "      <td>[- James Kurose, â€œTestimony before the House C...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nAI ethics and transparency\\nAlgori...</td>\n",
       "      <td>The GDPR (General Data Protection Regulation) ...</td>\n",
       "      <td>The GDPR impacts the transparency and accounta...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.971650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the trends in the usage of ChatGPT fo...</td>\n",
       "      <td>[37% of messages are work-related\\nfor users w...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n37% of messages are work-related f...</td>\n",
       "      <td>The trends in the usage of ChatGPT for Technic...</td>\n",
       "      <td>The usage of ChatGPT for Technical Help has sh...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.963631</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0                               What Alexa do in AI?   \n",
       "1  What happened to the UnitedHealthcare CEO Bria...   \n",
       "2  What is the current stance of the UK governmen...   \n",
       "3  How does the GDPR impact the transparency and ...   \n",
       "4  What are the trends in the usage of ChatGPT fo...   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [What is AI, how does it work and why are some...   \n",
       "1  [AI will reconfigure how society and the econo...   \n",
       "2  [AI will reconfigure how society and the econo...   \n",
       "3  [- James Kurose, â€œTestimony before the House C...   \n",
       "4  [37% of messages are work-related\\nfor users w...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  [What is AI, how does it work and why are some...   \n",
       "1  [Why is AI controversial?\\nWhile acknowledging...   \n",
       "2  [Are there laws governing AI?\\nSome government...   \n",
       "3  [<1-hop>\\n\\nAI ethics and transparency\\nAlgori...   \n",
       "4  [<1-hop>\\n\\n37% of messages are work-related f...   \n",
       "\n",
       "                                            response  \\\n",
       "0  Alexa is a voice-controlled virtual assistant ...   \n",
       "1         I don't have enough information to answer.   \n",
       "2  The current stance of the UK government regard...   \n",
       "3  The GDPR (General Data Protection Regulation) ...   \n",
       "4  The trends in the usage of ChatGPT for Technic...   \n",
       "\n",
       "                                           reference  faithfulness  \\\n",
       "0  Alexa is a voice-controlled virtual assistant ...           1.0   \n",
       "1  The BBC reported that Apple's AI falsely told ...           0.0   \n",
       "2  In the UK, Prime Minister Sir Keir Starmer has...           1.0   \n",
       "3  The GDPR impacts the transparency and accounta...           1.0   \n",
       "4  The usage of ChatGPT for Technical Help has sh...           1.0   \n",
       "\n",
       "   answer_relevancy  context_recall  factual_correctness(mode=f1)  \n",
       "0          0.917891             1.0                          0.67  \n",
       "1          0.000000             1.0                          0.00  \n",
       "2          0.979209             1.0                          0.65  \n",
       "3          0.971650             1.0                          0.53  \n",
       "4          0.963631             0.8                          0.76  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display results from concurrent evaluation\n",
    "print(\"Naive RAG - Async Evaluation Results:\")\n",
    "print(f\"Metrics: {naive_results_async['metrics']}\\n\")\n",
    "display(naive_results_async['evaluation_df'].head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"Hybrid RAG - Async Evaluation Results:\")\n",
    "print(f\"Metrics: {hybrid_results_async['metrics']}\\n\")\n",
    "display(hybrid_results_async['evaluation_df'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06db19ce",
   "metadata": {},
   "source": [
    "## Side-by-Side Performance Comparison\n",
    "\n",
    "Let's create a comprehensive comparison of both RAG systems to understand their relative performance across all metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4f2d91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE RAG SYSTEM COMPARISON (ASYNC EVALUATION)\n",
      "================================================================================\n",
      "                      Metric  Naive RAG  Hybrid RAG  Improvement (%) Better System\n",
      "                faithfulness     0.8972      0.8636            -3.74     Naive RAG\n",
      "            answer_relevancy     0.8696      0.8679            -0.20     Naive RAG\n",
      "              context_recall     0.9100      0.9267             1.83    Hybrid RAG\n",
      "factual_correctness(mode=f1)     0.5140      0.5940            15.56    Hybrid RAG\n",
      "\n",
      "================================================================================\n",
      "\n",
      "FINAL SCORECARD:\n",
      "Hybrid RAG wins: 2 metrics\n",
      "Naive RAG wins: 2 metrics\n",
      "Ties: 0 metrics\n",
      "\n",
      "RESULT: It's a tie between both systems!\n",
      "\n",
      "Average improvement by Hybrid RAG: 3.36%\n"
     ]
    }
   ],
   "source": [
    "# Update the comparison to use async results\n",
    "import pandas as pd\n",
    "\n",
    "# Extract average metrics from async evaluations\n",
    "naive_scores_async = naive_results_async['metrics'].scores\n",
    "hybrid_scores_async = hybrid_results_async['metrics'].scores\n",
    "\n",
    "# Compute averages for each metric\n",
    "naive_df_async = pd.DataFrame(naive_scores_async)\n",
    "hybrid_df_async = pd.DataFrame(hybrid_scores_async)\n",
    "\n",
    "naive_averages_async = naive_df_async.mean()\n",
    "hybrid_averages_async = hybrid_df_async.mean()\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_data_async = {\n",
    "    'Metric': list(naive_averages_async.index),\n",
    "    'Naive RAG': list(naive_averages_async.values),\n",
    "    'Hybrid RAG': list(hybrid_averages_async.values)\n",
    "}\n",
    "\n",
    "comparison_df_async = pd.DataFrame(comparison_data_async)\n",
    "comparison_df_async['Improvement (%)'] = (\n",
    "    (comparison_df_async['Hybrid RAG'] - comparison_df_async['Naive RAG']) / \n",
    "    comparison_df_async['Naive RAG'] * 100\n",
    ").round(2)\n",
    "comparison_df_async['Better System'] = comparison_df_async.apply(\n",
    "    lambda row: 'Hybrid RAG' if row['Hybrid RAG'] > row['Naive RAG'] \n",
    "    else 'Naive RAG' if row['Naive RAG'] > row['Hybrid RAG'] \n",
    "    else 'Tie', axis=1\n",
    ")\n",
    "\n",
    "# Round scores\n",
    "comparison_df_async['Naive RAG'] = comparison_df_async['Naive RAG'].round(4)\n",
    "comparison_df_async['Hybrid RAG'] = comparison_df_async['Hybrid RAG'].round(4)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPREHENSIVE RAG SYSTEM COMPARISON (ASYNC EVALUATION)\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df_async.to_string(index=False))\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Calculate overall winner\n",
    "hybrid_wins = sum(comparison_df_async['Hybrid RAG'] > comparison_df_async['Naive RAG'])\n",
    "naive_wins = sum(comparison_df_async['Naive RAG'] > comparison_df_async['Hybrid RAG'])\n",
    "ties = sum(comparison_df_async['Naive RAG'] == comparison_df_async['Hybrid RAG'])\n",
    "\n",
    "print(f\"\\nFINAL SCORECARD:\")\n",
    "print(f\"Hybrid RAG wins: {hybrid_wins} metrics\")\n",
    "print(f\"Naive RAG wins: {naive_wins} metrics\") \n",
    "print(f\"Ties: {ties} metrics\")\n",
    "\n",
    "if hybrid_wins > naive_wins:\n",
    "    print(f\"\\nOVERALL WINNER: Hybrid RAG SuperComponent!\")\n",
    "    print(f\"   Better performance in {hybrid_wins}/{len(comparison_df_async)} metrics\")\n",
    "elif naive_wins > hybrid_wins:\n",
    "    print(f\"\\nOVERALL WINNER: Naive RAG SuperComponent!\")  \n",
    "    print(f\"   Better performance in {naive_wins}/{len(comparison_df_async)} metrics\")\n",
    "else:\n",
    "    print(f\"\\nRESULT: It's a tie between both systems!\")\n",
    "    \n",
    "avg_improvement = comparison_df_async['Improvement (%)'].mean()\n",
    "print(f\"\\nAverage improvement by Hybrid RAG: {avg_improvement:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47be3490",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (haystack-nlp-dev)",
   "language": "python",
   "name": "haystack-nlp-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
