{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75f744c9",
   "metadata": {},
   "source": [
    "ðŸ”§ **Setup Required**: Before running this notebook, please follow the [setup instructions](../README.md#setup-instructions) to configure your environment and API keys. **You will need to ensure you've executed the Indexing pipeline before completing this exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ad4fc5",
   "metadata": {},
   "source": [
    "# Systematic RAG Evaluation: Naive vs Hybrid Comparison\n",
    "\n",
    "## ðŸ“‹ Overview\n",
    "\n",
    "This notebook demonstrates a **systematic evaluation workflow** for comparing two RAG (Retrieval-Augmented Generation) approaches using **Haystack custom components**. We'll create a reproducible pipeline to:\n",
    "\n",
    "1. **Load evaluation datasets** from CSV files\n",
    "2. **Process queries** through both Naive and Hybrid RAG SuperComponents \n",
    "3. **Generate comprehensive metrics** using the RAGAS framework\n",
    "4. **Compare performance** systematically\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand how to:\n",
    "- Create reusable evaluation components for RAG systems\n",
    "- Build scalable pipelines for systematic RAG comparison\n",
    "- Interpret RAGAS metrics in comparative context\n",
    "- Make data-driven decisions between RAG approaches\n",
    "\n",
    "## ðŸ”§ Evaluation Pipeline\n",
    "\n",
    "Our pipeline consists of three main components:\n",
    "\n",
    "```\n",
    "CSV Data â†’ RAGDataAugmenter â†’ RagasEvaluation â†’ Metrics & Results\n",
    "```\n",
    "\n",
    "**Key Benefits:**\n",
    "- **Systematic**: Same evaluation conditions for both RAG systems\n",
    "- **Reproducible**: Consistent evaluation across experiments\n",
    "- **Scalable**: Easy to add new RAG implementations\n",
    "- **Comprehensive**: Multiple metrics provide complete assessment\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4640ae5",
   "metadata": {},
   "source": [
    "## Component 1: CSV Data Loader ðŸ“Š\n",
    "\n",
    "The **CSVReaderComponent** serves as the entry point for our evaluation pipeline. It handles loading synthetic evaluation datasets and ensures data quality before processing.\n",
    "\n",
    "**Key Features:**\n",
    "- **Robust Error Handling**: Validates file existence and data integrity\n",
    "- **Pandas Integration**: Returns data as DataFrame for easy manipulation\n",
    "- **Pipeline Compatible**: Designed to work seamlessly with Haystack pipelines\n",
    "\n",
    "**Input:** File path to CSV containing evaluation queries and ground truth\n",
    "**Output:** Pandas DataFrame ready for RAG processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08a83513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from haystack import component, Pipeline\n",
    "from typing import List, Optional, Dict, Any, Union\n",
    "\n",
    "@component\n",
    "class CSVReaderComponent:\n",
    "    \"\"\"Reads a CSV file into a Pandas DataFrame.\"\"\"\n",
    "\n",
    "    @component.output_types(data_frame=pd.DataFrame)\n",
    "    def run(self, source: Union[str, Path]):\n",
    "        \"\"\"\n",
    "        Reads the CSV file from the first source in the list.\n",
    "        \n",
    "        Args:\n",
    "            sources: List of file paths to CSV files. Only the first file will be processed.\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary containing the loaded DataFrame under 'data_frame' key.\n",
    "            \n",
    "        Raises:\n",
    "            FileNotFoundError: If the file doesn't exist or can't be read.\n",
    "            ValueError: If the DataFrame is empty after loading.\n",
    "        \"\"\"\n",
    "        if not source:\n",
    "            raise ValueError(\"No sources provided\")\n",
    "            \n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(source)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"File not found at {source}\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error reading CSV file {source}: {str(e)}\")\n",
    "\n",
    "        # Check if DataFrame is empty using proper pandas method\n",
    "        if df.empty:\n",
    "            raise ValueError(f\"DataFrame is empty after loading from {source}\")\n",
    "\n",
    "        print(f\"Loaded DataFrame with {len(df)} rows from {source}.\")\n",
    "        return {\"data_frame\": df}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1861dfb2",
   "metadata": {},
   "source": [
    "## Component 2: RAG Data Augmentation ðŸ”„\n",
    "\n",
    "The **RAGDataAugmenterComponent** is the core of our evaluation workflow. It takes each query from our evaluation dataset and processes it through a RAG SuperComponent, collecting both the generated responses and retrieved contexts.\n",
    "\n",
    "**ðŸ”‘ Key Design Decisions:**\n",
    "\n",
    "1. **SuperComponent Flexibility**: Accepts any pre-configured RAG SuperComponent (Naive, Hybrid, or custom)\n",
    "2. **Batch Processing**: Efficiently processes entire evaluation datasets\n",
    "3. **Data Augmentation**: Enriches the original dataset with RAG outputs for evaluation\n",
    "4. **Context Extraction**: Captures retrieved documents for context-based metrics\n",
    "\n",
    "**Pipeline Integration:**\n",
    "- **Input**: DataFrame with queries from CSVReaderComponent  \n",
    "- **Process**: Runs each query through the specified RAG SuperComponent\n",
    "- **Output**: Augmented DataFrame with responses and retrieved contexts\n",
    "\n",
    "**ðŸ’¡ Why This Approach?**\n",
    "By separating RAG execution from evaluation, we can:\n",
    "- **Swap RAG systems** without changing evaluation logic\n",
    "- **Cache RAG results** for multiple evaluation runs  \n",
    "- **Debug RAG performance** independently of metrics calculation\n",
    "- **Scale evaluation** across different datasets and configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72bf8b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import SuperComponent, super_component\n",
    "\n",
    "@component\n",
    "class RAGDataAugmenterComponent:\n",
    "    \"\"\"\n",
    "    Applies a RAG SuperComponent to each query in a DataFrame and \n",
    "    augments the data with the generated answer and retrieved contexts.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rag_supercomponent: SuperComponent):\n",
    "        # We store the pre-initialized SuperComponent\n",
    "        self.rag_supercomponent = rag_supercomponent\n",
    "        self.output_names = [\"augmented_data_frame\"]\n",
    "\n",
    "    @component.output_types(augmented_data_frame=pd.DataFrame)\n",
    "    def run(self, data_frame: pd.DataFrame):\n",
    "        \n",
    "        # New columns to store RAG results\n",
    "        answers: List[str] = []\n",
    "        contexts: List[List[str]] = []\n",
    "\n",
    "        print(f\"Running RAG SuperComponent on {len(data_frame)} queries...\")\n",
    "\n",
    "        # Iterate through the queries (user_input column)\n",
    "        for _, row in data_frame.iterrows():\n",
    "            query = row[\"user_input\"]\n",
    "            \n",
    "            # 1. Run the RAG SuperComponent\n",
    "            # It expects 'query' as input and returns a dictionary.\n",
    "            rag_output = self.rag_supercomponent.run(query=query)\n",
    "            \n",
    "            # 2. Extract answer and contexts\n",
    "            # Based on the naive_rag_sc/hybrid_rag_sc structure:\n",
    "            answer = rag_output.get('replies', [''])[0]\n",
    "            \n",
    "            # Extract content from the Document objects\n",
    "            retrieved_docs = rag_output.get('documents', [])\n",
    "            retrieved_contexts = [doc.content for doc in retrieved_docs]\n",
    "            \n",
    "            answers.append(answer)\n",
    "            contexts.append(retrieved_contexts)\n",
    "        \n",
    "        # 3. Augment the DataFrame\n",
    "        data_frame['response'] = answers\n",
    "        data_frame['retrieved_contexts'] = contexts\n",
    "        \n",
    "        print(\"RAG processing complete.\")\n",
    "        return {\"augmented_data_frame\": data_frame}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272c9cff",
   "metadata": {},
   "source": [
    "## Component 3: RAGAS Evaluation Engine ðŸ“ˆ\n",
    "\n",
    "The **RagasEvaluationComponent** integrates the RAGAS framework into our Haystack pipeline, providing systematic evaluation metrics for RAG systems.\n",
    "\n",
    "**ðŸŽ¯ Core Evaluation Metrics:**\n",
    "\n",
    "| Metric | Purpose | What It Measures |\n",
    "|--------|---------|------------------|\n",
    "| **Faithfulness** | Response Quality | Factual consistency with retrieved context |\n",
    "| **ResponseRelevancy** | Relevance | How well responses answer the questions |\n",
    "| **LLMContextRecall** | Retrieval Quality | How well retrieval captures relevant information |\n",
    "| **FactualCorrectness** | Accuracy | Correctness of factual claims in responses |\n",
    "\n",
    "**ðŸ”§ Technical Implementation:**\n",
    "- **Focused Metrics**: Core metrics for reliable comparison\n",
    "- **LLM Integration**: Uses OpenAI GPT models for evaluation judgments  \n",
    "- **Data Format Handling**: Automatically formats data for RAGAS requirements\n",
    "- **Comprehensive Output**: Returns both aggregated metrics and detailed per-query results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5b842a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import EvaluationDataset, evaluate\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, ResponseRelevancy\n",
    "\n",
    "from ragas.llms import llm_factory\n",
    "from haystack.utils import Secret\n",
    "import os\n",
    "from ragas.llms import HaystackLLMWrapper\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "\n",
    "@component\n",
    "class RagasEvaluationComponent:\n",
    "    \"\"\"\n",
    "    Prepares data for Ragas, runs the evaluation, and returns the metrics.\n",
    "    Simplified for core metrics comparison.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 metrics: Optional[List[Any]] = None,\n",
    "                 llm_model: str = \"gpt-4o-mini\",\n",
    "                 openai_api_key: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Initialize the RagasEvaluationComponent.\n",
    "        \n",
    "        Args:\n",
    "            metrics: List of RAGAS metrics to evaluate (defaults to core metrics)\n",
    "            llm_model (str): OpenAI model for evaluation. Defaults to \"gpt-4o-mini\".\n",
    "            openai_api_key (Optional[str]): OpenAI API key. If None, will use environment variable.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Default to core metrics for systematic comparison\n",
    "        if metrics is None:\n",
    "            self.metrics = [\n",
    "                Faithfulness(), \n",
    "                ResponseRelevancy(),\n",
    "                LLMContextRecall(),\n",
    "                FactualCorrectness()\n",
    "            ]\n",
    "        else:\n",
    "            self.metrics = metrics\n",
    "            \n",
    "        self.llm_model = llm_model\n",
    "        self.openai_api_key = openai_api_key or os.getenv('OPENAI_API_KEY')\n",
    "        \n",
    "        if not self.openai_api_key:\n",
    "            raise ValueError(\"OpenAI API key not found. Please set OPENAI_API_KEY environment variable or pass openai_api_key parameter.\")\n",
    "        \n",
    "        # Configure RAGAS LLM for evaluation\n",
    "        self.ragas_llm = HaystackLLMWrapper(\n",
    "            OpenAIGenerator(\n",
    "                model=self.llm_model,\n",
    "                api_key=Secret.from_token(self.openai_api_key)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    @component.output_types(metrics=Dict[str, float], evaluation_df=pd.DataFrame)\n",
    "    def run(self, augmented_data_frame: pd.DataFrame):\n",
    "        \n",
    "        # 1. Map columns to Ragas requirements\n",
    "        ragas_data = pd.DataFrame({\n",
    "            'user_input': augmented_data_frame['user_input'],\n",
    "            'response': augmented_data_frame['response'], \n",
    "            'retrieved_contexts': augmented_data_frame['retrieved_contexts'],\n",
    "            'reference': augmented_data_frame['reference'],\n",
    "            'reference_contexts': augmented_data_frame['reference_contexts'].apply(eval)\n",
    "        })\n",
    "\n",
    "        print(\"Creating Ragas EvaluationDataset...\")\n",
    "        # 2. Create EvaluationDataset\n",
    "        dataset = EvaluationDataset.from_pandas(ragas_data)\n",
    "\n",
    "        print(\"Starting Ragas evaluation...\")\n",
    "        \n",
    "        # 3. Run Ragas Evaluation\n",
    "        results = evaluate(\n",
    "            dataset=dataset,\n",
    "            metrics=self.metrics,\n",
    "            llm=self.ragas_llm\n",
    "        )\n",
    "        \n",
    "        results_df = results.to_pandas()\n",
    "        \n",
    "        print(\"Ragas evaluation complete.\")\n",
    "        print(f\"Overall metrics: {results}\")\n",
    "        \n",
    "        return {\"metrics\": results, \"evaluation_df\": results_df}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b0aa01",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ§ª Systematic RAG Comparison: Naive vs Hybrid\n",
    "\n",
    "Now we'll systematically evaluate both RAG SuperComponents using the same evaluation pipeline. This ensures fair comparison with identical evaluation conditions.\n",
    "\n",
    "## ðŸŽ¯ Comparison Strategy\n",
    "\n",
    "Our approach enables **systematic comparison**:\n",
    "\n",
    "1. **Same Dataset**: Both systems evaluated on identical test queries\n",
    "2. **Same Metrics**: Consistent evaluation criteria across both approaches\n",
    "3. **Same Pipeline**: Identical processing workflow eliminates bias\n",
    "4. **Reproducible Results**: Pipeline ensures consistent evaluation conditions\n",
    "\n",
    "## ðŸ“Š Dataset Information\n",
    "\n",
    "We'll use a synthetic evaluation dataset:\n",
    "- **`synthetic_tests_advanced_branching_2.csv`**: Focused dataset for comparison\n",
    "\n",
    "**Dataset Structure:**\n",
    "- `user_input`: Questions to ask the RAG system\n",
    "- `reference`: Ground truth answers for comparison\n",
    "- `reference_contexts`: Expected retrieved contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0384e71c",
   "metadata": {},
   "source": [
    "### Setup: Initialize Both RAG SuperComponents\n",
    "\n",
    "First, we'll initialize both RAG SuperComponents with consistent parameters for fair comparison.\n",
    "\n",
    "**ðŸ”§ Configuration:**\n",
    "- **Same base parameters**: Both systems use identical core settings\n",
    "- **Document store**: Shared Elasticsearch document store\n",
    "- **Models**: Consistent LLM and embedding models for both systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a23faff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Both RAG SuperComponents initialized successfully!\n",
      "ðŸ“Š Naive RAG: NaiveRAGSuperComponent\n",
      "ðŸ“Š Hybrid RAG: HybridRAGSuperComponent\n"
     ]
    }
   ],
   "source": [
    "# --- Setup Environment & Dependencies ---\n",
    "from scripts.rag.hybridrag import HybridRAGSuperComponent\n",
    "from scripts.rag.naiverag import NaiveRAGSuperComponent\n",
    "from haystack_integrations.document_stores.elasticsearch import ElasticsearchDocumentStore\n",
    "import os\n",
    "\n",
    "# Initialize document store\n",
    "document_store = ElasticsearchDocumentStore(hosts=\"http://localhost:9200\")\n",
    "\n",
    "# Create both RAG SuperComponents with base parameters for fair comparison\n",
    "naive_rag_sc = NaiveRAGSuperComponent(\n",
    "    document_store=document_store\n",
    ")\n",
    "\n",
    "hybrid_rag_sc = HybridRAGSuperComponent(\n",
    "    document_store=document_store\n",
    ")\n",
    "\n",
    "print(\"âœ… Both RAG SuperComponents initialized successfully!\")\n",
    "print(f\"ðŸ“Š Naive RAG: {naive_rag_sc.__class__.__name__}\")\n",
    "print(f\"ðŸ“Š Hybrid RAG: {hybrid_rag_sc.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd5bfc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create RAG Evaluation SuperComponent for Systematic Comparison ---\n",
    "\n",
    "@super_component\n",
    "class RAGEvaluationSuperComponent:\n",
    "    def __init__(self, \n",
    "                 rag_supercomponent, \n",
    "                 system_name: str,\n",
    "                 llm_model: str = \"gpt-4o-mini\",\n",
    "                 openai_api_key: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Initialize the RAG Evaluation SuperComponent.\n",
    "        \n",
    "        Args:\n",
    "            rag_supercomponent: The RAG system to evaluate\n",
    "            system_name (str): Name for logging and identification\n",
    "            llm_model (str): OpenAI model for evaluation. Defaults to \"gpt-4o-mini\".\n",
    "            openai_api_key (Optional[str]): OpenAI API key. If None, will use environment variable.\n",
    "        \"\"\"\n",
    "        self.rag_supercomponent = rag_supercomponent\n",
    "        self.system_name = system_name\n",
    "        self.llm_model = llm_model\n",
    "        self.openai_api_key = openai_api_key or os.getenv('OPENAI_API_KEY')\n",
    "        \n",
    "        if not self.openai_api_key:\n",
    "            raise ValueError(\"OpenAI API key not found. Please set OPENAI_API_KEY environment variable or pass openai_api_key parameter.\")\n",
    "        \n",
    "        self._build_pipeline()\n",
    "    \n",
    "    def _build_pipeline(self):\n",
    "        \"\"\"Build the RAG evaluation pipeline with initialized components.\"\"\"\n",
    "        \n",
    "        print(f\"\\nðŸ”„ Building evaluation pipeline for {self.system_name}...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # --- 1. Initialize Evaluation Pipeline Components ---\n",
    "        \n",
    "        # CSV Reader: Loads evaluation dataset\n",
    "        reader = CSVReaderComponent()\n",
    "        \n",
    "        # RAG Data Augmenter: Processes queries through the RAG system\n",
    "        augmenter = RAGDataAugmenterComponent(rag_supercomponent=self.rag_supercomponent)\n",
    "        \n",
    "        # RAGAS Evaluator: Computes evaluation metrics\n",
    "        evaluator = RagasEvaluationComponent(\n",
    "            llm_model=self.llm_model,\n",
    "            openai_api_key=self.openai_api_key\n",
    "        )\n",
    "        \n",
    "        # --- 2. Build the Evaluation Pipeline ---\n",
    "        self.pipeline = Pipeline()\n",
    "        \n",
    "        # Add all components to the pipeline\n",
    "        self.pipeline.add_component(\"reader\", reader)\n",
    "        self.pipeline.add_component(\"augmenter\", augmenter)\n",
    "        self.pipeline.add_component(\"evaluator\", evaluator)\n",
    "        \n",
    "        # --- 3. Connect the Components in a Graph ---\n",
    "        \n",
    "        # CSV Data -> RAG Augmentation -> RAGAS Evaluation\n",
    "        self.pipeline.connect(\"reader.data_frame\", \"augmenter.data_frame\")\n",
    "        self.pipeline.connect(\"augmenter.augmented_data_frame\", \"evaluator.augmented_data_frame\")\n",
    "        \n",
    "        # --- 4. Define Input and Output Mappings ---\n",
    "        self.input_mapping = {\n",
    "            \"csv_source\": [\"reader.source\"]\n",
    "        }\n",
    "\n",
    "        self.output_mapping = {\n",
    "            \"evaluator.metrics\": \"metrics\",\n",
    "            \"evaluator.evaluation_df\": \"evaluation_df\"\n",
    "        }\n",
    "        \n",
    "        print(f\"âœ… Evaluation pipeline for {self.system_name} built successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08342b27",
   "metadata": {},
   "source": [
    "## Experiment 1: Naive RAG Evaluation ðŸ”¬\n",
    "\n",
    "Let's evaluate the Naive RAG SuperComponent first. This will establish our baseline performance metrics.\n",
    "\n",
    "**ðŸŽ¯ What to Observe:**\n",
    "- Processing time and efficiency\n",
    "- Core metric scores (Faithfulness, Relevancy, Context Recall, Factual Correctness)\n",
    "- Any errors or warnings during evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a9d17a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”„ Building evaluation pipeline for Naive RAG...\n",
      "==================================================\n",
      "âœ… Evaluation pipeline for Naive RAG built successfully!\n",
      "Loaded DataFrame with 10 rows from data_for_eval/synthetic_tests_advanced_branching_10.csv.\n",
      "Running RAG SuperComponent on 10 queries...\n",
      "RAG processing complete.\n",
      "Creating Ragas EvaluationDataset...\n",
      "Starting Ragas evaluation...\n",
      "RAG processing complete.\n",
      "Creating Ragas EvaluationDataset...\n",
      "Starting Ragas evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3791dfcb18a459f93481d584eae2385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Exception raised in Job[24]: TimeoutError()\n",
      "Exception raised in Job[27]: TimeoutError()\n",
      "Exception raised in Job[24]: TimeoutError()\n",
      "Exception raised in Job[27]: TimeoutError()\n",
      "Exception raised in Job[36]: TimeoutError()\n",
      "Exception raised in Job[36]: TimeoutError()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ragas evaluation complete.\n",
      "Overall metrics: {'faithfulness': 0.6411, 'answer_relevancy': 0.6678, 'context_recall': 0.6800, 'factual_correctness(mode=f1)': 0.3556}\n",
      "âœ… Naive RAG evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Naive RAG SuperComponent\n",
    "csv_file_path = \"data_for_eval/synthetic_tests_advanced_branching_10.csv\"\n",
    "\n",
    "# Create evaluation SuperComponent (csv_source now passed in run method)\n",
    "evaluation_sc = RAGEvaluationSuperComponent(\n",
    "    rag_supercomponent=naive_rag_sc,\n",
    "    system_name=\"Naive RAG\"\n",
    ")\n",
    "\n",
    "# Run evaluation with CSV source passed to run method\n",
    "naive_results = evaluation_sc.run(csv_source=csv_file_path)\n",
    "\n",
    "print(f\"âœ… Naive RAG evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b965d00",
   "metadata": {},
   "source": [
    "### Naive RAG Results Analysis ðŸ“Š\n",
    "\n",
    "Let's examine the detailed evaluation results from our Naive RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c9ce4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Naive RAG - Detailed Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>factual_correctness(mode=f1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wut is Meta AI and why do peeple have concerns...</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>Meta AI refers to the artificial intelligence ...</td>\n",
       "      <td>Meta AI is a generative AI tool that can hold ...</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.924937</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the implications of AI technology on ...</td>\n",
       "      <td>[In the same vein, the IEEE Global Initiative ...</td>\n",
       "      <td>[Why is AI controversial?\\nWhile acknowledging...</td>\n",
       "      <td>I don't have enough information to answer.</td>\n",
       "      <td>The controversy surrounding AI technology incl...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What does the Artificial Intelligence Act regu...</td>\n",
       "      <td>[AI will reconfigure how society and the econo...</td>\n",
       "      <td>[Are there laws governing AI?\\nSome government...</td>\n",
       "      <td>The Artificial Intelligence Act regulates high...</td>\n",
       "      <td>The EU's Artificial Intelligence Act places co...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What steps are federal officials considering t...</td>\n",
       "      <td>[A body of case law has shown that the situati...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nCreate a federal AI advisory commi...</td>\n",
       "      <td>Federal officials are considering several step...</td>\n",
       "      <td>Federal officials are considering the creation...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977361</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the trends in user satisfaction regar...</td>\n",
       "      <td>[The yellow line represents the first cohort o...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n5.5 Quality of Interactions\\nWe ad...</td>\n",
       "      <td>I don't have enough information to answer rega...</td>\n",
       "      <td>User satisfaction regarding Technical Help int...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How does the quality of interactions in Techni...</td>\n",
       "      <td>[Figure 9 disaggregates four of the seven Conv...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n5.5 Quality of Interactions\\nWe ad...</td>\n",
       "      <td>I don't have enough information to answer.</td>\n",
       "      <td>The quality of interactions in Technical Help ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What are the ethical implications and environm...</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nWhat is AI, how does it work and w...</td>\n",
       "      <td>The rapid growth of artificial intelligence (A...</td>\n",
       "      <td>The rapid growth of artificial intelligence (A...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.981306</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How do the user message classifications relate...</td>\n",
       "      <td>[Teams, Enterprise, Education), which we do no...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nThe yellow line represents the fir...</td>\n",
       "      <td>The user message classifications highlight a s...</td>\n",
       "      <td>The user message classifications indicate that...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.911098</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What trends can be observed in the user messag...</td>\n",
       "      <td>[Teams, Enterprise, Education), which we do no...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nThe yellow line represents the fir...</td>\n",
       "      <td>The trends observed in the user message classi...</td>\n",
       "      <td>The trends observed in the user message classi...</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.945481</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>In what ways do the challenges of data access ...</td>\n",
       "      <td>[Smart cities\\nMetropolitan governments are us...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nSmart cities\\nMetropolitan governm...</td>\n",
       "      <td>The challenges of data access problems signifi...</td>\n",
       "      <td>The challenges of data access problems signifi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.937816</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  Wut is Meta AI and why do peeple have concerns...   \n",
       "1  What are the implications of AI technology on ...   \n",
       "2  What does the Artificial Intelligence Act regu...   \n",
       "3  What steps are federal officials considering t...   \n",
       "4  What are the trends in user satisfaction regar...   \n",
       "5  How does the quality of interactions in Techni...   \n",
       "6  What are the ethical implications and environm...   \n",
       "7  How do the user message classifications relate...   \n",
       "8  What trends can be observed in the user messag...   \n",
       "9  In what ways do the challenges of data access ...   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [What is AI, how does it work and why are some...   \n",
       "1  [In the same vein, the IEEE Global Initiative ...   \n",
       "2  [AI will reconfigure how society and the econo...   \n",
       "3  [A body of case law has shown that the situati...   \n",
       "4  [The yellow line represents the first cohort o...   \n",
       "5  [Figure 9 disaggregates four of the seven Conv...   \n",
       "6  [What is AI, how does it work and why are some...   \n",
       "7  [Teams, Enterprise, Education), which we do no...   \n",
       "8  [Teams, Enterprise, Education), which we do no...   \n",
       "9  [Smart cities\\nMetropolitan governments are us...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  [What is AI, how does it work and why are some...   \n",
       "1  [Why is AI controversial?\\nWhile acknowledging...   \n",
       "2  [Are there laws governing AI?\\nSome government...   \n",
       "3  [<1-hop>\\n\\nCreate a federal AI advisory commi...   \n",
       "4  [<1-hop>\\n\\n5.5 Quality of Interactions\\nWe ad...   \n",
       "5  [<1-hop>\\n\\n5.5 Quality of Interactions\\nWe ad...   \n",
       "6  [<1-hop>\\n\\nWhat is AI, how does it work and w...   \n",
       "7  [<1-hop>\\n\\nThe yellow line represents the fir...   \n",
       "8  [<1-hop>\\n\\nThe yellow line represents the fir...   \n",
       "9  [<1-hop>\\n\\nSmart cities\\nMetropolitan governm...   \n",
       "\n",
       "                                            response  \\\n",
       "0  Meta AI refers to the artificial intelligence ...   \n",
       "1         I don't have enough information to answer.   \n",
       "2  The Artificial Intelligence Act regulates high...   \n",
       "3  Federal officials are considering several step...   \n",
       "4  I don't have enough information to answer rega...   \n",
       "5         I don't have enough information to answer.   \n",
       "6  The rapid growth of artificial intelligence (A...   \n",
       "7  The user message classifications highlight a s...   \n",
       "8  The trends observed in the user message classi...   \n",
       "9  The challenges of data access problems signifi...   \n",
       "\n",
       "                                           reference  faithfulness  \\\n",
       "0  Meta AI is a generative AI tool that can hold ...      0.928571   \n",
       "1  The controversy surrounding AI technology incl...      0.000000   \n",
       "2  The EU's Artificial Intelligence Act places co...      0.500000   \n",
       "3  Federal officials are considering the creation...      1.000000   \n",
       "4  User satisfaction regarding Technical Help int...      1.000000   \n",
       "5  The quality of interactions in Technical Help ...      0.000000   \n",
       "6  The rapid growth of artificial intelligence (A...           NaN   \n",
       "7  The user message classifications indicate that...      1.000000   \n",
       "8  The trends observed in the user message classi...      0.700000   \n",
       "9  The challenges of data access problems signifi...           NaN   \n",
       "\n",
       "   answer_relevancy  context_recall  factual_correctness(mode=f1)  \n",
       "0          0.924937             1.0                          0.60  \n",
       "1          0.000000             0.0                          0.00  \n",
       "2          1.000000             1.0                          0.80  \n",
       "3          0.977361             1.0                          0.40  \n",
       "4          0.000000             0.0                          0.00  \n",
       "5          0.000000             0.0                          0.00  \n",
       "6          0.981306             1.0                           NaN  \n",
       "7          0.911098             0.8                          0.58  \n",
       "8          0.945481             1.0                          0.50  \n",
       "9          0.937816             1.0                          0.32  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display Naive RAG detailed results\n",
    "print(\"ðŸ“Š Naive RAG - Detailed Results:\")\n",
    "naive_results['evaluation_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61bd01d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Naive RAG - Summary Metrics:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'faithfulness': 0.6411, 'answer_relevancy': 0.6678, 'context_recall': 0.6800, 'factual_correctness(mode=f1)': 0.3556}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display Naive RAG summary metrics\n",
    "print(\"ðŸ“ˆ Naive RAG - Summary Metrics:\")\n",
    "naive_metrics = naive_results['metrics']\n",
    "naive_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8308cf",
   "metadata": {},
   "source": [
    "## Experiment 2: Hybrid RAG Evaluation ðŸ”¬âš¡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1db6e1",
   "metadata": {},
   "source": [
    "Now let's evaluate the **Hybrid RAG SuperComponent** using the identical evaluation pipeline. This systematic approach ensures fair comparison.\n",
    "\n",
    "**ðŸ”„ Key Benefits of This Approach:**\n",
    "- **Identical Conditions**: Same pipeline, metrics, and dataset\n",
    "- **Systematic Comparison**: Eliminates evaluation bias\n",
    "- **Reproducible Results**: Consistent methodology across both systems\n",
    "\n",
    "**ðŸŽ¯ Expected Improvements:**\n",
    "Hybrid RAG typically shows better performance due to:\n",
    "- **Dense + Sparse Retrieval**: Combines semantic and keyword-based search\n",
    "- **Enhanced Context Quality**: Better retrieval often leads to better responses\n",
    "- **Improved Robustness**: Multiple retrieval methods reduce failure modes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60268ad4",
   "metadata": {},
   "source": [
    "# Evaluate Hybrid RAG SuperComponent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff5f12ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”„ Building evaluation pipeline for Hybrid RAG...\n",
      "==================================================\n",
      "âœ… Evaluation pipeline for Hybrid RAG built successfully!\n",
      "Loaded DataFrame with 10 rows from data_for_eval/synthetic_tests_advanced_branching_10.csv.\n",
      "Running RAG SuperComponent on 10 queries...\n",
      "RAG processing complete.\n",
      "Creating Ragas EvaluationDataset...\n",
      "Starting Ragas evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7014f73dd41b431fad0daaf69a8a7453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ragas evaluation complete.\n",
      "Overall metrics: {'faithfulness': 0.9626, 'answer_relevancy': 0.7374, 'context_recall': 0.7633, 'factual_correctness(mode=f1)': 0.4090}\n",
      "âœ… Hybrid RAG evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Hybrid RAG SuperComponent\n",
    "\n",
    "# Create evaluation SuperComponent (csv_source now passed in run method)\n",
    "hybrid_evaluation_sc = RAGEvaluationSuperComponent(\n",
    "    rag_supercomponent=hybrid_rag_sc,\n",
    "    system_name=\"Hybrid RAG\"\n",
    ")\n",
    "\n",
    "# Run evaluation with CSV source passed to run method\n",
    "hybrid_results = hybrid_evaluation_sc.run(csv_source=csv_file_path)\n",
    "\n",
    "print(f\"âœ… Hybrid RAG evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fb55e6",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f152688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Hybrid RAG - Detailed Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>factual_correctness(mode=f1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wut is Meta AI and why do peeple have concerns...</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>Meta AI is a generative AI tool developed by M...</td>\n",
       "      <td>Meta AI is a generative AI tool that can hold ...</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.889708</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the implications of AI technology on ...</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>[Why is AI controversial?\\nWhile acknowledging...</td>\n",
       "      <td>I don't have enough information to answer.</td>\n",
       "      <td>The controversy surrounding AI technology incl...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What does the Artificial Intelligence Act regu...</td>\n",
       "      <td>[- James Kurose, â€œTestimony before the House C...</td>\n",
       "      <td>[Are there laws governing AI?\\nSome government...</td>\n",
       "      <td>The \"Future of Artificial Intelligence Act\" ai...</td>\n",
       "      <td>The EU's Artificial Intelligence Act places co...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.908452</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What steps are federal officials considering t...</td>\n",
       "      <td>[AI will reconfigure how society and the econo...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nCreate a federal AI advisory commi...</td>\n",
       "      <td>Federal officials are considering the establis...</td>\n",
       "      <td>Federal officials are considering the creation...</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.912998</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the trends in user satisfaction regar...</td>\n",
       "      <td>[Figure 9 disaggregates four of the seven Conv...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n5.5 Quality of Interactions\\nWe ad...</td>\n",
       "      <td>The trends in user satisfaction regarding Tech...</td>\n",
       "      <td>User satisfaction regarding Technical Help int...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.944705</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How does the quality of interactions in Techni...</td>\n",
       "      <td>[Overall, the majority of ChatGPT usage\\nat wo...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n5.5 Quality of Interactions\\nWe ad...</td>\n",
       "      <td>I don't have enough information to answer.</td>\n",
       "      <td>The quality of interactions in Technical Help ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What are the ethical implications and environm...</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nWhat is AI, how does it work and w...</td>\n",
       "      <td>Experts have raised several ethical implicatio...</td>\n",
       "      <td>The rapid growth of artificial intelligence (A...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.979982</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How do the user message classifications relate...</td>\n",
       "      <td>[Teams, Enterprise, Education), which we do no...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nThe yellow line represents the fir...</td>\n",
       "      <td>The user message classifications reveal signif...</td>\n",
       "      <td>The user message classifications indicate that...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.875714</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What trends can be observed in the user messag...</td>\n",
       "      <td>[Teams, Enterprise, Education), which we do no...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nThe yellow line represents the fir...</td>\n",
       "      <td>The trends observed in the user message classi...</td>\n",
       "      <td>The trends observed in the user message classi...</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.924762</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>In what ways do the challenges of data access ...</td>\n",
       "      <td>[AI will reconfigure how society and the econo...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nSmart cities\\nMetropolitan governm...</td>\n",
       "      <td>The challenges of data access problems signifi...</td>\n",
       "      <td>The challenges of data access problems signifi...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.937708</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  Wut is Meta AI and why do peeple have concerns...   \n",
       "1  What are the implications of AI technology on ...   \n",
       "2  What does the Artificial Intelligence Act regu...   \n",
       "3  What steps are federal officials considering t...   \n",
       "4  What are the trends in user satisfaction regar...   \n",
       "5  How does the quality of interactions in Techni...   \n",
       "6  What are the ethical implications and environm...   \n",
       "7  How do the user message classifications relate...   \n",
       "8  What trends can be observed in the user messag...   \n",
       "9  In what ways do the challenges of data access ...   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [What is AI, how does it work and why are some...   \n",
       "1  [What is AI, how does it work and why are some...   \n",
       "2  [- James Kurose, â€œTestimony before the House C...   \n",
       "3  [AI will reconfigure how society and the econo...   \n",
       "4  [Figure 9 disaggregates four of the seven Conv...   \n",
       "5  [Overall, the majority of ChatGPT usage\\nat wo...   \n",
       "6  [What is AI, how does it work and why are some...   \n",
       "7  [Teams, Enterprise, Education), which we do no...   \n",
       "8  [Teams, Enterprise, Education), which we do no...   \n",
       "9  [AI will reconfigure how society and the econo...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  [What is AI, how does it work and why are some...   \n",
       "1  [Why is AI controversial?\\nWhile acknowledging...   \n",
       "2  [Are there laws governing AI?\\nSome government...   \n",
       "3  [<1-hop>\\n\\nCreate a federal AI advisory commi...   \n",
       "4  [<1-hop>\\n\\n5.5 Quality of Interactions\\nWe ad...   \n",
       "5  [<1-hop>\\n\\n5.5 Quality of Interactions\\nWe ad...   \n",
       "6  [<1-hop>\\n\\nWhat is AI, how does it work and w...   \n",
       "7  [<1-hop>\\n\\nThe yellow line represents the fir...   \n",
       "8  [<1-hop>\\n\\nThe yellow line represents the fir...   \n",
       "9  [<1-hop>\\n\\nSmart cities\\nMetropolitan governm...   \n",
       "\n",
       "                                            response  \\\n",
       "0  Meta AI is a generative AI tool developed by M...   \n",
       "1         I don't have enough information to answer.   \n",
       "2  The \"Future of Artificial Intelligence Act\" ai...   \n",
       "3  Federal officials are considering the establis...   \n",
       "4  The trends in user satisfaction regarding Tech...   \n",
       "5         I don't have enough information to answer.   \n",
       "6  Experts have raised several ethical implicatio...   \n",
       "7  The user message classifications reveal signif...   \n",
       "8  The trends observed in the user message classi...   \n",
       "9  The challenges of data access problems signifi...   \n",
       "\n",
       "                                           reference  faithfulness  \\\n",
       "0  Meta AI is a generative AI tool that can hold ...      0.928571   \n",
       "1  The controversy surrounding AI technology incl...      1.000000   \n",
       "2  The EU's Artificial Intelligence Act places co...      1.000000   \n",
       "3  Federal officials are considering the creation...      0.928571   \n",
       "4  User satisfaction regarding Technical Help int...      1.000000   \n",
       "5  The quality of interactions in Technical Help ...      1.000000   \n",
       "6  The rapid growth of artificial intelligence (A...      1.000000   \n",
       "7  The user message classifications indicate that...      1.000000   \n",
       "8  The trends observed in the user message classi...      0.769231   \n",
       "9  The challenges of data access problems signifi...      1.000000   \n",
       "\n",
       "   answer_relevancy  context_recall  factual_correctness(mode=f1)  \n",
       "0          0.889708        1.000000                          0.48  \n",
       "1          0.000000        0.500000                          0.00  \n",
       "2          0.908452        0.500000                          0.00  \n",
       "3          0.912998        0.500000                          0.50  \n",
       "4          0.944705        0.666667                          0.62  \n",
       "5          0.000000        0.666667                          0.00  \n",
       "6          0.979982        1.000000                          0.62  \n",
       "7          0.875714        0.800000                          0.52  \n",
       "8          0.924762        1.000000                          0.89  \n",
       "9          0.937708        1.000000                          0.46  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display Hybrid RAG detailed results\n",
    "print(\"ðŸ“Š Hybrid RAG - Detailed Results:\")\n",
    "hybrid_results['evaluation_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd0607d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Hybrid RAG - Summary Metrics:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'faithfulness': 0.9626, 'answer_relevancy': 0.7374, 'context_recall': 0.7633, 'factual_correctness(mode=f1)': 0.4090}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display Hybrid RAG summary metrics\n",
    "print(\"ðŸ“ˆ Hybrid RAG - Summary Metrics:\")\n",
    "hybrid_metrics = hybrid_results['metrics']\n",
    "hybrid_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f64eeb",
   "metadata": {},
   "source": [
    "## ðŸ“Š Side-by-Side Performance Comparison\n",
    "\n",
    "Let's create a comprehensive comparison of both RAG systems to understand their relative performance across all metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "787c31f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Processing individual query scores...\n",
      "Naive RAG: 10 queries evaluated\n",
      "Hybrid RAG: 10 queries evaluated\n",
      "\n",
      "ðŸ“Š Average Scores Computed:\n",
      "Naive RAG averages: {'faithfulness': np.float64(0.6410714285714286), 'answer_relevancy': np.float64(0.6678000145616547), 'context_recall': np.float64(0.6799999999999999), 'factual_correctness(mode=f1)': np.float64(0.35555555555555557)}\n",
      "Hybrid RAG averages: {'faithfulness': np.float64(0.9626373626373628), 'answer_relevancy': np.float64(0.7374028997143484), 'context_recall': np.float64(0.7633333333333333), 'factual_correctness(mode=f1)': np.float64(0.4090000000000001)}\n",
      "\n",
      "================================================================================\n",
      "ðŸ” COMPREHENSIVE RAG SYSTEM COMPARISON\n",
      "================================================================================\n",
      "                      Metric  Naive RAG  Hybrid RAG  Improvement (%) Better System\n",
      "                faithfulness     0.6411      0.9626            50.16  ðŸ† Hybrid RAG\n",
      "            answer_relevancy     0.6678      0.7374            10.42  ðŸ† Hybrid RAG\n",
      "              context_recall     0.6800      0.7633            12.25  ðŸ† Hybrid RAG\n",
      "factual_correctness(mode=f1)     0.3556      0.4090            15.03  ðŸ† Hybrid RAG\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ðŸ FINAL SCORECARD:\n",
      "ðŸ† Hybrid RAG wins: 4 metrics\n",
      "ðŸ† Naive RAG wins: 0 metrics\n",
      "ðŸ¤ Ties: 0 metrics\n",
      "\n",
      "ðŸŽ‰ OVERALL WINNER: Hybrid RAG SuperComponent!\n",
      "   Better performance in 4/4 metrics\n",
      "\n",
      "ðŸ“ˆ Average improvement by Hybrid RAG: 21.96%\n",
      "\n",
      "ðŸ“Š Score Consistency (Standard Deviation):\n",
      "   faithfulness:\n",
      "     Naive RAG: 0.4333\n",
      "     Hybrid RAG: 0.0742\n",
      "     More consistent: Hybrid RAG\n",
      "   answer_relevancy:\n",
      "     Naive RAG: 0.4616\n",
      "     Hybrid RAG: 0.3897\n",
      "     More consistent: Hybrid RAG\n",
      "   context_recall:\n",
      "     Naive RAG: 0.4733\n",
      "     Hybrid RAG: 0.2241\n",
      "     More consistent: Hybrid RAG\n",
      "   factual_correctness(mode=f1):\n",
      "     Naive RAG: 0.2981\n",
      "     Hybrid RAG: 0.3074\n",
      "     More consistent: Naive RAG\n"
     ]
    }
   ],
   "source": [
    "# --- Create Side-by-Side Performance Comparison ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Extract average metrics from both evaluations using proper RAGAS data structure methods\n",
    "# The EvaluationResult object contains individual scores for each query\n",
    "naive_scores = naive_metrics.scores\n",
    "hybrid_scores = hybrid_metrics.scores\n",
    "\n",
    "print(\"ðŸ” Processing individual query scores...\")\n",
    "print(f\"Naive RAG: {len(naive_scores)} queries evaluated\")\n",
    "print(f\"Hybrid RAG: {len(hybrid_scores)} queries evaluated\")\n",
    "\n",
    "# Compute averages for each metric using pandas for easier calculation\n",
    "naive_df = pd.DataFrame(naive_scores)\n",
    "hybrid_df = pd.DataFrame(hybrid_scores)\n",
    "\n",
    "# Calculate mean values for each metric\n",
    "naive_averages = naive_df.mean()\n",
    "hybrid_averages = hybrid_df.mean()\n",
    "\n",
    "print(\"\\nðŸ“Š Average Scores Computed:\")\n",
    "print(f\"Naive RAG averages: {dict(naive_averages)}\")\n",
    "print(f\"Hybrid RAG averages: {dict(hybrid_averages)}\")\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_data = {\n",
    "    'Metric': list(naive_averages.index),\n",
    "    'Naive RAG': list(naive_averages.values),\n",
    "    'Hybrid RAG': list(hybrid_averages.values)\n",
    "}\n",
    "\n",
    "# Calculate improvement percentages\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df['Improvement (%)'] = ((comparison_df['Hybrid RAG'] - comparison_df['Naive RAG']) / comparison_df['Naive RAG'] * 100).round(2)\n",
    "comparison_df['Better System'] = comparison_df.apply(\n",
    "    lambda row: 'ðŸ† Hybrid RAG' if row['Hybrid RAG'] > row['Naive RAG'] \n",
    "    else 'ðŸ† Naive RAG' if row['Naive RAG'] > row['Hybrid RAG'] \n",
    "    else 'ðŸ¤ Tie', axis=1\n",
    ")\n",
    "\n",
    "# Round scores for better display\n",
    "comparison_df['Naive RAG'] = comparison_df['Naive RAG'].round(4)\n",
    "comparison_df['Hybrid RAG'] = comparison_df['Hybrid RAG'].round(4)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸ” COMPREHENSIVE RAG SYSTEM COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Calculate overall winner\n",
    "hybrid_wins = sum(comparison_df['Hybrid RAG'] > comparison_df['Naive RAG'])\n",
    "naive_wins = sum(comparison_df['Naive RAG'] > comparison_df['Hybrid RAG'])\n",
    "ties = sum(comparison_df['Naive RAG'] == comparison_df['Hybrid RAG'])\n",
    "\n",
    "print(f\"\\nðŸ FINAL SCORECARD:\")\n",
    "print(f\"ðŸ† Hybrid RAG wins: {hybrid_wins} metrics\")\n",
    "print(f\"ðŸ† Naive RAG wins: {naive_wins} metrics\") \n",
    "print(f\"ðŸ¤ Ties: {ties} metrics\")\n",
    "\n",
    "if hybrid_wins > naive_wins:\n",
    "    print(f\"\\nðŸŽ‰ OVERALL WINNER: Hybrid RAG SuperComponent!\")\n",
    "    print(f\"   Better performance in {hybrid_wins}/{len(comparison_df)} metrics\")\n",
    "elif naive_wins > hybrid_wins:\n",
    "    print(f\"\\nðŸŽ‰ OVERALL WINNER: Naive RAG SuperComponent!\")  \n",
    "    print(f\"   Better performance in {naive_wins}/{len(comparison_df)} metrics\")\n",
    "else:\n",
    "    print(f\"\\nðŸ¤ RESULT: It's a tie between both systems!\")\n",
    "    \n",
    "avg_improvement = comparison_df['Improvement (%)'].mean()\n",
    "print(f\"\\nðŸ“ˆ Average improvement by Hybrid RAG: {avg_improvement:.2f}%\")\n",
    "\n",
    "# Show standard deviations to understand score consistency\n",
    "naive_stds = naive_df.std()\n",
    "hybrid_stds = hybrid_df.std()\n",
    "\n",
    "print(f\"\\nðŸ“Š Score Consistency (Standard Deviation):\")\n",
    "for metric in naive_averages.index:\n",
    "    print(f\"   {metric}:\")\n",
    "    print(f\"     Naive RAG: {naive_stds[metric]:.4f}\")\n",
    "    print(f\"     Hybrid RAG: {hybrid_stds[metric]:.4f}\")\n",
    "    stability_winner = \"Hybrid RAG\" if hybrid_stds[metric] < naive_stds[metric] else \"Naive RAG\"\n",
    "    print(f\"     More consistent: {stability_winner}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47be3490",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-with-haystack-ch6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
