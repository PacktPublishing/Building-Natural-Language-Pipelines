{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba5f0310",
   "metadata": {},
   "source": [
    "# Hybrid RAG Embedding Model Comparison with Weights & Biases üìäüî¨\n",
    "\n",
    "## üìã Overview\n",
    "\n",
    "This notebook focuses on using **Weights & Biases (W&B)** to systematically compare **embedding models** in our **Hybrid RAG** system. We will run two experiments comparing `text-embedding-3-small` vs `text-embedding-3-large` to understand the **quality vs cost tradeoff**.\n",
    "\n",
    "### Hybrid RAG System Configuration (Embedding Model Comparison):\n",
    "\n",
    "| Parameter | Small Model Experiment | Large Model Experiment |\n",
    "| :--- | :--- | :--- |\n",
    "| **`embedder_model`** | **`text-embedding-3-small`** | **`text-embedding-3-large`** |\n",
    "| `llm_model` | `gpt-4o-mini` | `gpt-4o-mini` |\n",
    "| `retriever_top_k` | `5` | `5` |\n",
    "| `rag_type` | `hybrid` | `hybrid` |\n",
    "| `reranker_model` | `BAAI/bge-reranker-base` | `BAAI/bge-reranker-base` |\n",
    "| `bm25_enabled` | `True` | `True` |\n",
    "\n",
    "### üî¨ Experimental Hypothesis:\n",
    "- **Large embedding model** may provide better semantic understanding ‚Üí higher faithfulness/context recall\n",
    "- **Small embedding model** will be more cost-effective ‚Üí better cost-per-query metrics\n",
    "- Both models will be re-indexed with fresh documents to ensure fair comparison\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All custom components imported successfully\n",
      "Setup: Imports and Environment variables loaded.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import tiktoken\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# Add the current directory to Python path to ensure imports work\n",
    "current_dir = Path.cwd()\n",
    "if str(current_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(current_dir))\n",
    "\n",
    "# Import Haystack/Ragas components\n",
    "from haystack import Pipeline\n",
    "from ragas.metrics import (LLMContextRecall,\\\n",
    "    Faithfulness,\\\n",
    "    FactualCorrectness,\\\n",
    "    ResponseRelevancy,\\\n",
    "    ContextEntityRecall, NoiseSensitivity)\n",
    "from haystack_integrations.document_stores.elasticsearch import ElasticsearchDocumentStore\n",
    "\n",
    "# Import custom components (assuming these paths exist relative to the notebook)\n",
    "try:\n",
    "    from scripts.rag.hybridrag import HybridRAGSuperComponent\n",
    "    from scripts.ragas_evaluation.ragasevalsupercomponent import RAGEvaluationSuperComponent\n",
    "    from scripts.wandb_experiments.rag_analytics import RAGAnalytics\n",
    "    from scripts.rag.indexing import IndexingPipelineSuperComponent\n",
    "    print(\"‚úÖ All custom components imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"WARNING: Custom components could not be imported: {e}\")\n",
    "    print(\"Ensure all required components are available.\")\n",
    "\n",
    "# Environment setup (reduced logging)\n",
    "os.environ[\"HAYSTACK_CONTENT_TRACING_ENABLED\"] = \"false\"\n",
    "print(\"Setup: Imports and Environment variables loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "raganalytics_class",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "class RAGEvaluationExperiment:\n",
    "    \"\"\"Enhanced RAG evaluation workflow with streamlined W&B integration using RAGEvaluationSuperComponent.\"\"\"\n",
    "    \n",
    "    def __init__(self, project_name: str, experiment_name: str):\n",
    "        self.project_name = project_name\n",
    "        self.experiment_name = experiment_name\n",
    "        self.run = None\n",
    "        self.evaluation_supercomponent = None\n",
    "    \n",
    "    def setup_pipeline(self, rag_supercomponent, metrics_list: list, config: dict = None):\n",
    "        \"\"\"Set up the evaluation pipeline with W&B tracking using RAGEvaluationSuperComponent.\"\"\"\n",
    "        self.run = wandb.init(\n",
    "            project=self.project_name,\n",
    "            name=self.experiment_name,\n",
    "            config=config,\n",
    "            reinit=True\n",
    "        )\n",
    "        print(f\"W&B STARTED: {self.experiment_name} | URL: {self.run.url}\")\n",
    "        \n",
    "        # Initialize the RAGEvaluationSuperComponent with the RAG system to evaluate\n",
    "        self.evaluation_supercomponent = RAGEvaluationSuperComponent(\n",
    "            rag_supercomponent=rag_supercomponent,\n",
    "            system_name=self.experiment_name,\n",
    "            llm_model=config.get('llm_model', 'gpt-4o-mini') if config else 'gpt-4o-mini'\n",
    "        )\n",
    "        \n",
    "        # Override the metrics in the evaluator component if custom metrics are provided\n",
    "        if metrics_list:\n",
    "            # Access the evaluator component and update its metrics\n",
    "            evaluator = self.evaluation_supercomponent.pipeline.get_component(\"evaluator\")\n",
    "            evaluator.metrics = metrics_list\n",
    "        \n",
    "        return self.evaluation_supercomponent\n",
    "    \n",
    "    def run_evaluation(self, csv_file_path: str):\n",
    "        \"\"\"Execute the pipeline using RAGEvaluationSuperComponent, log high-level metrics, and return results.\"\"\"\n",
    "        if not self.evaluation_supercomponent:\n",
    "            raise ValueError(\"Pipeline not set up. Call setup_pipeline() first.\")\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        print(f\"\\nRunning RAGEvaluationSuperComponent on {csv_file_path}...\")\n",
    "        \n",
    "        # Run the supercomponent with the CSV file\n",
    "        results = self.evaluation_supercomponent.run(csv_source=csv_file_path)\n",
    "        end_time = datetime.now()\n",
    "        \n",
    "        execution_time = (end_time - start_time).total_seconds()\n",
    "        metrics = results[\"metrics\"]\n",
    "        evaluation_df = results[\"evaluation_df\"].rename(columns={\n",
    "            'factual_correctness(mode=f1)': 'factual_correctness_f1'\n",
    "        })\n",
    "        \n",
    "        # Log dataset artifact\n",
    "        dataset_artifact = wandb.Artifact(name=f\"evaluation-dataset-{Path(csv_file_path).stem}\", type=\"dataset\")\n",
    "        dataset_artifact.add_file(csv_file_path)\n",
    "        self.run.log_artifact(dataset_artifact)\n",
    "        \n",
    "        # Extract and log summary metrics\n",
    "        wandb_metrics = {\n",
    "            \"execution_time_seconds\": execution_time,\n",
    "            \"num_queries_evaluated\": len(evaluation_df),\n",
    "        }\n",
    "        # Simple conversion of Ragas EvaluationResult metrics to flat dictionary\n",
    "        if hasattr(metrics, 'to_dict'):\n",
    "            metrics_dict = metrics.to_dict()\n",
    "            for metric_name, metric_value in metrics_dict.items():\n",
    "                if isinstance(metric_value, (int, float)):\n",
    "                    # Standardize metric names for W&B comparison\n",
    "                    clean_name = metric_name.replace('(mode=f1)', '').replace('ragas_', '').strip()\n",
    "                    wandb_metrics[f\"ragas_{clean_name}\"] = metric_value\n",
    "        \n",
    "        self.run.log(wandb_metrics)\n",
    "        print(f\"Evaluation Complete: Logged {len(evaluation_df)} queries and {len(wandb_metrics)} metrics.\")\n",
    "        \n",
    "        return {\n",
    "            \"metrics\": metrics, # Full EvaluationResult object\n",
    "            \"evaluation_df\": evaluation_df,\n",
    "            \"execution_time\": execution_time,\n",
    "            \"wandb_url\": self.run.url\n",
    "        }\n",
    "    \n",
    "    def finish_experiment(self):\n",
    "        \"\"\"Finish the W&B run.\"\"\"\n",
    "        if self.run:\n",
    "            url = self.run.url\n",
    "            self.run.finish()\n",
    "            print(f\"\\nW&B COMPLETED: {self.experiment_name} | View Results: {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5d5e6d",
   "metadata": {},
   "source": [
    "## üóÇÔ∏è Document Store Setup and Indexing\n",
    "\n",
    "Before running our experiments, we need to set up our document sources and create a clean indexing process for each embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b87424f",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store_small = ElasticsearchDocumentStore(hosts=\"http://localhost:9200\")\n",
    "document_store_large = ElasticsearchDocumentStore(hosts=\"http://localhost:9201\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610afd8a",
   "metadata": {},
   "source": [
    "## üî¨ Experiment 1: Hybrid RAG with Small Embedding Model (`text-embedding-3-small`)\n",
    "\n",
    "We establish a performance baseline for the Hybrid RAG system using the smaller, more cost-effective embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_1_setup",
   "metadata": {
    "tags": [
     "setup"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlgutierrwr\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/laurafunderburk/Documents/GitHub/Building-Natural-Language-Pipelines/ch6/jupyter-notebooks/wandb/run-20251103_190018-kzm3h5zb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/kzm3h5zb' target=\"_blank\">hybrid-rag-small-embedding</a></strong> to <a href='https://wandb.ai/lgutierrwr/embedding-model-comparison' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lgutierrwr/embedding-model-comparison' target=\"_blank\">https://wandb.ai/lgutierrwr/embedding-model-comparison</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/kzm3h5zb' target=\"_blank\">https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/kzm3h5zb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [instructor, openai] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Weave is installed but not imported. Add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B STARTED: hybrid-rag-small-embedding | URL: https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/kzm3h5zb\n",
      "\n",
      "üîÑ Building evaluation pipeline for hybrid-rag-small-embedding...\n",
      "==================================================\n",
      "‚úÖ Evaluation pipeline for hybrid-rag-small-embedding built successfully!\n",
      "\n",
      "Running RAGEvaluationSuperComponent on data_for_eval/synthetic_tests_advanced_branching_3.csv...\n",
      "Loaded DataFrame with 4 rows from data_for_eval/synthetic_tests_advanced_branching_3.csv.\n",
      "Running RAG SuperComponent on 4 queries...\n",
      "RAG processing complete.\n",
      "Creating Ragas EvaluationDataset...\n",
      "Starting Ragas evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f955a440332456fa6b72c48e5e0a899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n"
     ]
    }
   ],
   "source": [
    "# 1. Define evaluation metrics (Focusing on core RAGAS metrics)\n",
    "evaluation_metrics = [LLMContextRecall(), \\\n",
    "                Faithfulness(), \\\n",
    "                FactualCorrectness(), \\\n",
    "                ResponseRelevancy(), \\\n",
    "                ContextEntityRecall(), \\\n",
    "                NoiseSensitivity()]\n",
    "csv_file_path = \"data_for_eval/synthetic_tests_advanced_branching_3.csv\"\n",
    "\n",
    "# 2. Configuration for the Small Embedding Model Experiment\n",
    "small_embedding_config = {\n",
    "    \"embedder_model\": \"text-embedding-3-small\",\n",
    "    \"llm_model\": \"gpt-4o-mini\",\n",
    "    \"retriever_top_k\": 5,  # Fixed value\n",
    "    \"rag_type\": \"hybrid\",\n",
    "    \"document_store\": \"elasticsearch\",\n",
    "    \"reranker_model\": \"BAAI/bge-reranker-base\",\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# 4. Initialize RAG component with small embedder\n",
    "small_embedding_rag_sc = HybridRAGSuperComponent(\n",
    "    document_store=document_store_small,\n",
    "    embedder_model=small_embedding_config[\"embedder_model\"]\n",
    ")\n",
    "\n",
    "# 5. Initialize and Setup Small Embedding Experiment\n",
    "small_embedding_experiment = RAGEvaluationExperiment(\n",
    "    project_name=\"embedding-model-comparison\",\n",
    "    experiment_name=\"hybrid-rag-small-embedding\"\n",
    ")\n",
    "\n",
    "small_embedding_pipeline = small_embedding_experiment.setup_pipeline(\n",
    "    rag_supercomponent=small_embedding_rag_sc, \n",
    "    metrics_list=evaluation_metrics,\n",
    "    config=small_embedding_config\n",
    ")\n",
    "\n",
    "# 6. Run the evaluation and store results\n",
    "small_embedding_results = small_embedding_experiment.run_evaluation(\n",
    "    csv_file_path=csv_file_path\n",
    ")\n",
    "\n",
    "# 7. Run Analytics and log to W&B\n",
    "small_embedding_analytics = RAGAnalytics(small_embedding_results, model_name=small_embedding_config['llm_model'])\n",
    "small_embedding_summary = small_embedding_analytics.log_to_wandb(small_embedding_experiment.run)\n",
    "\n",
    "# 8. Finish the experiment run\n",
    "small_embedding_experiment.finish_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d0130f",
   "metadata": {},
   "source": [
    "## üöÄ Experiment 2: Hybrid RAG with Large Embedding Model (`text-embedding-3-large`)\n",
    "\n",
    "To potentially improve semantic understanding and retrieval quality, we'll use the larger, more powerful embedding model. This tests the tradeoff between **embedding quality vs. cost/latency**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_2_setup",
   "metadata": {
    "tags": [
     "optimization"
    ]
   },
   "outputs": [],
   "source": [
    "# 1. Configuration for the Large Embedding Model Experiment\n",
    "large_embedding_config = {\n",
    "    \"embedder_model\": \"text-embedding-3-large\",\n",
    "    \"llm_model\": \"gpt-4o-mini\",\n",
    "    \"retriever_top_k\": 5,  # Same as small embedding experiment\n",
    "    \"rag_type\": \"hybrid\",\n",
    "    \"document_store\": \"elasticsearch\",\n",
    "    \"reranker_model\": \"BAAI/bge-reranker-base\",\n",
    "}\n",
    "\n",
    "# 2. Initialize RAG component with large embedder\n",
    "large_embedding_rag_sc = HybridRAGSuperComponent(\n",
    "    document_store=document_store_large,\n",
    "    embedder_model=large_embedding_config[\"embedder_model\"]\n",
    ")\n",
    "\n",
    "# 4. Initialize and Setup Large Embedding Experiment\n",
    "large_embedding_experiment = RAGEvaluationExperiment(\n",
    "    project_name=\"embedding-model-comparison\",\n",
    "    experiment_name=\"hybrid-rag-large-embedding\"\n",
    ")\n",
    "\n",
    "large_embedding_pipeline = large_embedding_experiment.setup_pipeline(\n",
    "    rag_supercomponent=large_embedding_rag_sc, \n",
    "    metrics_list=evaluation_metrics,\n",
    "    config=large_embedding_config\n",
    ")\n",
    "\n",
    "# 5. Run the evaluation and store results\n",
    "large_embedding_results = large_embedding_experiment.run_evaluation(\n",
    "    csv_file_path=csv_file_path\n",
    ")\n",
    "\n",
    "# 6. Run Analytics and log to W&B\n",
    "large_embedding_analytics = RAGAnalytics(large_embedding_results, model_name=large_embedding_config['llm_model'])\n",
    "large_embedding_summary = large_embedding_analytics.log_to_wandb(large_embedding_experiment.run)\n",
    "\n",
    "# 7. Finish the experiment run\n",
    "large_embedding_experiment.finish_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd620687",
   "metadata": {},
   "source": [
    "## üìä Comparative Analysis & Key Insights\n",
    "\n",
    "Now we can programmatically compare the key metrics between the two runs. The full comparison is available in the W&B dashboard, but a quick summary confirms the tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison_analysis",
   "metadata": {
    "tags": [
     "summary"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def extract_ragas_metrics(metrics_obj):\n",
    "    \"\"\"Extract and flatten RAGAS metrics from the result object for comparison.\"\"\"\n",
    "    metrics_dict = {} \n",
    "    if hasattr(metrics_obj, 'to_dict'):\n",
    "        raw_metrics = metrics_obj.to_dict()\n",
    "        for k, v in raw_metrics.items():\n",
    "            if isinstance(v, (float, int)):\n",
    "                # Clean metric name for the output table\n",
    "                clean_name = k.replace('(mode=f1)', '').strip()\n",
    "                metrics_dict[clean_name] = v\n",
    "    return metrics_dict\n",
    "\n",
    "# 1. Extract and combine summary data\n",
    "small_embedding_data = {\n",
    "    'System': 'Small Embedding (text-embedding-3-small)',\n",
    "    'embedder_model': 'text-embedding-3-small',\n",
    "    'Execution Time (s)': small_embedding_results['execution_time'],\n",
    "    'Avg Cost (USD)': small_embedding_summary['average_cost_per_query_usd'],\n",
    "    'Avg Tokens/Query': small_embedding_summary['average_tokens_per_query'],\n",
    "    'Faithfulness': np.array(small_embedding_results['metrics']['faithfulness']).mean(),\n",
    "    'Context Recall': np.array(small_embedding_results['metrics']['context_recall']).mean(),\n",
    "    'Factual Correctness': np.array(small_embedding_results['metrics']['factual_correctness(mode=f1)']).mean(),\n",
    "    'Response Relevancy': np.array(small_embedding_results['metrics']['answer_relevancy']).mean(),\n",
    "    'Noise Sensitivity': np.array(small_embedding_results['metrics']['noise_sensitivity(mode=relevant)']).mean(),\n",
    "    'Context Entity Recall': np.array(small_embedding_results['metrics']['context_entity_recall']).mean()\n",
    "}\n",
    "\n",
    "large_embedding_data = {\n",
    "    'System': 'Large Embedding (text-embedding-3-large)',\n",
    "    'embedder_model': 'text-embedding-3-large',\n",
    "    'Execution Time (s)': large_embedding_results['execution_time'],\n",
    "    'Avg Cost (USD)': large_embedding_summary['average_cost_per_query_usd'],\n",
    "    'Avg Tokens/Query': large_embedding_summary['average_tokens_per_query'],\n",
    "    'Faithfulness': np.array(large_embedding_results['metrics']['faithfulness']).mean(),\n",
    "    'Context Recall': np.array(large_embedding_results['metrics']['context_recall']).mean(),\n",
    "    'Factual Correctness': np.array(large_embedding_results['metrics']['factual_correctness(mode=f1)']).mean(),\n",
    "    'Response Relevancy': np.array(large_embedding_results['metrics']['answer_relevancy']).mean(),\n",
    "    'Noise Sensitivity': np.array(large_embedding_results['metrics']['noise_sensitivity(mode=relevant)']).mean(),\n",
    "    'Context Entity Recall': np.array(large_embedding_results['metrics']['context_entity_recall']).mean()\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame([small_embedding_data, large_embedding_data])\n",
    "comparison_df = comparison_df.set_index('System')\n",
    "\n",
    "# Display the comparison table\n",
    "print(\"üìä EMBEDDING MODEL COMPARISON RESULTS:\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "# Log final summary table to W&B for easy comparison\n",
    "final_run = wandb.init(project=\"embedding-model-comparison\", name=\"final-comparison\", reinit=True)\n",
    "final_run.log({\"embedding_comparison_table\": wandb.Table(dataframe=comparison_df.reset_index())})\n",
    "final_run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b90b3b",
   "metadata": {},
   "source": [
    "## üéì Summary and Next Steps\n",
    "\n",
    "You have successfully executed an **embedding model comparison experiment** for the Hybrid RAG system and logged all results to a single W&B project: `embedding-model-comparison`.\n",
    "\n",
    "### Key Accomplishments:\n",
    "1.  **Document Store Management:** Implemented proper document wiping and re-indexing workflow to ensure fair comparison between embedding models.\n",
    "2.  **Small vs Large Embedding Comparison:** Tested `text-embedding-3-small` vs `text-embedding-3-large` to understand the **quality vs cost tradeoff**.\n",
    "3.  **Controlled Variables:** Fixed all other parameters (`retriever_top_k=5`, `llm_model=gpt-4o-mini`) to isolate the impact of embedding model choice.\n",
    "4.  **Comprehensive Logging:** Logged both runs with their configurations, metrics, and cost analysis to W&B for detailed comparison.\n",
    "\n",
    "### Expected Insights:\n",
    "- **Performance:** Large embedding model may show improved semantic understanding (higher context recall, faithfulness)\n",
    "- **Cost:** Large embedding model will likely have higher embedding costs but same LLM costs\n",
    "- **Latency:** Large embeddings may have slightly higher processing time\n",
    "- **Use Case Guidance:** Data will help decide if quality improvement justifies additional cost\n",
    "\n",
    "### Next Steps in W&B:\n",
    "1.  **View Comparison:** Navigate to the **`embedding-model-comparison`** project dashboard on W&B. Compare the **faithfulness vs. embedding cost** and **quality metrics vs. model size** visualizations.\n",
    "2.  **Cost Analysis:** Analyze the embedding cost difference and determine ROI for your specific use case.\n",
    "3.  **Deeper Optimization:** Use W&B Sweeps to test additional embedding models or explore hybrid approaches using both models for different query types.\n",
    "4.  **Production Decision:** Use these results to make an informed choice for your production RAG system based on your quality requirements and budget constraints."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-with-haystack-ch6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
