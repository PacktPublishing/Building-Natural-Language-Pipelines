{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba5f0310",
   "metadata": {},
   "source": [
    "# Hybrid RAG Embedding Model Comparison with Weights & Biases üìäüî¨\n",
    "\n",
    "## üìã Overview\n",
    "\n",
    "This notebook focuses on using **Weights & Biases (W&B)** to systematically compare **embedding models** in our **Hybrid RAG** system. We will run two experiments comparing `text-embedding-3-small` vs `text-embedding-3-large` to understand the **quality vs cost tradeoff**.\n",
    "\n",
    "### Hybrid RAG System Configuration (Embedding Model Comparison):\n",
    "\n",
    "| Parameter | Small Model Experiment | Large Model Experiment |\n",
    "| :--- | :--- | :--- |\n",
    "| **`embedder_model`** | **`text-embedding-3-small`** | **`text-embedding-3-large`** |\n",
    "| `llm_model` | `gpt-4o-mini` | `gpt-4o-mini` |\n",
    "| `retriever_top_k` | `5` | `5` |\n",
    "| `rag_type` | `hybrid` | `hybrid` |\n",
    "| `reranker_model` | `BAAI/bge-reranker-base` | `BAAI/bge-reranker-base` |\n",
    "| `bm25_enabled` | `True` | `True` |\n",
    "\n",
    "### üî¨ Experimental Hypothesis:\n",
    "- **Large embedding model** may provide better semantic understanding ‚Üí higher faithfulness/context recall\n",
    "- **Small embedding model** will be more cost-effective ‚Üí better cost-per-query metrics\n",
    "- Both models will be re-indexed with fresh documents to ensure fair comparison\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All custom components imported successfully\n",
      "Setup: Imports and Environment variables loaded.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import tiktoken\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# Add the current directory to Python path to ensure imports work\n",
    "current_dir = Path.cwd()\n",
    "if str(current_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(current_dir))\n",
    "\n",
    "# Import Haystack/Ragas components\n",
    "from haystack import Pipeline\n",
    "from ragas.metrics import (LLMContextRecall,\\\n",
    "    Faithfulness,\\\n",
    "    FactualCorrectness,\\\n",
    "    ResponseRelevancy,\\\n",
    "    ContextEntityRecall, NoiseSensitivity)\n",
    "from haystack_integrations.document_stores.elasticsearch import ElasticsearchDocumentStore\n",
    "\n",
    "# Import custom components (assuming these paths exist relative to the notebook)\n",
    "try:\n",
    "    from scripts.rag.hybridrag import HybridRAGSuperComponent\n",
    "    from scripts.ragas_evaluation.ragasevalsupercomponent import RAGEvaluationSuperComponent\n",
    "    from scripts.wandb_experiments.rag_analytics import RAGAnalytics\n",
    "    from scripts.rag.indexing import IndexingPipelineSuperComponent\n",
    "    print(\"‚úÖ All custom components imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"WARNING: Custom components could not be imported: {e}\")\n",
    "    print(\"Ensure all required components are available.\")\n",
    "\n",
    "# Environment setup (reduced logging)\n",
    "os.environ[\"HAYSTACK_CONTENT_TRACING_ENABLED\"] = \"false\"\n",
    "print(\"Setup: Imports and Environment variables loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "raganalytics_class",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "class RAGEvaluationExperiment:\n",
    "    \"\"\"Enhanced RAG evaluation workflow with streamlined W&B integration using RAGEvaluationSuperComponent.\"\"\"\n",
    "    \n",
    "    def __init__(self, project_name: str, experiment_name: str):\n",
    "        self.project_name = project_name\n",
    "        self.experiment_name = experiment_name\n",
    "        self.run = None\n",
    "        self.evaluation_supercomponent = None\n",
    "    \n",
    "    def setup_pipeline(self, rag_supercomponent, metrics_list: list, config: dict = None):\n",
    "        \"\"\"Set up the evaluation pipeline with W&B tracking using RAGEvaluationSuperComponent.\"\"\"\n",
    "        self.run = wandb.init(\n",
    "            project=self.project_name,\n",
    "            name=self.experiment_name,\n",
    "            config=config,\n",
    "            reinit=True\n",
    "        )\n",
    "        print(f\"W&B STARTED: {self.experiment_name} | URL: {self.run.url}\")\n",
    "        \n",
    "        # Initialize the RAGEvaluationSuperComponent with the RAG system to evaluate\n",
    "        self.evaluation_supercomponent = RAGEvaluationSuperComponent(\n",
    "            rag_supercomponent=rag_supercomponent,\n",
    "            system_name=self.experiment_name,\n",
    "            llm_model=config.get('llm_model', 'gpt-4o-mini') if config else 'gpt-4o-mini'\n",
    "        )\n",
    "        \n",
    "        # Override the metrics in the evaluator component if custom metrics are provided\n",
    "        if metrics_list:\n",
    "            # Access the evaluator component and update its metrics\n",
    "            evaluator = self.evaluation_supercomponent.pipeline.get_component(\"evaluator\")\n",
    "            evaluator.metrics = metrics_list\n",
    "        \n",
    "        return self.evaluation_supercomponent\n",
    "    \n",
    "    def run_evaluation(self, csv_file_path: str):\n",
    "        \"\"\"Execute the pipeline using RAGEvaluationSuperComponent, log high-level metrics, and return results.\"\"\"\n",
    "        if not self.evaluation_supercomponent:\n",
    "            raise ValueError(\"Pipeline not set up. Call setup_pipeline() first.\")\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        print(f\"\\nRunning RAGEvaluationSuperComponent on {csv_file_path}...\")\n",
    "        \n",
    "        # Run the supercomponent with the CSV file\n",
    "        results = self.evaluation_supercomponent.run(csv_source=csv_file_path)\n",
    "        end_time = datetime.now()\n",
    "        \n",
    "        execution_time = (end_time - start_time).total_seconds()\n",
    "        metrics = results[\"metrics\"]\n",
    "        evaluation_df = results[\"evaluation_df\"].rename(columns={\n",
    "            'factual_correctness(mode=f1)': 'factual_correctness_f1'\n",
    "        })\n",
    "        \n",
    "        # Log dataset artifact\n",
    "        dataset_artifact = wandb.Artifact(name=f\"evaluation-dataset-{Path(csv_file_path).stem}\", type=\"dataset\")\n",
    "        dataset_artifact.add_file(csv_file_path)\n",
    "        self.run.log_artifact(dataset_artifact)\n",
    "        \n",
    "        # Extract and log summary metrics\n",
    "        wandb_metrics = {\n",
    "            \"execution_time_seconds\": execution_time,\n",
    "            \"num_queries_evaluated\": len(evaluation_df),\n",
    "        }\n",
    "        # Simple conversion of Ragas EvaluationResult metrics to flat dictionary\n",
    "        if hasattr(metrics, 'to_dict'):\n",
    "            metrics_dict = metrics.to_dict()\n",
    "            for metric_name, metric_value in metrics_dict.items():\n",
    "                if isinstance(metric_value, (int, float)):\n",
    "                    # Standardize metric names for W&B comparison\n",
    "                    clean_name = metric_name.replace('(mode=f1)', '').replace('ragas_', '').strip()\n",
    "                    wandb_metrics[f\"ragas_{clean_name}\"] = metric_value\n",
    "        \n",
    "        self.run.log(wandb_metrics)\n",
    "        print(f\"Evaluation Complete: Logged {len(evaluation_df)} queries and {len(wandb_metrics)} metrics.\")\n",
    "        \n",
    "        return {\n",
    "            \"metrics\": metrics, # Full EvaluationResult object\n",
    "            \"evaluation_df\": evaluation_df,\n",
    "            \"execution_time\": execution_time,\n",
    "            \"wandb_url\": self.run.url\n",
    "        }\n",
    "    \n",
    "    def finish_experiment(self):\n",
    "        \"\"\"Finish the W&B run.\"\"\"\n",
    "        if self.run:\n",
    "            url = self.run.url\n",
    "            self.run.finish()\n",
    "            print(f\"\\nW&B COMPLETED: {self.experiment_name} | View Results: {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5d5e6d",
   "metadata": {},
   "source": [
    "## üóÇÔ∏è Document Store Setup and Indexing\n",
    "\n",
    "Before running our experiments, we need to set up our document sources and create a clean indexing process for each embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b87424f",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store_small = ElasticsearchDocumentStore(hosts=\"http://localhost:9200\")\n",
    "document_store_large = ElasticsearchDocumentStore(hosts=\"http://localhost:9201\", index=\"large_embeddings\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610afd8a",
   "metadata": {},
   "source": [
    "## üî¨ Experiment 1: Hybrid RAG with Small Embedding Model (`text-embedding-3-small`)\n",
    "\n",
    "We establish a performance baseline for the Hybrid RAG system using the smaller, more cost-effective embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_1_setup",
   "metadata": {
    "tags": [
     "setup"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlgutierrwr\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/laurafunderburk/Documents/GitHub/Building-Natural-Language-Pipelines/ch6/jupyter-notebooks/wandb/run-20251103_190654-hnyo2tbe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/hnyo2tbe' target=\"_blank\">hybrid-rag-small-embedding</a></strong> to <a href='https://wandb.ai/lgutierrwr/embedding-model-comparison' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lgutierrwr/embedding-model-comparison' target=\"_blank\">https://wandb.ai/lgutierrwr/embedding-model-comparison</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/hnyo2tbe' target=\"_blank\">https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/hnyo2tbe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [instructor, openai] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Weave is installed but not imported. Add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B STARTED: hybrid-rag-small-embedding | URL: https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/hnyo2tbe\n",
      "\n",
      "üîÑ Building evaluation pipeline for hybrid-rag-small-embedding...\n",
      "==================================================\n",
      "‚úÖ Evaluation pipeline for hybrid-rag-small-embedding built successfully!\n",
      "\n",
      "Running RAGEvaluationSuperComponent on data_for_eval/synthetic_tests_advanced_branching_10.csv...\n",
      "Loaded DataFrame with 10 rows from data_for_eval/synthetic_tests_advanced_branching_10.csv.\n",
      "Running RAG SuperComponent on 10 queries...\n",
      "RAG processing complete.\n",
      "Creating Ragas EvaluationDataset...\n",
      "Starting Ragas evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d89ba1674d647c28423d3db4755bd13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Exception raised in Job[59]: TimeoutError()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ragas evaluation complete.\n",
      "Overall metrics: {'context_recall': 0.9400, 'faithfulness': 0.8612, 'factual_correctness(mode=f1)': 0.4770, 'answer_relevancy': 0.7712, 'context_entity_recall': 0.3943, 'noise_sensitivity(mode=relevant)': 0.3532}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Complete: Logged 10 queries and 2 metrics.\n",
      "Analytics: Logged comprehensive analysis for 10 queries.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_cost_per_query_usd</td><td>‚ñÅ</td></tr><tr><td>average_tokens_per_query</td><td>‚ñÅ</td></tr><tr><td>execution_time_seconds</td><td>‚ñÅ</td></tr><tr><td>num_queries_evaluated</td><td>‚ñÅ</td></tr><tr><td>token_efficiency_tps_per_dollar</td><td>‚ñÅ</td></tr><tr><td>total_cost_usd</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_cost_per_query_usd</td><td>0.00084</td></tr><tr><td>average_tokens_per_query</td><td>4981.2</td></tr><tr><td>execution_time_seconds</td><td>282.73325</td></tr><tr><td>num_queries_evaluated</td><td>10</td></tr><tr><td>token_efficiency_tps_per_dollar</td><td>5928623.71235</td></tr><tr><td>total_cost_usd</td><td>0.0084</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">hybrid-rag-small-embedding</strong> at: <a href='https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/hnyo2tbe' target=\"_blank\">https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/hnyo2tbe</a><br> View project at: <a href='https://wandb.ai/lgutierrwr/embedding-model-comparison' target=\"_blank\">https://wandb.ai/lgutierrwr/embedding-model-comparison</a><br>Synced 4 W&B file(s), 1 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251103_190654-hnyo2tbe/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "W&B COMPLETED: hybrid-rag-small-embedding | View Results: https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/hnyo2tbe\n"
     ]
    }
   ],
   "source": [
    "# 1. Define evaluation metrics (Focusing on core RAGAS metrics)\n",
    "evaluation_metrics = [LLMContextRecall(), \\\n",
    "                Faithfulness(), \\\n",
    "                FactualCorrectness(), \\\n",
    "                ResponseRelevancy(), \\\n",
    "                ContextEntityRecall(), \\\n",
    "                NoiseSensitivity()]\n",
    "csv_file_path = \"data_for_eval/synthetic_tests_advanced_branching_10.csv\"\n",
    "\n",
    "# 2. Configuration for the Small Embedding Model Experiment\n",
    "small_embedding_config = {\n",
    "    \"embedder_model\": \"text-embedding-3-small\",\n",
    "    \"llm_model\": \"gpt-4o-mini\",\n",
    "    \"retriever_top_k\": 5,  # Fixed value\n",
    "    \"rag_type\": \"hybrid\",\n",
    "    \"document_store\": \"elasticsearch\",\n",
    "    \"reranker_model\": \"BAAI/bge-reranker-base\",\n",
    "}\n",
    "\n",
    "# 4. Initialize RAG component with small embedder\n",
    "small_embedding_rag_sc = HybridRAGSuperComponent(\n",
    "    document_store=document_store_small,\n",
    "    embedder_model=small_embedding_config[\"embedder_model\"]\n",
    ")\n",
    "\n",
    "# 5. Initialize and Setup Small Embedding Experiment\n",
    "small_embedding_experiment = RAGEvaluationExperiment(\n",
    "    project_name=\"embedding-model-comparison\",\n",
    "    experiment_name=\"hybrid-rag-small-embedding\"\n",
    ")\n",
    "\n",
    "small_embedding_pipeline = small_embedding_experiment.setup_pipeline(\n",
    "    rag_supercomponent=small_embedding_rag_sc, \n",
    "    metrics_list=evaluation_metrics,\n",
    "    config=small_embedding_config\n",
    ")\n",
    "\n",
    "# 6. Run the evaluation and store results\n",
    "small_embedding_results = small_embedding_experiment.run_evaluation(\n",
    "    csv_file_path=csv_file_path\n",
    ")\n",
    "\n",
    "# 7. Run Analytics and log to W&B with updated RAGAnalytics\n",
    "# Pass the specific embedding model being used as a list\n",
    "small_embedding_analytics = RAGAnalytics(\n",
    "    results=small_embedding_results, \n",
    "    model_name=small_embedding_config['llm_model'],\n",
    "    embedding_models=[small_embedding_config['embedder_model']]  # Pass as list\n",
    ")\n",
    "small_embedding_summary = small_embedding_analytics.log_to_wandb(small_embedding_experiment.run)\n",
    "\n",
    "# 8. Finish the experiment run\n",
    "small_embedding_experiment.finish_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d0130f",
   "metadata": {},
   "source": [
    "## üöÄ Experiment 2: Hybrid RAG with Large Embedding Model (`text-embedding-3-large`)\n",
    "\n",
    "To potentially improve semantic understanding and retrieval quality, we'll use the larger, more powerful embedding model. This tests the tradeoff between **embedding quality vs. cost/latency**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_2_setup",
   "metadata": {
    "tags": [
     "optimization"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/laurafunderburk/Documents/GitHub/Building-Natural-Language-Pipelines/ch6/jupyter-notebooks/wandb/run-20251103_191140-bimrym87</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/bimrym87' target=\"_blank\">hybrid-rag-large-embedding</a></strong> to <a href='https://wandb.ai/lgutierrwr/embedding-model-comparison' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lgutierrwr/embedding-model-comparison' target=\"_blank\">https://wandb.ai/lgutierrwr/embedding-model-comparison</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/bimrym87' target=\"_blank\">https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/bimrym87</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference, instructor, openai] in use.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B STARTED: hybrid-rag-large-embedding | URL: https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/bimrym87\n",
      "\n",
      "üîÑ Building evaluation pipeline for hybrid-rag-large-embedding...\n",
      "==================================================\n",
      "‚úÖ Evaluation pipeline for hybrid-rag-large-embedding built successfully!\n",
      "\n",
      "Running RAGEvaluationSuperComponent on data_for_eval/synthetic_tests_advanced_branching_10.csv...\n",
      "Loaded DataFrame with 10 rows from data_for_eval/synthetic_tests_advanced_branching_10.csv.\n",
      "Running RAG SuperComponent on 10 queries...\n",
      "RAG processing complete.\n",
      "Creating Ragas EvaluationDataset...\n",
      "Starting Ragas evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d991d6ff60cf4396bda626d1df337551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ragas evaluation complete.\n",
      "Overall metrics: {'context_recall': 0.9600, 'faithfulness': 0.8000, 'factual_correctness(mode=f1)': 0.5280, 'answer_relevancy': 0.8642, 'context_entity_recall': 0.3376, 'noise_sensitivity(mode=relevant)': 0.2989}\n",
      "Evaluation Complete: Logged 10 queries and 2 metrics.\n",
      "Analytics: Logged comprehensive analysis for 10 queries.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_cost_per_query_usd</td><td>‚ñÅ</td></tr><tr><td>average_tokens_per_query</td><td>‚ñÅ</td></tr><tr><td>execution_time_seconds</td><td>‚ñÅ</td></tr><tr><td>num_queries_evaluated</td><td>‚ñÅ</td></tr><tr><td>token_efficiency_tps_per_dollar</td><td>‚ñÅ</td></tr><tr><td>total_cost_usd</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_cost_per_query_usd</td><td>0.00082</td></tr><tr><td>average_tokens_per_query</td><td>4865</td></tr><tr><td>execution_time_seconds</td><td>242.19803</td></tr><tr><td>num_queries_evaluated</td><td>10</td></tr><tr><td>token_efficiency_tps_per_dollar</td><td>5906850.24647</td></tr><tr><td>total_cost_usd</td><td>0.00824</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">hybrid-rag-large-embedding</strong> at: <a href='https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/bimrym87' target=\"_blank\">https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/bimrym87</a><br> View project at: <a href='https://wandb.ai/lgutierrwr/embedding-model-comparison' target=\"_blank\">https://wandb.ai/lgutierrwr/embedding-model-comparison</a><br>Synced 4 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251103_191140-bimrym87/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "W&B COMPLETED: hybrid-rag-large-embedding | View Results: https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/bimrym87\n"
     ]
    }
   ],
   "source": [
    "# 1. Configuration for the Large Embedding Model Experiment\n",
    "large_embedding_config = {\n",
    "    \"embedder_model\": \"text-embedding-3-large\",\n",
    "    \"llm_model\": \"gpt-4o-mini\",\n",
    "    \"retriever_top_k\": 5,  # Same as small embedding experiment\n",
    "    \"rag_type\": \"hybrid\",\n",
    "    \"document_store\": \"elasticsearch\",\n",
    "    \"reranker_model\": \"BAAI/bge-reranker-base\",\n",
    "}\n",
    "\n",
    "# 2. Initialize RAG component with large embedder\n",
    "large_embedding_rag_sc = HybridRAGSuperComponent(\n",
    "    document_store=document_store_large,\n",
    "    embedder_model=large_embedding_config[\"embedder_model\"]\n",
    ")\n",
    "\n",
    "# 4. Initialize and Setup Large Embedding Experiment\n",
    "large_embedding_experiment = RAGEvaluationExperiment(\n",
    "    project_name=\"embedding-model-comparison\",\n",
    "    experiment_name=\"hybrid-rag-large-embedding\"\n",
    ")\n",
    "\n",
    "large_embedding_pipeline = large_embedding_experiment.setup_pipeline(\n",
    "    rag_supercomponent=large_embedding_rag_sc, \n",
    "    metrics_list=evaluation_metrics,\n",
    "    config=large_embedding_config\n",
    ")\n",
    "\n",
    "# 5. Run the evaluation and store results\n",
    "large_embedding_results = large_embedding_experiment.run_evaluation(\n",
    "    csv_file_path=csv_file_path\n",
    ")\n",
    "\n",
    "# 6. Run Analytics and log to W&B with updated RAGAnalytics\n",
    "# Pass the specific embedding model being used as a list\n",
    "large_embedding_analytics = RAGAnalytics(\n",
    "    results=large_embedding_results, \n",
    "    model_name=large_embedding_config['llm_model'],\n",
    "    embedding_models=[large_embedding_config['embedder_model']]  # Pass as list\n",
    ")\n",
    "large_embedding_summary = large_embedding_analytics.log_to_wandb(large_embedding_experiment.run)\n",
    "\n",
    "# 7. Finish the experiment run\n",
    "large_embedding_experiment.finish_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd620687",
   "metadata": {},
   "source": [
    "## üìä Comparative Analysis & Key Insights\n",
    "\n",
    "Now we can programmatically compare the key metrics between the two runs. The full comparison is available in the W&B dashboard, but a quick summary confirms the tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "comparison_analysis",
   "metadata": {
    "tags": [
     "summary"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/laurafunderburk/Documents/GitHub/Building-Natural-Language-Pipelines/ch6/jupyter-notebooks/wandb/run-20251103_191813-2hw2du8e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/2hw2du8e' target=\"_blank\">final-comparison</a></strong> to <a href='https://wandb.ai/lgutierrwr/embedding-model-comparison' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lgutierrwr/embedding-model-comparison' target=\"_blank\">https://wandb.ai/lgutierrwr/embedding-model-comparison</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/2hw2du8e' target=\"_blank\">https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/2hw2du8e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">final-comparison</strong> at: <a href='https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/2hw2du8e' target=\"_blank\">https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/2hw2du8e</a><br> View project at: <a href='https://wandb.ai/lgutierrwr/embedding-model-comparison' target=\"_blank\">https://wandb.ai/lgutierrwr/embedding-model-comparison</a><br>Synced 4 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251103_191813-2hw2du8e/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def extract_ragas_metrics(metrics_obj):\n",
    "    \"\"\"Extract and flatten RAGAS metrics from the result object for comparison.\"\"\"\n",
    "    metrics_dict = {} \n",
    "    if hasattr(metrics_obj, 'to_dict'):\n",
    "        raw_metrics = metrics_obj.to_dict()\n",
    "        for k, v in raw_metrics.items():\n",
    "            if isinstance(v, (float, int)):\n",
    "                # Clean metric name for the output table\n",
    "                clean_name = k.replace('(mode=f1)', '').strip()\n",
    "                metrics_dict[clean_name] = v\n",
    "    return metrics_dict\n",
    "\n",
    "# 1. Extract and combine summary data\n",
    "small_embedding_data = {\n",
    "    'System': 'Small Embedding (text-embedding-3-small)',\n",
    "    'embedder_model': 'text-embedding-3-small',\n",
    "    'Execution Time (s)': small_embedding_results['execution_time'],\n",
    "    'Avg Cost (USD)': small_embedding_summary['average_cost_per_query_usd'],\n",
    "    'Avg Tokens/Query': small_embedding_summary['average_tokens_per_query'],\n",
    "    'Faithfulness': np.array(small_embedding_results['metrics']['faithfulness']).mean(),\n",
    "    'Context Recall': np.array(small_embedding_results['metrics']['context_recall']).mean(),\n",
    "    'Factual Correctness': np.array(small_embedding_results['metrics']['factual_correctness(mode=f1)']).mean(),\n",
    "    'Response Relevancy': np.array(small_embedding_results['metrics']['answer_relevancy']).mean(),\n",
    "    'Noise Sensitivity': np.array(small_embedding_results['metrics']['noise_sensitivity(mode=relevant)']).mean(),\n",
    "    'Context Entity Recall': np.array(small_embedding_results['metrics']['context_entity_recall']).mean()\n",
    "}\n",
    "\n",
    "large_embedding_data = {\n",
    "    'System': 'Large Embedding (text-embedding-3-large)',\n",
    "    'embedder_model': 'text-embedding-3-large',\n",
    "    'Execution Time (s)': large_embedding_results['execution_time'],\n",
    "    'Avg Cost (USD)': large_embedding_summary['average_cost_per_query_usd'],\n",
    "    'Avg Tokens/Query': large_embedding_summary['average_tokens_per_query'],\n",
    "    'Faithfulness': np.array(large_embedding_results['metrics']['faithfulness']).mean(),\n",
    "    'Context Recall': np.array(large_embedding_results['metrics']['context_recall']).mean(),\n",
    "    'Factual Correctness': np.array(large_embedding_results['metrics']['factual_correctness(mode=f1)']).mean(),\n",
    "    'Response Relevancy': np.array(large_embedding_results['metrics']['answer_relevancy']).mean(),\n",
    "    'Noise Sensitivity': np.array(large_embedding_results['metrics']['noise_sensitivity(mode=relevant)']).mean(),\n",
    "    'Context Entity Recall': np.array(large_embedding_results['metrics']['context_entity_recall']).mean()\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame([small_embedding_data, large_embedding_data])\n",
    "comparison_df = comparison_df.set_index('System')\n",
    "\n",
    "\n",
    "\n",
    "# Log final summary table to W&B for easy comparison\n",
    "final_run = wandb.init(project=\"embedding-model-comparison\", name=\"final-comparison\", reinit=True)\n",
    "final_run.log({\"embedding_comparison_table\": wandb.Table(dataframe=comparison_df.reset_index())})\n",
    "final_run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262365b8",
   "metadata": {},
   "source": [
    "## üí∞ Enhanced Cost Analysis with Updated RAGAnalytics\n",
    "\n",
    "The updated `RAGAnalytics` class provides comprehensive cost tracking including embedding costs, LLM costs, and detailed breakdowns. Let's create a final analysis that demonstrates the full capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2540f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive analytics comparison using both models\n",
    "print(\"üîç Creating Enhanced Cost Analysis with Updated RAGAnalytics...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Create a dual-embedding analytics for comparison purposes\n",
    "# This simulates analyzing a system that uses both embedding models\n",
    "dual_embedding_analytics = RAGAnalytics(\n",
    "    results=small_embedding_results,  # Use one set of results as base\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    embedding_models=[\"text-embedding-3-small\", \"text-embedding-3-large\"]  # Both models\n",
    ")\n",
    "\n",
    "# 2. Extract detailed cost breakdowns from individual experiments\n",
    "print(\"\\nüìä Individual Experiment Cost Analysis:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(f\"üîπ Small Embedding Experiment:\")\n",
    "print(f\"   Total Cost: ${small_embedding_summary['total_cost_usd']:.6f}\")\n",
    "print(f\"   LLM Cost: ${small_embedding_summary['llm_cost_usd']:.6f}\")\n",
    "print(f\"   Embedding Cost: ${small_embedding_summary['embedding_cost_usd']:.6f}\")\n",
    "print(f\"   Cost per Query: ${small_embedding_summary['average_cost_per_query_usd']:.6f}\")\n",
    "\n",
    "print(f\"\\nüîπ Large Embedding Experiment:\")\n",
    "print(f\"   Total Cost: ${large_embedding_summary['total_cost_usd']:.6f}\")\n",
    "print(f\"   LLM Cost: ${large_embedding_summary['llm_cost_usd']:.6f}\")\n",
    "print(f\"   Embedding Cost: ${large_embedding_summary['embedding_cost_usd']:.6f}\")\n",
    "print(f\"   Cost per Query: ${large_embedding_summary['average_cost_per_query_usd']:.6f}\")\n",
    "\n",
    "# 3. Calculate cost differences and efficiency metrics\n",
    "cost_difference = large_embedding_summary['total_cost_usd'] - small_embedding_summary['total_cost_usd']\n",
    "embedding_cost_difference = large_embedding_summary['embedding_cost_usd'] - small_embedding_summary['embedding_cost_usd']\n",
    "cost_increase_percentage = (cost_difference / small_embedding_summary['total_cost_usd']) * 100\n",
    "\n",
    "print(f\"\\nüìà Cost Comparison Analysis:\")\n",
    "print(f\"   Total Cost Difference: ${cost_difference:.6f} ({cost_increase_percentage:.1f}% increase)\")\n",
    "print(f\"   Embedding Cost Difference: ${embedding_cost_difference:.6f}\")\n",
    "print(f\"   Cost Increase per Query: ${cost_difference / len(small_embedding_results['evaluation_df']):.6f}\")\n",
    "\n",
    "# 4. Performance vs Cost Analysis\n",
    "faithfulness_improvement = (large_embedding_data['Faithfulness'] - small_embedding_data['Faithfulness'])\n",
    "context_recall_improvement = (large_embedding_data['Context Recall'] - small_embedding_data['Context Recall'])\n",
    "\n",
    "print(f\"\\n‚öñÔ∏è Performance vs Cost Trade-off:\")\n",
    "print(f\"   Faithfulness Improvement: {faithfulness_improvement:.4f} ({faithfulness_improvement/small_embedding_data['Faithfulness']*100:.1f}%)\")\n",
    "print(f\"   Context Recall Improvement: {context_recall_improvement:.4f} ({context_recall_improvement/small_embedding_data['Context Recall']*100:.1f}%)\")\n",
    "print(f\"   Cost per 1% Faithfulness Improvement: ${cost_difference/(faithfulness_improvement*100):.6f}\" if faithfulness_improvement > 0 else \"   Faithfulness: No improvement\")\n",
    "\n",
    "# 5. Create enhanced comparison DataFrame with cost details\n",
    "enhanced_comparison = pd.DataFrame({\n",
    "    'Metric': ['Small Embedding', 'Large Embedding'],\n",
    "    'Total Cost ($)': [small_embedding_summary['total_cost_usd'], large_embedding_summary['total_cost_usd']],\n",
    "    'LLM Cost ($)': [small_embedding_summary['llm_cost_usd'], large_embedding_summary['llm_cost_usd']],\n",
    "    'Embedding Cost ($)': [small_embedding_summary['embedding_cost_usd'], large_embedding_summary['embedding_cost_usd']],\n",
    "    'Avg Cost/Query ($)': [small_embedding_summary['average_cost_per_query_usd'], large_embedding_summary['average_cost_per_query_usd']],\n",
    "    'Faithfulness': [small_embedding_data['Faithfulness'], large_embedding_data['Faithfulness']],\n",
    "    'Context Recall': [small_embedding_data['Context Recall'], large_embedding_data['Context Recall']],\n",
    "    'Embedding Model': ['text-embedding-3-small', 'text-embedding-3-large']\n",
    "})\n",
    "\n",
    "print(f\"\\nüìã Enhanced Comparison Table:\")\n",
    "print(enhanced_comparison.round(6).to_string(index=False))\n",
    "\n",
    "# 6. Log the enhanced comparison to W&B\n",
    "enhanced_run = wandb.init(project=\"embedding-model-comparison\", name=\"enhanced-cost-analysis\", reinit=True)\n",
    "\n",
    "# Log the enhanced comparison table\n",
    "enhanced_run.log({\"enhanced_cost_comparison\": wandb.Table(dataframe=enhanced_comparison)})\n",
    "\n",
    "# Log key insights as metrics\n",
    "enhanced_run.log({\n",
    "    \"cost_increase_percentage\": cost_increase_percentage,\n",
    "    \"cost_difference_usd\": cost_difference,\n",
    "    \"embedding_cost_difference_usd\": embedding_cost_difference,\n",
    "    \"faithfulness_improvement\": faithfulness_improvement,\n",
    "    \"context_recall_improvement\": context_recall_improvement,\n",
    "    \"cost_per_faithfulness_improvement\": cost_difference/(faithfulness_improvement*100) if faithfulness_improvement > 0 else 0\n",
    "})\n",
    "\n",
    "enhanced_run.finish()\n",
    "\n",
    "print(f\"\\n‚úÖ Enhanced cost analysis complete and logged to W&B!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07ad1b87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embedder_model</th>\n",
       "      <th>Execution Time (s)</th>\n",
       "      <th>Avg Cost (USD)</th>\n",
       "      <th>Avg Tokens/Query</th>\n",
       "      <th>Faithfulness</th>\n",
       "      <th>Context Recall</th>\n",
       "      <th>Factual Correctness</th>\n",
       "      <th>Response Relevancy</th>\n",
       "      <th>Noise Sensitivity</th>\n",
       "      <th>Context Entity Recall</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>System</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Small Embedding (text-embedding-3-small)</th>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>282.733249</td>\n",
       "      <td>0.000840</td>\n",
       "      <td>4981.2</td>\n",
       "      <td>0.861166</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.771222</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.394305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Large Embedding (text-embedding-3-large)</th>\n",
       "      <td>text-embedding-3-large</td>\n",
       "      <td>242.198028</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>4865.0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.864172</td>\n",
       "      <td>0.298932</td>\n",
       "      <td>0.337561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  embedder_model  \\\n",
       "System                                                             \n",
       "Small Embedding (text-embedding-3-small)  text-embedding-3-small   \n",
       "Large Embedding (text-embedding-3-large)  text-embedding-3-large   \n",
       "\n",
       "                                          Execution Time (s)  Avg Cost (USD)  \\\n",
       "System                                                                         \n",
       "Small Embedding (text-embedding-3-small)          282.733249        0.000840   \n",
       "Large Embedding (text-embedding-3-large)          242.198028        0.000824   \n",
       "\n",
       "                                          Avg Tokens/Query  Faithfulness  \\\n",
       "System                                                                     \n",
       "Small Embedding (text-embedding-3-small)            4981.2      0.861166   \n",
       "Large Embedding (text-embedding-3-large)            4865.0      0.800000   \n",
       "\n",
       "                                          Context Recall  Factual Correctness  \\\n",
       "System                                                                          \n",
       "Small Embedding (text-embedding-3-small)            0.94                0.477   \n",
       "Large Embedding (text-embedding-3-large)            0.96                0.528   \n",
       "\n",
       "                                          Response Relevancy  \\\n",
       "System                                                         \n",
       "Small Embedding (text-embedding-3-small)            0.771222   \n",
       "Large Embedding (text-embedding-3-large)            0.864172   \n",
       "\n",
       "                                          Noise Sensitivity  \\\n",
       "System                                                        \n",
       "Small Embedding (text-embedding-3-small)                NaN   \n",
       "Large Embedding (text-embedding-3-large)           0.298932   \n",
       "\n",
       "                                          Context Entity Recall  \n",
       "System                                                           \n",
       "Small Embedding (text-embedding-3-small)               0.394305  \n",
       "Large Embedding (text-embedding-3-large)               0.337561  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b90b3b",
   "metadata": {},
   "source": [
    "## üéì Summary and Next Steps\n",
    "\n",
    "You have successfully executed an **embedding model comparison experiment** with **enhanced cost analytics** for the Hybrid RAG system and logged all results to W&B project: `embedding-model-comparison`.\n",
    "\n",
    "### Key Accomplishments:\n",
    "1. **Document Store Management:** Used separate Elasticsearch instances (small: port 9200, large: port 9201) for proper comparison between embedding models.\n",
    "2. **Small vs Large Embedding Comparison:** Tested `text-embedding-3-small` vs `text-embedding-3-large` with **current OpenAI pricing** (November 2024).\n",
    "3. **Enhanced Cost Analytics:** Implemented comprehensive cost tracking including:\n",
    "   - **LLM costs** (input/output tokens)\n",
    "   - **Embedding costs** (indexing and retrieval operations)\n",
    "   - **Cost breakdowns** and efficiency metrics\n",
    "   - **Performance vs cost trade-offs**\n",
    "4. **Controlled Variables:** Fixed all parameters except embedding model to isolate impact.\n",
    "5. **Comprehensive W&B Logging:** Enhanced logging with cost breakdowns, embedding comparisons, and efficiency metrics.\n",
    "\n",
    "### Current OpenAI Pricing Integration:\n",
    "- **text-embedding-3-small**: $0.02 per 1M tokens\n",
    "- **text-embedding-3-large**: $0.13 per 1M tokens  \n",
    "- **gpt-4o-mini**: $0.15/$0.60 per 1M input/output tokens\n",
    "- **Automatic cost calculation** for both LLM and embedding operations\n",
    "\n",
    "### Expected Insights from Enhanced Analytics:\n",
    "- **Cost Breakdown**: Percentage split between LLM vs embedding costs\n",
    "- **ROI Analysis**: Cost per performance improvement metrics\n",
    "- **Efficiency Metrics**: Tokens per dollar and cost per query optimization\n",
    "- **Model Comparison**: Direct cost and performance comparison between embedding models\n",
    "\n",
    "### Next Steps in W&B:\n",
    "1. **Enhanced Dashboard**: View the `embedding-model-comparison` project with new cost breakdown visualizations\n",
    "2. **Performance ROI**: Analyze cost per improvement in faithfulness and context recall\n",
    "3. **Production Planning**: Use cost vs performance data for budget planning\n",
    "4. **Optimization Strategy**: Consider hybrid approaches or query-based model selection based on cost/performance profiles\n",
    "5. **Scaling Analysis**: Project costs for production volumes using the detailed metrics\n",
    "\n",
    "### Key Files Updated:\n",
    "- ‚úÖ **RAGAnalytics**: Enhanced with current OpenAI pricing and embedding cost tracking\n",
    "- ‚úÖ **Notebook**: Compatible with updated analytics class and dual Elasticsearch setup\n",
    "- ‚úÖ **Cost Analysis**: Comprehensive cost breakdown and efficiency metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-with-haystack-ch6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
