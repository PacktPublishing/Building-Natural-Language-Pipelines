{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba5f0310",
   "metadata": {},
   "source": [
    "# Hybrid RAG Optimization with Weights & Biases ðŸ“ŠðŸ”¬\n",
    "\n",
    "## ðŸ“‹ Overview\n",
    "\n",
    "This notebook focuses on using **Weights & Biases (W&B)** to systematically evaluate and optimize our **Hybrid RAG** system. We will run two experiments: a baseline and an optimized configuration, with the goal of improving the `faithfulness` metric while monitoring cost.\n",
    "\n",
    "### Hybrid RAG System Configuration (to be optimized):\n",
    "\n",
    "| Parameter | Baseline Value | Optimization Target |\n",
    "| :--- | :--- | :--- |\n",
    "| `embedder_model` | `sentence-transformers/all-MiniLM-L6-v2` | Fixed |\n",
    "| `llm_model` | `gpt-4o-mini` | Fixed |\n",
    "| **`retriever_top_k`** | **3** | **7** |\n",
    "| `rag_type` | `hybrid` | Fixed |\n",
    "| `reranker_model` | `BAAI/bge-reranker-base` | Fixed |\n",
    "| `bm25_enabled` | `True` | Fixed |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup: Imports and Environment variables loaded.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import tiktoken\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# Import Haystack/Ragas components\n",
    "from haystack import Pipeline\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, ResponseRelevancy, ContextEntityRecall, NoiseSensitivity\n",
    "\n",
    "# Import custom components (assuming these paths exist relative to the notebook)\n",
    "try:\n",
    "    from scripts.ragasevaluation import CSVReaderComponent, RAGDataAugmenterComponent, RagasEvaluationComponent\n",
    "    from scripts.rag.hybridrag import hybrid_rag_sc\n",
    "    from scripts.wandb_experiments.rag_analytics import RAGAnalytics\n",
    "except ImportError:\n",
    "    print(\"WARNING: Custom components could not be imported. Ensure 'scripts/ragasevaluation.py' and 'scripts/rag/hybridrag.py' are available.\")\n",
    "\n",
    "# Environment setup (reduced logging)\n",
    "os.environ[\"HAYSTACK_CONTENT_TRACING_ENABLED\"] = \"false\"\n",
    "print(\"Setup: Imports and Environment variables loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "raganalytics_class",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "class RAGEvaluationExperiment:\n",
    "    \"\"\"Enhanced RAG evaluation workflow with streamlined W&B integration.\"\"\"\n",
    "    \n",
    "    def __init__(self, project_name: str, experiment_name: str):\n",
    "        self.project_name = project_name\n",
    "        self.experiment_name = experiment_name\n",
    "        self.run = None\n",
    "        self.pipeline = None\n",
    "    \n",
    "    def setup_pipeline(self, rag_supercomponent, metrics_list: list, config: dict = None):\n",
    "        \"\"\"Set up the evaluation pipeline with W&B tracking.\"\"\"\n",
    "        self.run = wandb.init(\n",
    "            project=self.project_name,\n",
    "            name=self.experiment_name,\n",
    "            config=config,\n",
    "            reinit=True\n",
    "        )\n",
    "        print(f\"W&B STARTED: {self.experiment_name} | URL: {self.run.url}\")\n",
    "        \n",
    "        # Initialize components\n",
    "        reader = CSVReaderComponent()\n",
    "        # The RAGDataAugmenterComponent needs access to the current config (specifically top_k)\n",
    "        # We assume the underlying implementation of hybrid_rag_sc takes the config or is already correctly set up.\n",
    "        # Since the goal is only to change the parameter in the run config, we leave the RAG SuperComponent initialization simple here.\n",
    "        # The actual RAG configuration modification logic should ideally be inside RAGDataAugmenterComponent/hybrid_rag_sc.\n",
    "        # For this minimal example, we assume the wrapper logic or environment variables handle the parameter change for hybrid_rag_sc based on 'config'.\n",
    "        augmenter = RAGDataAugmenterComponent(rag_supercomponent=rag_supercomponent)\n",
    "        evaluator = RagasEvaluationComponent(metrics=metrics_list)\n",
    "        \n",
    "        self.pipeline = Pipeline()\n",
    "        self.pipeline.add_component(\"reader\", reader)\n",
    "        self.pipeline.add_component(\"augmenter\", augmenter)\n",
    "        self.pipeline.add_component(\"evaluator\", evaluator)\n",
    "        \n",
    "        self.pipeline.connect(\"reader.data_frame\", \"augmenter.data_frame\")\n",
    "        self.pipeline.connect(\"augmenter.augmented_data_frame\", \"evaluator.augmented_data_frame\")\n",
    "        \n",
    "        return self.pipeline\n",
    "    \n",
    "    def run_evaluation(self, csv_file_path: str):\n",
    "        \"\"\"Execute the pipeline, log high-level metrics, and return results.\"\"\"\n",
    "        if not self.pipeline:\n",
    "            raise ValueError(\"Pipeline not set up. Call setup_pipeline() first.\")\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        print(f\"\\nRunning pipeline on {csv_file_path}...\")\n",
    "        results = self.pipeline.run({\"reader\": {\"source\": csv_file_path}})\n",
    "        end_time = datetime.now()\n",
    "        \n",
    "        execution_time = (end_time - start_time).total_seconds()\n",
    "        metrics = results[\"evaluator\"][\"metrics\"]\n",
    "        evaluation_df = results[\"evaluator\"][\"evaluation_df\"].rename(columns={\n",
    "            'factual_correctness(mode=f1)': 'factual_correctness_f1'\n",
    "        })\n",
    "        \n",
    "        # Log dataset artifact\n",
    "        dataset_artifact = wandb.Artifact(name=f\"evaluation-dataset-{Path(csv_file_path).stem}\", type=\"dataset\")\n",
    "        dataset_artifact.add_file(csv_file_path)\n",
    "        self.run.log_artifact(dataset_artifact)\n",
    "        \n",
    "        # Extract and log summary metrics\n",
    "        wandb_metrics = {\n",
    "            \"execution_time_seconds\": execution_time,\n",
    "            \"num_queries_evaluated\": len(evaluation_df),\n",
    "        }\n",
    "        # Simple conversion of Ragas EvaluationResult metrics to flat dictionary\n",
    "        if hasattr(metrics, 'to_dict'):\n",
    "            metrics_dict = metrics.to_dict()\n",
    "            for metric_name, metric_value in metrics_dict.items():\n",
    "                if isinstance(metric_value, (int, float)):\n",
    "                    # Standardize metric names for W&B comparison\n",
    "                    clean_name = metric_name.replace('(mode=f1)', '').replace('ragas_', '').strip()\n",
    "                    wandb_metrics[f\"ragas_{clean_name}\"] = metric_value\n",
    "        \n",
    "        self.run.log(wandb_metrics)\n",
    "        print(f\"Evaluation Complete: Logged {len(evaluation_df)} queries and {len(wandb_metrics)} metrics.\")\n",
    "        \n",
    "        return {\n",
    "            \"metrics\": metrics, # Full EvaluationResult object\n",
    "            \"evaluation_df\": evaluation_df,\n",
    "            \"execution_time\": execution_time,\n",
    "            \"wandb_url\": self.run.url\n",
    "        }\n",
    "    \n",
    "    def finish_experiment(self):\n",
    "        \"\"\"Finish the W&B run.\"\"\"\n",
    "        if self.run:\n",
    "            url = self.run.url\n",
    "            self.run.finish()\n",
    "            print(f\"\\nW&B COMPLETED: {self.experiment_name} | View Results: {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610afd8a",
   "metadata": {},
   "source": [
    "## ðŸ”¬ Experiment 1: Hybrid RAG Baseline (`retriever_top_k=3`)\n",
    "\n",
    "We establish a performance baseline for the Hybrid RAG system using a retrieval limit of `top_k=3` documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_1_setup",
   "metadata": {
    "tags": [
     "setup"
    ]
   },
   "outputs": [],
   "source": [
    "# 1. Define evaluation metrics (Focusing on core RAGAS metrics)\n",
    "evaluation_metrics = [LLMContextRecall(), \\\n",
    "                Faithfulness(), \\\n",
    "                FactualCorrectness(), \\\n",
    "                ResponseRelevancy(), \\\n",
    "                ContextEntityRecall(), \\\n",
    "                NoiseSensitivity()]\n",
    "csv_file_path = \"data_for_eval/synthetic_tests_advanced_branching_3.csv\"\n",
    "\n",
    "# 2. Configuration for the Baseline Experiment\n",
    "baseline_config = {\n",
    "    \"embedder_model\": \"text-embedding-3-small\",\n",
    "    \"llm_model\": \"gpt-4o-mini\",\n",
    "    \"retriever_top_k\": 3,  # Baseline value\n",
    "    \"rag_type\": \"hybrid\",\n",
    "    \"document_store\": \"elasticsearch\",\n",
    "    \"reranker_model\": \"BAAI/bge-reranker-base\",\n",
    "}\n",
    "\n",
    "# 3. Initialize and Setup Baseline Experiment\n",
    "baseline_experiment = RAGEvaluationExperiment(\n",
    "    project_name=\"hybrid-rag-optimization\",\n",
    "    experiment_name=\"hybrid-rag-baseline-k3\"\n",
    ")\n",
    "\n",
    "baseline_pipeline = baseline_experiment.setup_pipeline(\n",
    "    rag_supercomponent=hybrid_rag_sc, \n",
    "    metrics_list=evaluation_metrics,\n",
    "    config=baseline_config\n",
    ")\n",
    "\n",
    "# 4. Run the evaluation and store results\n",
    "baseline_results = baseline_experiment.run_evaluation(\n",
    "    csv_file_path=csv_file_path\n",
    ")\n",
    "\n",
    "# 5. Run Analytics and log to W&B\n",
    "baseline_analytics = RAGAnalytics(baseline_results, model_name=baseline_config['llm_model'])\n",
    "baseline_summary = baseline_analytics.log_to_wandb(baseline_experiment.run)\n",
    "\n",
    "# 6. Finish the experiment run\n",
    "baseline_experiment.finish_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d0130f",
   "metadata": {},
   "source": [
    "## ðŸš€ Experiment 2: Hybrid RAG Optimization (`retriever_top_k=7`)\n",
    "\n",
    "To potentially improve context recall and faithfulness, we'll increase the number of retrieved documents from 3 to 7. This is a common **hyperparameter tuning** strategy to check the tradeoff between performance and cost/latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_2_setup",
   "metadata": {
    "tags": [
     "optimization"
    ]
   },
   "outputs": [],
   "source": [
    "# 1. Configuration for the Optimized Experiment (change top_k)\n",
    "optimized_config = baseline_config.copy()\n",
    "optimized_config['retriever_top_k'] = 7  # Optimized value\n",
    "\n",
    "# 2. Initialize and Setup Optimized Experiment\n",
    "optimized_experiment = RAGEvaluationExperiment(\n",
    "    project_name=\"hybrid-rag-optimization\",\n",
    "    experiment_name=\"hybrid-rag-optimized-k7\"\n",
    ")\n",
    "\n",
    "optimized_pipeline = optimized_experiment.setup_pipeline(\n",
    "    rag_supercomponent=hybrid_rag_sc, \n",
    "    metrics_list=evaluation_metrics,\n",
    "    config=optimized_config\n",
    ")\n",
    "\n",
    "# 3. Run the evaluation and store results\n",
    "optimized_results = optimized_experiment.run_evaluation(\n",
    "    csv_file_path=csv_file_path\n",
    ")\n",
    "\n",
    "# 4. Run Analytics and log to W&B\n",
    "optimized_analytics = RAGAnalytics(optimized_results, model_name=optimized_config['llm_model'])\n",
    "optimized_summary = optimized_analytics.log_to_wandb(optimized_experiment.run)\n",
    "\n",
    "# 5. Finish the experiment run\n",
    "optimized_experiment.finish_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd620687",
   "metadata": {},
   "source": [
    "## ðŸ“Š Comparative Analysis & Key Insights\n",
    "\n",
    "Now we can programmatically compare the key metrics between the two runs. The full comparison is available in the W&B dashboard, but a quick summary confirms the tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b8518c",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison_analysis",
   "metadata": {
    "tags": [
     "summary"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def extract_ragas_metrics(metrics_obj):\n",
    "    \"\"\"Extract and flatten RAGAS metrics from the result object for comparison.\"\"\"\n",
    "    metrics_dict = {} \n",
    "    if hasattr(metrics_obj, 'to_dict'):\n",
    "        raw_metrics = metrics_obj.to_dict()\n",
    "        for k, v in raw_metrics.items():\n",
    "            if isinstance(v, (float, int)):\n",
    "                # Clean metric name for the output table\n",
    "                clean_name = k.replace('(mode=f1)', '').strip()\n",
    "                metrics_dict[clean_name] = v\n",
    "    return metrics_dict\n",
    "\n",
    "# 1. Extract and combine summary data\n",
    "baseline_data = {\n",
    "    'System': 'Baseline (k=3)',\n",
    "    'retriever_top_k': 3,\n",
    "    'Execution Time (s)': baseline_results['execution_time'],\n",
    "    'Avg Cost (USD)': baseline_summary['average_cost_per_query_usd'],\n",
    "    'Avg Tokens/Query': baseline_summary['average_tokens_per_query'],\n",
    "    'Faithfulness': np.array(baseline_results['metrics']['faithfulness']).mean(),\n",
    "    'Context Recall': np.array(baseline_results['metrics']['context_recall']).mean(),\n",
    "    'Factual Correctness': np.array(baseline_results['metrics']['factual_correctness(mode=f1)']).mean(),\n",
    "    'Response Relevancy': np.array(baseline_results['metrics']['answer_relevancy']).mean(),\n",
    "    'Noise Sensitivity': np.array(baseline_results['metrics']['noise_sensitivity(mode=relevant)']).mean(),\n",
    "    'Context Entity Recall': np.array(baseline_results['metrics']['context_entity_recall']).mean()\n",
    "}\n",
    "\n",
    "optimized_data = {\n",
    "    'System': 'Optimized (k=7)',\n",
    "    'retriever_top_k': 7,\n",
    "    'Execution Time (s)': optimized_results['execution_time'],\n",
    "    'Avg Cost (USD)': optimized_summary['average_cost_per_query_usd'],\n",
    "    'Avg Tokens/Query': optimized_summary['average_tokens_per_query'],\n",
    "    'Faithfulness': np.array(optimized_results['metrics']['faithfulness']).mean(),\n",
    "    'Context Recall': np.array(optimized_results['metrics']['context_recall']).mean(),\n",
    "    'Factual Correctness': np.array(optimized_results['metrics']['factual_correctness(mode=f1)']).mean(),\n",
    "    'Response Relevancy': np.array(optimized_results['metrics']['answer_relevancy']).mean(),\n",
    "    'Noise Sensitivity': np.array(optimized_results['metrics']['noise_sensitivity(mode=relevant)']).mean(),\n",
    "    'Context Entity Recall': np.array(optimized_results['metrics']['context_entity_recall']).mean()\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame([baseline_data, optimized_data])\n",
    "comparison_df = comparison_df.set_index('System')\n",
    "\n",
    "\n",
    "# Log final summary table to W&B for easy comparison\n",
    "final_run = wandb.init(project=\"hybrid-rag-optimization\", name=\"final-comparison\", reinit=True)\n",
    "final_run.log({\"final_comparison_table\": wandb.Table(dataframe=comparison_df.reset_index())})\n",
    "final_run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34812c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_results['metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5438aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_results['metrics']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b90b3b",
   "metadata": {},
   "source": [
    "## ðŸŽ“ Summary and Next Steps\n",
    "\n",
    "You have successfully executed a targeted optimization experiment for the Hybrid RAG system and logged all results to a single W&B project: `hybrid-rag-optimization`.\n",
    "\n",
    "### Key Accomplishments:\n",
    "1.  **Baseline Established:** Created a baseline measurement of Hybrid RAG performance (`top_k=3`) across RAGAS metrics, cost, and latency.\n",
    "2.  **Parameter Tuning:** Successfully ran an experiment with an updated hyperparameter (`top_k=7`).\n",
    "3.  **Comprehensive Logging:** Logged both runs, their configurations, raw data samples, and **meaningful visualizations** to W&B.\n",
    "\n",
    "### Next Steps in W&B:\n",
    "1.  **View Comparison:** Navigate to the **`hybrid-rag-optimization`** project dashboard on W&B. Use the **Compare Runs** feature to analyze the **faithfulness vs. cost scatter plot** and the **cost vs. input tokens scatter plot** to understand the true impact of increasing `top_k`.\n",
    "2.  **Deeper Optimization:** Utilize W&B Sweeps to automate the search for the optimal `retriever_top_k` value, or try tuning other parameters like `reranker_model` or document chunking strategies.\n",
    "3.  **Reproduce:** Every run is versioned, ensuring you can reproduce these results precisely later."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-with-haystack-ch6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
