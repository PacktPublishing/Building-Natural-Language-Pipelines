{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14c78581",
   "metadata": {},
   "source": [
    "üîß **Setup Required**: Before running this notebook, please follow the [setup instructions](../README.md#setup-instructions) to configure your environment and API keys. **You will need to ensure you've executed the Indexing pipeline before completing this exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5f0310",
   "metadata": {},
   "source": [
    "# Hybrid RAG Embedding Model Comparison with Weights & Biases üìäüî¨\n",
    "\n",
    "## üìã Overview\n",
    "\n",
    "This notebook focuses on using **Weights & Biases (W&B)** to systematically compare **embedding models** in our **Hybrid RAG** system. We will run two experiments comparing `text-embedding-3-small` vs `text-embedding-3-large` to understand the **quality vs cost tradeoff**.\n",
    "\n",
    "### Hybrid RAG System Configuration (Embedding Model Comparison):\n",
    "\n",
    "| Parameter | Small Model Experiment | Large Model Experiment |\n",
    "| :--- | :--- | :--- |\n",
    "| **`embedder_model`** | **`text-embedding-3-small`** | **`text-embedding-3-large`** |\n",
    "| `llm_model` | `gpt-4o-mini` | `gpt-4o-mini` |\n",
    "| `retriever_top_k` | `5` | `5` |\n",
    "| `rag_type` | `hybrid` | `hybrid` |\n",
    "| `reranker_model` | `BAAI/bge-reranker-base` | `BAAI/bge-reranker-base` |\n",
    "| `bm25_enabled` | `True` | `True` |\n",
    "\n",
    "### üî¨ Experimental Hypothesis:\n",
    "- **Large embedding model** may provide better semantic understanding ‚Üí higher faithfulness/context recall\n",
    "- **Small embedding model** will be more cost-effective ‚Üí better cost-per-query metrics\n",
    "- Both models will be re-indexed with fresh documents to ensure fair comparison\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All custom components imported successfully\n",
      "Setup: Imports and Environment variables loaded.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Add the current directory to Python path to ensure imports work\n",
    "current_dir = Path.cwd()\n",
    "if str(current_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(current_dir))\n",
    "\n",
    "# Import Haystack/Ragas components\n",
    "from ragas.metrics import (LLMContextRecall,\\\n",
    "    Faithfulness,\\\n",
    "    FactualCorrectness,\\\n",
    "    ResponseRelevancy,\\\n",
    "    ContextEntityRecall, NoiseSensitivity)\n",
    "from haystack_integrations.document_stores.elasticsearch import ElasticsearchDocumentStore\n",
    "\n",
    "# Import custom components (assuming these paths exist relative to the notebook)\n",
    "try:\n",
    "    from scripts.rag.hybridrag import HybridRAGSuperComponent\n",
    "    from scripts.ragas_evaluation.ragasevalsupercomponent import RAGEvaluationSuperComponent\n",
    "    from scripts.wandb_experiments.rag_analytics import RAGAnalytics\n",
    "    print(\"‚úÖ All custom components imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"WARNING: Custom components could not be imported: {e}\")\n",
    "    print(\"Ensure all required components are available.\")\n",
    "\n",
    "# Environment setup (reduced logging)\n",
    "os.environ[\"HAYSTACK_CONTENT_TRACING_ENABLED\"] = \"false\"\n",
    "print(\"Setup: Imports and Environment variables loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174fb00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Set W&B to offline mode to avoid network timeouts\n",
    "# Uncomment the line below if you experience timeout errors\n",
    "# os.environ[\"WANDB_MODE\"] = \"offline\"\n",
    "\n",
    "# Optional: Disable W&B console logging to reduce noise\n",
    "# os.environ[\"WANDB_SILENT\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "raganalytics_class",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "class RAGEvaluationExperiment:\n",
    "    \"\"\"Enhanced RAG evaluation workflow with streamlined W&B integration using RAGEvaluationSuperComponent.\"\"\"\n",
    "    \n",
    "    def __init__(self, project_name: str, experiment_name: str):\n",
    "        self.project_name = project_name\n",
    "        self.experiment_name = experiment_name\n",
    "        self.run = None\n",
    "        self.evaluation_supercomponent = None\n",
    "    \n",
    "    def setup_pipeline(self, rag_supercomponent, generator, metrics_list: list, config: dict = None):\n",
    "        \"\"\"Set up the evaluation pipeline with W&B tracking using RAGEvaluationSuperComponent.\"\"\"\n",
    "        self.run = wandb.init(\n",
    "            project=self.project_name,\n",
    "            name=self.experiment_name,\n",
    "            config=config,\n",
    "            reinit=True\n",
    "        )\n",
    "        print(f\"W&B STARTED: {self.experiment_name} | URL: {self.run.url}\")\n",
    "        \n",
    "        # Initialize the RAGEvaluationSuperComponent with the RAG system to evaluate\n",
    "        self.evaluation_supercomponent = RAGEvaluationSuperComponent(\n",
    "            rag_supercomponent=rag_supercomponent,\n",
    "            system_name=self.experiment_name,\n",
    "            generator=generator\n",
    "        )\n",
    "        \n",
    "        # Override the metrics in the evaluator component if custom metrics are provided\n",
    "        if metrics_list:\n",
    "            # Access the evaluator component and update its metrics\n",
    "            evaluator = self.evaluation_supercomponent.pipeline.get_component(\"evaluator\")\n",
    "            evaluator.metrics = metrics_list\n",
    "        \n",
    "        return self.evaluation_supercomponent\n",
    "    \n",
    "    def run_evaluation(self, csv_file_path: str):\n",
    "        \"\"\"Execute the pipeline using RAGEvaluationSuperComponent, log high-level metrics, and return results.\"\"\"\n",
    "        if not self.evaluation_supercomponent:\n",
    "            raise ValueError(\"Pipeline not set up. Call setup_pipeline() first.\")\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        print(f\"\\nRunning RAGEvaluationSuperComponent on {csv_file_path}...\")\n",
    "        \n",
    "        # Run the supercomponent with the CSV file\n",
    "        results = self.evaluation_supercomponent.run(csv_source=csv_file_path)\n",
    "        end_time = datetime.now()\n",
    "        \n",
    "        execution_time = (end_time - start_time).total_seconds()\n",
    "        metrics = results[\"metrics\"]\n",
    "        evaluation_df = results[\"evaluation_df\"].rename(columns={\n",
    "            'factual_correctness(mode=f1)': 'factual_correctness_f1'\n",
    "        })\n",
    "        \n",
    "        # Log dataset artifact\n",
    "        dataset_artifact = wandb.Artifact(name=f\"evaluation-dataset-{Path(csv_file_path).stem}\", type=\"dataset\")\n",
    "        dataset_artifact.add_file(csv_file_path)\n",
    "        self.run.log_artifact(dataset_artifact)\n",
    "        \n",
    "        # Extract and log summary metrics\n",
    "        wandb_metrics = {\n",
    "            \"execution_time_seconds\": execution_time,\n",
    "            \"num_queries_evaluated\": len(evaluation_df),\n",
    "        }\n",
    "        # Simple conversion of Ragas EvaluationResult metrics to flat dictionary\n",
    "        if hasattr(metrics, 'to_dict'):\n",
    "            metrics_dict = metrics.to_dict()\n",
    "            for metric_name, metric_value in metrics_dict.items():\n",
    "                if isinstance(metric_value, (int, float)):\n",
    "                    # Standardize metric names for W&B comparison\n",
    "                    clean_name = metric_name.replace('(mode=f1)', '').replace('ragas_', '').strip()\n",
    "                    wandb_metrics[f\"ragas_{clean_name}\"] = metric_value\n",
    "        \n",
    "        self.run.log(wandb_metrics)\n",
    "        print(f\"Evaluation Complete: Logged {len(evaluation_df)} queries and {len(wandb_metrics)} metrics.\")\n",
    "        \n",
    "        return {\n",
    "            \"metrics\": metrics, # Full EvaluationResult object\n",
    "            \"evaluation_df\": evaluation_df,\n",
    "            \"execution_time\": execution_time,\n",
    "            \"wandb_url\": self.run.url\n",
    "        }\n",
    "    \n",
    "    def finish_experiment(self):\n",
    "        \"\"\"Finish the W&B run.\"\"\"\n",
    "        if self.run:\n",
    "            url = self.run.url\n",
    "            self.run.finish()\n",
    "            print(f\"\\nW&B COMPLETED: {self.experiment_name} | View Results: {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5d5e6d",
   "metadata": {},
   "source": [
    "## üóÇÔ∏è Document Store Setup and Indexing\n",
    "\n",
    "Before running our experiments, we need to set up our document sources and create a clean indexing process for each embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b87424f",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store_small = ElasticsearchDocumentStore(hosts=\"http://localhost:9200\")\n",
    "document_store_large = ElasticsearchDocumentStore(hosts=\"http://localhost:9201\", index=\"large_embeddings\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610afd8a",
   "metadata": {},
   "source": [
    "## üî¨ Experiment 1: Hybrid RAG with Small Embedding Model (`text-embedding-3-small`)\n",
    "\n",
    "We establish a performance baseline for the Hybrid RAG system using the smaller, more cost-effective embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "run_1_setup",
   "metadata": {
    "tags": [
     "setup"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlgutierrwr\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/laurafunderburk/Documents/GitHub/Building-Natural-Language-Pipelines/ch6/jupyter-notebooks/wandb/run-20251121_205551-dxha4k60</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/dxha4k60' target=\"_blank\">hybrid-rag-small-embedding</a></strong> to <a href='https://wandb.ai/lgutierrwr/embedding-model-comparison' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lgutierrwr/embedding-model-comparison' target=\"_blank\">https://wandb.ai/lgutierrwr/embedding-model-comparison</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/dxha4k60' target=\"_blank\">https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/dxha4k60</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [instructor, openai] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Weave is installed but not imported. Add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Weave is installed but not imported. Add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B STARTED: hybrid-rag-small-embedding | URL: https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/dxha4k60\n",
      "\n",
      "üîÑ Building evaluation pipeline for hybrid-rag-small-embedding...\n",
      "==================================================\n",
      "‚úÖ Evaluation pipeline for hybrid-rag-small-embedding built successfully!\n",
      "\n",
      "Running RAGEvaluationSuperComponent on data_for_eval/synthetic_tests_advanced_branching_10.csv...\n",
      "Loaded DataFrame with 10 rows from data_for_eval/synthetic_tests_advanced_branching_10.csv.\n",
      "Running RAG SuperComponent on 10 queries...\n",
      "RAG processing complete.\n",
      "Creating Ragas EvaluationDataset...\n",
      "Starting Ragas evaluation...\n",
      "RAG processing complete.\n",
      "Creating Ragas EvaluationDataset...\n",
      "Starting Ragas evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d41fafc9939042efa60939dbde64accc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Exception raised in Job[53]: TimeoutError()\n",
      "Exception raised in Job[53]: TimeoutError()\n",
      "Exception raised in Job[59]: TimeoutError()\n",
      "Exception raised in Job[59]: TimeoutError()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ragas evaluation complete.\n",
      "Overall metrics: {'context_recall': 0.9600, 'faithfulness': 0.9121, 'factual_correctness(mode=f1)': 0.5110, 'answer_relevancy': 0.8557, 'context_entity_recall': 0.3280, 'noise_sensitivity(mode=relevant)': 0.3562}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Complete: Logged 10 queries and 2 metrics.\n",
      "Analytics: Logged comprehensive analysis for 10 queries.\n",
      "Total Cost: $0.009311 (LLM: $0.008356, Embeddings: $0.000955)\n",
      "Embedding Models: text-embedding-3-small\n",
      "Analytics: Logged comprehensive analysis for 10 queries.\n",
      "Total Cost: $0.009311 (LLM: $0.008356, Embeddings: $0.000955)\n",
      "Embedding Models: text-embedding-3-small\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_cost_per_query_usd</td><td>‚ñÅ</td></tr><tr><td>average_embedding_tokens_per_operation</td><td>‚ñÅ</td></tr><tr><td>average_input_tokens_per_query</td><td>‚ñÅ</td></tr><tr><td>average_llm_tokens_per_query</td><td>‚ñÅ</td></tr><tr><td>average_output_tokens_per_query</td><td>‚ñÅ</td></tr><tr><td>cost_breakdown_embedding_percentage</td><td>‚ñÅ</td></tr><tr><td>cost_breakdown_llm_percentage</td><td>‚ñÅ</td></tr><tr><td>embedding_cost_usd</td><td>‚ñÅ</td></tr><tr><td>execution_time_seconds</td><td>‚ñÅ</td></tr><tr><td>llm_cost_usd</td><td>‚ñÅ</td></tr><tr><td>+6</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_cost_per_query_usd</td><td>0.00093</td></tr><tr><td>average_embedding_tokens_per_operation</td><td>1193.6</td></tr><tr><td>average_input_tokens_per_query</td><td>4774.5</td></tr><tr><td>average_llm_tokens_per_query</td><td>4973.6</td></tr><tr><td>average_output_tokens_per_query</td><td>199.1</td></tr><tr><td>cost_breakdown_embedding_percentage</td><td>10.25514</td></tr><tr><td>cost_breakdown_llm_percentage</td><td>89.74486</td></tr><tr><td>embedding_cost_usd</td><td>0.00095</td></tr><tr><td>embedding_models</td><td>text-embedding-3-sma...</td></tr><tr><td>execution_time_seconds</td><td>301.86398</td></tr><tr><td>+8</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">hybrid-rag-small-embedding</strong> at: <a href='https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/dxha4k60' target=\"_blank\">https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/dxha4k60</a><br> View project at: <a href='https://wandb.ai/lgutierrwr/embedding-model-comparison' target=\"_blank\">https://wandb.ai/lgutierrwr/embedding-model-comparison</a><br>Synced 4 W&B file(s), 2 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251121_205551-dxha4k60/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "W&B COMPLETED: hybrid-rag-small-embedding | View Results: https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/dxha4k60\n"
     ]
    }
   ],
   "source": [
    "# 1. Define evaluation metrics (Focusing on core RAGAS metrics)\n",
    "evaluation_metrics = [LLMContextRecall(), \\\n",
    "                Faithfulness(), \\\n",
    "                FactualCorrectness(), \\\n",
    "                ResponseRelevancy(), \\\n",
    "                ContextEntityRecall(), \\\n",
    "                NoiseSensitivity()]\n",
    "csv_file_path = \"data_for_eval/synthetic_tests_advanced_branching_10.csv\"\n",
    "\n",
    "# 2. Configuration for the Small Embedding Model Experiment\n",
    "small_embedding_config = {\n",
    "    \"embedder_model\": \"text-embedding-3-small\",\n",
    "    \"llm_model\": \"gpt-4o-mini\",\n",
    "    \"retriever_top_k\": 5,  # Fixed value\n",
    "    \"rag_type\": \"hybrid\",\n",
    "    \"document_store\": \"elasticsearch\",\n",
    "    \"reranker_model\": \"BAAI/bge-reranker-base\",\n",
    "}\n",
    "\n",
    "# 3. Create generator for evaluation\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.utils import Secret\n",
    "\n",
    "eval_generator = OpenAIGenerator(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=Secret.from_token(os.getenv(\"OPENAI_API_KEY\"))\n",
    ")\n",
    "\n",
    "# 4. Initialize RAG component with small embedder\n",
    "small_embedding_rag_sc = HybridRAGSuperComponent(\n",
    "    document_store=document_store_small,\n",
    "    embedder_model=small_embedding_config[\"embedder_model\"]\n",
    ")\n",
    "\n",
    "# 5. Initialize and Setup Small Embedding Experiment\n",
    "small_embedding_experiment = RAGEvaluationExperiment(\n",
    "    project_name=\"embedding-model-comparison\",\n",
    "    experiment_name=\"hybrid-rag-small-embedding\"\n",
    ")\n",
    "\n",
    "small_embedding_pipeline = small_embedding_experiment.setup_pipeline(\n",
    "    rag_supercomponent=small_embedding_rag_sc,\n",
    "    generator=eval_generator,\n",
    "    metrics_list=evaluation_metrics,\n",
    "    config=small_embedding_config\n",
    ")\n",
    "\n",
    "# 6. Run the evaluation and store results\n",
    "small_embedding_results = small_embedding_experiment.run_evaluation(\n",
    "    csv_file_path=csv_file_path\n",
    ")\n",
    "\n",
    "# 7. Run Analytics and log to W&B with updated RAGAnalytics\n",
    "# Pass the specific embedding model being used as a list\n",
    "small_embedding_analytics = RAGAnalytics(\n",
    "    results=small_embedding_results, \n",
    "    model_name=small_embedding_config['llm_model'],\n",
    "    embedding_models=[small_embedding_config['embedder_model']]  # Pass as list\n",
    ")\n",
    "small_embedding_summary = small_embedding_analytics.log_to_wandb(small_embedding_experiment.run)\n",
    "\n",
    "# 8. Finish the experiment run\n",
    "small_embedding_experiment.finish_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d0130f",
   "metadata": {},
   "source": [
    "## üöÄ Experiment 2: Hybrid RAG with Large Embedding Model (`text-embedding-3-large`)\n",
    "\n",
    "To potentially improve semantic understanding and retrieval quality, we'll use the larger, more powerful embedding model. This tests the tradeoff between **embedding quality vs. cost/latency**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "run_2_setup",
   "metadata": {
    "tags": [
     "optimization"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/laurafunderburk/Documents/GitHub/Building-Natural-Language-Pipelines/ch6/jupyter-notebooks/wandb/run-20251121_210108-wcshznyi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/wcshznyi' target=\"_blank\">hybrid-rag-large-embedding</a></strong> to <a href='https://wandb.ai/lgutierrwr/embedding-model-comparison' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lgutierrwr/embedding-model-comparison' target=\"_blank\">https://wandb.ai/lgutierrwr/embedding-model-comparison</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/wcshznyi' target=\"_blank\">https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/wcshznyi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference, instructor, openai] in use.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B STARTED: hybrid-rag-large-embedding | URL: https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/wcshznyi\n",
      "\n",
      "üîÑ Building evaluation pipeline for hybrid-rag-large-embedding...\n",
      "==================================================\n",
      "‚úÖ Evaluation pipeline for hybrid-rag-large-embedding built successfully!\n",
      "\n",
      "Running RAGEvaluationSuperComponent on data_for_eval/synthetic_tests_advanced_branching_10.csv...\n",
      "Loaded DataFrame with 10 rows from data_for_eval/synthetic_tests_advanced_branching_10.csv.\n",
      "Running RAG SuperComponent on 10 queries...\n",
      "RAG processing complete.\n",
      "Creating Ragas EvaluationDataset...\n",
      "Starting Ragas evaluation...\n",
      "RAG processing complete.\n",
      "Creating Ragas EvaluationDataset...\n",
      "Starting Ragas evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7cd2a17a6ef4d4ba3d1245bcdb1f044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "wandb-core(46659) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(46659) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(47196) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(47196) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(47653) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(47653) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(48182) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(48182) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(48654) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(48654) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(49119) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(49119) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Exception raised in Job[41]: TimeoutError()\n",
      "Exception raised in Job[41]: TimeoutError()\n",
      "wandb-core(49656) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(49656) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(50132) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(50132) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Exception raised in Job[53]: TimeoutError()\n",
      "Exception raised in Job[53]: TimeoutError()\n",
      "Exception raised in Job[55]: TimeoutError()\n",
      "Exception raised in Job[55]: TimeoutError()\n",
      "wandb-core(50610) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(50610) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Exception raised in Job[59]: TimeoutError()\n",
      "Exception raised in Job[59]: TimeoutError()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ragas evaluation complete.\n",
      "Overall metrics: {'context_recall': 0.8067, 'faithfulness': 0.8581, 'factual_correctness(mode=f1)': 0.5760, 'answer_relevancy': 0.8675, 'context_entity_recall': 0.3746, 'noise_sensitivity(mode=relevant)': 0.2096}\n",
      "Evaluation Complete: Logged 10 queries and 2 metrics.\n",
      "Evaluation Complete: Logged 10 queries and 2 metrics.\n",
      "Analytics: Logged comprehensive analysis for 10 queries.\n",
      "Total Cost: $0.014451 (LLM: $0.008398, Embeddings: $0.006053)\n",
      "Embedding Models: text-embedding-3-large\n",
      "Analytics: Logged comprehensive analysis for 10 queries.\n",
      "Total Cost: $0.014451 (LLM: $0.008398, Embeddings: $0.006053)\n",
      "Embedding Models: text-embedding-3-large\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_cost_per_query_usd</td><td>‚ñÅ</td></tr><tr><td>average_embedding_tokens_per_operation</td><td>‚ñÅ</td></tr><tr><td>average_input_tokens_per_query</td><td>‚ñÅ</td></tr><tr><td>average_llm_tokens_per_query</td><td>‚ñÅ</td></tr><tr><td>average_output_tokens_per_query</td><td>‚ñÅ</td></tr><tr><td>cost_breakdown_embedding_percentage</td><td>‚ñÅ</td></tr><tr><td>cost_breakdown_llm_percentage</td><td>‚ñÅ</td></tr><tr><td>embedding_cost_usd</td><td>‚ñÅ</td></tr><tr><td>execution_time_seconds</td><td>‚ñÅ</td></tr><tr><td>llm_cost_usd</td><td>‚ñÅ</td></tr><tr><td>+6</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_cost_per_query_usd</td><td>0.00145</td></tr><tr><td>average_embedding_tokens_per_operation</td><td>1164.05</td></tr><tr><td>average_input_tokens_per_query</td><td>4656.4</td></tr><tr><td>average_llm_tokens_per_query</td><td>4891.9</td></tr><tr><td>average_output_tokens_per_query</td><td>235.5</td></tr><tr><td>cost_breakdown_embedding_percentage</td><td>41.88778</td></tr><tr><td>cost_breakdown_llm_percentage</td><td>58.11222</td></tr><tr><td>embedding_cost_usd</td><td>0.00605</td></tr><tr><td>embedding_models</td><td>text-embedding-3-lar...</td></tr><tr><td>execution_time_seconds</td><td>333.63414</td></tr><tr><td>+8</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">hybrid-rag-large-embedding</strong> at: <a href='https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/wcshznyi' target=\"_blank\">https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/wcshznyi</a><br> View project at: <a href='https://wandb.ai/lgutierrwr/embedding-model-comparison' target=\"_blank\">https://wandb.ai/lgutierrwr/embedding-model-comparison</a><br>Synced 4 W&B file(s), 2 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251121_210108-wcshznyi/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "W&B COMPLETED: hybrid-rag-large-embedding | View Results: https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/wcshznyi\n"
     ]
    }
   ],
   "source": [
    "# 1. Configuration for the Large Embedding Model Experiment\n",
    "large_embedding_config = {\n",
    "    \"embedder_model\": \"text-embedding-3-large\",\n",
    "    \"llm_model\": \"gpt-4o-mini\",\n",
    "    \"retriever_top_k\": 5,  # Same as small embedding experiment\n",
    "    \"rag_type\": \"hybrid\",\n",
    "    \"document_store\": \"elasticsearch\",\n",
    "    \"reranker_model\": \"BAAI/bge-reranker-base\",\n",
    "}\n",
    "\n",
    "# 2. Create generator for evaluation\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.utils import Secret\n",
    "\n",
    "eval_generator_large = OpenAIGenerator(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=Secret.from_token(os.getenv(\"OPENAI_API_KEY\"))\n",
    ")\n",
    "\n",
    "# 3. Initialize RAG component with large embedder\n",
    "large_embedding_rag_sc = HybridRAGSuperComponent(\n",
    "    document_store=document_store_large,\n",
    "    embedder_model=large_embedding_config[\"embedder_model\"]\n",
    ")\n",
    "\n",
    "# 4. Initialize and Setup Large Embedding Experiment\n",
    "large_embedding_experiment = RAGEvaluationExperiment(\n",
    "    project_name=\"embedding-model-comparison\",\n",
    "    experiment_name=\"hybrid-rag-large-embedding\"\n",
    ")\n",
    "\n",
    "large_embedding_pipeline = large_embedding_experiment.setup_pipeline(\n",
    "    rag_supercomponent=large_embedding_rag_sc,\n",
    "    generator=eval_generator_large,\n",
    "    metrics_list=evaluation_metrics,\n",
    "    config=large_embedding_config\n",
    ")\n",
    "\n",
    "# 5. Run the evaluation and store results\n",
    "large_embedding_results = large_embedding_experiment.run_evaluation(\n",
    "    csv_file_path=csv_file_path\n",
    ")\n",
    "\n",
    "# 6. Run Analytics and log to W&B with updated RAGAnalytics\n",
    "# Pass the specific embedding model being used as a list\n",
    "large_embedding_analytics = RAGAnalytics(\n",
    "    results=large_embedding_results, \n",
    "    model_name=large_embedding_config['llm_model'],\n",
    "    embedding_models=[large_embedding_config['embedder_model']]  # Pass as list\n",
    ")\n",
    "large_embedding_summary = large_embedding_analytics.log_to_wandb(large_embedding_experiment.run)\n",
    "\n",
    "# 7. Finish the experiment run\n",
    "large_embedding_experiment.finish_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd620687",
   "metadata": {},
   "source": [
    "## üìä Comparative Analysis & Key Insights\n",
    "\n",
    "Now we can programmatically compare the key metrics between the two runs. The full comparison is available in the W&B dashboard, but a quick summary confirms the tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44c5461f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context_recall': 0.8067, 'faithfulness': 0.8581, 'factual_correctness(mode=f1)': 0.5760, 'answer_relevancy': 0.8675, 'context_entity_recall': 0.3746, 'noise_sensitivity(mode=relevant)': 0.2096}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_embedding_results['metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "comparison_analysis",
   "metadata": {
    "tags": [
     "summary"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb-core(50883) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(50886) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/laurafunderburk/Documents/GitHub/Building-Natural-Language-Pipelines/ch6/jupyter-notebooks/wandb/run-20251121_210645-ral0qf59</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/ral0qf59' target=\"_blank\">final-comparison</a></strong> to <a href='https://wandb.ai/lgutierrwr/embedding-model-comparison' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lgutierrwr/embedding-model-comparison' target=\"_blank\">https://wandb.ai/lgutierrwr/embedding-model-comparison</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/ral0qf59' target=\"_blank\">https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/ral0qf59</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">final-comparison</strong> at: <a href='https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/ral0qf59' target=\"_blank\">https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/ral0qf59</a><br> View project at: <a href='https://wandb.ai/lgutierrwr/embedding-model-comparison' target=\"_blank\">https://wandb.ai/lgutierrwr/embedding-model-comparison</a><br>Synced 4 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251121_210645-ral0qf59/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def extract_ragas_metrics(metrics_obj):\n",
    "    \"\"\"Extract and flatten RAGAS metrics from the result object for comparison.\"\"\"\n",
    "    metrics_dict = {} \n",
    "    if hasattr(metrics_obj, 'to_dict'):\n",
    "        raw_metrics = metrics_obj.to_dict()\n",
    "        for k, v in raw_metrics.items():\n",
    "            if isinstance(v, (float, int)):\n",
    "                # Clean metric name for the output table\n",
    "                clean_name = k.replace('(mode=f1)', '').strip()\n",
    "                metrics_dict[clean_name] = v\n",
    "    return metrics_dict\n",
    "\n",
    "# 1. Extract and combine summary data\n",
    "small_embedding_data = {\n",
    "    'System': 'Small Embedding (text-embedding-3-small)',\n",
    "    'embedder_model': 'text-embedding-3-small',\n",
    "    'Execution Time (s)': small_embedding_results['execution_time'],\n",
    "    'Avg Cost (USD)': small_embedding_summary['average_cost_per_query_usd'],\n",
    "    'Avg Input Tokens/Query': small_embedding_summary['average_input_tokens_per_query'],\n",
    "    'Faithfulness': np.array(small_embedding_results['metrics']['faithfulness']).mean(),\n",
    "    'Context Recall': np.array(small_embedding_results['metrics']['context_recall']).mean(),\n",
    "    'Factual Correctness': np.array(small_embedding_results['metrics']['factual_correctness(mode=f1)']).mean(),\n",
    "    'Response Relevancy': np.array(small_embedding_results['metrics']['answer_relevancy']).mean(),\n",
    "    'Noise Sensitivity': np.array(small_embedding_results['metrics']['noise_sensitivity(mode=relevant)']).mean(),\n",
    "    'Context Entity Recall': np.array(small_embedding_results['metrics']['context_entity_recall']).mean()\n",
    "}\n",
    "\n",
    "large_embedding_data = {\n",
    "    'System': 'Large Embedding (text-embedding-3-large)',\n",
    "    'embedder_model': 'text-embedding-3-large',\n",
    "    'Execution Time (s)': large_embedding_results['execution_time'],\n",
    "    'Avg Cost (USD)': large_embedding_summary['average_cost_per_query_usd'],\n",
    "    'Avg Input Tokens/Query': large_embedding_summary['average_input_tokens_per_query'],\n",
    "    'Faithfulness': np.array(large_embedding_results['metrics']['faithfulness']).mean(),\n",
    "    'Context Recall': np.array(large_embedding_results['metrics']['context_recall']).mean(),\n",
    "    'Factual Correctness': np.array(large_embedding_results['metrics']['factual_correctness(mode=f1)']).mean(),\n",
    "    'Response Relevancy': np.array(large_embedding_results['metrics']['answer_relevancy']).mean(),\n",
    "    'Noise Sensitivity': np.array(large_embedding_results['metrics']['noise_sensitivity(mode=relevant)']).mean(),\n",
    "    'Context Entity Recall': np.array(large_embedding_results['metrics']['context_entity_recall']).mean()\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame([small_embedding_data, large_embedding_data])\n",
    "comparison_df = comparison_df.set_index('System')\n",
    "\n",
    "\n",
    "\n",
    "# Log final summary table to W&B for easy comparison\n",
    "final_run = wandb.init(project=\"embedding-model-comparison\", name=\"final-comparison\", reinit=True)\n",
    "final_run.log({\"embedding_comparison_table\": wandb.Table(dataframe=comparison_df.reset_index())})\n",
    "final_run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262365b8",
   "metadata": {},
   "source": [
    "## üí∞ Enhanced Cost Analysis with Updated RAGAnalytics\n",
    "\n",
    "The updated `RAGAnalytics` class provides comprehensive cost tracking including embedding costs, LLM costs, and detailed breakdowns. Let's create a final analysis that demonstrates the full capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec2540f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Creating Enhanced Cost Analysis with Updated RAGAnalytics...\n",
      "======================================================================\n",
      "\n",
      "üìä Individual Experiment Cost Analysis:\n",
      "--------------------------------------------------\n",
      "üîπ Small Embedding Experiment:\n",
      "   Total Cost: $0.009311\n",
      "   LLM Cost: $0.008356\n",
      "   Embedding Cost: $0.000955\n",
      "   Cost per Query: $0.000931\n",
      "\n",
      "üîπ Large Embedding Experiment:\n",
      "   Total Cost: $0.014451\n",
      "   LLM Cost: $0.008398\n",
      "   Embedding Cost: $0.006053\n",
      "   Cost per Query: $0.001445\n",
      "\n",
      "üìà Cost Comparison Analysis:\n",
      "   Total Cost Difference: $0.005139 (55.2% increase)\n",
      "   Embedding Cost Difference: $0.005098\n",
      "   Cost Increase per Query: $0.000514\n",
      "\n",
      "‚öñÔ∏è Performance vs Cost Trade-off:\n",
      "   Faithfulness Improvement: nan (nan%)\n",
      "   Context Recall Improvement: -0.1533 (-16.0%)\n",
      "   Faithfulness: No improvement\n",
      "\n",
      "üìã Enhanced Comparison Table:\n",
      "         Metric  Total Cost ($)  LLM Cost ($)  Embedding Cost ($)  Avg Cost/Query ($)  Faithfulness  Context Recall  Factual Correctness  Response Relevancy  Context Entity Recall        Embedding Model\n",
      "Small Embedding        0.009311      0.008356            0.000955            0.000931      0.912088        0.960000                0.511            0.855721               0.328016 text-embedding-3-small\n",
      "Large Embedding        0.014451      0.008398            0.006053            0.001445           NaN        0.806667                0.576            0.867501               0.374618 text-embedding-3-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb-core(51471) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(51472) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/laurafunderburk/Documents/GitHub/Building-Natural-Language-Pipelines/ch6/jupyter-notebooks/wandb/run-20251121_210702-6ig9c91t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/6ig9c91t' target=\"_blank\">enhanced-cost-analysis</a></strong> to <a href='https://wandb.ai/lgutierrwr/embedding-model-comparison' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lgutierrwr/embedding-model-comparison' target=\"_blank\">https://wandb.ai/lgutierrwr/embedding-model-comparison</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/6ig9c91t' target=\"_blank\">https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/6ig9c91t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>context_recall_improvement</td><td>‚ñÅ</td></tr><tr><td>cost_difference_usd</td><td>‚ñÅ</td></tr><tr><td>cost_increase_percentage</td><td>‚ñÅ</td></tr><tr><td>cost_per_faithfulness_improvement</td><td>‚ñÅ</td></tr><tr><td>embedding_cost_difference_usd</td><td>‚ñÅ</td></tr><tr><td>+1</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>context_recall_improvement</td><td>-0.15333</td></tr><tr><td>cost_difference_usd</td><td>0.00514</td></tr><tr><td>cost_increase_percentage</td><td>55.19604</td></tr><tr><td>cost_per_faithfulness_improvement</td><td>0</td></tr><tr><td>embedding_cost_difference_usd</td><td>0.0051</td></tr><tr><td>faithfulness_improvement</td><td>nan</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">enhanced-cost-analysis</strong> at: <a href='https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/6ig9c91t' target=\"_blank\">https://wandb.ai/lgutierrwr/embedding-model-comparison/runs/6ig9c91t</a><br> View project at: <a href='https://wandb.ai/lgutierrwr/embedding-model-comparison' target=\"_blank\">https://wandb.ai/lgutierrwr/embedding-model-comparison</a><br>Synced 4 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251121_210702-6ig9c91t/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Enhanced cost analysis complete and logged to W&B!\n"
     ]
    }
   ],
   "source": [
    "# Create comprehensive analytics comparison using both models\n",
    "print(\"üîç Creating Enhanced Cost Analysis with Updated RAGAnalytics...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Create a dual-embedding analytics for comparison purposes\n",
    "# This simulates analyzing a system that uses both embedding models\n",
    "dual_embedding_analytics = RAGAnalytics(\n",
    "    results=small_embedding_results,  # Use one set of results as base\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    embedding_models=[\"text-embedding-3-small\", \"text-embedding-3-large\"]  # Both models\n",
    ")\n",
    "\n",
    "# 2. Extract detailed cost breakdowns from individual experiments\n",
    "print(\"\\nüìä Individual Experiment Cost Analysis:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(f\"üîπ Small Embedding Experiment:\")\n",
    "print(f\"   Total Cost: ${small_embedding_summary['total_cost_usd']:.6f}\")\n",
    "print(f\"   LLM Cost: ${small_embedding_summary['llm_cost_usd']:.6f}\")\n",
    "print(f\"   Embedding Cost: ${small_embedding_summary['embedding_cost_usd']:.6f}\")\n",
    "print(f\"   Cost per Query: ${small_embedding_summary['average_cost_per_query_usd']:.6f}\")\n",
    "\n",
    "print(f\"\\nüîπ Large Embedding Experiment:\")\n",
    "print(f\"   Total Cost: ${large_embedding_summary['total_cost_usd']:.6f}\")\n",
    "print(f\"   LLM Cost: ${large_embedding_summary['llm_cost_usd']:.6f}\")\n",
    "print(f\"   Embedding Cost: ${large_embedding_summary['embedding_cost_usd']:.6f}\")\n",
    "print(f\"   Cost per Query: ${large_embedding_summary['average_cost_per_query_usd']:.6f}\")\n",
    "\n",
    "# 3. Calculate cost differences and efficiency metrics\n",
    "cost_difference = large_embedding_summary['total_cost_usd'] - small_embedding_summary['total_cost_usd']\n",
    "embedding_cost_difference = large_embedding_summary['embedding_cost_usd'] - small_embedding_summary['embedding_cost_usd']\n",
    "cost_increase_percentage = (cost_difference / small_embedding_summary['total_cost_usd']) * 100\n",
    "\n",
    "print(f\"\\nüìà Cost Comparison Analysis:\")\n",
    "print(f\"   Total Cost Difference: ${cost_difference:.6f} ({cost_increase_percentage:.1f}% increase)\")\n",
    "print(f\"   Embedding Cost Difference: ${embedding_cost_difference:.6f}\")\n",
    "print(f\"   Cost Increase per Query: ${cost_difference / len(small_embedding_results['evaluation_df']):.6f}\")\n",
    "\n",
    "# 4. Performance vs Cost Analysis\n",
    "faithfulness_improvement = (large_embedding_data['Faithfulness'] - small_embedding_data['Faithfulness'])\n",
    "context_recall_improvement = (large_embedding_data['Context Recall'] - small_embedding_data['Context Recall'])\n",
    "factual_correctness_improvement = (large_embedding_data['Factual Correctness'] - small_embedding_data['Factual Correctness'])\n",
    "response_relevancy_improvement = (large_embedding_data['Response Relevancy'] - small_embedding_data['Response Relevancy'])\n",
    "context_entity_recall_improvement = (large_embedding_data['Context Entity Recall'] - small_embedding_data['Context Entity Recall'])\n",
    "\n",
    "print(f\"\\n‚öñÔ∏è Performance vs Cost Trade-off:\")\n",
    "print(f\"   Faithfulness Improvement: {faithfulness_improvement:.4f} ({faithfulness_improvement/small_embedding_data['Faithfulness']*100:.1f}%)\")\n",
    "print(f\"   Context Recall Improvement: {context_recall_improvement:.4f} ({context_recall_improvement/small_embedding_data['Context Recall']*100:.1f}%)\")\n",
    "print(f\"   Cost per 1% Faithfulness Improvement: ${cost_difference/(faithfulness_improvement*100):.6f}\" if faithfulness_improvement > 0 else \"   Faithfulness: No improvement\")\n",
    "\n",
    "# 5. Create enhanced comparison DataFrame with cost details\n",
    "enhanced_comparison = pd.DataFrame({\n",
    "    'Metric': ['Small Embedding', 'Large Embedding'],\n",
    "    'Total Cost ($)': [small_embedding_summary['total_cost_usd'], large_embedding_summary['total_cost_usd']],\n",
    "    'LLM Cost ($)': [small_embedding_summary['llm_cost_usd'], large_embedding_summary['llm_cost_usd']],\n",
    "    'Embedding Cost ($)': [small_embedding_summary['embedding_cost_usd'], large_embedding_summary['embedding_cost_usd']],\n",
    "    'Avg Cost/Query ($)': [small_embedding_summary['average_cost_per_query_usd'], large_embedding_summary['average_cost_per_query_usd']],\n",
    "    'Faithfulness': [small_embedding_data['Faithfulness'], large_embedding_data['Faithfulness']],\n",
    "    'Context Recall': [small_embedding_data['Context Recall'], large_embedding_data['Context Recall']],\n",
    "    'Factual Correctness' : [small_embedding_data['Factual Correctness'], large_embedding_data['Factual Correctness']],\n",
    "    'Response Relevancy': [small_embedding_data['Response Relevancy'], large_embedding_data['Response Relevancy']],\n",
    "    'Context Entity Recall': [small_embedding_data['Context Entity Recall'], large_embedding_data['Context Entity Recall']],\n",
    "    'Embedding Model': ['text-embedding-3-small', 'text-embedding-3-large']\n",
    "})\n",
    "\n",
    "print(f\"\\nüìã Enhanced Comparison Table:\")\n",
    "print(enhanced_comparison.round(6).to_string(index=False))\n",
    "\n",
    "# 6. Log the enhanced comparison to W&B\n",
    "enhanced_run = wandb.init(project=\"embedding-model-comparison\", name=\"enhanced-cost-analysis\", reinit=True)\n",
    "\n",
    "# Log the enhanced comparison table\n",
    "enhanced_run.log({\"enhanced_cost_comparison\": wandb.Table(dataframe=enhanced_comparison)})\n",
    "\n",
    "# Log key insights as metrics\n",
    "enhanced_run.log({\n",
    "    \"cost_increase_percentage\": cost_increase_percentage,\n",
    "    \"cost_difference_usd\": cost_difference,\n",
    "    \"embedding_cost_difference_usd\": embedding_cost_difference,\n",
    "    \"faithfulness_improvement\": faithfulness_improvement,\n",
    "    \"context_recall_improvement\": context_recall_improvement,\n",
    "    \"cost_per_faithfulness_improvement\": cost_difference/(faithfulness_improvement*100) if faithfulness_improvement > 0 else 0\n",
    "})\n",
    "\n",
    "enhanced_run.finish()\n",
    "\n",
    "print(f\"\\n‚úÖ Enhanced cost analysis complete and logged to W&B!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07ad1b87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Total Cost ($)</th>\n",
       "      <th>LLM Cost ($)</th>\n",
       "      <th>Embedding Cost ($)</th>\n",
       "      <th>Avg Cost/Query ($)</th>\n",
       "      <th>Faithfulness</th>\n",
       "      <th>Context Recall</th>\n",
       "      <th>Factual Correctness</th>\n",
       "      <th>Response Relevancy</th>\n",
       "      <th>Context Entity Recall</th>\n",
       "      <th>Embedding Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Small Embedding</td>\n",
       "      <td>0.009311</td>\n",
       "      <td>0.008356</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>0.000931</td>\n",
       "      <td>0.912088</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.855721</td>\n",
       "      <td>0.328016</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Large Embedding</td>\n",
       "      <td>0.014451</td>\n",
       "      <td>0.008398</td>\n",
       "      <td>0.006053</td>\n",
       "      <td>0.001445</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.806667</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.867501</td>\n",
       "      <td>0.374618</td>\n",
       "      <td>text-embedding-3-large</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Metric  Total Cost ($)  LLM Cost ($)  Embedding Cost ($)  \\\n",
       "0  Small Embedding        0.009311      0.008356            0.000955   \n",
       "1  Large Embedding        0.014451      0.008398            0.006053   \n",
       "\n",
       "   Avg Cost/Query ($)  Faithfulness  Context Recall  Factual Correctness  \\\n",
       "0            0.000931      0.912088        0.960000                0.511   \n",
       "1            0.001445           NaN        0.806667                0.576   \n",
       "\n",
       "   Response Relevancy  Context Entity Recall         Embedding Model  \n",
       "0            0.855721               0.328016  text-embedding-3-small  \n",
       "1            0.867501               0.374618  text-embedding-3-large  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enhanced_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b90b3b",
   "metadata": {},
   "source": [
    "## üéì Summary and Next Steps\n",
    "\n",
    "You have successfully executed an **embedding model comparison experiment** with **enhanced cost analytics** for the Hybrid RAG system and logged all results to W&B project: `embedding-model-comparison`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffea587",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-with-haystack-ch6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
