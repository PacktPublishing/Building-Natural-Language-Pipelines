{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75f744c9",
   "metadata": {},
   "source": [
    "🔧 **Setup Required**: Before running this notebook, please follow the [setup instructions](../README.md#setup-instructions) to configure your environment and API keys. **You will need to ensure you've executed the Indexing pipeline before completing this exercise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08a83513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from haystack import component, Pipeline\n",
    "from typing import List, Optional, Dict, Any, Union\n",
    "\n",
    "@component\n",
    "class CSVReaderComponent:\n",
    "    \"\"\"Reads a CSV file into a Pandas DataFrame.\"\"\"\n",
    "\n",
    "    @component.output_types(data_frame=pd.DataFrame)\n",
    "    def run(self, source: Union[str, Path]):\n",
    "        \"\"\"\n",
    "        Reads the CSV file from the first source in the list.\n",
    "        \n",
    "        Args:\n",
    "            sources: List of file paths to CSV files. Only the first file will be processed.\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary containing the loaded DataFrame under 'data_frame' key.\n",
    "            \n",
    "        Raises:\n",
    "            FileNotFoundError: If the file doesn't exist or can't be read.\n",
    "            ValueError: If the DataFrame is empty after loading.\n",
    "        \"\"\"\n",
    "        if not source:\n",
    "            raise ValueError(\"No sources provided\")\n",
    "            \n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(source)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"File not found at {source}\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error reading CSV file {source}: {str(e)}\")\n",
    "\n",
    "        # Check if DataFrame is empty using proper pandas method\n",
    "        if df.empty:\n",
    "            raise ValueError(f\"DataFrame is empty after loading from {source}\")\n",
    "\n",
    "        print(f\"Loaded DataFrame with {len(df)} rows from {source}.\")\n",
    "        return {\"data_frame\": df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72bf8b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import SuperComponent\n",
    "\n",
    "@component\n",
    "class RAGDataAugmenterComponent:\n",
    "    \"\"\"\n",
    "    Applies a RAG SuperComponent to each query in a DataFrame and \n",
    "    augments the data with the generated answer and retrieved contexts.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rag_supercomponent: SuperComponent):\n",
    "        # We store the pre-initialized SuperComponent\n",
    "        self.rag_supercomponent = rag_supercomponent\n",
    "        self.output_names = [\"augmented_data_frame\"]\n",
    "\n",
    "    @component.output_types(augmented_data_frame=pd.DataFrame)\n",
    "    def run(self, data_frame: pd.DataFrame):\n",
    "        \n",
    "        # New columns to store RAG results\n",
    "        answers: List[str] = []\n",
    "        contexts: List[List[str]] = []\n",
    "\n",
    "        print(f\"Running RAG SuperComponent on {len(data_frame)} queries...\")\n",
    "\n",
    "        # Iterate through the queries (user_input column)\n",
    "        for _, row in data_frame.iterrows():\n",
    "            query = row[\"user_input\"]\n",
    "            \n",
    "            # 1. Run the RAG SuperComponent\n",
    "            # It expects 'query' as input and returns a dictionary.\n",
    "            rag_output = self.rag_supercomponent.run(query=query)\n",
    "            \n",
    "            # 2. Extract answer and contexts\n",
    "            # Based on the naive_rag_sc/hybrid_rag_sc structure:\n",
    "            answer = rag_output.get('replies', [''])[0]\n",
    "            \n",
    "            # Extract content from the Document objects\n",
    "            retrieved_docs = rag_output.get('documents', [])\n",
    "            retrieved_contexts = [doc.content for doc in retrieved_docs]\n",
    "            \n",
    "            answers.append(answer)\n",
    "            contexts.append(retrieved_contexts)\n",
    "        \n",
    "        # 3. Augment the DataFrame\n",
    "        data_frame['response'] = answers\n",
    "        data_frame['retrieved_contexts'] = contexts\n",
    "        \n",
    "        print(\"RAG processing complete.\")\n",
    "        return {\"augmented_data_frame\": data_frame}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5b842a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurafunderburk/Documents/GitHub/Building-Natural-Language-Pipelines/ch6/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from ragas import EvaluationDataset, evaluate\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, ResponseRelevancy, ContextEntityRecall, NoiseSensitivity\n",
    "\n",
    "from ragas.llms import llm_factory\n",
    "from haystack.utils import Secret\n",
    "import os\n",
    "from ragas.llms import HaystackLLMWrapper\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "\n",
    "# Note: Ensure ragas and its dependencies (like litellm or openai) are installed\n",
    "@component\n",
    "class RagasEvaluationComponent:\n",
    "    \"\"\"\n",
    "    Prepares data for Ragas, runs the evaluation, and returns the metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 metrics: Optional[List[Any]] = None,\n",
    "                 ragas_llm: Optional[Any] = None):\n",
    "        \n",
    "        # Default metrics for RAG evaluation\n",
    "        self.metrics = metrics\n",
    "        \n",
    "        # Ragas requires an LLM for evaluation, often provided through OpenAI or Anthropic.\n",
    "        # It's best practice to use a strong model like gpt-4o-mini or gpt-4.\n",
    "        if ragas_llm is None:\n",
    "            # Assumes OPENAI_API_KEY is set in the environment\n",
    "            self.ragas_llm = HaystackLLMWrapper(OpenAIGenerator(model=\"gpt-4o-mini\",\n",
    "                                                               api_key=Secret.from_env_var(\"OPENAI_API_KEY\")))\n",
    "        else:\n",
    "            self.ragas_llm = ragas_llm\n",
    "\n",
    "    @component.output_types(metrics=Dict[str, float], evaluation_df=pd.DataFrame)\n",
    "    def run(self, augmented_data_frame: pd.DataFrame):\n",
    "        \n",
    "        # 1. Map columns to Ragas requirements - correct column mapping for SingleTurnSample\n",
    "        ragas_data = pd.DataFrame({\n",
    "            'user_input': augmented_data_frame['user_input'],\n",
    "            'response': augmented_data_frame['response'], \n",
    "            'retrieved_contexts': augmented_data_frame['retrieved_contexts'],\n",
    "            'reference': augmented_data_frame['reference'],\n",
    "            'reference_contexts': augmented_data_frame['reference_contexts'].apply(eval)\n",
    "        })\n",
    "\n",
    "        print(\"Creating Ragas EvaluationDataset...\")\n",
    "        # 2. Create EvaluationDataset using from_pandas which handles the format correctly\n",
    "        dataset = EvaluationDataset.from_pandas(ragas_data)\n",
    "\n",
    "        print(\"Starting Ragas evaluation...\")\n",
    "        \n",
    "        # 3. Run Ragas Evaluation\n",
    "        # Pass the configured LLM to Ragas\n",
    "        results = evaluate(\n",
    "            dataset=dataset,\n",
    "            metrics=self.metrics,\n",
    "            llm=self.ragas_llm\n",
    "        )\n",
    "        \n",
    "\n",
    "        results_df = results.to_pandas()\n",
    "        \n",
    "        print(\"Ragas evaluation complete.\")\n",
    "        print(f\"Overall metrics: {results}\")\n",
    "        \n",
    "        return {\"metrics\": results, \"evaluation_df\": results_df}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51104c10",
   "metadata": {},
   "source": [
    "Naive RAG evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a23faff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x169eabf20>\n",
       "🚅 Components\n",
       "  - reader: CSVReaderComponent\n",
       "  - augmenter: RAGDataAugmenterComponent\n",
       "  - evaluator: RagasEvaluationComponent\n",
       "🛤️ Connections\n",
       "  - reader.data_frame -> augmenter.data_frame (DataFrame)\n",
       "  - augmenter.augmented_data_frame -> evaluator.augmented_data_frame (DataFrame)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Setup Environment & Dependencies ---\n",
    "# You need to ensure:\n",
    "# 1. Elasticsearch is running (as NaiveRAG/HybridRAG rely on it, see files).\n",
    "# 2. OPENAI_API_KEY is set in your environment.\n",
    "# 3. The document store has been indexed with your data.\n",
    "\n",
    "# --- 1. Import RAG SuperComponents ---\n",
    "# Assuming naiverag.py and hybridrag.py are in your environment\n",
    "from scripts.rag.naiverag import naive_rag_sc\n",
    "from scripts.rag.hybridrag import hybrid_rag_sc\n",
    "from pathlib import Path\n",
    "\n",
    "# --- 2. Define Configurations to Test ---\n",
    "\n",
    "# The RAG SuperComponent to test (change this to swap RAG configurations)\n",
    "rag_sc_to_test = naive_rag_sc # OR hybrid_rag_sc\n",
    "\n",
    "# If you want to test different internal configurations (e.g., chunk size, embedder model), \n",
    "# you should create and index new SuperComponents with those changes \n",
    "# and then choose the appropriate object here.\n",
    "\n",
    "# --- 3. Instantiate Custom Components ---\n",
    "\n",
    "metrics = [LLMContextRecall(), \\\n",
    "                Faithfulness(), \\\n",
    "                FactualCorrectness(), \\\n",
    "                ResponseRelevancy(), \\\n",
    "                ContextEntityRecall(), \\\n",
    "                NoiseSensitivity()]\n",
    "\n",
    "\n",
    "reader = CSVReaderComponent()\n",
    "augmenter = RAGDataAugmenterComponent(rag_supercomponent=rag_sc_to_test)\n",
    "evaluator = RagasEvaluationComponent(metrics=metrics)\n",
    "\n",
    "# --- 4. Build the Evaluation Pipeline ---\n",
    "\n",
    "evaluation_pipeline = Pipeline()\n",
    "\n",
    "evaluation_pipeline.add_component(\"reader\", reader)\n",
    "evaluation_pipeline.add_component(\"augmenter\", augmenter)\n",
    "evaluation_pipeline.add_component(\"evaluator\", evaluator)\n",
    "\n",
    "# Connect the flow: CSV -> Augment -> Evaluate\n",
    "evaluation_pipeline.connect(\"reader.data_frame\", \"augmenter.data_frame\")\n",
    "evaluation_pipeline.connect(\"augmenter.augmented_data_frame\", \"evaluator.augmented_data_frame\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a9d17a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation of SuperComponent...\n",
      "Loaded DataFrame with 4 rows from data_for_eval/synthetic_tests_advanced_branching_3.csv.\n",
      "Running RAG SuperComponent on 4 queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.77it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG processing complete.\n",
      "Creating Ragas EvaluationDataset...\n",
      "Starting Ragas evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/24 [00:00<?, ?it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:   4%|▍         | 1/24 [00:02<00:59,  2.60s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  17%|█▋        | 4/24 [00:08<00:38,  1.92s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating: 100%|██████████| 24/24 [01:07<00:00,  2.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ragas evaluation complete.\n",
      "Overall metrics: {'context_recall': 1.0000, 'faithfulness': 0.4500, 'factual_correctness(mode=f1)': 0.1875, 'answer_relevancy': 0.2355, 'context_entity_recall': 0.1833, 'noise_sensitivity(mode=relevant)': 0.1333}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 5. Run the Evaluation Pipeline ---\n",
    "csv_file_path = \"data_for_eval/synthetic_tests_advanced_branching_3.csv\"\n",
    "print(f\"Starting evaluation of {rag_sc_to_test.__class__.__name__}...\")\n",
    "\n",
    "results = evaluation_pipeline.run({\"reader\": {\"source\": csv_file_path}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6b4c0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 6. Access Metrics ---\n",
    "final_metrics = results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c9ce4eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>factual_correctness(mode=f1)</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_entity_recall</th>\n",
       "      <th>noise_sensitivity(mode=relevant)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How is Amazon utilizing AI technology in its p...</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>The provided documents do not contain specific...</td>\n",
       "      <td>Amazon's AI technology is prominently featured...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What percentage of ChatGPT queries related to ...</td>\n",
       "      <td>[Sampling details available in Section 3.\\n5.2...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n5.1 What share of ChatGPT queries ...</td>\n",
       "      <td>As of June 2025, the percentage of ChatGPT que...</td>\n",
       "      <td>The LLM classifier identified that 53% of mess...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.941918</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How is ChatGPT Business used in different occu...</td>\n",
       "      <td>[X’s indicate that the ranking is\\nunavailable...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nCorporate users may also use ChatG...</td>\n",
       "      <td>The provided information does not specify deta...</td>\n",
       "      <td>ChatGPT Business is utilized across various oc...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does the adoption of generative AI, partic...</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nCorporate users may also use ChatG...</td>\n",
       "      <td>The provided documents do not specifically add...</td>\n",
       "      <td>The adoption of generative AI, such as ChatGPT...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  How is Amazon utilizing AI technology in its p...   \n",
       "1  What percentage of ChatGPT queries related to ...   \n",
       "2  How is ChatGPT Business used in different occu...   \n",
       "3  How does the adoption of generative AI, partic...   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [What is AI, how does it work and why are some...   \n",
       "1  [Sampling details available in Section 3.\\n5.2...   \n",
       "2  [X’s indicate that the ranking is\\nunavailable...   \n",
       "3  [What is AI, how does it work and why are some...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  [What is AI, how does it work and why are some...   \n",
       "1  [<1-hop>\\n\\n5.1 What share of ChatGPT queries ...   \n",
       "2  [<1-hop>\\n\\nCorporate users may also use ChatG...   \n",
       "3  [<1-hop>\\n\\nCorporate users may also use ChatG...   \n",
       "\n",
       "                                            response  \\\n",
       "0  The provided documents do not contain specific...   \n",
       "1  As of June 2025, the percentage of ChatGPT que...   \n",
       "2  The provided information does not specify deta...   \n",
       "3  The provided documents do not specifically add...   \n",
       "\n",
       "                                           reference  context_recall  \\\n",
       "0  Amazon's AI technology is prominently featured...        1.000000   \n",
       "1  The LLM classifier identified that 53% of mess...        1.000000   \n",
       "2  ChatGPT Business is utilized across various oc...        0.666667   \n",
       "3  The adoption of generative AI, such as ChatGPT...        1.000000   \n",
       "\n",
       "   faithfulness  factual_correctness(mode=f1)  answer_relevancy  \\\n",
       "0      1.000000                          0.00          0.000000   \n",
       "1      0.800000                          0.86          0.941918   \n",
       "2      0.750000                          0.17          0.000000   \n",
       "3      0.333333                          0.00          0.000000   \n",
       "\n",
       "   context_entity_recall  noise_sensitivity(mode=relevant)  \n",
       "0               0.333333                          1.000000  \n",
       "1               0.166667                          0.000000  \n",
       "2               0.000000                          0.000000  \n",
       "3               0.111111                          0.333333  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_metrics['evaluator']['evaluation_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61bd01d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context_recall': 1.0000, 'faithfulness': 0.4500, 'factual_correctness(mode=f1)': 0.1875, 'answer_relevancy': 0.2355, 'context_entity_recall': 0.1833, 'noise_sensitivity(mode=relevant)': 0.1333}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_metrics['evaluator']['metrics']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8308cf",
   "metadata": {},
   "source": [
    "Hybrid RAG evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39dadf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x35cfaa030>\n",
       "🚅 Components\n",
       "  - reader: CSVReaderComponent\n",
       "  - augmenter: RAGDataAugmenterComponent\n",
       "  - evaluator: RagasEvaluationComponent\n",
       "🛤️ Connections\n",
       "  - reader.data_frame -> augmenter.data_frame (DataFrame)\n",
       "  - augmenter.augmented_data_frame -> evaluator.augmented_data_frame (DataFrame)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_sc_to_test = hybrid_rag_sc\n",
    "metrics = [LLMContextRecall(), \\\n",
    "                Faithfulness(), \\\n",
    "                FactualCorrectness(), \\\n",
    "                ResponseRelevancy(), \\\n",
    "                ContextEntityRecall(), \\\n",
    "                NoiseSensitivity()]\n",
    "\n",
    "\n",
    "reader = CSVReaderComponent()\n",
    "augmenter = RAGDataAugmenterComponent(rag_supercomponent=rag_sc_to_test)\n",
    "evaluator = RagasEvaluationComponent(metrics=metrics)\n",
    "\n",
    "# --- 4. Build the Evaluation Pipeline ---\n",
    "\n",
    "evaluation_pipeline = Pipeline()\n",
    "\n",
    "evaluation_pipeline.add_component(\"reader\", reader)\n",
    "evaluation_pipeline.add_component(\"augmenter\", augmenter)\n",
    "evaluation_pipeline.add_component(\"evaluator\", evaluator)\n",
    "\n",
    "# Connect the flow: CSV -> Augment -> Evaluate\n",
    "evaluation_pipeline.connect(\"reader.data_frame\", \"augmenter.data_frame\")\n",
    "evaluation_pipeline.connect(\"augmenter.augmented_data_frame\", \"evaluator.augmented_data_frame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22c2531e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation of SuperComponent...\n",
      "Loaded DataFrame with 4 rows from data_for_eval/synthetic_tests_advanced_branching_3.csv.\n",
      "Running RAG SuperComponent on 4 queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG processing complete.\n",
      "Creating Ragas EvaluationDataset...\n",
      "Starting Ragas evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/24 [00:00<?, ?it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:   4%|▍         | 1/24 [00:02<00:48,  2.13s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:   8%|▊         | 2/24 [00:04<00:54,  2.49s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  29%|██▉       | 7/24 [00:09<00:16,  1.03it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating: 100%|██████████| 24/24 [01:01<00:00,  2.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ragas evaluation complete.\n",
      "Overall metrics: {'context_recall': 0.9167, 'faithfulness': 0.7208, 'factual_correctness(mode=f1)': 0.2575, 'answer_relevancy': 0.2355, 'context_entity_recall': 0.1528, 'noise_sensitivity(mode=relevant)': 0.3333}\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Run the Evaluation Pipeline ---\n",
    "csv_file_path = \"data_for_eval/synthetic_tests_advanced_branching_3.csv\"\n",
    "print(f\"Starting evaluation of {rag_sc_to_test.__class__.__name__}...\")\n",
    "\n",
    "results = evaluation_pipeline.run({\"reader\": {\"source\": csv_file_path}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff5f12ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>factual_correctness(mode=f1)</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_entity_recall</th>\n",
       "      <th>noise_sensitivity(mode=relevant)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How is Amazon utilizing AI technology in its p...</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>The provided documents do not contain specific...</td>\n",
       "      <td>Amazon's AI technology is prominently featured...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What percentage of ChatGPT queries related to ...</td>\n",
       "      <td>[Sampling details available in Section 3.\\n5.2...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n5.1 What share of ChatGPT queries ...</td>\n",
       "      <td>As of June 2025, the percentage of ChatGPT que...</td>\n",
       "      <td>The LLM classifier identified that 53% of mess...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.941918</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How is ChatGPT Business used in different occu...</td>\n",
       "      <td>[X’s indicate that the ranking is\\nunavailable...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nCorporate users may also use ChatG...</td>\n",
       "      <td>The provided information does not specify deta...</td>\n",
       "      <td>ChatGPT Business is utilized across various oc...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does the adoption of generative AI, partic...</td>\n",
       "      <td>[What is AI, how does it work and why are some...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nCorporate users may also use ChatG...</td>\n",
       "      <td>The provided documents do not specifically add...</td>\n",
       "      <td>The adoption of generative AI, such as ChatGPT...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  How is Amazon utilizing AI technology in its p...   \n",
       "1  What percentage of ChatGPT queries related to ...   \n",
       "2  How is ChatGPT Business used in different occu...   \n",
       "3  How does the adoption of generative AI, partic...   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [What is AI, how does it work and why are some...   \n",
       "1  [Sampling details available in Section 3.\\n5.2...   \n",
       "2  [X’s indicate that the ranking is\\nunavailable...   \n",
       "3  [What is AI, how does it work and why are some...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  [What is AI, how does it work and why are some...   \n",
       "1  [<1-hop>\\n\\n5.1 What share of ChatGPT queries ...   \n",
       "2  [<1-hop>\\n\\nCorporate users may also use ChatG...   \n",
       "3  [<1-hop>\\n\\nCorporate users may also use ChatG...   \n",
       "\n",
       "                                            response  \\\n",
       "0  The provided documents do not contain specific...   \n",
       "1  As of June 2025, the percentage of ChatGPT que...   \n",
       "2  The provided information does not specify deta...   \n",
       "3  The provided documents do not specifically add...   \n",
       "\n",
       "                                           reference  context_recall  \\\n",
       "0  Amazon's AI technology is prominently featured...        1.000000   \n",
       "1  The LLM classifier identified that 53% of mess...        1.000000   \n",
       "2  ChatGPT Business is utilized across various oc...        0.666667   \n",
       "3  The adoption of generative AI, such as ChatGPT...        1.000000   \n",
       "\n",
       "   faithfulness  factual_correctness(mode=f1)  answer_relevancy  \\\n",
       "0      1.000000                          0.00          0.000000   \n",
       "1      0.800000                          0.86          0.941918   \n",
       "2      0.750000                          0.17          0.000000   \n",
       "3      0.333333                          0.00          0.000000   \n",
       "\n",
       "   context_entity_recall  noise_sensitivity(mode=relevant)  \n",
       "0               0.333333                          1.000000  \n",
       "1               0.166667                          0.000000  \n",
       "2               0.000000                          0.000000  \n",
       "3               0.111111                          0.333333  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['evaluator']['evaluation_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86dfa950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context_recall': 1.0000, 'faithfulness': 0.4500, 'factual_correctness(mode=f1)': 0.1875, 'answer_relevancy': 0.2355, 'context_entity_recall': 0.1833, 'noise_sensitivity(mode=relevant)': 0.1333}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_metrics['evaluator']['metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdcdfb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-with-haystack-ch6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
