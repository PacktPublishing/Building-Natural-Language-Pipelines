user_input,reference_contexts,reference,synthesizer_name
How do peple use OpenAI's ChatGPT and what are the trends in its adoption?,"['NBER WORKING PAPER SERIES\nHOW PEOPLE USE CHATGPT\nAaron Chatterji\nThomas Cunningham\nDavid J. Deming\nZoe Hitzig\nChristopher Ong\nCarl Yan Shan\nKevin Wadman\nWorking Paper 34255\nhttp://www.nber.org/papers/w34255\nNATIONAL BUREAU OF ECONOMIC RESEARCH\n1050 Massachusetts Avenue\nCambridge, MA 02138\nSeptember 2025\nWe acknowledge help and comments from Joshua Achiam, Hemanth Asirvatham, Ryan Beiermeister, Rachel Brown, Cassandra Duchan Solis, Jason Kwon, Elliott Mokski, Kevin Rao, Harrison Satcher, Gawesha Weeratunga, Hannah Wong, and Analytics & Insights team. We especially thank Tyna Eloundou and Pamela Mishkin who in several ways laid the foundation for this work. This study was approved by Harvard IRB (IRB25-0983). A repository containing all code run to produce the analyses in this paper is available on request. The views expressed herein are those of the authors and do not necessarily reflect the views of the National Bureau of Economic Research.\nAt least one co-author has disclosed additional relationships of potential relevance for this research. Further information is available online at http://www.nber.org/papers/w34255\nNBER working papers are circulated for discussion and comment purposes. They have not been peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies official NBER publications.\n© 2025 by Aaron Chatterji, Thomas Cunningham, David J. Deming, Zoe Hitzig, Christopher Ong, Carl Yan Shan, and Kevin Wadman. All rights reserved. Short sections of text, not to exceed two paragraphs, may be quoted without explicit permission provided that full credit, including © notice, is given to the source.\x0cHow People Use ChatGPT\nAaron Chatterji, Thomas Cunningham, David J. Deming, Zoe Hitzig, Christopher Ong, Carl\nYan Shan, and Kevin Wadman\nNBER Working Paper No. 34255\nSeptember 2025\nJEL No. J01, O3, O4\nABSTRACT\nDespite the rapid adoption of LLM chatbots, little is known about how they are used. We document the growth of ChatGPT’s consumer product from its launch in November 2022 through July 2025, when it had been adopted by around 10% of the world’s adult population. Early adopters were disproportionately male but the gender gap has narrowed dramatically, and we find higher growth rates in lower-income countries. Using a privacy-preserving automated pipeline, we classify usage patterns within a representative sample of ChatGPT conversations. We find steady growth in work-related messages bu t even faster growth in non-work-related messages, which have grown from 53% to more than 70% of all usage. Work usage is more common for educated users in highly-paid professional occupations. We classify messages by conversation top\nic and find that “Practical Guidance,” “Seeking Information,” and “Writing” are the three most common topics and collectively account for nearly 80% of all conversations. Writing dominates work-related tasks, highlighting chatbots’ unique ability to generate digital outputs compared to traditional sear ch engines. Computer programming and self-expression both represent relatively small shares of use. Overall, we find that ChatGPT provides economic value through decision support, which is especially important in knowledge-intensive jobs.\nAaron Chatterji\nDuke University\nFuqua School of Business and OpenAI\nronnie@duke.edu\nThomas Cunningham OpenAI\ntom.cunningham@gmail.com\nDavid J. Deming\nHarvard University\nHarvard Kennedy School and NBER\ndavid_deming@harvard.edu\nZoe Hitzig\nOpenAI\nand Harvard Society of Fellows\nzhitzig@g.harvard.edu\nChristopher Ong\nHarvard University\nand OpenAI\nchristopherong@hks.harvard.edu\nCarl Yan Shan\nOpenAI\ncshan@openai.com\nKevin Wadman\nOpenAI\nkevin.wadman@c-openai.com\x0c']","The study documents the growth of ChatGPT’s consumer product from its launch in November 2022 through July 2025, noting that it had been adopted by around 10% of the world’s adult population. Early adopters were disproportionately male, but the gender gap has narrowed dramatically, with higher growth rates observed in lower-income countries. The analysis reveals that work-related messages have seen steady growth, but non-work-related messages have grown even faster, increasing from 53% to more than 70% of all usage. Work usage is more common among educated users in highly-paid professional occupations. The most common conversation topics include 'Practical Guidance,' 'Seeking Information,' and 'Writing,' which collectively account for nearly 80% of all conversations. Writing tasks dominate work-related usage, showcasing chatbots' unique ability to generate digital outputs compared to traditional search engines.",single_hop_specific_query_synthesizer
"When ChatGPT launched in November 2022, how many users did it have by July 2025?","['ABSTRACT Despite the rapid adoption of LLM chatbots, little is known about how they are used. We document the growth of ChatGPT’s consumer product from its launch in November 2022 through July 2025, when it had been adopted by around 10% of the world’s adult population. Early adopters were disproportionately male but the gender gap has narrowed dramatically, and we find higher growth rates in lower-income countries. Using a privacy-preserving automated pipeline, we classify usage patterns within a representative sample of ChatGPT conversations. We find steady growth in work-related messages bu t even faster growth in non-work-related messages, which have grown from 53% to more than 70% of all usage. Work usage is more common for educated users in highly-paid professional occupations. We classify messages by conversation top ic and find that “Practical Guidance,” “Seeking Information,” and “Writing” are the three most common topics and collectively account for nearly 80% of all conversations. Writing dominates work-related tasks, highlighting chatbots’ unique ability to generate digital outputs compared to traditional sear ch engines. Computer programming and self-expression both represent relatively small shares of use. Overall, we find that ChatGPT provides economic value through decision support, which is especially important in knowledge-intensive jobs. Aaron Chatterji Duke University Fuqua School of Business and OpenAI ronnie@duke.edu Thomas Cunningham OpenAI tom.cunningham@gmail.com David J. Deming Harvard University Harvard Kennedy School and NBER david_deming@harvard.edu Zoe Hitzig OpenAI and Harvard Society of Fellows zhitzig@g.harvard.edu Christopher Ong Harvard University and OpenAI christopherong@hks.harvard.edu Carl Yan Shan OpenAI cshan@openai.com Kevin Wadman OpenAI kevin.wadman@c-openai.com 1 Introduction ChatGPT launched in November 2022. By July 2025, 18 billion messages were being sent each week by 700 million users, representing around 10% of the global adult population. 1 For a new technology, this speed of global diffusion has no precedent (Bick et al., 2024). This paper studies consumer usage of ChatGPT, the first mass-market chatbot and likely the largest.2 ChatGPT is based on a Large Language Model (LLM), a type of Artificial Intelligence (AI) developed over the last decade and generally considered to represent an acceleration in AI capabilities.3 The sudden growth in LLM abilities and adoption has intensified interest in the effects of artificial intelligence on economic growth (Acemoglu, 2024; Korinek and Suh, 2024); employment (Eloundou et al., 2025); and society (Kulveit et al., 2025). However, despite the rapid adoption of LLMs, there is limited public information on how they are used. A number of surveys have measured self-reported adoption of LLMs (Bick et al., 2024; Pew Research Center, 2025); however there are reasons to expect bias in self-reports (Ling and Imas, 2025), and none of these papers have been able to directly track the quantity or nature of chatbot conversations. Two recent papers do report statistics on chatbot conversations, classified in a variety of ways (Handa et al., 2025; Tomlinson et al., 2025). We build on this work in several respects. First, the pool of users on ChatGPT is far larger, meaning we expect our data to be a closer approximation to the average chatbot user.4 Second, we use automated classifiers to report on the types of messages that users send using new classification taxonomies relative to the existing literature. Third, we report the diffusion of chatbot use across populations and the growth of different types of usage within cohorts. Fourth, we use a secure data clean room protocol to analyze aggregated employment and education categories for a sample of our users, lending new insights about differences in the types of messages sent by different groups while protecting user privacy. Our primary sample is a random selection of messages sent to ChatGPT on consumer plans (Free, Plus, Pro) between May 2024 and June 2025. 5 Messages from the user to chatbot are classified automatically using a number of different taxonomies: whether the message is used for paid work, the topic of conversation, and the type of interaction (asking, doing, or expressing), and the O*NET task the user is performing. Each taxonomy is defined in a prompt passed to an']","By July 2025, ChatGPT had been adopted by around 10% of the world’s adult population, with 700 million users sending 18 billion messages each week.",single_hop_specific_query_synthesizer
What did Bick et al. (2024) report about ChatGPT usage among US adults?,"['LLM, allowing us to classify messages without any human seeing them. We give the text of most prompts in Appendix A along with details about how the prompts were validated in Appendix B.6 The classification pipeline is protected by a series of privacy measures, detailed below, to ensure no leakage of sensitive information during the automated analysis. In a secure data clean room, we relate taxonomies of messages to aggregated employment and education categories. Table 1 shows the growth in total message volume for work and non-work usage. Both types of 1Reuters (2025), Roth (2025) 2Bick et al. (2024) report that 28% of US adults used ChatGPT in late 2024, higher than any other chatbot. 3We use the term LLM loosely here and give more details in the following section. 4Wiggers (2025) reports estimates that in April 2025 ChatGPT was receiving more than 10 times as many visitors as either Claude or Copilot. 5Our sample includes the three consumer plans (Free, Plus, or Pro). OpenAI also offers a variety of other ChatGPT plans (Business fka. Teams, Enterprise, Education), which we do not include in our sample. 6Our classifiers take into account not just the randomly-selected user message, but also a portion of the preceding messages in that conversation. 1 Month Non-Work (M)(%)Work (M)(%)Total Messages (M) Jun 2024 238 53% 213 47% 451 Jun 2025 1,911 73% 716 27% 2,627 Table 1:ChatGPT daily message counts (millions), broken down by likely work-related or non-work-related.\nTotal daily counts are exact measurements of message volume from all consumer plans. Daily counts of work\nand non-work related messages are estimated by classifying a random sample of conversations from that day.\n']","Bick et al. (2024) report that 28% of US adults used ChatGPT in late 2024, higher than any other chatbot.",single_hop_specific_query_synthesizer
Who is Collis in relation to AI communication?,"['Teams, Enterprise, Education), which we do not include in our sample. 6Our classifiers take into account not just the randomly-selected user message, but also a portion of the preceding messages in that conversation. 1 Month Non-Work (M)(%)Work (M)(%)Total Messages (M) Jun 2024 238 53% 213 47% 451 Jun 2025 1,911 73% 716 27% 2,627 Table 1:ChatGPT daily message counts (millions), broken down by likely work-related or non-work-related. Total daily counts are exact measurements of message volume from all consumer plans. Daily counts of work and non-work related messages are estimated by classifying a random sample of conversations from that day. Sampling is done to exclude users who opt-out of sharing their messages for model training, users who self- report their age as under 18, logged-out users, deleted conversations, and accounts which have been deactivated or banned (details available in Section 3). Reported values are 7-day averages (to smooth weekly fluctuation) ending on the 26th of June 2024 and 26th of June 2025. messages have grown continuously, but non-work messages have grown faster and now represent more than 70% of all consumer ChatGPT messages. While most economic analysis of AI has focused on its impact on productivity in paid work, the impact on activity outside of work (home production) is on a similar scale and possibly larger. The decrease in the share of work-related messages is primarily due to changing usage within each cohort of users rather than a change in the composition of new ChatGPT users. This finding is consistent with Collis and Brynjolfsson (2025), who use choice experiments to uncover willingness-to-pay for generative AI and estimate a consumer surplus of at least$97 billion in 2024 alone in the US. We next report on a classification of messages using a taxonomy developed at OpenAI for un- derstanding product usage (“conversation classifier”). Nearly 80% of all ChatGPT usage falls into three broad categories, which we callPractical Guidance,Seeking Information, andWriting.Practical Guidanceis the most common use case and includes activities like tutoring and teaching, how-to advice about a variety of topics, and creative ideation. 7 Seeking Informationincludes searching for information about people, current events, products, and recipes, and appears to be a very close sub- stitute for web search.Writingincludes the automated production of emails, documents and other communications, but also editing, critiquing, summarizing, and translating text provided by the user. Writingis the most common use case at work, accounting for 40% of work-related messages on average in June 2025. About two-thirds of allWritingmessages ask ChatGPT to modify user text (editing, critiquing, translating, etc.) rather than creating new text from scratch. About 10% of all messages are requests for tutoring or teaching, suggesting that education is a key use case for ChatGPT. Two of our findings stand in contrast to other work. First, we find the share of messages related to computer coding is relatively small: only 4.2% of ChatGPT messages are related to computer programming, compared to 33% of work-related Claude conversations Handa et al. (2025).8 Second, we find the share of messages related to companionship or social-emotional issues is fairly small: only 1.9% of ChatGPT messages are on the topic ofRelationships and Personal Reflectionand 0.4% are related 7The difference betweenPractical GuidanceandSeeking Informationis that the former is highly customized to the user and can be adapted based on conversation and follow-up, whereas the latter is factual information that should be the same for all users. For example, users interested in running might ask ChatGPT for the Boston Marathon qualifying times by age and gender (Seeking Information), or they might ask for a customized workout plan that matches their goals and current level of fitness (Practical Guidance). 8Handa et al. (2025) report that 37% of conversations are mapped to a “computer and mathematical” occupation category, and their Figure 12 shows 30% or more of all imputed tasks are programming or IT-related. We believe the discrepancy is partly due to the difference in types of users between Claude and ChatGPT, additionally Handa et al. (2025) only includes queries that ”possibly involve an occupational task”. 2 toGames and Role Play. In contrast, Zao-Sanders (2025) estimates thatTherapy/Companionshipis the most prevalent use case for generative AI. 9']","Collis is referenced in the context of a study by Collis and Brynjolfsson (2025), which uses choice experiments to uncover willingness-to-pay for generative AI and estimates a consumer surplus of at least $97 billion in 2024 alone in the US.",single_hop_specific_query_synthesizer
What are some creative writing prompts that involve mathematical calculations?,"['<1-hop>\n\n- ""Create a short story about a time-traveling astronaut.""- ""Make a rap in the style of Drake about the ocean.""- ""Escribe un cuento sobre un ni~ no que descubre un tesoro, pero despu´ es viene un\npirata."",→\n- ""Compose a sonnet about time.""** how_to_advice**:\n- ""How do I turn off my screensaver?""\n44\x0c- ""My car won’t start; what should I try?""\n- ""Comment faire pour me connecter ` a mon wifi?""\n- ""What’s the best way to clean hardwood floors?""\n- ""How can I replace a flat tire?""\n** creative_ideation**:\n- ""What should I talk about on my future podcast episodes?""\n- ""Give me some themes for a photography project.""- ""Necesito ideas para un regalo de aniversario.""- ""Brainstorm names for a new coffee shop.""- ""What are some unique app ideas for startups?""\n** tutoring_or_teaching**:\n- ""How do black holes work?""\n- ""Can you explain derivatives and integrals?""\n- ""No entiendo la diferencia entre ser y estar.""- ""Explain the causes of the French Revolution.""- ""What is the significance of the Pythagorean theorem?""\n** translation**:\n- ""How do you say Happy Birthday in Hindi?""\n- ""Traduis Je taime en anglais.""- ""What’s Good morning in Japanese?""\n- ""Translate I love coding to German.""- ""¿C´ omo se dice Thank you en franc´ es?""\n**', '<2-hop>\n\nmathematical_calculation**:\n- ""What is 400000 divided by 23?""\n- ""Calculate the square root of 144.""- ""Solve for x in the equation 2x + 5 = 15.""- ""What’s the integral of sin(x)?""\n- ""Convert 150 kilometers to miles.""**computer_programming**:\n- ""How to group by and filter for biggest groups in SQL.""- ""Im getting a TypeError in JavaScript when I try to call this function.""- ""Write a function to retrieve the first and last value of an array in Python.""45\x0c- ""Escribe un programa en Python que cuente las palabras en un texto.""- ""Explain how inheritance works in Java.""**purchasable_products**:\n- ""iPhone 15.""- ""What’s the best streaming service?""\n- ""How much are Nikes?""\n- ""Cu´ anto cuesta un Google Pixel?""\n- ""Recommend a good laptop under $1000.""**cooking_and_recipes**:\n- ""How to cook salmon.""- ""Recipe for lasagna.""- ""Is turkey bacon halal?""\n- ""Comment faire des cr^ epes?""\n- ""Give me a step-by-step guide to make sushi.""**health_fitness_beauty_or_self_care**:\n- ""How to do my eyebrows.""- ""Quiero perder peso, ¿c´ omo empiezo?""\n- ""Whats a good skincare routine for oily skin?""\n- ""How can I improve my cardio fitness?""\n- ""Give me tips for reducing stress.""**specific_info**:\n- ""What is regenerative agriculture?""\n- ""Whats the name of the song that has the lyrics I was born to run?""\n- ""Tell me about Marie Curie and her main contributions to science.""- ""What conflicts are happening in the Middle East right now?""\n- ""Quelles ´ equipes sont en finale de la ligue des champions ce mois-ci?""\n- ""Tell me about recent breakthroughs in cancer research.""**greetings_and_chitchat**:\n- ""Ciao!""- ""Hola.""- ""I had an awesome day today; how was yours?""\n46\x0c- ""Whats your favorite animal?""\n- ""Do you like ice cream?""\n**relationships_and_personal_reflection**:\n- ""what should I do for my 10th anniversary?""\n- ""Im feeling worried.""- ""My wife is mad at me, and I don’t know what to do.""- ""Im so happy about my promotion!""- ""Je sais pas ce que je fais pour que les gens me d´ etestent. Quest-ce que je fais\nmal?"",→\n**games_and_role_play**:\n- ""You are a Klingon. Lets discuss the pros and cons of working with humans.""- ""Ill say a word, and then you say the opposite of that word!""- ""Youre the dungeon master; tell us about the mysterious cavern we encountered.""- ""I want you to be my AI girlfriend.""- ""Faisons semblant que nous sommes des astronautes. Comment on fait pour atterrir\nsur Mars?"",→\n**asking_about_the_model**:\n- ""Who made you?""\n- ""What do you know?""\n- ""How many languages do you speak?""\n- ""Are you an AI or a human?""\n- ""As-tu des sentiments?""\n**create_an_image**:\n- ""Draw an astronaut riding a unicorn.""- ""Photorealistic image of a sunset over the mountains.""- ""Quiero que hagas un dibujo de un conejo con una corbata.""- ""Generate an image of a futuristic cityscape.""- ""Make an illustration of a space shuttle launch.""**analyze_an_image**:\n- ""Who is in this photo?""\n- ""What does this sign say?""\n47\x0c- ""Soy ciega, ¿puedes describirme esta foto?""\n- ""Interpret the data shown in this chart.""- ""Describe the facial expressions in this photo.""**generate_or_retrieve_other_media**:\n- ""Make a YouTube video about goal kicks.""- ""Write PPT slides for a tax law conference.""- ""Create a spreadsheet for mortgage payments.""- ""Find me a podcast about ancient history.""- ""Busca un video que explique la teor´ ıa de la relatividad.""**data_analysis**:\n- ""Heres a spreadsheet with my expenses; tell me how much I spent on which\ncategories."",→\n- ""Whats the mean, median, and mode of this dataset?""\n- ""Create a CSV with the top 10 most populated countries and their populations over\ntime. Give me the mean annual growth rate for each country."",→\n- ""Perform a regression analysis on this data.""- ""Analyse these survey results and summarize the key findings.""']","Some creative writing prompts that involve mathematical calculations include: ""Create a short story about a time-traveling astronaut,"" which could incorporate mathematical concepts of time and space, and prompts like ""Calculate the square root of 144"" could inspire a narrative involving a character solving a mystery through math.",multi_hop_abstract_query_synthesizer
How does the usage of ChatGPT for Seeking Information compare to its use for Practical Guidance among different occupations?,"['<1-hop>\n\nThis is consistent with the fact that almost half of all ChatGPT usage is\neitherPractical GuidanceorSeeking Information. We also show thatAskingis growing faster than\n9Zao-Sanders (2025) is based on a manual collection and labeling of online resources (Reddit, Quora, online articles),\nand so we believe it likely resulted in an unrepresentative distribution of use cases.\n10Among those with names commonly associated with a particular gender.\n11Appendix A gives the full prompt text and Appendix B gives detail about how the prompts were validated against\npublic conversation data.\n3\x0cDoing, and thatAskingmessages are consistently rated as having higher quality both by a classifier\nthat measures user satisfaction and from direct user feedback.\nHow does ChatGPT provide economic value, and for whom is its value the greatest? We argue that\nChatGPT likely improves worker output by providingdecision support, which is especially important in\nknowledge-intensive jobs where better decision-making increases productivity (Deming, 2021; Caplin et\nal., 2023). This explains whyAskingis relatively more common for educated users who are employed\nin highly-paid, professional occupations. Our findings are most consistent with Ide and Talamas\n(2025), who develop a model where AI agents can serve either asco-workersthat produce output or\nasco-pilotsthat give advice and improve the productivity of human problem-solving.\n2 ', '<2-hop>\n\nWhat is ChatGPT?\nHere we give a simplified overview of LLMs and chatbots. For more precise details, refer to the papers\nand system cards that OpenAI has released with each model e.g., (OpenAI, 2023, 2024a, 2025b). A\nchatbot is a statistical model trained to generate a text response given some text input, so as to\nmaximize the “quality” of that response, where the quality is measured with a variety of metrics.\nIn a prototypical interaction, a user submits a plain-text message (“prompt”) and ChatGPT\nreturns the message (“response”) generated from an underlying LLM. A large set of additional features\nhave been added to ChatGPT—including the possibility for the LLM to search the web or external\ndatabases, and generate images based on text—but the exchange of text-based messages remains the\nmost typical interaction.\nSince its launch ChatGPT has used a variety of different underlying LLMs e.g., GPT-3.5, GPT-4,\nGPT-4o, o1, o3, and GPT-5. 12 In addition there are occasional updates to the model’s weights and\nto the model’s system prompt (text instructions sent to the model along with all the queries).\nAn LLM can be thought of as a function from a string of words to a probability distribution over\nthe set of all possible words (more precisely, “tokens,” which very roughly correspond to words13). The\nfunctions are implemented with deep neural nets, typically with a transformer architecture (Vaswani\net al., 2017), parameterized with billions of model “weights”. We will refer to all of ChatGPT’s models\nas language models, though most can additionally process tokens representing images, audio, or other\nmedia.\nThe weights in an LLM-based chatbot are often trained in two stages, commonly called “pre-\ntraining” and “post-training”. In the first stage (“pre-training”), the LLMs are trained to predict the\nnext word in a string, given the preceding words, over an enormous corpus of text. At that point the\nmodels are purely predictors of the likelihood of the next word given a prior context, and as such they\nhave a relatively narrow application. In the second stage (“post-training”), the models are trained to\nproduce words that comprise “good” responses to some prompt. This stage often consists of a variety\nof different strategies: fine-tuning on a dataset of queries and ideal responses, reinforcement learning\nagainst another model that is trained to grade the quality of a response (Ouyang et al., 2022), or\nreinforcement learning against a function that knows the true response to queries (OpenAI (2024b),\n12For a timeline of model launches, see Appendix C.\n13Tokenization is a way of cutting a string of text into discrete chunks, chosen to be statistically efficient. In many\ntokenization schemes, one token corresponds to roughly three-quarters of an English word.\n4\x0cLambert et al. (2024)). This second stage also typically includes a number of “safety” constraints to\navoid certain classes of response, especially those which are deemed harmful or dangerous (OpenAI,\n2025a).\nThis two-stage process has a common statistical interpretation: the first stage teaches the model a\nlatent representation of the world; the second stage fits a function using that representation (Bengio\net al., 2014). Pre-training the model to predict the next word effectively teaches the model a low-\ndimensional representation of text, representing only the key semantic features, and therefore rendering\nthe prompt-response problem tractable with a reasonable set of training examples.\nTwo common ways of evaluating chatbots are with benchmarks (batteries of questions with known\nanswers, e.g. Measuring Massive Multitask Language Understanding (Hendrycks et al., 2021)) and\ncomparisons of human preferences over two alternative responses to the same message (e.g. Chatbot\nArena (Chiang et al., 2024)).\n3 ', '<3-hop>\n\nPanel C2.Technical Help. Panel C3.Seeking Information. Panel C4.Practical Guidance. Figure 23:Variation in ChatGPT usage by occupation. Panel A shows the share of messages that are work-related across broad occupation categories. Panel B presents variation in the share of Asking and Doing messages within work-related usage. Panel C presents the distribution of work-related conversation topics by occupation, focusing on Writing and Practical Guidance. The regression for these figures is the same one as the one used in Figure 22. 34 Occupation Group Documenting/ Recording Information Making Decisions And Solving Problems Thinking Creatively Working With Computers Interpreting The Meaning Of Information For Others Getting Information Providing Consultation And Advice To Others Management 2 1 3 6 4 5 8 Business 2 1 3 6 4 5 7 Computer/Math 4 2 5 1 3 6 7 Engineering 3 1 5 2 4 6 7 Science 2 1 4 3 6 5 7 Social Service 2 1 3 X 5 4 X Legal 1 X X X X X X Education 1 2 3 4 6 5 7 Arts/Design/Media 2 1 3 5 4 6 7 Health Professionals 1 2 3 X 5 4 6 Food Service 1 X X X X X X Personal Service 1 2 3 X 4 5 X Sales 2 1 3 6 4 5 7 Administrative 2 1 3 7 4 5 8 Transportation 2 1 3 X X 4 X Military 2 1 X X X X X Figure 24:The seven most commonly requested GWAs for work-related queries. Table reports the frequency ranking of each of these GWAs for each broad occupation groups (two-digit SOC codes). 1 represents the most frequently requested GWA for that occupation. X’s indicate that the ranking is unavailable since fewer than 100 users from that occupation group requested that specific GWA within the sample. Seven occupation groups are omitted because no GWA was requested by more than 100 users from a single occupation group. These omitted occupation groups (with corresponding SOC2 codes) are ”Healthcare Support” (31), ”Protective Service” (33), ”Building and Grounds Cleaning and Maintenance” (37), ”Farming, Fishing, and Forestry” (45), ”Construction and Extraction” (47), ”Installation, Maintenance, and Repair” (49), and ”Production” (51). Not pictured are twelve other GWAs which are less frequently requested and are reported fully in Appendix D. See Appendix for full cross-tabulations between GWA and two-digit SOC2 codes. 35 7 Conclusion This paper studies the rapid growth of ChatGPT, which launched in November 2022. By July 2025, ChatGPT had been used weekly by more than 700 million users, who were collectively sending more than 2.5 billion messages per day, or about 29,000 messages per second. Yet despite the rapid adop- tion of ChatGPT and Generative AI more broadly, little previous evidence existed on how this new technology is used and who is using it. This is the first economics paper to use internal ChatGPT message data, and we do so while introducing a novel privacy-preserving methodology. No user messages were observed by humans during any part of the work on this paper. This paper documents eight important facts about ChatGPT. First, as of July 2025 about 70% of ChatGPT consumer queries were unrelated to work; while both work-related and non-work-related queries have been increasing, non-work queries have been increasing faster. Second, the three most common ChatGPT conversation topics arePractical Guidance,Writing, andSeeking Information, collectively accounting for nearly 78% of all messages.Computer Pro- grammingandRelationships and Personal Reflectionaccount for only 4.2% and 1.9% of messages respectively. Third,Writingis by far the most common work use, accounting for 42% of work-related messages overall and more than half of all messages for users in management and business occupations. About two-thirds ofWritingmessages are requests to modify user text rather than to produce novel text from scratch. Fourth, we classify messages according to the kind of output users are seeking with a rubric we callAsking, Doing,orExpressing.About 49% of messages are users asking ChatGPT for guidance, advice, or information (Asking), 40% are requests to']","The usage of ChatGPT for Seeking Information and Practical Guidance varies significantly across different occupations. According to the data, Seeking Information and Practical Guidance are among the three most common conversation topics, collectively accounting for nearly 78% of all messages. However, the distribution of these topics differs by occupation. For instance, in management and business occupations, Writing, which is closely related to Practical Guidance, accounts for 42% of work-related messages. In contrast, the frequency of Seeking Information messages is also notable, indicating that while both types of usage are prevalent, the emphasis on Practical Guidance is particularly strong in professional settings.",multi_hop_abstract_query_synthesizer
What trends can be observed in the usage of ChatGPT user cohorts and the nature of ChatGPT queries related to work between June 2024 and June 2025?,"['<1-hop>\n\nThe yellow line represents the first cohort of ChatGPT users: their usage declined somewhat over\n2023, but started growing again in late 2024 and is now higher than it has ever been. The pink line\nrepresents messages from users who signed up in Q3 of 2023 or earlier, and so thedifferencebetween\n20Note that we expect our counts of distinct accounts to somewhat exceed distinct people when one person has two\naccounts (or, for logged-out users, one person using two devices). For logged-in users, the count is based on distinct\nlogin credentials (email addresses), and one person may have multiple accounts. For logged-out users, the count is based\non distinct browser cookies; this would double-count people if someone returns to ChatGPT after clearing their cookies,\nor if they access ChatGPT with two different devices in the same week.\n10\x0cFigure 4:Daily message volumes from ChatGPT consumer plans (Free, Plus, Pro), split by sign-up date of\nthe requesting user. Reported values are moving averages of the past 90 days. Y-axis is an index normalized\nto the reported value for ”All Cohorts” at the end of Q1 2024 (April 1, 2024).\nthe yellow and pink lines represents the messages sent by users who signed up in Q2 and Q3 of 2023.\nThere has been dramatic growth in message volume both by new cohorts of users, and from growth\nin existing cohorts.\nFigure 5 normalizes each cohort, plotting daily messages per weekly active user. Each line rep-\nresents an individual cohort (instead of a cumulative cohort, as in Figure 4). The figure shows that\nearlier sign-ups have consistently had higher usage, but that usage has also consistently grown within\nevery cohort, which we interpret as due to both (1) improvements in the capabilities of the models,\nand (2) users slowly discovering new uses for existing capabilities.\n5  How ChatGPT is Used\nWe next report on thecontentof ChatGPT conversations using a variety of different taxonomies. For\neach taxonomy we describe a “prompt” which defines a set of categories, and then apply an LLM\nto map each message to a category. Our categories often apply to the user’sintention, rather than\nthe text of the conversation, and as such we never directly observe the ground truth. Nevertheless\nthe classifier results can be interpreted as the best-guess inferences that a human would make: the\nguesses from the LLM correlate highly with human guesses from the same prompt, and we get similar\nqualitative results when the prompt includes a third category for “uncertain.”\n11\x0cFigure 5:Daily messages sent per weekly active user, split by sign-up cohort. Sample only considers users of\nChatGPT consumer plans (Free, Plus, Pro). Reported values are moving averages of the past 90 days and are\nreported starting 90 days after the cohort is fully formed. Y-axis is an index normalized to the first reported\nvalue for the Q1 2023 cohort.\n5.1 ', '<2-hop>\n\nWhat share of ChatGPT queries are related to paid work?\nWe label each user message in our dataset based on whether it appears to be related to work, using\nan LLM classifier. The critical part of the prompt is as follows: 21\nDoes the last user message of this conversation transcript seem likely to be related to doing\nsome work/employment? Answer with one of the following:\n(1) likely part of work (e.g., “rewrite this HR complaint”)\n(0) likely not part of work (e.g., “does ice reduce pimples?”)\nTable 1 shows that both types of queries grew rapidly between June 2024 and June 2025, however\nnon-work-related messages grew faster: 53% of messages were not related to work in June 2024, which\nclimbed to 73% by June 2025.\nFigure 6 plots the share of non-work messages decomposed by cumulative sign-up cohorts. Succes-\nsive cohorts have had a higher share of non-work messages, but also within each cohort their non-work\nuse has increased. Comparing the share among all users (black line) to the share among the earliest\ncohort of users (yellow line), we can see that they track very closely.\n21See Appendix A for the full prompt, see Appendix B for validation.\n12\x0cFigure 6:The solid black line represents the probability that a messages on a given day is not related to\nwork, as determined by an automated classifier. Values are averaged over a 28-day lagging window. The\ndotted orange line shows the same calculation, but conditioned on messages being from users who first used\nChatGPT during or before Q2 of 2024. The remaining lines are defined similarly for successive quarters, with\ncoloring cooling for more recent cohorts. Counts are calculated from a sample of approximately 1.1 million\nsampled conversations from May 15, 2024 through June 26, 2025. Observations are reweighted to reflect total\nmessage volumes on a given day. Sampling details available in Section 3.\n5.2 ']","Between June 2024 and June 2025, there was a notable trend in the usage of ChatGPT user cohorts, particularly with the first cohort experiencing a decline in usage early in 2023, followed by significant growth towards the end of 2024. This growth was mirrored across new cohorts, indicating an overall increase in message volume. Additionally, the nature of ChatGPT queries showed that while both work-related and non-work-related messages grew rapidly, non-work-related messages outpaced work-related ones, rising from 53% in June 2024 to 73% by June 2025. This suggests that newer cohorts of users are increasingly engaging with ChatGPT for non-work purposes, reflecting a broader trend in user interaction with the platform.",multi_hop_abstract_query_synthesizer
"What are the primary use cases for ChatGPT, and how do they relate to the economic value it provides, particularly in knowledge-intensive jobs?","['<1-hop>\n\nThis is consistent with the fact that almost half of all ChatGPT usage is\neitherPractical GuidanceorSeeking Information. We also show thatAskingis growing faster than\n9Zao-Sanders (2025) is based on a manual collection and labeling of online resources (Reddit, Quora, online articles),\nand so we believe it likely resulted in an unrepresentative distribution of use cases.\n10Among those with names commonly associated with a particular gender.\n11Appendix A gives the full prompt text and Appendix B gives detail about how the prompts were validated against\npublic conversation data.\n3\x0cDoing, and thatAskingmessages are consistently rated as having higher quality both by a classifier\nthat measures user satisfaction and from direct user feedback.\nHow does ChatGPT provide economic value, and for whom is its value the greatest? We argue that\nChatGPT likely improves worker output by providingdecision support, which is especially important in\nknowledge-intensive jobs where better decision-making increases productivity (Deming, 2021; Caplin et\nal., 2023). This explains whyAskingis relatively more common for educated users who are employed\nin highly-paid, professional occupations. Our findings are most consistent with Ide and Talamas\n(2025), who develop a model where AI agents can serve either asco-workersthat produce output or\nasco-pilotsthat give advice and improve the productivity of human problem-solving.\n2 ', '<2-hop>\n\nWhat is ChatGPT?\nHere we give a simplified overview of LLMs and chatbots. For more precise details, refer to the papers\nand system cards that OpenAI has released with each model e.g., (OpenAI, 2023, 2024a, 2025b). A\nchatbot is a statistical model trained to generate a text response given some text input, so as to\nmaximize the “quality” of that response, where the quality is measured with a variety of metrics.\nIn a prototypical interaction, a user submits a plain-text message (“prompt”) and ChatGPT\nreturns the message (“response”) generated from an underlying LLM. A large set of additional features\nhave been added to ChatGPT—including the possibility for the LLM to search the web or external\ndatabases, and generate images based on text—but the exchange of text-based messages remains the\nmost typical interaction.\nSince its launch ChatGPT has used a variety of different underlying LLMs e.g., GPT-3.5, GPT-4,\nGPT-4o, o1, o3, and GPT-5. 12 In addition there are occasional updates to the model’s weights and\nto the model’s system prompt (text instructions sent to the model along with all the queries).\nAn LLM can be thought of as a function from a string of words to a probability distribution over\nthe set of all possible words (more precisely, “tokens,” which very roughly correspond to words13). The\nfunctions are implemented with deep neural nets, typically with a transformer architecture (Vaswani\net al., 2017), parameterized with billions of model “weights”. We will refer to all of ChatGPT’s models\nas language models, though most can additionally process tokens representing images, audio, or other\nmedia.\nThe weights in an LLM-based chatbot are often trained in two stages, commonly called “pre-\ntraining” and “post-training”. In the first stage (“pre-training”), the LLMs are trained to predict the\nnext word in a string, given the preceding words, over an enormous corpus of text. At that point the\nmodels are purely predictors of the likelihood of the next word given a prior context, and as such they\nhave a relatively narrow application. In the second stage (“post-training”), the models are trained to\nproduce words that comprise “good” responses to some prompt. This stage often consists of a variety\nof different strategies: fine-tuning on a dataset of queries and ideal responses, reinforcement learning\nagainst another model that is trained to grade the quality of a response (Ouyang et al., 2022), or\nreinforcement learning against a function that knows the true response to queries (OpenAI (2024b),\n12For a timeline of model launches, see Appendix C.\n13Tokenization is a way of cutting a string of text into discrete chunks, chosen to be statistically efficient. In many\ntokenization schemes, one token corresponds to roughly three-quarters of an English word.\n4\x0cLambert et al. (2024)). This second stage also typically includes a number of “safety” constraints to\navoid certain classes of response, especially those which are deemed harmful or dangerous (OpenAI,\n2025a).\nThis two-stage process has a common statistical interpretation: the first stage teaches the model a\nlatent representation of the world; the second stage fits a function using that representation (Bengio\net al., 2014). Pre-training the model to predict the next word effectively teaches the model a low-\ndimensional representation of text, representing only the key semantic features, and therefore rendering\nthe prompt-response problem tractable with a reasonable set of training examples.\nTwo common ways of evaluating chatbots are with benchmarks (batteries of questions with known\nanswers, e.g. Measuring Massive Multitask Language Understanding (Hendrycks et al., 2021)) and\ncomparisons of human preferences over two alternative responses to the same message (e.g. Chatbot\nArena (Chiang et al., 2024)).\n3 ', '<3-hop>\n\nTeams, Enterprise, Education), which we do not include in our sample. 6Our classifiers take into account not just the randomly-selected user message, but also a portion of the preceding messages in that conversation. 1 Month Non-Work (M)(%)Work (M)(%)Total Messages (M) Jun 2024 238 53% 213 47% 451 Jun 2025 1,911 73% 716 27% 2,627 Table 1:ChatGPT daily message counts (millions), broken down by likely work-related or non-work-related. Total daily counts are exact measurements of message volume from all consumer plans. Daily counts of work and non-work related messages are estimated by classifying a random sample of conversations from that day. Sampling is done to exclude users who opt-out of sharing their messages for model training, users who self- report their age as under 18, logged-out users, deleted conversations, and accounts which have been deactivated or banned (details available in Section 3). Reported values are 7-day averages (to smooth weekly fluctuation) ending on the 26th of June 2024 and 26th of June 2025. messages have grown continuously, but non-work messages have grown faster and now represent more than 70% of all consumer ChatGPT messages. While most economic analysis of AI has focused on its impact on productivity in paid work, the impact on activity outside of work (home production) is on a similar scale and possibly larger. The decrease in the share of work-related messages is primarily due to changing usage within each cohort of users rather than a change in the composition of new ChatGPT users. This finding is consistent with Collis and Brynjolfsson (2025), who use choice experiments to uncover willingness-to-pay for generative AI and estimate a consumer surplus of at least$97 billion in 2024 alone in the US. We next report on a classification of messages using a taxonomy developed at OpenAI for un- derstanding product usage (“conversation classifier”). Nearly 80% of all ChatGPT usage falls into three broad categories, which we callPractical Guidance,Seeking Information, andWriting.Practical Guidanceis the most common use case and includes activities like tutoring and teaching, how-to advice about a variety of topics, and creative ideation. 7 Seeking Informationincludes searching for information about people, current events, products, and recipes, and appears to be a very close sub- stitute for web search.Writingincludes the automated production of emails, documents and other communications, but also editing, critiquing, summarizing, and translating text provided by the user. Writingis the most common use case at work, accounting for 40% of work-related messages on average in June 2025. About two-thirds of allWritingmessages ask ChatGPT to modify user text (editing, critiquing, translating, etc.) rather than creating new text from scratch. About 10% of all messages are requests for tutoring or teaching, suggesting that education is a key use case for ChatGPT. Two of our findings stand in contrast to other work. First, we find the share of messages related to computer coding is relatively small: only 4.2% of ChatGPT messages are related to computer programming, compared to 33% of work-related Claude conversations Handa et al. (2025).8 Second, we find the share of messages related to companionship or social-emotional issues is fairly small: only 1.9% of ChatGPT messages are on the topic ofRelationships and Personal Reflectionand 0.4% are related 7The difference betweenPractical GuidanceandSeeking Informationis that the former is highly customized to the user and can be adapted based on conversation and follow-up, whereas the latter is factual information that should be the same for all users. For example, users interested in running might ask ChatGPT for the Boston Marathon qualifying times by age and gender (Seeking Information), or they might ask for a customized workout plan that matches their goals and current level of fitness (Practical Guidance). 8Handa et al. (2025) report that 37% of conversations are mapped to a “computer and mathematical” occupation category, and their Figure 12 shows 30% or more of all imputed tasks are programming or IT-related. We believe the discrepancy is partly due to the difference in types of users between Claude and ChatGPT, additionally Handa et al. (2025) only includes queries that ”possibly involve an occupational task”. 2 toGames and Role Play. In contrast, Zao-Sanders (2025) estimates thatTherapy/Companionshipis the most prevalent use case for generative AI. 9']","The primary use cases for ChatGPT include Practical Guidance, Seeking Information, and Writing. Practical Guidance is the most common use case, encompassing activities like tutoring, teaching, and creative ideation. Seeking Information involves searching for factual data about various topics, which serves as a close substitute for web search. Writing includes the automated production of emails, documents, and other communications, with a significant portion of these messages focused on editing and critiquing existing text. The economic value of ChatGPT is particularly pronounced in knowledge-intensive jobs, where it provides decision support that enhances productivity. This is especially relevant for educated users in high-paying professional occupations, as better decision-making facilitated by ChatGPT can lead to improved worker output.",multi_hop_abstract_query_synthesizer
"What is the ratio of good to bad interactions in Technical Help compared to other conversation topics, and how does education level affect the frequency of Technical Help messages?","['<1-hop>\n\n5.5 Quality of Interactions\nWe additionally used automated classifiers to study the user’s apparent satisfaction with the chatbot’s\nresponse to their request. OurInteraction Qualityclassifier looks for an expression of satisfaction or\ndissatisfaction in the user’s subsequent message in the same conversation (if one exists), with three\npossible categories:Good,Bad, andUnknown. 23\nFigure 16 plots the overall growth of messages in these three buckets. In late 2024Goodinteractions\nwere about three times as common asBadinteractions, butGoodinteractions grew much more rapidly\nover the next nine months, and by July 2025 they were more than four times more common.\nFigure 16:Interaction quality shares, based on automated sentiment analysis of thenext responseprovided\nby the user. See Appendix B to understand how this classifier was validated. Values are averaged over a 28\nday lagging window. Shares are calculated from a sample of approximately 1.1 million sampled conversations\nfrom May 15, 2024 through June 26, 2025. Observations are reweighted to reflect total message volumes on a\ngiven day. Sampling details available in Section 3.\nDetails on the validation of this classifier, along with measurements of how it correlates with\nexplicit thumbs up/thumbs down annotations from users, are included in Appendix B.\nFigure 17 shows the ratio of good-to-bad messages by conversation topic and interaction type, as\nrated by Interaction Quality. Panel A shows thatSelf-Expressionis the highest rated topic, with a\ngood-to-bad ratio of more than seven, consistent with the growth in this category.Multimediaand\nTechnical Helphave the lowest good-to-bad ratios (1.7 and 2.7 respectively). Panel B shows that\nAskingmessages are substantially more likely to receive a good rating thanDoingorExpressing\nmessages.\n23For this classifier we do not disclose the prompt.\n23\x0cFigure 17:AverageGoodtoBadratio for user interactions by Conversation Topic (Panel A) and Ask-\ning/Doing/Expressing classification (Panel B). The prompts for each of these automated classifiers (with the\nexception of interaction quality) are available in Appendix A. Values represent the average ratio from May 15,\n2024 through June 26, 2025, where observations are reweighted to reflect total message volumes on a given\nday. Sampling details available in Section 3.\n24\x0c', '<2-hop>\n\n37% of messages are work-related for users with less than a bachelor’s degree, compared to 46% for users with exactly a bachelor’s degree and 48% for those with some graduate education. Those differences are cut roughly in half after adjusting for other characteristics, but they are still statistically significant at the less than 1 percent level. Educated users are more likely to send work-related messages. Panel B explores variation by education in user intent.Askingconstitutes about 49% of messages for users with less than a bachelor’s degree, with little variation for more educated users. After regression adjustment, we find that users with a graduate degree are about two percentage points more likely to use ChatGPT forAskingmessages, a difference that is statistically significant at the 5% level. Prior to regression adjustment, the frequency ofDoingmessages is increasing in education. However, this pattern reverses after adjusting for other characteristics such as occupation. Users with a graduate degree are about 1.6 percentage points less likely to sendDoingmessages than users with less than a bachelor’s degree, and the difference is statistically significant at the 10% level. Panel C studies variation by education in the frequency of four different conversation topics – Practical Guidance,Seeking Information,Technical Help, andWriting. We find only modest differ- ences by education across most of these categories. The one exception is that the share of messages related toWritingis increasing in relation to education. 28 Panel A.Work Related Panel B1.Asking. Panel B2.Doing. Panel B3.Expressing. Figure 22:(continued on next page) 29 Panel C1.Writing. Panel C2.Technical Help. Panel C3.Seeking Information. Panel C4.Practical Guidance. Figure 22:Variation in ChatGPT usage by education. Each plot shows unadjusted vs. regression-adjusted estimates, with 95% confidence intervals. We regress each message share on education and occupation, control- ling for the following covariates: age, whether the name was typically masculine or feminine, seniority within role, company size, and industry. (To guarantee user privacy, we coarsen all covariates to broad categories and programmatically enforce that each group has at least 100 members prior to running the regression) We add the coefficients on each education and occupation category to the unadjusted value for the reference category and compute 95% confidence intervals using the standard errors from the regression coefficients. The sample for this regression is the approximately 40,000 users of the original 130,000 sample whose publicly available occupation was not blank or consisted of strictly special characters (as determined by a classification script). Shares for each user are calculated by randomly sampling up to six conversations attributed to the user from May 2024 through July 2025. 30 6.5 Variation by Occupation Figure 23 presents variation in ChatGPT usage by user occupation. Due to privacy-preserving aggre- gation limits, we report results for the following broad occupation categories – (1) all nonprofessional occupations, including administrative, clerical, service, and blue-collar occupations; (2) computer- related occupations; (3) engineering and science occupations; (4) management and business occupa- tions; and (5) all other professional occupations, including law, education, and health care. 26 As above, the left-hand side of the figure shows unadjusted comparisons and the right-hand side presents the coefficients on each occupation category from a regression of message shares on age, whether the name was typically masculine or feminine, education, occupation categories, job seniority, firm size, and industry. Users in highly paid professional and technical occupations are more likely to use ChatGPT for work.27 Panel A shows that the unadjusted work shares are 57% for computer-related occupations; 50% for management and business; 48% for engineering and science; 44% for other professional oc- cupations; and only 40% for all non-professional occupations. Regression adjustment moves these figures around slightly, but the gaps by occupation remain highly statistically significant. Users in highly-paid professional occupations are more likely to send work-related messages. Because work usage is so different by occupation, we restrict the sample only to work-related messages in Panels B and C. Panel B presents the share of work-related messages that areAsking messages, by occupation. We find that users in highly paid professional occupations are more likely to use ChatGPT forAskingrather thanDoing. 28 This is especially true in scientific and technical occupations. 47% of the work-related messages sent by users employed in computer-related occupa-']","The ratio of good to bad interactions in Technical Help is 2.7, which is lower than the highest rated topic, Self-Expression, which has a good-to-bad ratio of more than seven. Regarding education level, 37% of messages are work-related for users with less than a bachelor’s degree, compared to 46% for users with exactly a bachelor’s degree and 48% for those with some graduate education. Users with a graduate degree are about two percentage points more likely to use ChatGPT for Asking messages, but the share of messages related to Technical Help shows only modest differences by education.",multi_hop_specific_query_synthesizer
"What was the percentage of Asking messages compared to Doing messages in conversations sampled from May 15, 2024, through June 26, 2025?","['<1-hop>\n\nNearly 35% of all work-related queries areDoingmessages related toWriting.DoingandAsking\ncomprise equal shares ofTechnical Helpqueries.\n17\x0cFigure 10:Breakdown of Conversation Topics by Asking/Doing/Expressing category, with topic columns\nsorted by relative share of ”Doing” messages. Prompts for these automated classifiers are available in Appendix\nA. For a detailed breakdown of conversation topic contents, see Table 3. Each bin reports a percentage of\nthe total population. Shares are calculated from a sample of approximately 1.1 million sampled conversations\nfrom May 15, 2024 through June 26, 2025. Observations are reweighted to reflect total message volumes on a\ngiven day. Sampling details available in Section 3.\nFigure 11:Breakdown of Conversation Topics by Asking/Doing/Expressing category foronly work-related\nmessages, with topic columns sorted by relative share of ”Doing” messages. Prompts for these automated\nclassifiers are available in Appendix A. For a detailed breakdown of conversation topic contents, see Table 3.\nEach bin reports a percentage of the total population. Shares are calculated from a sample of approximately\n1.1 million sampled conversations from May 15, 2024 through June 26, 2025. Observations are reweighted to\nreflect total message volumes on a given day. Sampling details available in Section 3.\n18\x0cFigure 12 presents changes over time in the composition of messages by user intent. In July\n2024, usage was evenly split betweenAskingandDoing, with just under 8% of messages classified as\nExpressing.AskingandExpressinggrew much faster thanDoingover the next year, and by late June\n2025 the split was 51.6%Asking, 34.6%Doing, and 13.8%Expressing.\nFigure 12:Shares of messages classified as Asking, Doing, or Expressing by an automated ternary classifier.\nValues are averaged over a 28 day lagging window. Shares are calculated from a sample of approximately\n1.1 million sampled conversations from May 15, 2024 through June 26, 2025. Observations are reweighted to\nreflect total message volumes on a given day. Sampling details available in Section 3.\nFigure 13 presents the share of work-related messages by user intent.Doingmessages, which\naccount for approximately 40% of messages, have an even split of messages between work-related and\nnon-work related.\n']","By late June 2025, the split of messages was 51.6% classified as Asking and 34.6% classified as Doing, indicating that Asking messages outnumbered Doing messages in the sampled conversations.",multi_hop_specific_query_synthesizer
"What percentage of ChatGPT conversations are categorized under Practical Guidance, and what specific topics fall under this category?","['<1-hop>\n\nFigure 9 disaggregates four of the seven Conversation Topics into smaller groups and sums up\nmessages of each type over a one-year period. For example, the five sub-categories withinWriting\nare (in order of frequency)Editing or Critiquing Provided Text,Personal Writing or Communication,\nTranslation,Argument or Summary Generation, andWriting Fiction. Three of those five categories\n(Editing or Critiquing Provided Text,Translation, andArgument or Summary Generation) are re-\nquests to modify text that has been provided to ChatGPT by the user, whereas the other two are\nrequests to produce novel text. The former constitute two thirds of allWritingconversations, which\n14\x0cFigure 7:Share of consumer ChatGPT messages broken down by high level conversation topic, according\nto the mapping in Table 3. Values are averaged over a 28 day lagging window. Shares are calculated from\na sample of approximately 1.1 million sampled conversations from May 15, 2024 through June 26, 2025.\nObservations are reweighted to reflect total message volumes on a given day. Sampling details available in\nSection 3.\nFigure 8:Share ofwork relatedconsumer ChatGPT messages broken down by high level conversation\ntopic, according to the mapping in Table 3. Values are averaged over a 28 day lagging window. Shares are\ncalculated from a sample of approximately 1.1 million sampled conversations from May 15, 2024 through June\n26, 2025. Observations are reweighted to reflect total message volumes on a given day. Sampling details\navailable in Section 3.\n15\x0csuggests that most userWritingconversations with ChatGPT are requests to modify user inputs\nrather than to create something new. Education is a major use case for ChatGPT. 10.2% of all user\nmessages and 36% ofPractical Guidancemessages are requests forTutoring or Teaching. Another\nlarge share - 8.5% in total and 30% ofPractical Guidance- is general how-to advice on a variety\nof topics.Technical HelpincludesComputer Programming(4.2% of messages),Mathematical Calcu-\nlations(3%), andData Analysis(0.4%). Looking at the topic ofSelf-Expression, only 2.4% of all\nChatGPT messages are aboutRelationships and Personal Reflection(1.9%) orGames and Role Play\n(0.4%).\nWhile users can seek information and advice from traditional web search engines as well as from\nChatGPT, the ability to produce writing, software code, spreadsheets, and other digital products\ndistinguishes generative AI from existing technologies. ChatGPT is also more flexible than web\nsearch even for traditional applications likeSeeking InformationandPractical Guidance, because\nusers receive customized responses (e.g., tailored workout plans, new product ideas, ideas for fantasy\nfootball team names) that represent newly generated content or novel modification of user-provided\ncontent and follow-up requests.\nFigure 9:Breakdown of granular conversation topic shares within the coarse mapping defined in Table 3. The\nunderlying classifier prompt is available in Appendix A. Each bin reports a percentage of the total population.\nShares are calculated from a sample of approximately 1.1 million sampled conversations from May 15, 2024\nthrough June 26, 2025. Observations are reweighted to reflect total message volumes on a given day. Sampling\ndetails available in Section 3.\n5.3 ', '<2-hop>\n\nWhat are the topics of ChatGPT conversations?\nWe modify a classifier used by internal research teams at OpenAI that identifies which capabilities\nthe user is requesting from ChatGPT. The classifier itself directly assigns the user’s query into one\nof 24 categories. We aggregate these 24 categories into seven topical groupings (the full conversation-\ncategorization prompt is given in Appendix A):\nTopic Conversation Category\nWriting Edit or Critique Provided Text\nPersonal Writing or Communication\nTranslation\nArgument or Summary Generation\nWrite Fiction\nPractical Guidance How-To Advice\nTutoring or Teaching\nCreative Ideation\nHealth, Fitness, Beauty, or Self-Care\nTechnical Help Mathematical Calculation\nData Analysis\n13\x0cTopic Conversation Category\nComputer Programming\nMultimedia Create an Image\nAnalyze an Image\nGenerate or Retrieve Other Media\nSeeking Information Specific Info\nPurchasable Products\nCooking and Recipes\nSelf-Expression Greetings and Chitchat\nRelationships and Personal Reflection\nGames and Role Play\nOther/Unknown Asking About the Model\nOther\nUnclear\nTable 3:Coarse Conversation Topics and Underlying Classifier Categories\nFigure 7 shows the composition of user messages over time. The three most common Conversation\nTopics arePractical Guidance,Seeking Information, andWriting, collectively accounting for about\n77% of all ChatGPT conversations.Practical Guidancehas remained constant at roughly 29% of\noverall usage.Writinghas declined from 36% of all usage in July 2024 to 24% a year later.Seeking\nInformationhas grown from 14% to 24% of all usage over the same period. The share ofTechnical\nHelpdeclined from 12% from all usage in July 2024 to around 5% a year later – this may be because\nthe use of LLMs for programming has grown very rapidly through the API (outside of ChatGPT),\nfor AI assistance in code editing and for autonomous programming agents (e.g. Codex).Multimedia\ngrew from 2% to just over 7%, with a large spike in April 2025 after ChatGPT released new image-\ngeneration capabilities: the spike attenuated but the elevated level has persisted.\nFigure 8 shows Conversation Topics, restricting the sample to only work-related messages. About\n40% of all work-related messages in July 2025 areWriting, by far the most common Conversation\nTopic.Practical Guidanceis the second most common use case at 24%.Technical Helphas declined\nfrom 18% of all work-related messages in July 2024 to just over 10% in July 2025.\nFigure 9 disaggregates four of the seven Conversation Topics into smaller groups and sums up\nmessages of each type over a one-year period. For example, the five sub-categories withinWriting\nare (in order of frequency)Editing or Critiquing Provided Text,Personal Writing or Communication,\nTranslation,Argument or Summary Generation, andWriting Fiction. Three of those five categories\n(Editing or Critiquing Provided Text,Translation, andArgument or Summary Generation) are re-\nquests to modify text that has been provided to ChatGPT by the user, whereas the other two are\nrequests to produce novel text. The former constitute two thirds of allWritingconversations, which\n14\x0cFigure 7:Share of consumer ChatGPT messages broken down by high level conversation topic, according\nto the mapping in Table 3. Values are averaged over a 28 day lagging window. ']","Practical Guidance accounts for roughly 29% of overall ChatGPT usage. The specific topics that fall under this category include How-To Advice, Tutoring or Teaching, Creative Ideation, Health, Fitness, Beauty, or Self-Care, Technical Help, Mathematical Calculation, and Data Analysis.",multi_hop_specific_query_synthesizer
"What are the key features and advancements of ChatGPT, particularly focusing on the introduction of GPT-4o and its impact on user interaction quality?","['<1-hop>\n\nWe retain this classifier because theseκ\nstatistics primarily highlight the inherent difficulty of inferring the user’s latent satisfaction from text\nalone.\nWhile this latent “prior” is unobserved in our validation data, it is partially observable when users\nprovide explicit thumbs-up/down feedback. To assess whether the classifier captures a signal aligned\n56\x0cFigure 30:Agreement Between Model and Plurality for Interaction Quality\nFigure 31:Bias Between Model and Plurality for Interaction Quality\n57\x0cwith user experience, we link model predictions to voluntary feedback on assistant messages. We\ndraw a 1-in-10,000 sample of conversations from June 2024 to June 2025 and retain cases where (i)\nthe assistant message received explicit feedback and (ii) the user sent a subsequent message that our\nclassifier can score, yielding roughly 60,000 eligible items. This is a restricted sample that may not\nbe fully representative of all interactions, but it offers a unique lens on the classifier’s ability to proxy\nuser satisfaction.\nFigure 32 shows thatUnknownclassifications are split roughly evenly between thumbs-down and\nthumbs-up feedback. Thumbs-up comprises 86% of all feedback. Conversations with thumbs-down\nfeedback are about equally likely to be classified asGoodorBad, whereas thumbs-up feedback is 9.5\ntimes more likely to be followed by a message classified asGood.\nFigure 32:Correlation of User Rating and Interaction Quality Annotation\n58\x0cC  Appendix: ChatGPT Timeline\ndate event\n2022-11-30 Public launch of ChatGPT as a “research preview” (using GPT-3.5)\n2023-02-01 Launch of ChatGPT Plus subscription\n2023-03-14 Launch of GPT-4 in ChatGPT Plus\n2024-04-01 Launch of logged-out ChatGPT\n2024-05-13 Launch of GPT-4o in ChatGPT Free and Plus\n2024-09-12 Launch of o1-preview and o1-mini in ChatGPT Plus\n2024-12-01 Launch of o1-pro in ChatGPT\n2024-12-05 Launch of ChatGPT Pro subscription\n2025-01-03 Launch of o3-mini in ChatGPT\n2025-03-25 Launch of GPT-4o image generation\n2025-04-16 Launch of o3 and o4-mini\n2025-06-10 Launch of o3-pro\n2025-08-07 Launch of GPT-5 in ChatGPT\n59\x0cD ', '<2-hop>\n\nWhat is ChatGPT?\nHere we give a simplified overview of LLMs and chatbots. For more precise details, refer to the papers\nand system cards that OpenAI has released with each model e.g., (OpenAI, 2023, 2024a, 2025b). A\nchatbot is a statistical model trained to generate a text response given some text input, so as to\nmaximize the “quality” of that response, where the quality is measured with a variety of metrics.\nIn a prototypical interaction, a user submits a plain-text message (“prompt”) and ChatGPT\nreturns the message (“response”) generated from an underlying LLM. A large set of additional features\nhave been added to ChatGPT—including the possibility for the LLM to search the web or external\ndatabases, and generate images based on text—but the exchange of text-based messages remains the\nmost typical interaction.\nSince its launch ChatGPT has used a variety of different underlying LLMs e.g., GPT-3.5, GPT-4,\nGPT-4o, o1, o3, and GPT-5. 12 In addition there are occasional updates to the model’s weights and\nto the model’s system prompt (text instructions sent to the model along with all the queries).\nAn LLM can be thought of as a function from a string of words to a probability distribution over\nthe set of all possible words (more precisely, “tokens,” which very roughly correspond to words13). The\nfunctions are implemented with deep neural nets, typically with a transformer architecture (Vaswani\net al., 2017), parameterized with billions of model “weights”. We will refer to all of ChatGPT’s models\nas language models, though most can additionally process tokens representing images, audio, or other\nmedia.\nThe weights in an LLM-based chatbot are often trained in two stages, commonly called “pre-\ntraining” and “post-training”. In the first stage (“pre-training”), the LLMs are trained to predict the\nnext word in a string, given the preceding words, over an enormous corpus of text. At that point the\nmodels are purely predictors of the likelihood of the next word given a prior context, and as such they\nhave a relatively narrow application. In the second stage (“post-training”), the models are trained to\nproduce words that comprise “good” responses to some prompt. This stage often consists of a variety\nof different strategies: fine-tuning on a dataset of queries and ideal responses, reinforcement learning\nagainst another model that is trained to grade the quality of a response (Ouyang et al., 2022), or\nreinforcement learning against a function that knows the true response to queries (OpenAI (2024b),\n12For a timeline of model launches, see Appendix C.\n13Tokenization is a way of cutting a string of text into discrete chunks, chosen to be statistically efficient. In many\ntokenization schemes, one token corresponds to roughly three-quarters of an English word.\n4\x0cLambert et al. (2024)). This second stage also typically includes a number of “safety” constraints to\navoid certain classes of response, especially those which are deemed harmful or dangerous (OpenAI,\n2025a).\nThis two-stage process has a common statistical interpretation: the first stage teaches the model a\nlatent representation of the world; the second stage fits a function using that representation (Bengio\net al., 2014). Pre-training the model to predict the next word effectively teaches the model a low-\ndimensional representation of text, representing only the key semantic features, and therefore rendering\nthe prompt-response problem tractable with a reasonable set of training examples.\nTwo common ways of evaluating chatbots are with benchmarks (batteries of questions with known\nanswers, e.g. Measuring Massive Multitask Language Understanding (Hendrycks et al., 2021)) and\ncomparisons of human preferences over two alternative responses to the same message (e.g. Chatbot\nArena (Chiang et al., 2024)).\n3 ']","ChatGPT is a statistical model designed to generate text responses based on user input, maximizing the quality of those responses through various metrics. Since its launch, ChatGPT has utilized several underlying large language models (LLMs), including GPT-3.5, GPT-4, and notably GPT-4o, which was launched on May 13, 2024. The introduction of GPT-4o brought enhancements that improved user interaction quality. Feedback mechanisms, such as thumbs-up/down ratings, indicate that conversations classified as 'Good' are significantly more likely to follow thumbs-up feedback, suggesting that the advancements in GPT-4o have positively influenced user satisfaction. The model's ability to process and generate responses has been refined through a two-stage training process, which includes pre-training on a vast corpus of text and post-training to produce high-quality responses, further optimizing engagement strategies.",multi_hop_specific_query_synthesizer
