{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building question-and-answer pipelines for complex knowledge bases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build an indexing pipeline from Haystack's documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x34dbfbe60>\n",
       "ðŸš… Components\n",
       "  - link_fetcher: LinkContentFetcher\n",
       "  - converter: HTMLToDocument\n",
       "  - splitter: DocumentSplitter\n",
       "  - embedder: SentenceTransformersDocumentEmbedder\n",
       "  - writer: DocumentWriter\n",
       "ðŸ›¤ï¸ Connections\n",
       "  - link_fetcher.streams -> converter.sources (List[ByteStream])\n",
       "  - converter.documents -> splitter.documents (List[Document])\n",
       "  - splitter.documents -> embedder.documents (List[Document])\n",
       "  - embedder.documents -> writer.documents (List[Document])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.fetchers import LinkContentFetcher\n",
    "from haystack.components.converters import HTMLToDocument\n",
    "from haystack.components.preprocessors import DocumentSplitter\n",
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.document_stores.types import DuplicatePolicy\n",
    "\n",
    "\n",
    "\n",
    "document_store = InMemoryDocumentStore()\n",
    "\n",
    "link_fetcher = LinkContentFetcher()\n",
    "converter = HTMLToDocument()\n",
    "splitter = DocumentSplitter(split_length=150, split_overlap=5, split_by=\"sentence\")\n",
    "embedder = SentenceTransformersDocumentEmbedder()\n",
    "writer = DocumentWriter(document_store=document_store, \n",
    "                        policy=DuplicatePolicy.OVERWRITE)\n",
    "\n",
    "indexing_pipeline = Pipeline()\n",
    "indexing_pipeline.add_component(\"link_fetcher\", link_fetcher)\n",
    "indexing_pipeline.add_component(\"converter\", converter)\n",
    "indexing_pipeline.add_component(\"splitter\", splitter)\n",
    "indexing_pipeline.add_component(\"embedder\", embedder)\n",
    "indexing_pipeline.add_component(\"writer\", writer)\n",
    "\n",
    "indexing_pipeline.connect(\"link_fetcher\", \"converter\")\n",
    "indexing_pipeline.connect(\"converter\", \"splitter\")\n",
    "indexing_pipeline.connect(\"splitter\", \"embedder\")\n",
    "indexing_pipeline.connect(\"embedder\", \"writer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexing_pipeline.draw(\"indexing_complex_knowledge.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./indexing_complex_knowledge.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Populate the document store with relevant links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.11s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'writer': {'documents_written': 20}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexing_pipeline.run(data={\"link_fetcher\":{\"urls\": [\"https://haystack.deepset.ai/integrations/elasticsearch-document-store\",\n",
    "                                                    \"https://haystack.deepset.ai/tutorials/27_first_rag_pipeline/\",\n",
    "                                                     \"https://haystack.deepset.ai/tutorials/28_structured_output_with_loop/\",\n",
    "                                                     \"https://haystack.deepset.ai/tutorials/36_building_fallbacks_with_conditional_routing/\",\n",
    "                                                     \"https://haystack.deepset.ai/tutorials/40_building_chat_application_with_function_calling/\",\n",
    "                                                     \"https://haystack.deepset.ai/tutorials/42_sentence_window_retriever/\",\n",
    "                                                     \"https://haystack.deepset.ai/tutorials/41_query_classification_with_transformerstextrouter_and_transformerszeroshottextrouter/\",\n",
    "                                                     \"https://haystack.deepset.ai/tutorials/31_metadata_filtering/\",\n",
    "                                                     \"https://haystack.deepset.ai/tutorials/30_file_type_preprocessing_index_pipeline/\",\n",
    "                                                    \"https://haystack.deepset.ai/tutorials/39_embedding_metadata_for_improved_retrieval/\",\n",
    "                                                    \"https://haystack.deepset.ai/tutorials/29_serializing_pipelines/\",\n",
    "                                                    \"https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline/\",\n",
    "                                                    \"https://haystack.deepset.ai/tutorials/33_hybrid_retrieval/\",\n",
    "                                                    \"https://haystack.deepset.ai/tutorials/32_classifying_documents_and_queries_by_language/\",\n",
    "                                                    \"https://haystack.deepset.ai/tutorials/35_evaluating_rag_pipelines/\",\n",
    "                                                    \"https://haystack.deepset.ai/tutorials/37_simplifying_pipeline_inputs_with_multiplexer/\",\n",
    "                                                    \"https://haystack.deepset.ai/integrations/ragas\"]}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"./../.env\")\n",
    "\n",
    "open_ai_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer questions using prompt templating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "from haystack.components.builders.prompt_builder import PromptBuilder\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "\n",
    "######## Complete this section #############\n",
    "prompt_template = \"\"\"\n",
    "You are an expert Python software engineer, you are asked to write Haystack 2.0 pipelines for indexing and querying documents., \n",
    "explain code and you use the context provided to generate accurate and functional code along with clear explanations.\n",
    "After you define a class, you also provide examples of using the class and its methods.\n",
    "You must only use information from the given documents and cite the documents you used by mentioning their URL in the answer.\n",
    "For example, begin your answer with â€˜As stated in URL, ...â€™.\n",
    "If the documents do not contain the answer to the question, say that â€˜Answer is unknown.â€™\n",
    "Context:\n",
    "{% for doc in documents %}\n",
    "    Document: {{ doc.content }} URL: {{ doc.meta['url'] }} \\n\n",
    "{% endfor %};\n",
    "Question: {{query}}\n",
    "\\nAs stated in\n",
    "\"\"\"\n",
    "prompt_builder = PromptBuilder(prompt_template)\n",
    "############################################\n",
    "query_embedder = SentenceTransformersTextEmbedder()\n",
    "retriever = InMemoryEmbeddingRetriever(document_store=document_store, top_k=2)\n",
    "llm = OpenAIGenerator(model='gpt-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x350d3a6c0>\n",
       "ðŸš… Components\n",
       "  - query_embedder: SentenceTransformersTextEmbedder\n",
       "  - retriever: InMemoryEmbeddingRetriever\n",
       "  - prompt_builder: PromptBuilder\n",
       "  - llm: OpenAIGenerator\n",
       "ðŸ›¤ï¸ Connections\n",
       "  - query_embedder.embedding -> retriever.query_embedding (List[float])\n",
       "  - retriever.documents -> prompt_builder.documents (List[Document])\n",
       "  - prompt_builder.prompt -> llm.prompt (str)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline()\n",
    "pipeline.add_component(instance=query_embedder, name=\"query_embedder\")\n",
    "pipeline.add_component(instance=retriever, name=\"retriever\")\n",
    "pipeline.add_component(instance=prompt_builder, name=\"prompt_builder\")\n",
    "pipeline.add_component(instance=llm, name=\"llm\")\n",
    "\n",
    "pipeline.connect(\"query_embedder.embedding\", \"retriever.query_embedding\")\n",
    "pipeline.connect(\"retriever.documents\", \"prompt_builder.documents\")\n",
    "pipeline.connect(\"prompt_builder\", \"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.draw(\"answer_generation_pipeline.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./answer_generation_pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 17.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://haystack.deepset.ai/integrations/elasticsearch-document-store and https://haystack.deepset.ai/tutorials/27_first_rag_pipeline/ , to start the writing of pipeline, we need to first import the necessary modules from Haystack and initialize the ElasticSearch Document store. \n",
      "\n",
      "```python\n",
      "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
      "from haystack.components.builders import PromptBuilder\n",
      "from haystack.components.generators import RAGenerator\n",
      "from haystack.components.evaluators.rag import RAGAsEvaluater\n",
      "from haystack import Pipeline\n",
      "from haystack.document_stores.elasticsearch import ElasticsearchDocumentStore\n",
      "\n",
      "document_store = ElasticsearchDocumentStore(hosts=\"http://localhost:9200\")\n",
      "```\n",
      "\n",
      "Next, initialize the TextEmbedder that will create embeddings for the user query.\n",
      "\n",
      "```python\n",
      "text_embedder = SentenceTransformersTextEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "```\n",
      "\n",
      "Define a template prompt that will guide the generation of answers. \n",
      "\n",
      "```python\n",
      "template = \"\"\"\n",
      "Given the following information, answer the question.\n",
      "Context:\n",
      "{% for document in documents %}\n",
      "{{ document.content }}\n",
      "{% endfor %}\n",
      "Question: {{question}}\n",
      "Answer:\n",
      "\"\"\"\n",
      "prompt_builder = PromptBuilder(template=template)\n",
      "```\n",
      "    \n",
      "Then, initialize the Generators and Evaluators. We are using RAGenerator for generating answers with RAG approach and RAGAsEvaluater for evaluating generated answers.\n",
      "\n",
      "```python\n",
      "generator = RAGenerator(model=\"rag-token-nq\")\n",
      "evaluator = RAGAsEvaluater()\n",
      "```\n",
      "Now, build the pipeline by adding the components and connecting them. \n",
      "\n",
      "```python\n",
      "pipeline = Pipeline()\n",
      "pipeline.add_component(\"text_embedder\", text_embedder)\n",
      "pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
      "pipeline.add_component(\"generator\", generator)\n",
      "pipeline.add_component(\"evaluator\", evaluator)\n",
      "\n",
      "pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
      "pipeline.connect(\"retriever\", \"prompt_builder.documents\")\n",
      "pipeline.connect(\"prompt_builder\", \"generator\")\n",
      "pipeline.connect(\"generator\", \"evaluator\")\n",
      "```\n",
      "\n",
      "Finally, run the pipeline with a question to generate and evaluate an answer.\n",
      "\n",
      "```python\n",
      "question = \"What does Rhodes Statue look like?\"\n",
      "response = pipeline.run({\"text_embedder\": {\"text\": question}, \"prompt_builder\": {\"question\": question}})\n",
      "print(response[\"evaluator\"][\"evaluation\"])\n",
      "```\n",
      "Please note that the code is hypothetical given the documents provided and may need modifications as per actual setup and requirement. The model name used for SentenceTransformersTextEmbedder and RAGenerator may need to change based on the desired performance and characteristics. The connection of the components in the pipeline may vary based on the actual need.\n"
     ]
    }
   ],
   "source": [
    "question = \"Write a Haystack 2.0 pipeline that connects to Elastic Search and answers questions about its knowledge, the pipeline\\\n",
    "    should include a prompt template with instructions and the template should iterate over all documents in the \\\n",
    "        context, the pipeline should also incorporate evaluation of the generated answers through RAGAS.\"\n",
    "result = pipeline.run(data={\"query_embedder\": {\"text\": question}, \"prompt_builder\": {\"query\": question}})\n",
    "print(result['llm']['replies'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (haystack-nlp)",
   "language": "python",
   "name": "haystack-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
