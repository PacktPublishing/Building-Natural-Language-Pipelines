{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with LLMs from various providers using Haystack "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt node initialization - no base instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "load_dotenv(\"./../../.env\")\n",
    "\n",
    "# Open AI API Key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.preview import Pipeline, Document\n",
    "from haystack.preview.document_stores import InMemoryDocumentStore\n",
    "from haystack.preview.components.writers import DocumentWriter\n",
    "from haystack.preview.components.retrievers import InMemoryBM25Retriever, InMemoryEmbeddingRetriever\n",
    "from haystack.preview.components.embedders import SentenceTransformersTextEmbedder, SentenceTransformersDocumentEmbedder\n",
    "from haystack.preview.components.generators.openai.gpt import GPTGenerator\n",
    "from haystack.preview.components.builders.answer_builder import AnswerBuilder\n",
    "from haystack.preview.components.builders.prompt_builder import PromptBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = GPTGenerator(api_key=os.getenv(\"OPENAI_API_KEY\"),  \n",
    "                                                  model_name='gpt-4', \n",
    "                                                  api_base_url = 'https://api.openai.com/v1',\n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt.run(\"Who lives in Paris?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "    Given a question, you provide with a helpful answer using Shakespeare's language.\n",
    "\n",
    "    \\nQuestion: {{question}}\n",
    "    \\nAnswer:\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "init_pipeline = Pipeline()\n",
    "init_pipeline.add_component(instance=PromptBuilder(template=prompt_template), name=\"prompt_builder\")\n",
    "init_pipeline.add_component(instance=GPTGenerator(api_key=os.getenv(\"OPENAI_API_KEY\"),  \n",
    "                                                  model_name='gpt-4', \n",
    "                                                  api_base_url = 'https://api.openai.com/v1',\n",
    "                                                  ), \n",
    "                            name=\"llm\")\n",
    "init_pipeline.add_component(instance=AnswerBuilder(), name=\"answer_builder\")\n",
    "init_pipeline.connect(\"prompt_builder\", \"llm\")\n",
    "init_pipeline.connect(\"llm.replies\", \"answer_builder.replies\")\n",
    "init_pipeline.connect(\"llm.metadata\", \"answer_builder.metadata\")\n",
    "\n",
    "questions = [\"Who lives in Paris?\", \"Who lives in Berlin?\", \"Who lives in Rome?\"]\n",
    "\n",
    "\n",
    "for question in questions:\n",
    "    result = init_pipeline.run(\n",
    "        {\n",
    "            \"prompt_builder\": {\"question\": question},\n",
    "            \"answer_builder\": {\"query\": question},\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f\"Question: {question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\"Who lives in Paris?\", \"Who lives in Berlin?\", \"Who lives in Rome?\"]\n",
    "\n",
    "\n",
    "for question in questions:\n",
    "    result = init_pipeline.run(\n",
    "        {\n",
    "            \"prompt_builder\": {\"question\": question},\n",
    "            \"answer_builder\": {\"query\": question},\n",
    "        }\n",
    "    )\n",
    "\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in result.keys():\n",
    "    print(key)\n",
    "    print(result[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['answer_builder']['answers'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.huggingfaceendpoints import HuggingFaceModelQuery\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hugging_face_token_endpoint = os.getenv(\"mistral_hf_token\")\n",
    "hf_url = \"https://je31lgyj1c1qtn2m.us-east-1.aws.endpoints.huggingface.cloud\"\n",
    "\n",
    "mistral = HuggingFaceModelQuery(api_url=hf_url, api_key=hugging_face_token_endpoint, parameters={\n",
    "    \"temperature\": 0.1,\n",
    "    \"max_length\": 500\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'replies': ['\\n\\nThe answer is:\\n\\n- 2,200,000 people']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistral.run(\"Who lives in Paris?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "PipelineConnectError",
     "evalue": "'llm.replies does not exist. Output connections of llm are: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPipelineConnectError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m/Users/macpro/Documents/GitHub/Building-Natural-Language-Pipelines/ch2/jupyter-notebooks/interacting-with-llms-haystack2.0.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/macpro/Documents/GitHub/Building-Natural-Language-Pipelines/ch2/jupyter-notebooks/interacting-with-llms-haystack2.0.ipynb#X54sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m init_pipeline\u001b[39m.\u001b[39madd_component(instance\u001b[39m=\u001b[39mAnswerBuilder(), name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39manswer_builder\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/macpro/Documents/GitHub/Building-Natural-Language-Pipelines/ch2/jupyter-notebooks/interacting-with-llms-haystack2.0.ipynb#X54sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m init_pipeline\u001b[39m.\u001b[39mconnect(\u001b[39m\"\u001b[39m\u001b[39mprompt_builder\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mllm\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/macpro/Documents/GitHub/Building-Natural-Language-Pipelines/ch2/jupyter-notebooks/interacting-with-llms-haystack2.0.ipynb#X54sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m init_pipeline\u001b[39m.\u001b[39;49mconnect(\u001b[39m\"\u001b[39;49m\u001b[39mllm.replies\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39manswer_builder.replies\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/macpro/Documents/GitHub/Building-Natural-Language-Pipelines/ch2/jupyter-notebooks/interacting-with-llms-haystack2.0.ipynb#X54sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m questions \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mWho lives in Paris?\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mWho lives in Berlin?\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mWho lives in Rome?\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/macpro/Documents/GitHub/Building-Natural-Language-Pipelines/ch2/jupyter-notebooks/interacting-with-llms-haystack2.0.ipynb#X54sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mfor\u001b[39;00m question \u001b[39min\u001b[39;00m questions:\n",
      "File \u001b[0;32m~/anaconda3/envs/llm-pipelines/lib/python3.10/site-packages/canals/pipeline/pipeline.py:270\u001b[0m, in \u001b[0;36mPipeline.connect\u001b[0;34m(self, connect_from, connect_to)\u001b[0m\n\u001b[1;32m    268\u001b[0m     from_socket \u001b[39m=\u001b[39m from_sockets\u001b[39m.\u001b[39mget(from_socket_name, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    269\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m from_socket:\n\u001b[0;32m--> 270\u001b[0m         \u001b[39mraise\u001b[39;00m PipelineConnectError(\n\u001b[1;32m    271\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mfrom_node\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mfrom_socket_name\u001b[39m}\u001b[39;00m\u001b[39m does not exist. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    272\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOutput connections of \u001b[39m\u001b[39m{\u001b[39;00mfrom_node\u001b[39m}\u001b[39;00m\u001b[39m are: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    273\u001b[0m             \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m (type \u001b[39m\u001b[39m{\u001b[39;00m_type_name(socket\u001b[39m.\u001b[39mtype)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m name, socket \u001b[39min\u001b[39;00m from_sockets\u001b[39m.\u001b[39mitems()])\n\u001b[1;32m    274\u001b[0m         )\n\u001b[1;32m    275\u001b[0m \u001b[39mif\u001b[39;00m to_socket_name:\n\u001b[1;32m    276\u001b[0m     to_socket \u001b[39m=\u001b[39m to_sockets\u001b[39m.\u001b[39mget(to_socket_name, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[0;31mPipelineConnectError\u001b[0m: 'llm.replies does not exist. Output connections of llm are: "
     ]
    }
   ],
   "source": [
    "prompt_template = \"\"\"\n",
    "    Given a question, you provide with a helpful answer using Shakespeare's language.\n",
    "\n",
    "    \\nQuestion: {{question}}\n",
    "    \\nAnswer:\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "init_pipeline = Pipeline()\n",
    "init_pipeline.add_component(instance=PromptBuilder(template=prompt_template), name=\"prompt_builder\")\n",
    "init_pipeline.add_component(instance=mistral, name=\"llm\")\n",
    "init_pipeline.add_component(instance=AnswerBuilder(), name=\"answer_builder\")\n",
    "init_pipeline.connect(\"prompt_builder\", \"llm\")\n",
    "init_pipeline.connect(\"llm.replies\", \"answer_builder.replies\")\n",
    "\n",
    "questions = [\"Who lives in Paris?\", \"Who lives in Berlin?\", \"Who lives in Rome?\"]\n",
    "\n",
    "\n",
    "for question in questions:\n",
    "    result = init_pipeline.run(\n",
    "        {\n",
    "            \"prompt_builder\": {\"question\": question},\n",
    "            \"answer_builder\": {\"query\": question},\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f\"Question: {question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-pipelines",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
