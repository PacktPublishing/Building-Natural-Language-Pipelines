{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with LLMs from various providers using Haystack "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt node initialization - no base instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "load_dotenv(\"./../../.env\")\n",
    "\n",
    "# Open AI API Key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.preview import Pipeline, Document\n",
    "from haystack.preview.document_stores import InMemoryDocumentStore\n",
    "from haystack.preview.components.writers import DocumentWriter\n",
    "from haystack.preview.components.retrievers import InMemoryBM25Retriever, InMemoryEmbeddingRetriever\n",
    "from haystack.preview.components.embedders import SentenceTransformersTextEmbedder, SentenceTransformersDocumentEmbedder\n",
    "from haystack.preview.components.generators.openai.gpt import GPTGenerator\n",
    "from haystack.preview.components.builders.answer_builder import AnswerBuilder\n",
    "from haystack.preview.components.builders.prompt_builder import PromptBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "    Given a question, you provide with a helpful answer using Shakespeare's language.\n",
    "\n",
    "    \\nQuestion: {{question}}\n",
    "    \\nAnswer:\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "init_pipeline = Pipeline()\n",
    "init_pipeline.add_component(instance=PromptBuilder(template=prompt_template), name=\"prompt_builder\")\n",
    "init_pipeline.add_component(instance=GPTGenerator(api_key=os.getenv(\"OPENAI_API_KEY\"),  \n",
    "                                                  model_name='gpt-4', \n",
    "                                                  api_base_url = 'https://api.openai.com/v1',\n",
    "                                                  ), \n",
    "                            name=\"llm\")\n",
    "init_pipeline.add_component(instance=AnswerBuilder(), name=\"answer_builder\")\n",
    "init_pipeline.connect(\"prompt_builder\", \"llm\")\n",
    "init_pipeline.connect(\"llm.replies\", \"answer_builder.replies\")\n",
    "init_pipeline.connect(\"llm.metadata\", \"answer_builder.metadata\")\n",
    "\n",
    "questions = [\"Who lives in Paris?\", \"Who lives in Berlin?\", \"Who lives in Rome?\"]\n",
    "\n",
    "\n",
    "for question in questions:\n",
    "    result = init_pipeline.run(\n",
    "        {\n",
    "            \"prompt_builder\": {\"question\": question},\n",
    "            \"answer_builder\": {\"query\": question},\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f\"Question: {question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\"Who lives in Paris?\", \"Who lives in Berlin?\", \"Who lives in Rome?\"]\n",
    "\n",
    "\n",
    "for question in questions:\n",
    "    result = init_pipeline.run(\n",
    "        {\n",
    "            \"prompt_builder\": {\"question\": question},\n",
    "            \"answer_builder\": {\"query\": question},\n",
    "        }\n",
    "    )\n",
    "\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.component_to_connect_to_hf import HuggingFaceModelQuery\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hugging_face_token_endpoint = os.getenv(\"mistral_hf_token\")\n",
    "hf_url = \"https://je31lgyj1c1qtn2m.us-east-1.aws.endpoints.huggingface.cloud\"\n",
    "\n",
    "mistral = HuggingFaceModelQuery(api_url=hf_url, api_key=hugging_face_token_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"temperature\": 0.1,\n",
    "    \"max_length\": 500\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '\\n\\nThe answer is:\\n\\nEveryone.\\n\\nParis is a city of '}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistral.run(\"Who lives in Paris?\", parameters=parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-pipelines",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
