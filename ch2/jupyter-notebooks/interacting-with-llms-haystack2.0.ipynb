{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with LLMs from various providers using Haystack \n",
    "\n",
    "![](./pipeline-diagrams/dag.jpg)\n",
    "\n",
    "Note: if you are running this notebook using Google colab, a requirements.txt file has been prepared with the following dependencies:\n",
    "\n",
    "```bash\n",
    "transformers==4.34.1\n",
    "haystack-ai==0.93.0\n",
    "cohere==4.31\n",
    "tiktoken==0.5.1\n",
    "python-dotenv\n",
    "```\n",
    "\n",
    "Upload the requirements_colab.txt file onto your Google Colab session. You can install them using the following command:\n",
    "\n",
    "```bash\n",
    "!pip install -r requirements_colab.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to OpenAI's GPT models\n",
    "\n",
    "Ensure you create an OpenAI account and set the OPENAI_API_KEY environment variable. You can do this by creating a .env file in the same directory as this notebook and adding the following line:\n",
    "\n",
    "```bash\n",
    "OPENAI_API_KEY=<your-api-key>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.preview.components.generators.openai import GPTGenerator\n",
    "from haystack.preview.components.generators.hugging_face_local import HuggingFaceLocalGenerator\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "\n",
    "# Get the current and parent working directory\n",
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "\n",
    "# Append parent directory to sys.path\n",
    "sys.path.append(parent_directory)\n",
    "from scripts.pipelines import initialize_simple_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "# Applicable only if your env file is stored two levels above the current directory\n",
    "load_dotenv(\"./../../.env\")\n",
    "\n",
    "\n",
    "try:\n",
    "    # Open AI API Key\n",
    "    openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "    if openai_key==None:\n",
    "        raise Exception(\"Please set OPENAI_API_KEY as an environment variable\")\n",
    "except Exception as e:\n",
    "    print(\"Please set OPENAI_API_KEY as an environment variable\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = GPTGenerator(api_key=os.getenv(\"OPENAI_API_KEY\"),  \n",
    "                                                  model_name='gpt-4', \n",
    "                                                  api_base_url = 'https://api.openai.com/v1',\n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'replies': ['The capital of France is Paris.'],\n",
       " 'metadata': [{'model': 'gpt-4-0613',\n",
       "   'index': 0,\n",
       "   'finish_reason': 'stop',\n",
       "   'usage': {'prompt_tokens': 25,\n",
       "    'completion_tokens': 7,\n",
       "    'total_tokens': 32}}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.run(\"You are a helpful assistant who provides answers to questions. What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "        Given a question, you provide a short answer.\n",
    "        \\nQuestion: {{question}}\n",
    "        \\nAnswer:\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initializing the pipeline\n",
    "simple_pipeline = initialize_simple_pipeline(llm_generator=gpt, llm_generator_name=\"gpt_generator\", prompt_template=prompt_template)\n",
    "\n",
    "# Running a question through the pipeline\n",
    "question = \"What is the capital of France?\"\n",
    "result = simple_pipeline.run(\n",
    "    {\n",
    "        \"prompt_builder\": {\"question\": question},\n",
    "    }\n",
    ")\n",
    "\n",
    "# Printing the result\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_pipeline.draw( \"pipeline-diagrams/gpt-pipeline.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to Local HuggingFace LLM\n",
    "\n",
    "Important: the code below will download a local copy of the model on your local computer. Running the code below can be time consuming and will require GPU and RAM resources.\n",
    "\n",
    "You can use Google Colab to run this notebook. It is recommended that you use a Pro account with a T4 GPU runtime enabled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_generator = HuggingFaceLocalGenerator(model_name_or_path=\"bigscience/bloom-1b\",\n",
    "                                          task=\"text-generation\",\n",
    "                                          generation_kwargs={\n",
    "                                            \"max_new_tokens\": 1000,\n",
    "                                            \"temperature\": 0.1,\n",
    "                                            \"do_sample\":  True\n",
    "                                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_generator.warm_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = local_generator.run(\"Who lives in Paris?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_pipeline = initialize_simple_pipeline(llm_generator=local_generator, \n",
    "                                          llm_generator_name=\"hf_generator\", \n",
    "                                          prompt_template=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"You are a helpful assistant who provides answers to questions. What is the capital of France?\"\n",
    "result = hf_pipeline.run(\n",
    "    {\n",
    "        \"prompt_builder\": {\"question\": question},\n",
    "    }\n",
    ")\n",
    "\n",
    "# Printing the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_pipeline.draw( \"pipeline-diagrams/hf-pipeline.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom component: connecting to Mistral using a custom component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.huggingfaceendpoints import InferenceEndpointAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hugging_face_token_endpoint = os.getenv(\"mistral_hf_token\")\n",
    "hf_url = os.getenv(\"mistral_hf_endpoint\")\n",
    "\n",
    "mistral = InferenceEndpointAPI(api_url=hf_url, api_key=hugging_face_token_endpoint, parameters={\n",
    "    \"temperature\": 0.01,\n",
    "    \"max_length\": 100\n",
    "})\n",
    "\n",
    "mistral.run(\"Answer using one word: What is the Capital of France\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mistral_pipeline = initialize_simple_pipeline(llm_generator=mistral, \n",
    "                                          llm_generator_name=\"mistral_generator\", \n",
    "                                          prompt_template=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_pipeline.draw( \"pipeline-diagrams/mistral-pipeline.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the capital of Rome?\"\n",
    "result = mistral_pipeline.run(\n",
    "    {\n",
    "        \"prompt_builder\": {\"question\": question},\n",
    "    }\n",
    ")\n",
    "\n",
    "# Printing the result\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-pipelines",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
