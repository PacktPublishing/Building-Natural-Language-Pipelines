{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with LLMs from various providers using Haystack "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to OpenAI's GPT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.preview import Pipeline\n",
    "from haystack.preview.components.builders.prompt_builder import PromptBuilder\n",
    "from haystack.preview.components.generators.openai.gpt import GPTGenerator\n",
    "from haystack.preview.components.generators.hugging_face.hugging_face_local import HuggingFaceLocalGenerator\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"./../../.env\")\n",
    "\n",
    "# Open AI API Key\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_simple_pipeline(llm_generator, llm_generator_name, prompt_template):\n",
    "    # Creating a pipeline\n",
    "    pipeline = Pipeline()\n",
    "\n",
    "    # Adding a PromptBuilder\n",
    "    prompt_builder = PromptBuilder(template=prompt_template)\n",
    "    pipeline.add_component(instance=prompt_builder, name=\"prompt_builder\")\n",
    "\n",
    "    # Adding a GPT-based Generator\n",
    "    # Ensure that you have the OPENAI_API_KEY environment variable set\n",
    "    gpt_generator = llm_generator # GPTGenerator(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "    pipeline.add_component(instance=gpt_generator, name=llm_generator_name) #\"gpt_generator\")\n",
    "\n",
    "    # Connecting the components\n",
    "    pipeline.connect(\"prompt_builder\",llm_generator_name)\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "    Given a question, you provide with an accurate answer using Shakespeare's style.\n",
    "\n",
    "    \\nQuestion: {{question}}\n",
    "    \\nAnswer:\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = GPTGenerator(api_key=os.getenv(\"OPENAI_API_KEY\"),  \n",
    "                                                  model_name='gpt-4', \n",
    "                                                  api_base_url = 'https://api.openai.com/v1',\n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt.run(\"Who lives in Paris?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initializing the pipeline\n",
    "simple_pipeline = initialize_simple_pipeline(llm_generator=gpt, llm_generator_name=\"gpt_generator\", prompt_template=prompt_template)\n",
    "\n",
    "# Running a question through the pipeline\n",
    "question = \"What is the capital of France?\"\n",
    "result = simple_pipeline.run(\n",
    "    {\n",
    "        \"prompt_builder\": {\"question\": question},\n",
    "    }\n",
    ")\n",
    "\n",
    "# Printing the result\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_pipeline.draw( \"gpt-pipeline.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to Local HuggingFace LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_generator = HuggingFaceLocalGenerator(model_name_or_path=\"bigscience/bloom-3b\",\n",
    "                                          task=\"text-generation\",\n",
    "                                          generation_kwargs={\n",
    "                                            \"max_new_tokens\": 1000,\n",
    "                                            \"temperature\": 0.1,\n",
    "                                            \"do_sample\":  True\n",
    "                                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_generator.warm_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = local_generator.run(\"Who lives in Paris?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom component: connecting to Mistral using a custom component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.huggingfaceendpoints import HuggingFaceModelQuery\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hugging_face_token_endpoint = os.getenv(\"mistral_hf_token\")\n",
    "hf_url = \"https://g683oq4smsqs4s4b.us-east-1.aws.endpoints.huggingface.cloud\"\n",
    "\n",
    "mistral = HuggingFaceModelQuery(api_url=hf_url, api_key=hugging_face_token_endpoint, parameters={\n",
    "    \"temperature\": 0.01,\n",
    "    \"max_length\": 100\n",
    "})\n",
    "\n",
    "mistral.run(\"Answer using one word: What is the Capital of France\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "    Given a question, answer using one word.\n",
    "\n",
    "    \\nQuestion: {{question}}\n",
    "    \\nAnswer:\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "    Given a question, you provide with a helpful answer using Shakespeare's language.\n",
    "\n",
    "    \\nQuestion: {{question}}\n",
    "    \\nAnswer:\n",
    "    \"\"\"\n",
    "def initialize_simple_pipeline(hf_url, hugging_face_token_endpoint):\n",
    "    # Creating a pipeline\n",
    "    pipeline = Pipeline()\n",
    "\n",
    "    # Adding a PromptBuilder\n",
    "    prompt_builder = PromptBuilder(template=prompt_template)\n",
    "    pipeline.add_component(instance=prompt_builder, name=\"prompt_builder\")\n",
    "\n",
    "    # Adding a GPT-based Generator\n",
    "    # Ensure that you have the OPENAI_API_KEY environment variable set\n",
    "    mistral = HuggingFaceModelQuery(api_url=hf_url, api_key=hugging_face_token_endpoint, parameters={\n",
    "            \"temperature\": 0.1,\n",
    "            \"max_length\": 500\n",
    "        })\n",
    "    pipeline.add_component(instance=mistral, name=\"mistral\")\n",
    "\n",
    "    # Connecting the components\n",
    "    pipeline.connect(\"prompt_builder\", \"mistral\")\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "# Initializing the pipeline\n",
    "simple_pipeline = initialize_simple_pipeline(hf_url=hf_url, hugging_face_token_endpoint=hugging_face_token_endpoint)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_pipeline.draw( \"mistral-pipeline.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the capital of France?\"\n",
    "result = simple_pipeline.run(\n",
    "    {\n",
    "        \"prompt_builder\": {\"question\": question},\n",
    "    }\n",
    ")\n",
    "\n",
    "# Printing the result\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-pipelines",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
