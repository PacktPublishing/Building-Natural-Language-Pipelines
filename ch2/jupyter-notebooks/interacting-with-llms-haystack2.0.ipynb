{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with LLMs from various providers using Haystack \n",
    "\n",
    "Note: if you are running this notebook using Google colab, a requirements.txt file has been prepared with the following dependencies:\n",
    "\n",
    "```bash\n",
    "transformers==4.34.1\n",
    "haystack-ai==0.93.0\n",
    "cohere==4.31\n",
    "tiktoken==0.5.1\n",
    "python-dotenv\n",
    "```\n",
    "\n",
    "Upload the requirements_colab.txt file onto your Google Colab session. You can install them using the following command:\n",
    "\n",
    "```bash\n",
    "!pip install -r requirements_colab.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to OpenAI's GPT models\n",
    "\n",
    "Ensure you create an OpenAI account and set the OPENAI_API_KEY environment variable. You can do this by creating a .env file in the same directory as this notebook and adding the following line:\n",
    "\n",
    "```bash\n",
    "OPENAI_API_KEY=<your-api-key>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.preview import Pipeline\n",
    "from haystack.preview.components.builders.prompt_builder import PromptBuilder\n",
    "from haystack.preview.components.generators.openai.gpt import GPTGenerator\n",
    "from haystack.preview.components.generators.hugging_face.hugging_face_local import HuggingFaceLocalGenerator\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "# Applicable only if your env file is stored two levels above the current directory\n",
    "load_dotenv(\"./../../.env\")\n",
    "\n",
    "\n",
    "try:\n",
    "    # Open AI API Key\n",
    "    openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "    if openai_key==None:\n",
    "        raise Exception(\"Please set OPENAI_API_KEY as an environment variable\")\n",
    "except Exception as e:\n",
    "    print(\"Please set OPENAI_API_KEY as an environment variable\")\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_simple_pipeline(llm_generator, llm_generator_name, prompt_template):\n",
    "    # Creating a pipeline\n",
    "    pipeline = Pipeline()\n",
    "\n",
    "    # Adding a PromptBuilder\n",
    "    prompt_builder = PromptBuilder(template=prompt_template)\n",
    "    pipeline.add_component(instance=prompt_builder, name=\"prompt_builder\")\n",
    "\n",
    "    # Adding a GPT-based Generator\n",
    "    # Ensure that you have the OPENAI_API_KEY environment variable set\n",
    "    gpt_generator = llm_generator # GPTGenerator(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "    pipeline.add_component(instance=gpt_generator, name=llm_generator_name) #\"gpt_generator\")\n",
    "\n",
    "    # Connecting the components\n",
    "    pipeline.connect(\"prompt_builder\",llm_generator_name)\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "    Given a question, you provide with an accurate answer using Shakespeare's style.\n",
    "\n",
    "    \\nQuestion: {{question}}\n",
    "    \\nAnswer:\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = GPTGenerator(api_key=os.getenv(\"OPENAI_API_KEY\"),  \n",
    "                                                  model_name='gpt-4', \n",
    "                                                  api_base_url = 'https://api.openai.com/v1',\n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt.run(\"Who lives in Paris?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initializing the pipeline\n",
    "simple_pipeline = initialize_simple_pipeline(llm_generator=gpt, llm_generator_name=\"gpt_generator\", prompt_template=prompt_template)\n",
    "\n",
    "# Running a question through the pipeline\n",
    "question = \"What is the capital of France?\"\n",
    "result = simple_pipeline.run(\n",
    "    {\n",
    "        \"prompt_builder\": {\"question\": question},\n",
    "    }\n",
    ")\n",
    "\n",
    "# Printing the result\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_pipeline.draw( \"pipeline-diagrams/gpt-pipeline.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to Local HuggingFace LLM\n",
    "\n",
    "Important: the code below will download a local copy of the model on your local computer. Running the code below can be time consuming and will require GPU and RAM resources.\n",
    "\n",
    "You can use Google Colab to run this notebook. It is recommended that you use a Pro account with a T4 GPU runtime enabled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_generator = HuggingFaceLocalGenerator(model_name_or_path=\"bigscience/bloom-1b\",\n",
    "                                          task=\"text-generation\",\n",
    "                                          generation_kwargs={\n",
    "                                            \"max_new_tokens\": 1000,\n",
    "                                            \"temperature\": 0.1,\n",
    "                                            \"do_sample\":  True\n",
    "                                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_generator.warm_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = local_generator.run(\"Who lives in Paris?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_pipeline = initialize_simple_pipeline(llm_generator=local_generator, \n",
    "                                          llm_generator_name=\"hf_generator\", \n",
    "                                          prompt_template=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who is the smartest man in the universe\"\n",
    "result = hf_pipeline.run(\n",
    "    {\n",
    "        \"prompt_builder\": {\"question\": question},\n",
    "    }\n",
    ")\n",
    "\n",
    "# Printing the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_pipeline.draw( \"pipeline-diagrams/hf-pipeline.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom component: connecting to Mistral using a custom component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.huggingfaceendpoints import InferenceEndpointAPI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'replies': ['?\\nAnswer: Paris']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hugging_face_token_endpoint = os.getenv(\"mistral_hf_token\")\n",
    "hf_url = os.getenv(\"mistral_hf_endpoint\")\n",
    "\n",
    "mistral = InferenceEndpointAPI(api_url=hf_url, api_key=hugging_face_token_endpoint, parameters={\n",
    "    \"temperature\": 0.01,\n",
    "    \"max_length\": 100\n",
    "})\n",
    "\n",
    "mistral.run(\"Answer using one word: What is the Capital of France\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mistral_pipeline = initialize_simple_pipeline(llm_generator=mistral, \n",
    "                                          llm_generator_name=\"mistral_generator\", \n",
    "                                          prompt_template=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt_builder': \"question: InputSocket(name='question', type=typing.Any, is_optional=False, sender=None)\", 'mistral_generator': \"prompt: InputSocket(name='prompt', type=<class 'str'>, is_optional=False, sender='prompt_builder')\"}\n"
     ]
    }
   ],
   "source": [
    "mistral_pipeline.draw( \"pipeline-diagrams/mistral-pipeline.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the capital of France?\"\n",
    "result = mistral_pipeline.run(\n",
    "    {\n",
    "        \"prompt_builder\": {\"question\": question},\n",
    "    }\n",
    ")\n",
    "\n",
    "# Printing the result\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-pipelines",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
