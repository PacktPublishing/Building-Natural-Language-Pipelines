{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4557d56a",
   "metadata": {},
   "source": [
    "# Interactive Prompting with Ollama\n",
    "\n",
    "This notebook allows you to directly prompt the Mistral-Nemo model running locally with Ollama.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **Ollama installed**: Download from [ollama.com](https://ollama.com)\n",
    "- **Mistral-Nemo model**: Run `ollama pull mistral-nemo:12b` in your terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0bd6c4",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e4b7b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurafunderburk/Documents/GitHub/Building-Natural-Language-Pipelines/ch2/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0433ce",
   "metadata": {},
   "source": [
    "## Step 2: Initialize the Ollama Model\n",
    "\n",
    "We'll create a ChatOllama instance with the Mistral-Nemo model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81efa0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Ollama model initialized!\n",
      "  Model: mistral-nemo:12b\n",
      "  Temperature: 0 (deterministic responses)\n"
     ]
    }
   ],
   "source": [
    "# Create a local model with Ollama\n",
    "model = ChatOllama(\n",
    "    model=\"mistral-nemo:12b\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(\"âœ“ Ollama model initialized!\")\n",
    "print(f\"  Model: mistral-nemo:12b\")\n",
    "print(f\"  Temperature: 0 (deterministic responses)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb6c932",
   "metadata": {},
   "source": [
    "## Step 3: Simple Prompt\n",
    "\n",
    "Let's send a simple prompt to the model and get a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b67891c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: What is artificial intelligence? Explain in 2-3 sentences.\n",
      "\n",
      "Response:\n",
      "Artificial Intelligence (AI) refers to the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and language understanding. AI aims to create intelligent agents that can perform tasks requiring human-like intelligence.\n"
     ]
    }
   ],
   "source": [
    "# Your prompt here\n",
    "prompt = \"What is artificial intelligence? Explain in 2-3 sentences.\"\n",
    "\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "print(\"Response:\")\n",
    "\n",
    "# Invoke the model\n",
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f19be5b",
   "metadata": {},
   "source": [
    "## Step 4: Interactive Prompting\n",
    "\n",
    "Change the prompt below to ask your own questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "138290f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Write a haiku about programming.\n",
      "\n",
      "Response:\n",
      "Silicon whispers,\n",
      "Binary secrets unfold,\n",
      "Code breathes life.\n"
     ]
    }
   ],
   "source": [
    "# Customize your prompt here\n",
    "my_prompt = \"Write a haiku about programming.\"\n",
    "\n",
    "response = model.invoke(my_prompt)\n",
    "print(f\"Prompt: {my_prompt}\\n\")\n",
    "print(f\"Response:\\n{response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca93084e",
   "metadata": {},
   "source": [
    "## Step 5: Conversational Prompting\n",
    "\n",
    "You can have a conversation by providing multiple messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a1b306d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation:\n",
      "System: You are a helpful AI assistant.\n",
      "Human: What are the three primary colors?\n",
      "AI: The three primary colors are red, blue, and yellow.\n",
      "Human: What colors do you get when you mix them?\n",
      "\n",
      "AI: When you mix the three primary colors (red, blue, and yellow) together, you get:\n",
      "\n",
      "* Red + Blue = Purple/Violet\n",
      "* Red + Yellow = Orange\n",
      "* Blue + Yellow = Green\n",
      "\n",
      "These are known as secondary colors. If you mix all three primaries together in equal amounts, you get a shade of brown or gray, depending on the specific hues used.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "# Create a conversation\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful AI assistant.\"),\n",
    "    HumanMessage(content=\"What are the three primary colors?\"),\n",
    "    AIMessage(content=\"The three primary colors are red, blue, and yellow.\"),\n",
    "    HumanMessage(content=\"What colors do you get when you mix them?\")\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "print(\"Conversation:\")\n",
    "for msg in messages:\n",
    "    role = msg.__class__.__name__.replace(\"Message\", \"\")\n",
    "    print(f\"{role}: {msg.content}\")\n",
    "\n",
    "print(f\"\\nAI: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6943538",
   "metadata": {},
   "source": [
    "## Step 6: Experiment with Temperature\n",
    "\n",
    "Temperature controls randomness:\n",
    "- **0**: Deterministic (same answer every time)\n",
    "- **0.5**: Balanced creativity\n",
    "- **1.0**: More creative/random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4c5cc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With temperature=0 (deterministic):\n",
      "Absolutely, here's one for you: Did you know that a day on Venus is longer than a year on Venus? This is because Venus has an extremely slow rotation on its axis. It takes about 243 Earth days for Venus to rotate once on its axis, but it only takes around 225 Earth days for Venus to orbit the Sun. Isn't that fascinating?\n",
      "\n",
      "==================================================\n",
      "\n",
      "With temperature=0.9 (creative):\n",
      "Sure, here's one for you: Did you know that a day on Venus is longer than a year on Venus? This is because Venus has an extremely slow rotation on its axis. It takes about 243 Earth days for Venus to rotate once on its axis, but it only takes around 225 Earth days for Venus to orbit the Sun. Isn't that fascinating?\n"
     ]
    }
   ],
   "source": [
    "# Create models with different temperatures\n",
    "model_creative = ChatOllama(\n",
    "    model=\"mistral-nemo:12b\",\n",
    "    temperature=0.9\n",
    ")\n",
    "\n",
    "prompt = \"Tell me a fun fact about space.\"\n",
    "\n",
    "print(\"With temperature=0 (deterministic):\")\n",
    "response1 = model.invoke(prompt)\n",
    "print(response1.content)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"With temperature=0.9 (creative):\")\n",
    "response2 = model_creative.invoke(prompt)\n",
    "print(response2.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0441fabe",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Try Your Own Prompts!\n",
    "\n",
    "Use the cell below to experiment with your own prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb0789bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today? Let's chat about anything you'd like. (Just keep in mind that I'm here to promote respectful and positive conversations.)\n"
     ]
    }
   ],
   "source": [
    "# Your custom prompt\n",
    "custom_prompt = \"Your prompt here\"\n",
    "\n",
    "response = model.invoke(custom_prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0de7f0",
   "metadata": {},
   "source": [
    "## Tips for Better Prompts\n",
    "\n",
    "1. **Be specific**: \"Write a 100-word summary\" is better than \"summarize this\"\n",
    "2. **Give context**: Include relevant background information\n",
    "3. **Use examples**: Show the model what you want\n",
    "4. **Set the role**: \"You are an expert Python programmer\"\n",
    "5. **Iterate**: Refine your prompts based on responses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-with-haystack-ch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
