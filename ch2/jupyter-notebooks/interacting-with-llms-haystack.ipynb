{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with LLMs from various providers using Haystack "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt node initialization - no base instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "load_dotenv(\"./../../.env\")\n",
    "\n",
    "# Open AI API Key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes import PromptTemplate, AnswerParser, PromptNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt node\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "pn = PromptNode(\"gpt-3.5-turbo\", \n",
    "                api_key=openai_api_key, \n",
    "                model_kwargs={\"stream\":False}, \n",
    "                max_length=1000)\n",
    "\n",
    "# Create a prompt\n",
    "prompt = \"What are the three most interesting things about Berlin? Be brief in your answer.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a prompt template\n",
    "tourist_recommendation_prompt = PromptTemplate(\n",
    "    prompt=\"\"\"You are a helpful assistant to a tourist looking find attractions in a city. \\ \n",
    "        If the request is vague, ask the tourist to provide the name of the city and what kind \\\n",
    "        of attractions they are looking for. \\ \n",
    "        You answer:\"\"\",\n",
    "    output_parser=AnswerParser(),\n",
    ")\n",
    "pn = PromptNode(\"gpt-3.5-turbo\", \n",
    "                api_key=openai_api_key, \n",
    "                model_kwargs={\"stream\":False},\n",
    "                default_prompt_template=tourist_recommendation_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.run(\"I am looking for a place with tropical wether in America\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompting LLMs from HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes import PromptNode\n",
    "\n",
    "# Initalize the node passing the model:\n",
    "prompt_node_gt5 = PromptNode(model_name_or_path=\"google/flan-t5-xl\",\n",
    "                             default_prompt_template=tourist_recommendation_prompt)\n",
    "\n",
    "# Go ahead and ask a question:\n",
    "prompt_node_gt5(\"What is the best city in Europe to live in?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting an LLM to an in-memory document store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.document_stores import InMemoryDocumentStore\n",
    "from datasets import load_dataset\n",
    "from haystack.nodes import BM25Retriever, PromptTemplate, AnswerParser, PromptNode\n",
    "import os\n",
    "from haystack.pipelines import Pipeline\n",
    "import openai\n",
    "# Initialize Haystack's QA system\n",
    "# Create a prompt node\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "document_store = InMemoryDocumentStore(use_bm25=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"cnn_dailymail\", '3.0.0',  split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating BM25 representation...: 100%|██████████| 482/482 [00:00<00:00, 2897.87 docs/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Extract and Transform Data\n",
    "transformed_documents = []\n",
    "\n",
    "for i in range(500):\n",
    "    content = dataset[i]['article']  \n",
    "    transformed_documents.append({\"content\": content})\n",
    "\n",
    "# Step 2: Write Transformed Data to Document Store\n",
    "document_store.write_documents(transformed_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rag_prompt = PromptTemplate(\n",
    "    prompt=\"\"\"Synthesize a brief answer from the following text for the given question.\n",
    "                             Provide a clear and concise response that summarizes the key points and information presented in the text.\n",
    "                             Your answer should be in your own words and be no longer than 50 words.\n",
    "                             \\n\\n Related text: {join(documents)} \\n\\n Question: {query} \\n\\n Answer:\"\"\",\n",
    "    output_parser=AnswerParser(),\n",
    ")\n",
    "\n",
    "# Set up nodes\n",
    "retriever = BM25Retriever(document_store=document_store, top_k=2)\n",
    "pn = PromptNode(\"gpt-3.5-turbo\", \n",
    "                api_key=openai_api_key, \n",
    "                model_kwargs={\"stream\":False},\n",
    "                default_prompt_template=rag_prompt)\n",
    "\n",
    "# Set up pipeline\n",
    "pipe = Pipeline()\n",
    "pipe.add_node(component=retriever, name=\"retriever\", inputs=[\"Query\"])\n",
    "pipe.add_node(component=pn, name=\"prompt_node\", inputs=[\"retriever\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = pipe.run(\"What Pakistan's former prime minister was disqualified from participating in parliamentary elections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['answers', 'invocation_context', 'documents', 'root_node', 'params', 'query', 'node_id'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-pipelines",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
