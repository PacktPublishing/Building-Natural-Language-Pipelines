{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with LLMs from various providers using Haystack "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt node initialization - no base instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"./../../.env\")\n",
    "\n",
    "# Open AI API Key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes import PromptTemplate, AnswerParser, PromptNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt node\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "pn = PromptNode(\"gpt-3.5-turbo\", \n",
    "                api_key=openai_api_key, \n",
    "                model_kwargs={\"stream\":False}, \n",
    "                max_length=1000)\n",
    "\n",
    "# Create a prompt\n",
    "prompt = \"What are the three most interesting things about Berlin? Be brief in your answer.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a prompt template\n",
    "tourist_recommendation_prompt = PromptTemplate(\n",
    "    prompt=\"\"\"You are a helpful assistant to a tourist looking find attractions in a city. \\ \n",
    "        If the request is vague, ask the tourist to provide the name of the city and what kind \\\n",
    "        of attractions they are looking for. \\ \n",
    "        You answer:\"\"\",\n",
    "    output_parser=AnswerParser(),\n",
    ")\n",
    "pn = PromptNode(\"gpt-3.5-turbo\", \n",
    "                api_key=openai_api_key, \n",
    "                model_kwargs={\"stream\":False},\n",
    "                default_prompt_template=tourist_recommendation_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.run(\"I am looking for a place with tropical wether in America\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompting LLMs from HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes import PromptNode\n",
    "\n",
    "# Initalize the node passing the model:\n",
    "prompt_node_gt5 = PromptNode(model_name_or_path=\"google/flan-t5-xl\",\n",
    "                             default_prompt_template=tourist_recommendation_prompt)\n",
    "\n",
    "# Go ahead and ask a question:\n",
    "prompt_node_gt5(\"What is the best city in Europe to live in?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting an LLM to an in-memory document store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba491d28a23049faa957453a55c65ecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33de2a2f4b11455eb163890c2d48f096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/159M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "888f732f7a9f4f5a81dce0ef30297a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/376M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from haystack.document_stores import InMemoryDocumentStore\n",
    "from datasets import load_dataset\n",
    "from haystack.nodes import BM25Retriever, PromptTemplate, AnswerParser, PromptNode\n",
    "import os\n",
    "from haystack.pipelines import Pipeline\n",
    "document_store = InMemoryDocumentStore()\n",
    "\n",
    "dataset = load_dataset(\"cnn_dailymail\", '3.0.0')\n",
    "document_store.write_documents(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Haystack's QA system\n",
    "document_store = InMemoryDocumentStore(use_bm25=True)\n",
    "dataset = load_dataset(\"bilgeyucel/seven-wonders\", split=\"train\")\n",
    "document_store.write_documents(dataset)\n",
    "\n",
    "\n",
    "rag_prompt = PromptTemplate(\n",
    "    prompt=\"\"\"Synthesize a brief answer from the following text for the given question.\n",
    "                             Provide a clear and concise response that summarizes the key points and information presented in the text.\n",
    "                             Your answer should be in your own words and be no longer than 50 words.\n",
    "                             \\n\\n Related text: {join(documents)} \\n\\n Question: {query} \\n\\n Answer:\"\"\",\n",
    "    output_parser=AnswerParser(),\n",
    ")\n",
    "\n",
    "# Set up nodes\n",
    "retriever = BM25Retriever(document_store=document_store, top_k=2)\n",
    "pn = PromptNode(\"gpt-3.5-turbo\", \n",
    "                api_key=MY_API_KEY, \n",
    "                model_kwargs={\"stream\":False},\n",
    "                default_prompt_template=rag_prompt)\n",
    "\n",
    "# Set up pipeline\n",
    "pipe = Pipeline()\n",
    "pipe.add_node(component=retriever, name=\"retriever\", inputs=[\"Query\"])\n",
    "pipe.add_node(component=pn, name=\"prompt_node\", inputs=[\"retriever\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-pipelines",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
