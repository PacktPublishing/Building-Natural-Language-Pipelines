{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec21db5c",
   "metadata": {},
   "source": [
    "ðŸ”§ **Setup Required**: Before running this notebook, please follow the [setup instructions](../README.md#setup-instructions) to configure your environment and API keys."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbebb135",
   "metadata": {},
   "source": [
    "# Document Q&A with LangChain 1.0\n",
    "\n",
    "## Welcome! ðŸ“š\n",
    "\n",
    "In this notebook, you'll learn how to extract information from documents using LangChain 1.0.\n",
    "\n",
    "## What is Document Q&A?\n",
    "\n",
    "**Document Q&A** allows you to:\n",
    "- Load documents from various sources (PDFs, text files, web pages)\n",
    "- Split them into manageable chunks\n",
    "- Create embeddings for semantic search\n",
    "- Ask questions and get answers based on document content\n",
    "\n",
    "## What You'll Build\n",
    "\n",
    "By the end of this notebook, you'll have a system that can:\n",
    "1. Load and process documents\n",
    "2. Create a vector store for efficient retrieval\n",
    "3. Answer questions based on document content\n",
    "4. Use Ollama for local LLM processing\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Make sure you have:\n",
    "- **Ollama installed**: Download from [ollama.com](https://ollama.com)\n",
    "- **Mistral-Nemo model**: Run `ollama pull mistral-nemo:12b` in your terminal\n",
    "- **nomic-embed-text model**: Run `ollama pull nomic-embed-text` for embeddings\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc41a54c",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries\n",
    "\n",
    "We'll need document loaders, text splitters, embeddings, vector stores, and our LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b16f2682",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurafunderburk/Documents/GitHub/Building-Natural-Language-Pipelines/ch2/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4e74c5",
   "metadata": {},
   "source": [
    "## Step 2: Initialize the Ollama Model\n",
    "\n",
    "We'll use Mistral-Nemo for generating answers and nomic-embed-text for creating embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfa296ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ LLM and embeddings initialized!\n",
      "  LLM Model: mistral-nemo:12b\n",
      "  Embeddings Model: nomic-embed-text\n"
     ]
    }
   ],
   "source": [
    "# Initialize the LLM\n",
    "llm = ChatOllama(\n",
    "    model=\"mistral-nemo:12b\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text\"\n",
    ")\n",
    "\n",
    "print(\"âœ“ LLM and embeddings initialized!\")\n",
    "print(f\"  LLM Model: mistral-nemo:12b\")\n",
    "print(f\"  Embeddings Model: nomic-embed-text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92fe31e",
   "metadata": {},
   "source": [
    "## Step 3: Create Sample Documents\n",
    "\n",
    "Let's create some sample text documents to work with. In practice, you'd load these from files or web pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b953b0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Created 4 sample documents\n",
      "\n",
      "Topics covered:\n",
      "  - MACHINE_LEARNING\n",
      "  - NLP\n",
      "  - LLM\n",
      "  - RAG\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "# Create sample documents about AI topics\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        Machine Learning is a subset of artificial intelligence that focuses on developing \n",
    "        systems that can learn from and make decisions based on data. It uses statistical \n",
    "        techniques to give computers the ability to learn without being explicitly programmed.\n",
    "        Common types include supervised learning, unsupervised learning, and reinforcement learning.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"ai_basics.txt\", \"topic\": \"machine_learning\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        Natural Language Processing (NLP) is a branch of AI that helps computers understand, \n",
    "        interpret, and generate human language. NLP combines computational linguistics with \n",
    "        machine learning and deep learning models. Applications include chatbots, translation \n",
    "        services, sentiment analysis, and text summarization.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"ai_basics.txt\", \"topic\": \"nlp\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        Large Language Models (LLMs) are neural networks trained on vast amounts of text data. \n",
    "        They can generate human-like text, answer questions, write code, and perform various \n",
    "        language tasks. Examples include GPT-4, Claude, and Mistral. LLMs use transformer \n",
    "        architecture and attention mechanisms to understand context.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"ai_advanced.txt\", \"topic\": \"llm\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval \n",
    "        with language generation. It allows LLMs to access external knowledge bases, reducing \n",
    "        hallucinations and providing more accurate, up-to-date information. RAG systems typically \n",
    "        use vector databases to store and retrieve relevant document chunks.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"ai_advanced.txt\", \"topic\": \"rag\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"âœ“ Created {len(documents)} sample documents\")\n",
    "print(\"\\nTopics covered:\")\n",
    "for doc in documents:\n",
    "    print(f\"  - {doc.metadata['topic'].upper()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b87f9d3",
   "metadata": {},
   "source": [
    "## Step 4: Split Documents into Chunks\n",
    "\n",
    "For better retrieval, we split long documents into smaller, overlapping chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cc51d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Split 4 documents into 4 chunks\n",
      "  Chunk size: 500 characters\n",
      "  Chunk overlap: 50 characters\n",
      "\n",
      "Example chunk:\n",
      "Machine Learning is a subset of artificial intelligence that focuses on developing \n",
      "        systems that can learn from and make decisions based on data. It uses statistical \n",
      "        techniques to giv...\n"
     ]
    }
   ],
   "source": [
    "# Create a text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,  # Size of each chunk in characters\n",
    "    chunk_overlap=50,  # Overlap between chunks to maintain context\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Split the documents\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"âœ“ Split {len(documents)} documents into {len(splits)} chunks\")\n",
    "print(f\"  Chunk size: 500 characters\")\n",
    "print(f\"  Chunk overlap: 50 characters\")\n",
    "print(f\"\\nExample chunk:\")\n",
    "print(f\"{splits[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7909059",
   "metadata": {},
   "source": [
    "## Step 5: Create a Vector Store\n",
    "\n",
    "We'll use FAISS (Facebook AI Similarity Search) to create a vector database for efficient retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c28647ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vector store... (this may take a moment)\n",
      "âœ“ Vector store created!\n",
      "  Total vectors: 4\n",
      "  Ready for semantic search\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating vector store... (this may take a moment)\")\n",
    "\n",
    "# Create vector store from documents\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "print(\"âœ“ Vector store created!\")\n",
    "print(f\"  Total vectors: {len(splits)}\")\n",
    "print(f\"  Ready for semantic search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4725ad",
   "metadata": {},
   "source": [
    "## Step 6: Test Similarity Search\n",
    "\n",
    "Let's test the vector store by finding documents similar to a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41a06f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is machine learning?\n",
      "\n",
      "Found 2 relevant documents:\n",
      "\n",
      "Document 1:\n",
      "Topic: machine_learning\n",
      "Content: Machine Learning is a subset of artificial intelligence that focuses on developing \n",
      "        systems that can learn from and make decisions based on data. It uses statistical \n",
      "        techniques to giv...\n",
      "\n",
      "Document 2:\n",
      "Topic: nlp\n",
      "Content: Natural Language Processing (NLP) is a branch of AI that helps computers understand, \n",
      "        interpret, and generate human language. NLP combines computational linguistics with \n",
      "        machine learn...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test query\n",
    "query = \"What is machine learning?\"\n",
    "\n",
    "# Find similar documents\n",
    "relevant_docs = vectorstore.similarity_search(query, k=2)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(f\"Found {len(relevant_docs)} relevant documents:\\n\")\n",
    "\n",
    "for i, doc in enumerate(relevant_docs, 1):\n",
    "    print(f\"Document {i}:\")\n",
    "    print(f\"Topic: {doc.metadata['topic']}\")\n",
    "    print(f\"Content: {doc.page_content.strip()[:200]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7d572d",
   "metadata": {},
   "source": [
    "## Step 7: Create a Retrieval QA Chain\n",
    "\n",
    "Now we'll combine the retriever with our LLM to answer questions based on the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcb79561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='521aa57a-f6c9-46b7-8387-15d9afa45643', metadata={'source': 'ai_advanced.txt', 'topic': 'rag'}, page_content='Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval \\n        with language generation. It allows LLMs to access external knowledge bases, reducing \\n        hallucinations and providing more accurate, up-to-date information. RAG systems typically \\n        use vector databases to store and retrieve relevant document chunks.'),\n",
       " Document(id='f0befea8-673b-43cb-a7bb-c7b4e28860bf', metadata={'source': 'ai_basics.txt', 'topic': 'machine_learning'}, page_content='Machine Learning is a subset of artificial intelligence that focuses on developing \\n        systems that can learn from and make decisions based on data. It uses statistical \\n        techniques to give computers the ability to learn without being explicitly programmed.\\n        Common types include supervised learning, unsupervised learning, and reinforcement learning.'),\n",
       " Document(id='1c8640bd-242c-4060-aafd-782717fd0d87', metadata={'source': 'ai_basics.txt', 'topic': 'nlp'}, page_content='Natural Language Processing (NLP) is a branch of AI that helps computers understand, \\n        interpret, and generate human language. NLP combines computational linguistics with \\n        machine learning and deep learning models. Applications include chatbots, translation \\n        services, sentiment analysis, and text summarization.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a retriever from the vector store\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}  # Retrieve top 3 most relevant chunks\n",
    ")\n",
    "\n",
    "\n",
    "retriever.invoke(input=\"types of reward hacking\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad037e38",
   "metadata": {},
   "source": [
    "## Step 8: Ask Questions About Your Documents\n",
    "\n",
    "Let's ask some questions and get answers based on our documents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6844e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is machine learning and what are its types?\n",
      "\n",
      "Retrieving relevant information...\n",
      "\n",
      "Answer:\n",
      "[Document(id='f0befea8-673b-43cb-a7bb-c7b4e28860bf', metadata={'source': 'ai_basics.txt', 'topic': 'machine_learning'}, page_content='Machine Learning is a subset of artificial intelligence that focuses on developing \\n        systems that can learn from and make decisions based on data. It uses statistical \\n        techniques to give computers the ability to learn without being explicitly programmed.\\n        Common types include supervised learning, unsupervised learning, and reinforcement learning.'), Document(id='cd675eb4-74e0-4b50-ad13-6ff306621e16', metadata={'source': 'ai_advanced.txt', 'topic': 'llm'}, page_content='Large Language Models (LLMs) are neural networks trained on vast amounts of text data. \\n        They can generate human-like text, answer questions, write code, and perform various \\n        language tasks. Examples include GPT-4, Claude, and Mistral. LLMs use transformer \\n        architecture and attention mechanisms to understand context.'), Document(id='1c8640bd-242c-4060-aafd-782717fd0d87', metadata={'source': 'ai_basics.txt', 'topic': 'nlp'}, page_content='Natural Language Processing (NLP) is a branch of AI that helps computers understand, \\n        interpret, and generate human language. NLP combines computational linguistics with \\n        machine learning and deep learning models. Applications include chatbots, translation \\n        services, sentiment analysis, and text summarization.')]\n",
      "\n",
      "============================================================\n",
      "\n",
      "Source Documents:\n",
      "\n",
      "2. Source: ai_basics.txt | Topic: machine_learning\n",
      "   Content: Machine Learning is a subset of artificial intelligence that focuses on developing \n",
      "        systems that can learn from and make decisions based on da...\n",
      "\n",
      "2. Source: ai_advanced.txt | Topic: llm\n",
      "   Content: Large Language Models (LLMs) are neural networks trained on vast amounts of text data. \n",
      "        They can generate human-like text, answer questions, w...\n",
      "\n",
      "2. Source: ai_basics.txt | Topic: nlp\n",
      "   Content: Natural Language Processing (NLP) is a branch of AI that helps computers understand, \n",
      "        interpret, and generate human language. NLP combines com...\n"
     ]
    }
   ],
   "source": [
    "# Ask a question\n",
    "question = \"What is machine learning and what are its types?\"\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(\"Retrieving relevant information...\\n\")\n",
    "\n",
    "# Get the answer\n",
    "result = retriever.invoke(input=question)\n",
    "\n",
    "print(\"Answer:\")\n",
    "print(result)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "print(\"Source Documents:\")\n",
    "for doc in result:\n",
    "    print(f\"\\n{i}. Source: {doc.metadata['source']} | Topic: {doc.metadata['topic']}\")\n",
    "    print(f\"   Content: {doc.page_content.strip()[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde9fbb6",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Congratulations!\n",
    "\n",
    "You've successfully built a Document Q&A system with LangChain 1.0!\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "âœ… **Document Loading**: How to create and load documents  \n",
    "âœ… **Text Splitting**: Breaking documents into manageable chunks  \n",
    "âœ… **Embeddings**: Creating vector representations of text  \n",
    "âœ… **Vector Stores**: Using FAISS for efficient similarity search  \n",
    "âœ… **Retrieval QA**: Combining retrieval with LLM for answers  \n",
    "âœ… **Local Processing**: Running everything with Ollama  \n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **Indexing**: Documents are split into chunks and embedded into vectors\n",
    "2. **Retrieval**: When you ask a question, the system finds relevant chunks\n",
    "3. **Generation**: The LLM uses retrieved chunks to generate an answer\n",
    "4. **Sources**: You can trace answers back to source documents\n",
    "\n",
    "### Key Components\n",
    "\n",
    "- **Document Loaders**: Load text from various sources\n",
    "- **Text Splitters**: Break long documents into chunks\n",
    "- **Embeddings**: Convert text to numerical vectors\n",
    "- **Vector Stores**: Store and search vectors efficiently\n",
    "- **Retrievers**: Find relevant documents for a query\n",
    "- **QA Chains**: Combine retrieval + LLM for answers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-with-haystack-ch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
