{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adc1b59e",
   "metadata": {},
   "source": [
    "# LangChain 1.0 Middleware: Advanced Agent Control\n",
    "\n",
    "## Overview\n",
    "**Middleware** is LangChain 1.0's killer feature for **context engineering** - the practice of getting the right information to your model at the right time.\n",
    "\n",
    "## What is Middleware?\n",
    "Middleware provides hooks into your agent's execution lifecycle, allowing you to:\n",
    "- **Control what happens before/after** LLM calls\n",
    "- **Intercept and modify** tool executions\n",
    "- **Add guardrails** without cluttering your main code\n",
    "- **Manage context** to prevent pollution\n",
    "- **Enforce limits** to control costs\n",
    "\n",
    "## Why Middleware Matters\n",
    "Think of middleware as **clean room protocols** for your agent:\n",
    "- ðŸ§¼ **Context quarantine**: Keep information separate\n",
    "- ðŸ›¡ï¸ **Safety guardrails**: Screen inputs and outputs\n",
    "- ðŸ“Š **Observability**: Log and monitor behavior\n",
    "- ðŸ’° **Cost control**: Limit expensive operations\n",
    "- ðŸ‘¤ **Human oversight**: Pause for approval\n",
    "\n",
    "## What You'll Learn\n",
    "1. Built-in middleware classes\n",
    "2. Creating custom middleware with decorators\n",
    "3. Building class-based middleware\n",
    "4. Combining multiple middleware layers\n",
    "5. Real-world use cases and patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2322ab0f",
   "metadata": {},
   "source": [
    "## Setup: Install and Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56f4c1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurafunderburk/Documents/GitHub/Building-Natural-Language-Pipelines/ch2/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All libraries imported successfully\n",
      "âœ“ Ready to explore middleware!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from typing import Any, Callable\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain 1.0 imports\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import (\n",
    "    ModelCallLimitMiddleware,\n",
    "    ToolCallLimitMiddleware,\n",
    "    SummarizationMiddleware,\n",
    "    HumanInTheLoopMiddleware,\n",
    "    TodoListMiddleware,\n",
    "    AgentMiddleware,\n",
    "    AgentState,\n",
    "    before_model,\n",
    "    after_model,\n",
    "    before_agent,\n",
    "    after_agent,\n",
    ")\n",
    "from langchain.tools.tool_node import ToolCallRequest\n",
    "from langchain.messages import ToolMessage, AIMessage, HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain.tools import tool\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.runtime import Runtime\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully\")\n",
    "print(\"âœ“ Ready to explore middleware!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ec87ea",
   "metadata": {},
   "source": [
    "## Part 1: Built-in Middleware Classes\n",
    "\n",
    "LangChain 1.0 provides several ready-to-use middleware classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56634017",
   "metadata": {},
   "source": [
    "### 1.1 Model Call Limit Middleware\n",
    "\n",
    "Prevents runaway costs by limiting how many times the model can be called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ff3f4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Agent with model call limits created\n",
      "  - Max 3 calls per run\n",
      "  - Max 10 calls per thread\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k7/8m84jpf52x7gvx628v82h2p80000gn/T/ipykernel_11169/391585432.py:2: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the `langchain-tavily package and should be used instead. To use it run `pip install -U `langchain-tavily` and import as `from `langchain_tavily import TavilySearch``.\n",
      "  search_tool = TavilySearchResults(max_results=2)\n"
     ]
    }
   ],
   "source": [
    "# Create a simple search tool\n",
    "search_tool = TavilySearchResults(max_results=2)\n",
    "\n",
    "# Limit model calls\n",
    "model_limiter = ModelCallLimitMiddleware(\n",
    "    run_limit=3,  # Max 3 LLM calls per single invocation\n",
    "    thread_limit=10,  # Max 10 calls across all runs in a thread\n",
    "    exit_behavior=\"end\"  # Gracefully end (vs \"error\" to raise exception)\n",
    ")\n",
    "\n",
    "# Create agent with middleware\n",
    "limited_agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[search_tool],\n",
    "    middleware=[model_limiter],\n",
    "    system_prompt=\"You are a research assistant. Be concise.\"\n",
    ")\n",
    "\n",
    "print(\"âœ“ Agent with model call limits created\")\n",
    "print(\"  - Max 3 calls per run\")\n",
    "print(\"  - Max 10 calls per thread\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bf29da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response: Here are the latest trends in AI for 2024:\n",
      "\n",
      "1. **Enhanced Performance Metrics**: AI is excelling in benchmarks related to image classification and language understanding, although it still struggles w...\n",
      "\n",
      "âœ“ Agent stayed within limits!\n"
     ]
    }
   ],
   "source": [
    "# Test the limited agent\n",
    "result = limited_agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What are the latest AI trends?\"}]\n",
    "})\n",
    "\n",
    "print(\"\\nResponse:\", result[\"messages\"][-1].content[:200] + \"...\")\n",
    "print(f\"\\nâœ“ Agent stayed within limits!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580b00c0",
   "metadata": {},
   "source": [
    "### 1.2 Tool Call Limit Middleware\n",
    "\n",
    "Limits how many times a specific tool can be called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd655ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Agent with tool limits created\n",
      "  - expensive_api limited to 2 calls per run\n"
     ]
    }
   ],
   "source": [
    "# Create a custom tool\n",
    "@tool\n",
    "def expensive_api(query: str) -> str:\n",
    "    \"\"\"An expensive API call that we want to limit.\"\"\"\n",
    "    logger.info(f\"ðŸ’° Expensive API called with: {query}\")\n",
    "    return f\"Expensive result for: {query}\"\n",
    "\n",
    "# Limit tool calls\n",
    "tool_limiter = ToolCallLimitMiddleware(\n",
    "    tool_name=\"expensive_api\",\n",
    "    run_limit=2,  # Max 2 calls per invocation\n",
    "    thread_limit=5  # Max 5 calls per thread\n",
    ")\n",
    "\n",
    "tool_limited_agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[expensive_api],\n",
    "    middleware=[tool_limiter],\n",
    "    system_prompt=\"You can use the expensive API, but use it wisely.\"\n",
    ")\n",
    "\n",
    "print(\"âœ“ Agent with tool limits created\")\n",
    "print(\"  - expensive_api limited to 2 calls per run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7789aa2",
   "metadata": {},
   "source": [
    "### 1.3 Summarization Middleware\n",
    "\n",
    "Automatically summarizes conversation history when it gets too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dfbe7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Agent with automatic summarization created\n",
      "  - Summarizes when conversation exceeds 500 tokens\n",
      "  - Keeps last 5 messages\n"
     ]
    }
   ],
   "source": [
    "# Create summarization middleware\n",
    "summarizer = SummarizationMiddleware(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    max_tokens_before_summary=500,  # Trigger at 500 tokens\n",
    "    messages_to_keep=5,  # Keep last 5 messages after summarizing\n",
    "    summary_prompt=\"Summarize the conversation concisely, keeping key details.\"\n",
    ")\n",
    "\n",
    "summarizing_agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[search_tool],\n",
    "    middleware=[summarizer],\n",
    "    system_prompt=\"You are a helpful assistant.\"\n",
    ")\n",
    "\n",
    "print(\"âœ“ Agent with automatic summarization created\")\n",
    "print(\"  - Summarizes when conversation exceeds 500 tokens\")\n",
    "print(\"  - Keeps last 5 messages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8620f787",
   "metadata": {},
   "source": [
    "### 1.4 Human-in-the-Loop Middleware\n",
    "\n",
    "Pauses execution to get human approval before taking actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e7c0c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Agent with human-in-the-loop created\n",
      "  - Pauses before sending emails\n",
      "  - Requires explicit approval\n"
     ]
    }
   ],
   "source": [
    "@tool\n",
    "def send_email(to: str, subject: str, body: str) -> str:\n",
    "    \"\"\"Sends an email (simulated).\"\"\"\n",
    "    return f\"Email sent to {to} with subject '{subject}'\"\n",
    "\n",
    "# Human approval for sensitive actions\n",
    "hitl_middleware = HumanInTheLoopMiddleware(\n",
    "    interrupt_on={\n",
    "        \"send_email\": {\n",
    "            \"allowed_decisions\": [\"approve\", \"reject\", \"edit\"]\n",
    "        }\n",
    "    },\n",
    "    description_prefix=\"âš ï¸ Action requires approval:\"\n",
    ")\n",
    "\n",
    "# Need a checkpointer for HITL\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "hitl_agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[send_email],\n",
    "    middleware=[hitl_middleware],\n",
    "    checkpointer=checkpointer,\n",
    "    system_prompt=\"You can send emails, but wait for approval.\"\n",
    ")\n",
    "\n",
    "print(\"âœ“ Agent with human-in-the-loop created\")\n",
    "print(\"  - Pauses before sending emails\")\n",
    "print(\"  - Requires explicit approval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e2776f",
   "metadata": {},
   "source": [
    "### 1.5 Todo List Middleware\n",
    "\n",
    "Tracks tasks and checks them off as the agent completes them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e30c9827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Agent with todo tracking created\n",
      "  - Automatically tracks planned tasks\n",
      "  - Updates as tasks complete\n"
     ]
    }
   ],
   "source": [
    "# Automatically tracks agent's plan\n",
    "todo_middleware = TodoListMiddleware()\n",
    "\n",
    "planning_agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[search_tool],\n",
    "    middleware=[todo_middleware],\n",
    "    system_prompt=\"You are a research assistant who plans before acting.\"\n",
    ")\n",
    "\n",
    "print(\"âœ“ Agent with todo tracking created\")\n",
    "print(\"  - Automatically tracks planned tasks\")\n",
    "print(\"  - Updates as tasks complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661ee14c",
   "metadata": {},
   "source": [
    "## Part 2: Decorator-Based Custom Middleware\n",
    "\n",
    "Create custom middleware using decorators for quick, function-based hooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e25ae6d",
   "metadata": {},
   "source": [
    "### 2.1 Input Validation Middleware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "deea5591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Input validation middleware defined\n"
     ]
    }
   ],
   "source": [
    "@before_model(can_jump_to=[\"end\"])\n",
    "def validate_input(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
    "    \"\"\"Check if the user's input is appropriate.\"\"\"\n",
    "    user_message = state['messages'][-1].content\n",
    "    \n",
    "    # Simple profanity check (in production, use a real filter)\n",
    "    banned_words = [\"badword1\", \"badword2\"]\n",
    "    \n",
    "    for word in banned_words:\n",
    "        if word.lower() in user_message.lower():\n",
    "            logger.warning(f\"ðŸš« Inappropriate content detected: {word}\")\n",
    "            return {\n",
    "                \"messages\": [AIMessage(\"I cannot respond to inappropriate requests.\")],\n",
    "                \"jump_to\": \"end\"\n",
    "            }\n",
    "    \n",
    "    logger.info(\"âœ… Input validation passed\")\n",
    "    return None  # Continue normally\n",
    "\n",
    "print(\"âœ“ Input validation middleware defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d2cffe",
   "metadata": {},
   "source": [
    "### 2.2 Output Filtering Middleware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15af8e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Output filtering middleware defined\n"
     ]
    }
   ],
   "source": [
    "@after_model\n",
    "def filter_output(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
    "    \"\"\"Remove sensitive information from the response.\"\"\"\n",
    "    last_message = state['messages'][-1]\n",
    "    \n",
    "    if hasattr(last_message, 'content') and last_message.content:\n",
    "        content = last_message.content\n",
    "        \n",
    "        # Redact email addresses (simple regex)\n",
    "        import re\n",
    "        email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "        if re.search(email_pattern, content):\n",
    "            filtered_content = re.sub(email_pattern, '[EMAIL_REDACTED]', content)\n",
    "            logger.info(\"ðŸ“§ Email addresses redacted\")\n",
    "            \n",
    "            # Replace the message\n",
    "            messages = state['messages'][:-1] + [AIMessage(filtered_content)]\n",
    "            return {\"messages\": messages}\n",
    "    \n",
    "    return None  # No changes needed\n",
    "\n",
    "print(\"âœ“ Output filtering middleware defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a7b708",
   "metadata": {},
   "source": [
    "### 2.3 Logging Middleware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "760be89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Logging middleware defined\n"
     ]
    }
   ],
   "source": [
    "@before_agent\n",
    "def log_start(state: AgentState, runtime: Runtime) -> None:\n",
    "    \"\"\"Log when agent execution starts.\"\"\"\n",
    "    user_msg = state['messages'][-1].content if state['messages'] else \"No message\"\n",
    "    logger.info(f\"ðŸš€ Agent started | User: {user_msg[:50]}...\")\n",
    "\n",
    "@after_agent\n",
    "def log_end(state: AgentState, runtime: Runtime) -> None:\n",
    "    \"\"\"Log when agent execution completes.\"\"\"\n",
    "    logger.info(f\"âœ… Agent completed | Total messages: {len(state['messages'])}\")\n",
    "\n",
    "print(\"âœ“ Logging middleware defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eace377",
   "metadata": {},
   "source": [
    "### 2.4 Create Agent with Custom Decorators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17e91b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Agent with custom decorator middleware created\n",
      "  - Input validation\n",
      "  - Output filtering (PII redaction)\n",
      "  - Start/end logging\n"
     ]
    }
   ],
   "source": [
    "# Agent with all custom decorator middleware\n",
    "custom_agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[search_tool],\n",
    "    middleware=[\n",
    "        validate_input,\n",
    "        filter_output,\n",
    "        log_start,\n",
    "        log_end\n",
    "    ],\n",
    "    system_prompt=\"You are a helpful assistant.\"\n",
    ")\n",
    "\n",
    "print(\"âœ“ Agent with custom decorator middleware created\")\n",
    "print(\"  - Input validation\")\n",
    "print(\"  - Output filtering (PII redaction)\")\n",
    "print(\"  - Start/end logging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cb1eef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:ðŸš€ Agent started | User: Tell me about AI safety....\n",
      "INFO:__main__:âœ… Input validation passed\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:âœ… Agent completed | Total messages: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response: AI safety refers to the field of research and practice focused on ensuring that artificial intelligence systems operate safely and align with human values. As AI technology advances rapidly, concerns ...\n"
     ]
    }
   ],
   "source": [
    "# Test the custom agent\n",
    "result = custom_agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Tell me about AI safety.\"}]\n",
    "})\n",
    "\n",
    "print(\"\\nResponse:\", result[\"messages\"][-1].content[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c45b14",
   "metadata": {},
   "source": [
    "## Part 3: Class-Based Custom Middleware\n",
    "\n",
    "For more complex middleware with state, extend `AgentMiddleware` class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9ee6d6",
   "metadata": {},
   "source": [
    "### 3.1 Tool Monitoring Middleware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93f0d8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ ToolMonitoringMiddleware class defined\n"
     ]
    }
   ],
   "source": [
    "class ToolMonitoringMiddleware(AgentMiddleware):\n",
    "    \"\"\"Monitors and logs all tool executions.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tool_calls = []\n",
    "    \n",
    "    def wrap_tool_call(\n",
    "        self,\n",
    "        request: ToolCallRequest,\n",
    "        handler: Callable[[ToolCallRequest], ToolMessage],\n",
    "    ) -> ToolMessage:\n",
    "        \"\"\"Intercept every tool call.\"\"\"\n",
    "        tool_name = request.tool_call['name']\n",
    "        tool_args = request.tool_call['args']\n",
    "        \n",
    "        logger.info(f\"ðŸ”§ Tool called: {tool_name}\")\n",
    "        logger.info(f\"   Arguments: {tool_args}\")\n",
    "        \n",
    "        try:\n",
    "            # Execute the tool\n",
    "            result = handler(request)\n",
    "            \n",
    "            # Track success\n",
    "            self.tool_calls.append({\n",
    "                \"tool\": tool_name,\n",
    "                \"args\": tool_args,\n",
    "                \"status\": \"success\"\n",
    "            })\n",
    "            \n",
    "            logger.info(f\"âœ… Tool completed successfully\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Track failure\n",
    "            self.tool_calls.append({\n",
    "                \"tool\": tool_name,\n",
    "                \"args\": tool_args,\n",
    "                \"status\": \"error\",\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "            \n",
    "            logger.error(f\"âŒ Tool failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get statistics about tool usage.\"\"\"\n",
    "        total = len(self.tool_calls)\n",
    "        successful = sum(1 for call in self.tool_calls if call['status'] == 'success')\n",
    "        failed = total - successful\n",
    "        \n",
    "        return {\n",
    "            \"total_calls\": total,\n",
    "            \"successful\": successful,\n",
    "            \"failed\": failed,\n",
    "            \"calls\": self.tool_calls\n",
    "        }\n",
    "\n",
    "print(\"âœ“ ToolMonitoringMiddleware class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99274dd4",
   "metadata": {},
   "source": [
    "### 3.2 Performance Tracking Middleware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2525546f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ PerformanceMiddleware class defined\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from langchain.agents.middleware import ModelRequest, ModelResponse\n",
    "\n",
    "class PerformanceMiddleware(AgentMiddleware):\n",
    "    \"\"\"Tracks execution time and token usage.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model_calls = []\n",
    "    \n",
    "    def wrap_model_call(\n",
    "        self,\n",
    "        request: ModelRequest,\n",
    "        handler: Callable[[ModelRequest], ModelResponse]\n",
    "    ) -> ModelResponse:\n",
    "        \"\"\"Time each model call.\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        logger.info(f\"â±ï¸  Model call starting...\")\n",
    "        \n",
    "        # Execute the model call\n",
    "        response = handler(request)\n",
    "        \n",
    "        # Calculate duration\n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        # Track the call\n",
    "        self.model_calls.append({\n",
    "            \"duration_seconds\": duration,\n",
    "            \"timestamp\": time.time()\n",
    "        })\n",
    "        \n",
    "        logger.info(f\"âœ… Model call completed in {duration:.2f}s\")\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get performance statistics.\"\"\"\n",
    "        if not self.model_calls:\n",
    "            return {\"total_calls\": 0, \"total_time\": 0, \"avg_time\": 0}\n",
    "        \n",
    "        total_time = sum(call['duration_seconds'] for call in self.model_calls)\n",
    "        avg_time = total_time / len(self.model_calls)\n",
    "        \n",
    "        return {\n",
    "            \"total_calls\": len(self.model_calls),\n",
    "            \"total_time\": total_time,\n",
    "            \"avg_time\": avg_time,\n",
    "            \"calls\": self.model_calls\n",
    "        }\n",
    "\n",
    "print(\"âœ“ PerformanceMiddleware class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91e5430",
   "metadata": {},
   "source": [
    "### 3.3 Create Agent with Class-Based Middleware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d358ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Agent with monitoring middleware created\n"
     ]
    }
   ],
   "source": [
    "# Initialize middleware instances\n",
    "tool_monitor = ToolMonitoringMiddleware()\n",
    "perf_tracker = PerformanceMiddleware()\n",
    "\n",
    "# Create agent\n",
    "monitored_agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[search_tool],\n",
    "    middleware=[tool_monitor, perf_tracker],\n",
    "    system_prompt=\"You are a research assistant.\"\n",
    ")\n",
    "\n",
    "print(\"âœ“ Agent with monitoring middleware created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ec521c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:â±ï¸  Model call starting...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:âœ… Model call completed in 3.89s\n",
      "INFO:__main__:ðŸ”§ Tool called: tavily_search_results_json\n",
      "INFO:__main__:   Arguments: {'query': 'latest news about quantum computing'}\n",
      "INFO:__main__:âœ… Tool completed successfully\n",
      "INFO:__main__:â±ï¸  Model call starting...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:âœ… Model call completed in 5.75s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RESPONSE:\n",
      "================================================================================\n",
      "Here are the latest developments in quantum computing:\n",
      "\n",
      "1. **Breakthroughs and Real-World Applications**: A recent article discusses a significant shift in the quantum computing field towards practical implementations rather than merely focusing on processor benchmarks. This transition is expected t...\n"
     ]
    }
   ],
   "source": [
    "# Test the monitored agent\n",
    "result = monitored_agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What is the latest news about quantum computing?\"}]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RESPONSE:\")\n",
    "print(\"=\" * 80)\n",
    "print(result[\"messages\"][-1].content[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07dbe336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TOOL MONITORING STATS:\n",
      "================================================================================\n",
      "Total tool calls: 1\n",
      "Successful: 1\n",
      "Failed: 0\n",
      "\n",
      "================================================================================\n",
      "PERFORMANCE STATS:\n",
      "================================================================================\n",
      "Total model calls: 2\n",
      "Total time: 9.65s\n",
      "Average time: 4.82s\n"
     ]
    }
   ],
   "source": [
    "# View monitoring statistics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TOOL MONITORING STATS:\")\n",
    "print(\"=\" * 80)\n",
    "tool_stats = tool_monitor.get_stats()\n",
    "print(f\"Total tool calls: {tool_stats['total_calls']}\")\n",
    "print(f\"Successful: {tool_stats['successful']}\")\n",
    "print(f\"Failed: {tool_stats['failed']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PERFORMANCE STATS:\")\n",
    "print(\"=\" * 80)\n",
    "perf_stats = perf_tracker.get_stats()\n",
    "print(f\"Total model calls: {perf_stats['total_calls']}\")\n",
    "print(f\"Total time: {perf_stats['total_time']:.2f}s\")\n",
    "print(f\"Average time: {perf_stats['avg_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be379fc5",
   "metadata": {},
   "source": [
    "## Part 4: Combining Multiple Middleware Layers\n",
    "\n",
    "The real power comes from stacking multiple middleware together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd81875",
   "metadata": {},
   "source": [
    "### 4.1 Production-Ready Agent with Full Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30689744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Production-ready agent created with full middleware stack:\n",
      "  1. Input validation\n",
      "  2. Model call limits\n",
      "  3. Tool call limits\n",
      "  4. Automatic summarization\n",
      "  5. Tool monitoring\n",
      "  6. Performance tracking\n",
      "  7. Output filtering\n",
      "  8. Comprehensive logging\n"
     ]
    }
   ],
   "source": [
    "# Initialize all middleware\n",
    "model_limiter = ModelCallLimitMiddleware(run_limit=5, exit_behavior=\"end\")\n",
    "tool_limiter = ToolCallLimitMiddleware(tool_name=\"tavily_search_results_json\", run_limit=3)\n",
    "summarizer = SummarizationMiddleware(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    max_tokens_before_summary=1000,\n",
    "    messages_to_keep=5\n",
    ")\n",
    "tool_monitor = ToolMonitoringMiddleware()\n",
    "perf_tracker = PerformanceMiddleware()\n",
    "\n",
    "# Create production agent with full middleware stack\n",
    "production_agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[search_tool],\n",
    "    middleware=[\n",
    "        validate_input,      # Security first\n",
    "        model_limiter,       # Cost control\n",
    "        tool_limiter,        # Tool usage control\n",
    "        summarizer,          # Context management\n",
    "        tool_monitor,        # Observability\n",
    "        perf_tracker,        # Performance tracking\n",
    "        filter_output,       # Output security\n",
    "        log_start,           # Logging\n",
    "        log_end              # Logging\n",
    "    ],\n",
    "    system_prompt=\"You are a production research assistant with full monitoring.\"\n",
    ")\n",
    "\n",
    "print(\"âœ“ Production-ready agent created with full middleware stack:\")\n",
    "print(\"  1. Input validation\")\n",
    "print(\"  2. Model call limits\")\n",
    "print(\"  3. Tool call limits\")\n",
    "print(\"  4. Automatic summarization\")\n",
    "print(\"  5. Tool monitoring\")\n",
    "print(\"  6. Performance tracking\")\n",
    "print(\"  7. Output filtering\")\n",
    "print(\"  8. Comprehensive logging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07056570",
   "metadata": {},
   "source": [
    "### 4.2 Test Production Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "698cdd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:ðŸš€ Agent started | User: Research the latest breakthroughs in renewable ene...\n",
      "INFO:__main__:âœ… Input validation passed\n",
      "INFO:__main__:â±ï¸  Model call starting...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:âœ… Model call completed in 0.83s\n",
      "INFO:__main__:ðŸ”§ Tool called: tavily_search_results_json\n",
      "INFO:__main__:   Arguments: {'query': 'latest breakthroughs in renewable energy 2023'}\n",
      "INFO:__main__:âœ… Tool completed successfully\n",
      "INFO:__main__:âœ… Input validation passed\n",
      "INFO:__main__:â±ï¸  Model call starting...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:âœ… Model call completed in 1.82s\n",
      "INFO:__main__:ðŸ”§ Tool called: tavily_search_results_json\n",
      "INFO:__main__:ðŸ”§ Tool called: tavily_search_results_json\n",
      "INFO:__main__:   Arguments: {'query': 'current breakthroughs in solar energy 2023'}\n",
      "INFO:__main__:   Arguments: {'query': 'current breakthroughs in wind energy 2023'}\n",
      "INFO:__main__:âœ… Tool completed successfully\n",
      "INFO:__main__:âœ… Tool completed successfully\n",
      "INFO:__main__:âœ… Input validation passed\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:â±ï¸  Model call starting...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:âœ… Model call completed in 9.68s\n",
      "INFO:__main__:âœ… Agent completed | Total messages: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PRODUCTION AGENT RESPONSE:\n",
      "================================================================================\n",
      "Here are some of the latest breakthroughs and developments in renewable energy from 2023:\n",
      "\n",
      "### 1. Solar Energy\n",
      "- **Perovskite Solar Cells**: Perovskite cells are emerging as a transformative technology in solar energy. These ultra-thin cells can be sprayed onto surfaces like windows and combined with silicon to enhance their energy absorption capabilities. Recent advancements include tandem solar cells reaching efficiencies of over 26.8%, indicating significant improvements in solar panel technology.\n",
      "    - Article: [Perovskite: The 'wonder material' that could transform solar](https://www.bbc.com/future/article/20251015-perovskite-the-wonder-material-that-could-transform-solar-energy)\n",
      "    - Additional trend: In early 2025, Trina Solar achieved a world record for solar conversion efficiency of 25.44% with n-type fully passivated heterojunction (HJT) solar modules, showcasing the continual improvement and competition within solar technology.\n",
      "    - Source: [7 New Solar Panel Technology Trends for 2025](https://www.greenlancer.com/post/solar-panel-technology-trends)\n",
      "\n",
      "### 2. Wind Energy\n",
      "- **Technological Innovations**: Despite a relatively slow year for new deployments in 2023, advancements in wind technology, such as longer blades and taller towers, could unlock an additional 80% of economically viable wind energy capacity in the U.S. by 2025. This indicates a strong potential for future growth driven by technological advancements.\n",
      "    - Source: [Technology Advancements Could Unlock 80% More Wind Energy Potential](https://www.nrel.gov/news/detail/program/2023/technology-advancements-could-unlock-80-more-wind-energy-potential-during-this-decade)\n",
      "- **Current Deployment**: In 2023, the U.S. added 6.5 GW of wind power, contributing 10% of the nation's electricity supply. Energy analysts expect a resurgence in wind installations, projecting an average growth of 15 GW per year from 2026 to 2028.\n",
      "    - Source: [Report Highlights Advancements in Wind Technology](https://newscenter.lbl.gov/2024/08/21/report-highlights-advancements-in-wind-technology-and-supply-chains/)\n",
      "\n",
      "### 3. Battery Technology\n",
      "*Note: Specific articles regarding breakthroughs in battery technology were not returned, but advancements in battery technology often accompany renewable energy developments to enhance energy storage and efficiency.*\n",
      "\n",
      "### Conclusion\n",
      "The renewable energy sector is experiencing significant technological advancements, particularly in solar and wind energy. Innovations such as perovskite solar cells and improved wind turbine designs promise a more efficient, affordable, and sustainable energy future. Further insights into battery technology may also complement these developments.\n",
      "\n",
      "Feel free to ask for more specific information or additional topics!\n"
     ]
    }
   ],
   "source": [
    "# Test with a complex query\n",
    "result = production_agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Research the latest breakthroughs in renewable energy.\"}]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PRODUCTION AGENT RESPONSE:\")\n",
    "print(\"=\" * 80)\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b8e6bb",
   "metadata": {},
   "source": [
    "## Part 5: Middleware Execution Order\n",
    "\n",
    "**Important**: Middleware execution order matters!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1d8a8e",
   "metadata": {},
   "source": [
    "### Understanding Middleware Flow\n",
    "\n",
    "```\n",
    "REQUEST FLOW (Top to Bottom):\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "User Input\n",
    "    â†“\n",
    "@before_agent    (ascending order)\n",
    "    â†“\n",
    "@before_model    (ascending order)\n",
    "    â†“\n",
    "ðŸ¤– LLM CALL\n",
    "    â†“\n",
    "@after_model     (descending order) âš ï¸\n",
    "    â†“\n",
    "Tool Execution (if needed)\n",
    "    â†“\n",
    "@after_agent     (descending order) âš ï¸\n",
    "    â†“\n",
    "Response to User\n",
    "```\n",
    "\n",
    "**Key Points:**\n",
    "- `before_*` hooks execute in **ascending order** (first to last)\n",
    "- `after_*` hooks execute in **descending order** (last to first)\n",
    "- Plan your middleware stack accordingly!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2b6254",
   "metadata": {},
   "source": [
    "## Part 6: Real-World Use Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c5d80b",
   "metadata": {},
   "source": [
    "### Use Case 1: Financial Analysis Agent with Compliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06d8739a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Financial agent with compliance checks created\n"
     ]
    }
   ],
   "source": [
    "@tool\n",
    "def get_stock_price(symbol: str) -> str:\n",
    "    \"\"\"Get current stock price (simulated).\"\"\"\n",
    "    return f\"Stock {symbol}: $150.25\"\n",
    "\n",
    "@before_model(can_jump_to=[\"end\"])\n",
    "def check_compliance(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
    "    \"\"\"Ensure queries comply with financial regulations.\"\"\"\n",
    "    user_msg = state['messages'][-1].content.lower()\n",
    "    \n",
    "    # Check for insider trading language\n",
    "    if \"insider\" in user_msg or \"secret\" in user_msg:\n",
    "        logger.warning(\"âš ï¸ Compliance violation detected\")\n",
    "        return {\n",
    "            \"messages\": [AIMessage(\"I cannot provide advice that may involve insider information.\")],\n",
    "            \"jump_to\": \"end\"\n",
    "        }\n",
    "    return None\n",
    "\n",
    "# Financial agent with compliance\n",
    "financial_agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[get_stock_price],\n",
    "    middleware=[\n",
    "        check_compliance,\n",
    "        ModelCallLimitMiddleware(run_limit=3, exit_behavior=\"end\"),\n",
    "        ToolMonitoringMiddleware()\n",
    "    ],\n",
    "    system_prompt=\"You are a financial analysis assistant. Always comply with regulations.\"\n",
    ")\n",
    "\n",
    "print(\"âœ“ Financial agent with compliance checks created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82513255",
   "metadata": {},
   "source": [
    "### Use Case 2: Customer Support Agent with Escalation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8dc03c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Customer support agent with escalation detection created\n"
     ]
    }
   ],
   "source": [
    "@after_model\n",
    "def detect_escalation(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
    "    \"\"\"Detect if issue needs escalation to human.\"\"\"\n",
    "    last_msg = state['messages'][-1].content if state['messages'] else \"\"\n",
    "    \n",
    "    # Check for escalation keywords\n",
    "    escalation_keywords = [\"angry\", \"frustrated\", \"manager\", \"lawsuit\", \"complaint\"]\n",
    "    \n",
    "    user_messages = [msg.content for msg in state['messages'] if hasattr(msg, 'content')]\n",
    "    combined = \" \".join(user_messages).lower()\n",
    "    \n",
    "    for keyword in escalation_keywords:\n",
    "        if keyword in combined:\n",
    "            logger.warning(f\"ðŸš¨ Escalation needed: '{keyword}' detected\")\n",
    "            escalation_msg = (\n",
    "                \"\\n\\n[SYSTEM NOTICE: This conversation has been flagged for human review \"\n",
    "                \"due to complexity or customer sentiment.]\"\n",
    "            )\n",
    "            updated_content = last_msg + escalation_msg\n",
    "            messages = state['messages'][:-1] + [AIMessage(updated_content)]\n",
    "            return {\"messages\": messages}\n",
    "    \n",
    "    return None\n",
    "\n",
    "support_agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[],\n",
    "    middleware=[detect_escalation],\n",
    "    system_prompt=\"You are a customer support agent. Be empathetic and helpful.\"\n",
    ")\n",
    "\n",
    "print(\"âœ“ Customer support agent with escalation detection created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6a9e20",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "âœ… **Built-in middleware**: Ready-to-use for common patterns  \n",
    "âœ… **Decorator-based**: Quick custom hooks with `@before_model` etc.  \n",
    "âœ… **Class-based**: Complex middleware with state  \n",
    "âœ… **Composability**: Stack multiple layers for production  \n",
    "âœ… **Execution order**: Critical for proper behavior  \n",
    "\n",
    "### Middleware Types\n",
    "\n",
    "| Type | Use Case | Complexity |\n",
    "|------|----------|------------|\n",
    "| Built-in | Standard patterns | Low |\n",
    "| Decorator | Quick custom hooks | Medium |\n",
    "| Class-based | Complex with state | High |\n",
    "\n",
    "### Best Practices\n",
    "1. **Layer logically**: Security â†’ Limits â†’ Processing â†’ Output\n",
    "2. **Test individually**: Verify each middleware works alone\n",
    "3. **Monitor performance**: Track overhead from middleware\n",
    "4. **Keep it focused**: Each middleware should have one job\n",
    "5. **Document order**: Be explicit about execution sequence\n",
    "\n",
    "### Common Patterns\n",
    "- **Security Stack**: Input validation â†’ Content filter â†’ Output sanitization\n",
    "- **Cost Control**: Model limits â†’ Tool limits â†’ Summarization\n",
    "- **Observability**: Logging â†’ Monitoring â†’ Performance tracking\n",
    "- **Compliance**: Regulatory checks â†’ Audit logging â†’ Approval gates\n",
    "\n",
    "### Next Steps\n",
    "1. **Implement guardrails**: Add safety layers for production\n",
    "2. **Create middleware library**: Build reusable components\n",
    "3. **Add telemetry**: Integrate with monitoring systems\n",
    "4. **Test edge cases**: Ensure middleware handles errors\n",
    "5. **Optimize performance**: Profile and reduce overhead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7750149",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- [LangChain 1.0 Middleware Guide](https://docs.langchain.com/oss/python/langchain/middleware)\n",
    "- [Middleware API Reference](https://reference.langchain.com/python/langchain/middleware/)\n",
    "- [Context Engineering Blog](https://blog.langchain.com/agent-middleware/)\n",
    "- [LangChain GitHub](https://github.com/langchain-ai/langchain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-with-haystack-ch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
