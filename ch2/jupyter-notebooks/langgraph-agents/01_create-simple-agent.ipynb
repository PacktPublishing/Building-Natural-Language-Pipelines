{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c539e8a3",
   "metadata": {},
   "source": [
    "# Simple Agent with LangChain 1.0's `create_agent`\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates **LangChain 1.0's new `create_agent` function** - the simplified way to build agents without needing to manually construct graphs.\n",
    "\n",
    "## What is `create_agent`?\n",
    "`create_agent` is LangChain 1.0's recommended way to build agents for most use cases. It provides:\n",
    "- **Simplified API**: No need to manually build state graphs\n",
    "- **Built on LangGraph**: Gets all LangGraph benefits (streaming, persistence, etc.)\n",
    "- **Middleware support**: Advanced context engineering capabilities\n",
    "- **Production-ready**: Designed for real applications\n",
    "\n",
    "## When to Use `create_agent` vs LangGraph\n",
    "- **Use `create_agent`**: Single agent, straightforward tool use, standard workflows\n",
    "- **Use LangGraph**: Complex multi-step orchestration, custom routing logic, multi-agent systems\n",
    "\n",
    "## Requirements\n",
    "- **LangChain 1.0+**: `pip install langchain>=1.0`\n",
    "- **API Key**: OpenAI or Ollama\n",
    "- **Tavily API**: For search tool (free tier available)\n",
    "\n",
    "## What You'll Learn\n",
    "1. How to create an agent with `create_agent`\n",
    "2. How to add tools to your agent\n",
    "3. How to invoke and stream agent responses\n",
    "4. How to add middleware for advanced control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57a3c8d",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries\n",
    "\n",
    "First, let's import the necessary components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59ee94bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Libraries imported successfully\n",
      "‚úì Using LangChain 1.0's create_agent function\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain 1.0 imports\n",
    "from langchain.agents import create_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "print(\"‚úì Libraries imported successfully\")\n",
    "print(\"‚úì Using LangChain 1.0's create_agent function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31292dc9",
   "metadata": {},
   "source": [
    "## Step 2: Set Up Tools\n",
    "\n",
    "Let's define the tools our agent can use. We'll start with a search tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "736a6b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì 1 tool(s) configured\n",
      "  - tavily_search_results_json: Search the web for current information. Use this when you need up-to-date facts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k7/8m84jpf52x7gvx628v82h2p80000gn/T/ipykernel_8942/3110299831.py:2: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the `langchain-tavily package and should be used instead. To use it run `pip install -U `langchain-tavily` and import as `from `langchain_tavily import TavilySearch``.\n",
      "  search_tool = TavilySearchResults(\n"
     ]
    }
   ],
   "source": [
    "# Initialize search tool\n",
    "search_tool = TavilySearchResults(\n",
    "    max_results=3,\n",
    "    description=\"Search the web for current information. Use this when you need up-to-date facts.\"\n",
    ")\n",
    "\n",
    "# Collect all tools\n",
    "tools = [search_tool]\n",
    "\n",
    "print(f\"‚úì {len(tools)} tool(s) configured\")\n",
    "print(f\"  - {search_tool.name}: {search_tool.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632912ec",
   "metadata": {},
   "source": [
    "## Step 3: Create the Agent (Basic Version)\n",
    "\n",
    "Now let's create our agent using `create_agent`. This is much simpler than manually building a graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc082813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Agent created successfully!\n",
      "  Model: gpt-4o-mini\n",
      "  Tools: 1\n",
      "  Ready to use!\n"
     ]
    }
   ],
   "source": [
    "# Create the agent - it's this simple!\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=tools,\n",
    "    system_prompt=\"You are a helpful research assistant. Use the search tool when you need current information.\"\n",
    ")\n",
    "\n",
    "print(\"‚úì Agent created successfully!\")\n",
    "print(\"  Model: gpt-4o-mini\")\n",
    "print(f\"  Tools: {len(tools)}\")\n",
    "print(\"  Ready to use!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2487f47",
   "metadata": {},
   "source": [
    "## Step 4: Invoke the Agent\n",
    "\n",
    "Let's test our agent with a question that requires current information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f98cebeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Query: What are the latest AI developments in 2025?\n",
      "\n",
      "‚öôÔ∏è  Agent is working...\n",
      "\n",
      "================================================================================\n",
      "AGENT RESPONSE:\n",
      "================================================================================\n",
      "Here are some of the latest AI developments and trends observed in 2025:\n",
      "\n",
      "1. **Advanced Learning Systems**: AI systems are now capable of continuous learning, allowing them to adapt based on recent interactions and updates without needing full retraining. This includes developments in large language models (LLMs) that can reason more like humans and manage complex tasks in real-time.\n",
      "\n",
      "   - [Source: Morgan Stanley](https://www.morganstanley.com/insights/articles/ai-trends-reasoning-frontier-models-2025-tmt)\n",
      "\n",
      "2. **Near-Infinite Memory**: AI technologies, such as Google's Gemini, are utilizing advanced memory capabilities that allow for ongoing conversations and the recall of past interactions over long periods. This leads to highly personalized and context-aware responses, enhancing user experience significantly.\n",
      "\n",
      "   - [Source: Forbes](https://www.forbes.com/councils/forbestechcouncil/2025/04/15/five-transformative-ai-technology-trends-shaping-2025/)\n",
      "\n",
      "3. **Integration of Multimodal Data**: New systems are being developed to integrate various types of data, including text, images, and video, which enhances their ability to process and interpret information more like a human would. This is pushing boundaries in natural language processing and coding.\n",
      "\n",
      "   - [Source: Morgan Stanley](https://www.morganstanley.com/insights/articles/ai-trends-reasoning-frontier-models-2025-tmt)\n",
      "\n",
      "4. **AI in Everyday Life**: The integration of AI into daily life has accelerated, with AI-enabled devices being increasingly common in healthcare and transportation. For example, self-driving cars have become a regular part of urban transport systems, providing hundreds of thousands of rides weekly.\n",
      "\n",
      "   - [Source: Stanford HAI](https://hai.stanford.edu/ai-index/2025-ai-index-report)\n",
      "\n",
      "5. **Emerging AI Startups**: While established companies like OpenAI and Google continue to innovate, new players, such as China's DeepSeek, are making significant contributions with fresh approaches to problem-solving in AI, showcasing that innovation can come from various sources.\n",
      "\n",
      "These advancements reflect a significant evolution in how AI interacts with humans and processes information, promising a more personalized and efficient future in technology.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Ask a question\n",
    "query = \"What are the latest AI developments in 2025?\"\n",
    "\n",
    "print(f\"üîç Query: {query}\\n\")\n",
    "print(\"‚öôÔ∏è  Agent is working...\\n\")\n",
    "\n",
    "# Invoke the agent\n",
    "result = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": query}]\n",
    "})\n",
    "\n",
    "# Display the response\n",
    "print(\"=\" * 80)\n",
    "print(\"AGENT RESPONSE:\")\n",
    "print(\"=\" * 80)\n",
    "print(result[\"messages\"][-1].content)\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2302c029",
   "metadata": {},
   "source": [
    "## Step 5: Stream Agent Responses\n",
    "\n",
    "For better user experience, we can stream the agent's responses in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d635e59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Query: What are the top tech companies by market cap in 2025?\n",
      "\n",
      "üì° Streaming response...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[tools]:\n",
      "[{\"title\": \"Top tech companies by market cap 2025\", \"url\": \"https://www.statista.com/statistics/1350976/leading-tech-companies-worldwide-by-market-cap/?srsltid=AfmBOoqxAyjX-FuMez7ui-_oSFzuPmzxsOEOFHEN...\n",
      "\n",
      "[model]:\n",
      "As of October 2025, the top tech companies by market capitalization are as follows:\n",
      "\n",
      "1. **NVIDIA** - $5,060 billion\n",
      "2. **Apple** - $4,015 billion\n",
      "3. **Microsoft** - $4,006 billion\n",
      "4. **Alphabet (Googl...\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the top tech companies by market cap in 2025?\"\n",
    "\n",
    "print(f\"üîç Query: {query}\\n\")\n",
    "print(\"üì° Streaming response...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Stream the response\n",
    "for chunk in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    stream_mode=\"updates\"\n",
    "):\n",
    "    for node_name, node_output in chunk.items():\n",
    "        if \"messages\" in node_output:\n",
    "            message = node_output[\"messages\"][-1]\n",
    "            if hasattr(message, \"content\") and message.content:\n",
    "                print(f\"\\n[{node_name}]:\")\n",
    "                print(message.content[:200] + \"...\" if len(message.content) > 200 else message.content)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6576acbe",
   "metadata": {},
   "source": [
    "## Step 6: Test Without Tools\n",
    "\n",
    "Let's see how the agent behaves when it doesn't need external tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "996ac734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Query: What is 25 * 4?\n",
      "\n",
      "Response: 25 * 4 equals 100.\n",
      "\n",
      "‚úì Agent answered directly without using tools!\n"
     ]
    }
   ],
   "source": [
    "# Simple question that doesn't need search\n",
    "simple_query = \"What is 25 * 4?\"\n",
    "\n",
    "print(f\"üîç Query: {simple_query}\\n\")\n",
    "\n",
    "result = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": simple_query}]\n",
    "})\n",
    "\n",
    "print(\"Response:\", result[\"messages\"][-1].content)\n",
    "print(\"\\n‚úì Agent answered directly without using tools!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1baf2cc",
   "metadata": {},
   "source": [
    "## Step 7: Add Custom Tools\n",
    "\n",
    "Let's create a custom tool and add it to our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6d61678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Enhanced agent created with 2 tools:\n",
      "  - Search tool\n",
      "  - Calculator tool\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"Evaluates a mathematical expression. Input should be a valid Python math expression.\"\"\"\n",
    "    try:\n",
    "        result = eval(expression)\n",
    "        return f\"The result is: {result}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Create a new agent with both tools\n",
    "enhanced_agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[search_tool, calculate],\n",
    "    system_prompt=\"You are a helpful assistant with search and calculation capabilities.\"\n",
    ")\n",
    "\n",
    "print(\"‚úì Enhanced agent created with 2 tools:\")\n",
    "print(\"  - Search tool\")\n",
    "print(\"  - Calculator tool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "482a13ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Query: What is 15% of 250?\n",
      "\n",
      "Response: 15% of 250 is 37.5.\n"
     ]
    }
   ],
   "source": [
    "# Test the calculator tool\n",
    "calc_query = \"What is 15% of 250?\"\n",
    "\n",
    "print(f\"üîç Query: {calc_query}\\n\")\n",
    "\n",
    "result = enhanced_agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": calc_query}]\n",
    "})\n",
    "\n",
    "print(\"Response:\", result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e41008",
   "metadata": {},
   "source": [
    "## Step 8: Add Middleware (Advanced)\n",
    "\n",
    "LangChain 1.0's killer feature is **middleware** - hooks that let you control agent behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b79a9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Agent created with middleware protection\n",
      "  - Max 5 model calls per run\n",
      "  - Automatic cost control\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents.middleware import ModelCallLimitMiddleware\n",
    "\n",
    "# Limit the number of model calls to prevent runaway costs\n",
    "call_limiter = ModelCallLimitMiddleware(\n",
    "    run_limit=5,  # Max 5 LLM calls per invocation\n",
    "    exit_behavior=\"end\"  # Gracefully end if limit reached\n",
    ")\n",
    "\n",
    "# Create agent with middleware\n",
    "safe_agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[search_tool],\n",
    "    middleware=[call_limiter],\n",
    "    system_prompt=\"You are a helpful research assistant.\"\n",
    ")\n",
    "\n",
    "print(\"‚úì Agent created with middleware protection\")\n",
    "print(\"  - Max 5 model calls per run\")\n",
    "print(\"  - Automatic cost control\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51d9723a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Recent breakthroughs in quantum computing in 2023 have been significant and transformative, spanning both theoretical advancements and practical applications. Here are some key highlights:\n",
      "\n",
      "1. **Shift to Practical Implementations**: There has been a notable transition in the quantum computing field ...\n"
     ]
    }
   ],
   "source": [
    "# Test the safe agent\n",
    "result = safe_agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Tell me about recent breakthroughs in quantum computing\"}]\n",
    "})\n",
    "\n",
    "print(\"Response:\", result[\"messages\"][-1].content[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a2be5c",
   "metadata": {},
   "source": [
    "## Step 9: Conversation with Memory\n",
    "\n",
    "Let's maintain a conversation across multiple turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9de51a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí¨ Starting conversation...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start a conversation\n",
    "conversation_history = []\n",
    "\n",
    "def chat(query: str):\n",
    "    \"\"\"Send a message and get a response\"\"\"\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": query})\n",
    "    \n",
    "    result = agent.invoke({\"messages\": conversation_history})\n",
    "    \n",
    "    assistant_message = result[\"messages\"][-1]\n",
    "    conversation_history.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": assistant_message.content\n",
    "    })\n",
    "    \n",
    "    return assistant_message.content\n",
    "\n",
    "print(\"üí¨ Starting conversation...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f04f155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What is the capital of France?\n",
      "Agent: The capital of France is Paris.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First message\n",
    "response1 = chat(\"What is the capital of France?\")\n",
    "print(\"User: What is the capital of France?\")\n",
    "print(f\"Agent: {response1}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1dbd1aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What's the population of that city?\n",
      "Agent: As of recent estimates, the population of the city of Paris is approximately **2,206,488**. However, when considering the larger urban agglomeration, the population rises to about **11,346,800**. \n",
      "\n",
      "For more details, you can check the sources:\n",
      "- [World Population Review](https://worldpopulationreview.com/cities/france/paris)\n",
      "- [City Population](https://www.citypopulation.de/en/france/paris/paris/75056__paris/)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Follow-up question (tests memory)\n",
    "response2 = chat(\"What's the population of that city?\")\n",
    "print(\"User: What's the population of that city?\")\n",
    "print(f\"Agent: {response2}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f3e5422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Full Conversation History:\n",
      "================================================================================\n",
      "1. User: What is the capital of France?\n",
      "\n",
      "2. Assistant: The capital of France is Paris.\n",
      "\n",
      "3. User: What's the population of that city?\n",
      "\n",
      "4. Assistant: As of recent estimates, the population of the city of Paris is approximately **2,206,488**. However,...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View conversation history\n",
    "print(\"\\nüìù Full Conversation History:\")\n",
    "print(\"=\" * 80)\n",
    "for i, msg in enumerate(conversation_history, 1):\n",
    "    role = msg[\"role\"].capitalize()\n",
    "    content = msg[\"content\"][:100] + \"...\" if len(msg[\"content\"]) > 100 else msg[\"content\"]\n",
    "    print(f\"{i}. {role}: {content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac7a04e",
   "metadata": {},
   "source": [
    "## Step 10: Using Ollama (Local LLM Alternative)\n",
    "\n",
    "You can also use local models with `create_agent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6974964f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Create agent with local model\n",
    "local_llm = ChatOllama(model=\"mistral-nemo:12b\", temperature=0)\n",
    "\n",
    "local_agent = create_agent(\n",
    "    model=local_llm,\n",
    "    tools=[calculate],  # Just calculator for local testing\n",
    "    system_prompt=\"You are a helpful math assistant.\"\n",
    ")\n",
    "\n",
    "print(\"‚úì Local agent created with Ollama\")\n",
    "print(\"  Model: mistral-nemo:12b\")\n",
    "print(\"  Running 100% locally!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cefe6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test local agent\n",
    "result = local_agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Calculate the square root of 144\"}]\n",
    "})\n",
    "\n",
    "print(\"Response:\", result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535e09a2",
   "metadata": {},
   "source": [
    "## Comparison: `create_agent` vs Manual LangGraph\n",
    "\n",
    "### `create_agent` (LangChain 1.0)\n",
    "‚úÖ **Pros:**\n",
    "- Simple, concise API\n",
    "- Fast to prototype\n",
    "- Built-in middleware support\n",
    "- Automatic tool routing\n",
    "- Good for 80% of use cases\n",
    "\n",
    "‚ö†Ô∏è **Cons:**\n",
    "- Less control over routing logic\n",
    "- Black-box behavior for complex flows\n",
    "- Harder to customize graph structure\n",
    "\n",
    "### Manual LangGraph\n",
    "‚úÖ **Pros:**\n",
    "- Full control over graph structure\n",
    "- Custom routing logic\n",
    "- Complex multi-agent orchestration\n",
    "- Conditional edges and loops\n",
    "- Great for complex workflows\n",
    "\n",
    "‚ö†Ô∏è **Cons:**\n",
    "- More verbose code\n",
    "- Steeper learning curve\n",
    "- More boilerplate\n",
    "\n",
    "### When to Use Each\n",
    "\n",
    "| Use Case | Recommended Approach |\n",
    "|----------|---------------------|\n",
    "| Simple Q&A with tools | `create_agent` |\n",
    "| Research assistant | `create_agent` |\n",
    "| Customer support bot | `create_agent` |\n",
    "| Multi-step workflows | LangGraph |\n",
    "| Multi-agent systems | LangGraph |\n",
    "| Complex conditional routing | LangGraph |\n",
    "| Custom error handling | LangGraph |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f36b5b",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "‚úÖ **`create_agent` simplicity**: Create powerful agents in just a few lines  \n",
    "‚úÖ **Tool integration**: Easy to add search, calculations, and custom tools  \n",
    "‚úÖ **Middleware power**: Advanced control without complexity  \n",
    "‚úÖ **Streaming support**: Real-time responses for better UX  \n",
    "‚úÖ **Memory management**: Built-in conversation history  \n",
    "\n",
    "### Best Practices\n",
    "1. **Start simple**: Use `create_agent` first, switch to LangGraph if needed\n",
    "2. **Clear system prompts**: Guide your agent's behavior\n",
    "3. **Tool descriptions**: Help the agent understand when to use each tool\n",
    "4. **Add middleware**: Use limiters to prevent runaway costs\n",
    "5. **Test both modes**: Try streaming and regular invoke\n",
    "\n",
    "### Next Steps\n",
    "1. **Add more tools**: File operations, database queries, API calls\n",
    "2. **Explore middleware**: Guardrails, summarization, human-in-the-loop\n",
    "3. **Try checkpointers**: Persist conversations across sessions\n",
    "4. **Learn LangGraph**: For complex orchestration needs\n",
    "5. **Build production apps**: Add error handling, logging, monitoring\n",
    "\n",
    "### Resources\n",
    "- [LangChain 1.0 Docs](https://docs.langchain.com/)\n",
    "- [create_agent API Reference](https://python.langchain.com/docs/modules/agents/)\n",
    "- [Middleware Guide](https://docs.langchain.com/oss/python/langchain/middleware)\n",
    "- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-with-haystack-ch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
