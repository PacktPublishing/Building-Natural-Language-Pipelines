{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with LLMs from OpenAI through its API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import initialization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"./../../.env\")\n",
    "\n",
    "# Open AI API Key\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load environment variables from .env file if it exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"./../../.env\")\n",
    "\n",
    "# Open AI API Key\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"role\": \"assistant\",\n",
      "  \"content\": \"Hello! How can I assist you today?\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "completion = openai.ChatCompletion.create( \n",
    "  model=\"gpt-3.5-turbo\", \n",
    "  messages=[ \n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, \n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"} \n",
    "  ] \n",
    ") \n",
    "\n",
    "print(completion.choices[0].message) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with Transformers through HuggingFace's API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing model access and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0472a630d6ab4925a6e02e25e0a2a609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/222 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a30062da06534e2d9c079d7a202f323d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c2c3fb9c4df41509b2fc8d869501679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c7d2f0dc54f49dab3a3dcc2ee5ce9a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/595 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9931c6ee59f4f9cb6ab9db21e4ee81f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)fetensors.index.json:   0%|          | 0.00/60.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd81323c257f4e0981b6a34f388bf02e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7136d79062f545e7a70a1e074459bbdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00072.safetensors:   0%|          | 0.00/7.19G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"bigscience/bloomz\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization: Before feeding text to a model, you need to tokenize it. Using the above tokenizer, you can tokenize a sentence like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(\"Translate to English: Je t’aime.\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Inference: Once tokenized, you can feed the input tensors to the model to get predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/macpro/Documents/GitHub/Building-Natural-Language-Pipelines/ch2/jupyter-notebooks/interacting-with-llms.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/macpro/Documents/GitHub/Building-Natural-Language-Pipelines/ch2/jupyter-notebooks/interacting-with-llms.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(inputs)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with LLMs from various providers using Haystack "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt node initialization - no base instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes import PromptTemplate, AnswerParser, PromptNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt node\n",
    "pn = PromptNode(\"gpt-3.5-turbo\", \n",
    "                api_key=openai_api_key, \n",
    "                model_kwargs={\"stream\":False}, \n",
    "                max_length=1000)\n",
    "\n",
    "# Create a prompt\n",
    "prompt = \"What are the three most interesting things about Berlin? Be brief in your answer.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a prompt template\n",
    "tourist_recommendation_prompt = PromptTemplate(\n",
    "    prompt=\"\"\"You are a helpful assistant to a tourists looking find places to\\\n",
    "            visit. You provide information on wether, tourist attractions\\\n",
    "            The tourist may or may not know what city they are interested in\\\n",
    "            so you recommend based on their request.\\\n",
    "            If the request is vague, ask the tourist to provide more information\\\n",
    "            such as the type of wether they want, what kind of experience they want\\\n",
    "            weather it is family friendly or not. If the tourist is interested in\\\n",
    "            a specific city, ask them to specify the city.\\\n",
    "            and cost of visiting\" You answer:\"\"\",\n",
    "    output_parser=AnswerParser(),\n",
    ")\n",
    "pn = PromptNode(\"gpt-3.5-turbo\", \n",
    "                api_key=openai_api_key, \n",
    "                model_kwargs={\"stream\":False},\n",
    "                default_prompt_template=tourist_recommendation_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'answers': [<Answer {'answer': \"Sure, I can help you with that! Could you please provide me with some more information? What type of weather are you looking for? Do you prefer a specific city or are you open to suggestions? Also, could you let me know if you have any specific interests or if you are traveling with family? Additionally, it would be helpful to know your budget for visiting. With these details, I'll be able to recommend some great tourist attractions for you!\", 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_ids': None, 'meta': {'prompt': \"You are a helpful assistant to a tourists looking find places to            visit. You provide information on wether, tourist attractions            The tourist may or may not know what city they are interested in            so you recommend based on their request.            If the request is vague, ask the tourist to provide more information            such as the type of wether they want, what kind of experience they want            weather it is family friendly or not. If the tourist is interested in            a specific city, ask them to specify the city.            and cost of visiting' You answer:\"}}>],\n",
       "  'invocation_context': {'query': 'I am looking for a place with tropical wether in America',\n",
       "   'answers': [<Answer {'answer': \"Sure, I can help you with that! Could you please provide me with some more information? What type of weather are you looking for? Do you prefer a specific city or are you open to suggestions? Also, could you let me know if you have any specific interests or if you are traveling with family? Additionally, it would be helpful to know your budget for visiting. With these details, I'll be able to recommend some great tourist attractions for you!\", 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_ids': None, 'meta': {'prompt': \"You are a helpful assistant to a tourists looking find places to            visit. You provide information on wether, tourist attractions            The tourist may or may not know what city they are interested in            so you recommend based on their request.            If the request is vague, ask the tourist to provide more information            such as the type of wether they want, what kind of experience they want            weather it is family friendly or not. If the tourist is interested in            a specific city, ask them to specify the city.            and cost of visiting' You answer:\"}}>],\n",
       "   'prompts': [\"You are a helpful assistant to a tourists looking find places to            visit. You provide information on wether, tourist attractions            The tourist may or may not know what city they are interested in            so you recommend based on their request.            If the request is vague, ask the tourist to provide more information            such as the type of wether they want, what kind of experience they want            weather it is family friendly or not. If the tourist is interested in            a specific city, ask them to specify the city.            and cost of visiting' You answer:\"]}},\n",
       " 'output_1')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pn.run(\"I am looking for a place with tropical wether in America\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompting LLMs from HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes import PromptNode\n",
    "\n",
    "# Initalize the node passing the model:\n",
    "prompt_node = PromptNode(model_name_or_path=\"google/flan-t5-xl\")\n",
    "\n",
    "# Go ahead and ask a question:\n",
    "prompt_node(\"What is the best city in Europe to live in?\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-pipelines",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
