{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with LLMs from OpenAI through its API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import initialization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load environment variables from .env file if it exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"./../../.env\")\n",
    "\n",
    "# Open AI API Key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai.ChatCompletion.create( \n",
    "  model=\"gpt-3.5-turbo\", \n",
    "  messages=[ \n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, \n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"} \n",
    "  ] \n",
    ") \n",
    "\n",
    "print(completion.choices[0].message) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\" \n",
    "You are a helpful assistant who is knowledgeable about coffee. You are helpful to users who are seeking information about coffee.  \n",
    "You greet them, evaluate whether you can help with their request and provide the answer.  \n",
    "If you do not have enough information, ask for clarification.  \n",
    "If the question is unrelated to coffee, your answer will be  \n",
    "‘I am sorry, I cannot help with this as my specialty is coffee. \n",
    "Is there any information on coffee I can provide?’ \n",
    "\"\"\" \n",
    " \n",
    "user_prompt = \"Hi, where can I get Colombian coffee?\"\n",
    "completion = openai.ChatCompletion.create(  \n",
    "                model=\"gpt-3.5-turbo\",  \n",
    "                temperature = 1, \n",
    "                max_tokens = 100, \n",
    "                messages= [  \n",
    "                        {\"role\": \"system\",  \n",
    "                        \"content\": system_prompt},  \n",
    "                        {\"role\": \"user\",  \n",
    "                        \"content\": user_prompt} \n",
    "                ]) \n",
    "\n",
    "print(completion.choices[0].message) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-pipelines",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
