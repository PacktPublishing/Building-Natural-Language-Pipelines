{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with LLMs from OpenAI through its API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import initialization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load environment variables from .env file if it exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"./../../.env\")\n",
    "\n",
    "# Open AI API Key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai.ChatCompletion.create( \n",
    "  model=\"gpt-3.5-turbo\", \n",
    "  messages=[ \n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, \n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"} \n",
    "  ] \n",
    ") \n",
    "\n",
    "print(completion.choices[0].message) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with Transformers through HuggingFace's API\n",
    "\n",
    "**Important** It is recommended that you execute this notebook on a GPU-enabled machine. You can do this from Google Colab as follows:\n",
    "\n",
    "1. Visit https://colab.research.google.com/\n",
    "2. Click on the GitHub tab\n",
    "3. Paste the URL to the repository or notebook. You will need to select A100 GPU runtime. \n",
    "\n",
    "https://github.com/PacktPublishing/Building-Natural-Language-Pipelines\n",
    "\n",
    "\n",
    "![](./images/colab.png)\n",
    "\n",
    "![](./images/runtime.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing model access and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"bigscience/mt0-small\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization: Before feeding text to a model, you need to tokenize it. Using the above tokenizer, you can tokenize a sentence like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(\"Translate to English: Je tâ€™aime.\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Inference: Once tokenized, you can feed the input tensors to the model to get predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(inputs)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with LLMs from various providers using Haystack "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt node initialization - no base instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes import PromptTemplate, AnswerParser, PromptNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt node\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "pn = PromptNode(\"gpt-3.5-turbo\", \n",
    "                api_key=openai_api_key, \n",
    "                model_kwargs={\"stream\":False}, \n",
    "                max_length=1000)\n",
    "\n",
    "# Create a prompt\n",
    "prompt = \"What are the three most interesting things about Berlin? Be brief in your answer.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a prompt template\n",
    "tourist_recommendation_prompt = PromptTemplate(\n",
    "    prompt=\"\"\"You are a helpful assistant to a tourists looking find places to\\\n",
    "            visit. You provide information on wether, tourist attractions\\\n",
    "            The tourist may or may not know what city they are interested in\\\n",
    "            so you recommend based on their request.\\\n",
    "            If the request is vague, ask the tourist to provide more information\\\n",
    "            such as the type of wether they want, what kind of experience they want\\\n",
    "            weather it is family friendly or not. If the tourist is interested in\\\n",
    "            a specific city, ask them to specify the city.\\\n",
    "            and cost of visiting\" You answer:\"\"\",\n",
    "    output_parser=AnswerParser(),\n",
    ")\n",
    "pn = PromptNode(\"gpt-3.5-turbo\", \n",
    "                api_key=openai_api_key, \n",
    "                model_kwargs={\"stream\":False},\n",
    "                default_prompt_template=tourist_recommendation_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.run(\"I am looking for a place with tropical wether in America\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompting LLMs from HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes import PromptNode\n",
    "\n",
    "# Initalize the node passing the model:\n",
    "prompt_node = PromptNode(model_name_or_path=\"google/flan-t5-xl\")\n",
    "\n",
    "# Go ahead and ask a question:\n",
    "prompt_node(\"What is the best city in Europe to live in?\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-pipelines",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
