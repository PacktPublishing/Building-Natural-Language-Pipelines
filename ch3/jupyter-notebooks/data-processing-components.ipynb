{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing components\n",
    "\n",
    "| Component category | Component name |\n",
    "| --- | --- |\n",
    "| Data preprocessing | DocumentCleaner |\n",
    "| Data preprocessing | DocumentSplitter |\n",
    "| Data extraction | LinkContentFetcher |\n",
    "| Data caching | URLCacheChecker |\n",
    "| Audio to text processing | LocalWhisperTranscriber |\n",
    "| Audio to text processing | RemoteWhisperTranscriber |\n",
    "| File converter | AzureOCRDocumentConverter |\n",
    "| File converter | HTMLToDocument |\n",
    "| File converter | MarkdownToDocument |\n",
    "| File converter | PyPDFToDocument |\n",
    "| File converter | TikaDocumentConverter |\n",
    "| File converter | TextFileToDocument |\n",
    "| Language classifier | DocumentLanguageClassifier |\n",
    "| Language classifier | TextLanguageClassifier |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install haystack-ai farm-haystack "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Cleaner\n",
    "\n",
    "Exercise: Remove white spaces and punctuation from a document using the DocumentCleaner component.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.preprocessors import DocumentCleaner \n",
    "from haystack.dataclasses import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple instance removing extra white spaces, specific characters. We can also remove special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Document Content: This is a simple document With some extra spaces and punctuation\n"
     ]
    }
   ],
   "source": [
    "# Define a regular expression for removing exclamation marks and punctuation\n",
    "punctuation_regex = r\"[!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~]\"\n",
    "\n",
    "# Create an instance of DocumentCleaner with the regex\n",
    "cleaner = DocumentCleaner(\n",
    "    remove_empty_lines=True,\n",
    "    remove_extra_whitespaces=True,\n",
    "    remove_repeated_substrings=False,\n",
    "    remove_substrings=punctuation_regex,\n",
    "    remove_regex=None\n",
    ")\n",
    "\n",
    "# Sample document with exclamation marks and punctuation\n",
    "sample_document = Document(content=\"This is a simple document! <<With some extra spaces... and punctuation!!\", meta={\"name\": \"test_doc\"})\n",
    "\n",
    "# Using the cleaner\n",
    "cleaned_documents = cleaner.run([sample_document])\n",
    "\n",
    "# Extracting the cleaned document\n",
    "cleaned_document = cleaned_documents['documents'][0]\n",
    "\n",
    "# Output the cleaned content\n",
    "print(\"Cleaned Document Content:\", cleaned_document.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Splitter\n",
    "\n",
    "Exercise: Document Splitting for Language Model Processing\n",
    "\n",
    "Objective:\n",
    "\n",
    "Write a Python script to split a long text document into smaller segments using the DocumentSplitter component. The script should be able to handle splitting by words, sentences, or passages. You'll test the splitter with different configurations and observe how it affects the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.preprocessors import DocumentSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Word Splits ---\n",
      "Segment 1:\n",
      "\n",
      "Your long text content goes here. It should include multiple paragraphs, sentences, and a variety of words.\n",
      "...\n",
      "\n",
      "\n",
      "--- Sentence Splits ---\n",
      "Segment 1:\n",
      "\n",
      "Your long text content goes here. It should include multiple paragraphs, sentences, and a variety of words.\n",
      "...\n",
      "\n",
      "Segment 2:\n",
      ".\n",
      "\n",
      "\n",
      "--- Passage Splits ---\n",
      "Segment 1:\n",
      "\n",
      "Your long text content goes here. It should include multiple paragraphs, sentences, and a variety of words.\n",
      "...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming all necessary imports are done and DocumentSplitter class is defined\n",
    "\n",
    "# Create a long text document\n",
    "text_content = \"\"\"\n",
    "Your long text content goes here. It should include multiple paragraphs, sentences, and a variety of words.\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "# Create a Document object\n",
    "long_document = Document(content=text_content, meta={\"name\": \"long_text_doc\"})\n",
    "\n",
    "# Initialize DocumentSplitters with different configurations\n",
    "word_splitter = DocumentSplitter(split_by=\"word\", split_length=50, split_overlap=10)\n",
    "sentence_splitter = DocumentSplitter(split_by=\"sentence\", split_length=5, split_overlap=1)\n",
    "passage_splitter = DocumentSplitter(split_by=\"passage\", split_length=2, split_overlap=0)\n",
    "\n",
    "# Function to print split documents\n",
    "def print_splits(documents, title):\n",
    "    print(f\"--- {title} ---\")\n",
    "    for i, doc in enumerate(documents['documents'], 1):\n",
    "        print(f\"Segment {i}:\\n{doc.content}\\n\")\n",
    "\n",
    "# Split the document in different ways\n",
    "word_splits = word_splitter.run([long_document])\n",
    "sentence_splits = sentence_splitter.run([long_document])\n",
    "passage_splits = passage_splitter.run([long_document])\n",
    "\n",
    "# Print the results\n",
    "print_splits(word_splits, \"Word Splits\")\n",
    "print_splits(sentence_splits, \"Sentence Splits\")\n",
    "print_splits(passage_splits, \"Passage Splits\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching data from a link\n",
    "\n",
    "Exercise: Implementing and Testing LinkContentFetcher\n",
    "\n",
    "Objective:\n",
    "\n",
    "In this exercise, you will implement and test the LinkContentFetcher component to fetch and extract content from various URLs. This component is designed to handle different content types, retry on failures, and rotate user agents for web requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.fetchers import LinkContentFetcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://en.wikipedia.org/wiki/Barbie_(film)\n",
      "Content Type: text/html\n",
      "First 10 characters: b'<!DOCTYPE ' ...\n",
      "\n",
      "\n",
      "URL: https://en.wikipedia.org/wiki/Oppenheimer_(film)\n",
      "Content Type: text/html\n",
      "First 10 characters: b'<!DOCTYPE ' ...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize LinkContentFetcher\n",
    "fetcher = LinkContentFetcher(\n",
    "    raise_on_failure=False,\n",
    "    user_agents=[\"UserAgent1\", \"UserAgent2\"],\n",
    "    retry_attempts=3,\n",
    "    timeout=5\n",
    ")\n",
    "\n",
    "# List of URLs to test\n",
    "urls = [\n",
    "    \"https://en.wikipedia.org/wiki/Barbie_(film)\",\n",
    "    \"https://en.wikipedia.org/wiki/Oppenheimer_(film)\",\n",
    "]\n",
    "\n",
    "# Fetch content from URLs\n",
    "results = fetcher.run(urls)\n",
    "\n",
    "# Analyze the fetched content\n",
    "for stream in results['streams']:\n",
    "    print(f\"URL: {stream.meta['url']}\")\n",
    "    print(f\"Content Type: {stream.meta['content_type']}\")\n",
    "    print(f\"First 10 characters: {stream.data[:10]} ...\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This extracts the content of a website and stores it into a `ByteStream` data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results['streams']\n",
    "\n",
    "###ByteStream(data=b'<!DOCTYPE html>\\n<html class=\"client-nojs vector-feature-language-in-header-enabled \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save this into a Document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.dataclasses import Document\n",
    "\n",
    "web_document_barbie = Document(blob=results['streams'][0], meta=results['streams'][0].meta)\n",
    "web_document_oppenheimer = Document(blob=results['streams'][1], meta=results['streams'][1].meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then save our Documents into a Document store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from haystack.document_stores.in_memory.document_store import InMemoryDocumentStore\n",
    "\n",
    "sample_docstore = InMemoryDocumentStore()\n",
    "web_docs = [web_document_barbie, web_document_oppenheimer]\n",
    "sample_docstore.write_documents(documents=web_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing URLCacheChecker\n",
    "\n",
    "In this exercise, you will implement the UrlCacheChecker component, which checks for the presence of documents from specific URLs in a document store. The goal is to understand how to implement caching functionality in web retrieval pipelines using a document store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hits (Found in Store):\n",
      "URL: https://en.wikipedia.org/wiki/Oppenheimer_(film) - Content: b'<!DOCTYPE ' ... \n",
      "\n",
      "Misses (Not Found in Store):\n",
      "https://en.wikipedia.org/wiki/Avengers:_Endgame\n"
     ]
    }
   ],
   "source": [
    "from haystack.components.caching import CacheChecker\n",
    "\n",
    "# Initialize UrlCacheChecker\n",
    "url_cache_checker = CacheChecker(document_store=sample_docstore, cache_field='url')\n",
    "\n",
    "# List of URLs to check\n",
    "urls_to_check = [\n",
    "    \"https://en.wikipedia.org/wiki/Oppenheimer_(film)\", # This URL should be a hit\n",
    "    \"https://en.wikipedia.org/wiki/Avengers:_Endgame\",  # This URL should be a miss\n",
    "\n",
    "]\n",
    "\n",
    "# Run UrlCacheChecker\n",
    "cache_results = url_cache_checker.run(urls_to_check)\n",
    "\n",
    "# Analyze Results\n",
    "print(\"Hits (Found in Store):\")\n",
    "for doc in cache_results['hits']:\n",
    "    print(f\"URL: {doc.meta['url']} - Content: {doc.blob.data[0:10]} ... \")\n",
    "\n",
    "print(\"\\nMisses (Not Found in Store):\")\n",
    "for url in cache_results['misses']:\n",
    "    print(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RemoteWhisperTranscriber\n",
    "\n",
    "Objective:\n",
    "\n",
    "Write a Python script to use the RemoteWhisperTranscriber component for transcribing audio files using OpenAI's Whisper API. The goal is to understand how to interact with remote machine learning models for audio transcription.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macpro/anaconda3/envs/llm-pipelines/lib/python3.10/site-packages/whisper/timing.py:58: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def backtrace(trace: np.ndarray):\n"
     ]
    }
   ],
   "source": [
    "from haystack.components.audio import RemoteWhisperTranscriber\n",
    "from haystack.dataclasses import ByteStream\n",
    "from pathlib import Path\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribed Text: The stale smell of old beer lingers. It takes heat to bring out the odor. A cold dip restores health and zest. A salt pickle tastes fine with ham. Tacos al pastor are my favorite. A zestful food is the hot cross bun.\n",
      "Metadata: {}\n",
      "-----------\n",
      "Transcribed Text: The stale smell of old beer lingers.\n",
      "Metadata: {}\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize RemoteWhisperTranscriber with your OpenAI API key\n",
    "load_dotenv(\"../../.env\")\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "transcriber = RemoteWhisperTranscriber()\n",
    "\n",
    "# List of audio file paths\n",
    "audio_file_paths = [\"./audio-files/harvard.wav\", \"./audio-files/jackhammer.wav\"]\n",
    "\n",
    "# Convert audio files to ByteStream objects\n",
    "audio_streams = [ByteStream.from_file_path(Path(file_path)) for file_path in audio_file_paths]\n",
    "\n",
    "# Transcribe audio files\n",
    "transcription_results = transcriber.run(audio_streams)\n",
    "\n",
    "# Process and display results\n",
    "for doc in transcription_results['documents']:\n",
    "    print(f\"Transcribed Text: {doc.content}\")\n",
    "    print(f\"Metadata: {doc.meta}\")\n",
    "    print(\"-----------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-pipelines",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
