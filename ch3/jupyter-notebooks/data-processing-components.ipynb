{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing components\n",
    "\n",
    "| Component category | Component name |\n",
    "| --- | --- |\n",
    "| Data preprocessing | DocumentCleaner |\n",
    "| Data preprocessing | DocumentSplitter |\n",
    "| Data extraction | LinkContentFetcher |\n",
    "| Data caching | URLCacheChecker |\n",
    "| Audio to text processing | LocalWhisperTranscriber |\n",
    "| Audio to text processing | RemoteWhisperTranscriber |\n",
    "| File converter | AzureOCRDocumentConverter |\n",
    "| File converter | HTMLToDocument |\n",
    "| File converter | MarkdownToDocument |\n",
    "| File converter | PyPDFToDocument |\n",
    "| File converter | TikaDocumentConverter |\n",
    "| File converter | TextFileToDocument |\n",
    "| Language classifier | DocumentLanguageClassifier |\n",
    "| Language classifier | TextLanguageClassifier |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Cleaner\n",
    "\n",
    "Exercise: Remove white spaces and punctuation from a document using the DocumentCleaner component.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.preview.components.preprocessors import DocumentCleaner \n",
    "from haystack.preview.dataclasses import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple instance removing extra white spaces, specific characters. We can also remove special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Document Content: This is a simple document With some extra spaces and punctuation\n"
     ]
    }
   ],
   "source": [
    "# Define a regular expression for removing exclamation marks and punctuation\n",
    "punctuation_regex = r\"[!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~]\"\n",
    "\n",
    "# Create an instance of DocumentCleaner with the regex\n",
    "cleaner = DocumentCleaner(\n",
    "    remove_empty_lines=True,\n",
    "    remove_extra_whitespaces=True,\n",
    "    remove_repeated_substrings=False,\n",
    "    remove_substrings=punctuation_regex,\n",
    "    remove_regex=None\n",
    ")\n",
    "\n",
    "# Sample document with exclamation marks and punctuation\n",
    "sample_document = Document(content=\"This is a simple document! <<With some extra spaces... and punctuation!!\", meta={\"name\": \"test_doc\"})\n",
    "\n",
    "# Using the cleaner\n",
    "cleaned_documents = cleaner.run([sample_document])\n",
    "\n",
    "# Extracting the cleaned document\n",
    "cleaned_document = cleaned_documents['documents'][0]\n",
    "\n",
    "# Output the cleaned content\n",
    "print(\"Cleaned Document Content:\", cleaned_document.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Splitter\n",
    "\n",
    "Exercise: Document Splitting for Language Model Processing\n",
    "\n",
    "Objective:\n",
    "\n",
    "Write a Python script to split a long text document into smaller segments using the DocumentSplitter component. The script should be able to handle splitting by words, sentences, or passages. You'll test the splitter with different configurations and observe how it affects the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.preview.components.preprocessors import DocumentSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Word Splits ---\n",
      "Segment 1:\n",
      "\n",
      "Your long text content goes here. It should include multiple paragraphs, sentences, and a variety of words.\n",
      "...\n",
      "\n",
      "\n",
      "--- Sentence Splits ---\n",
      "Segment 1:\n",
      "\n",
      "Your long text content goes here. It should include multiple paragraphs, sentences, and a variety of words.\n",
      "...\n",
      "\n",
      "Segment 2:\n",
      ".\n",
      "\n",
      "\n",
      "--- Passage Splits ---\n",
      "Segment 1:\n",
      "\n",
      "Your long text content goes here. It should include multiple paragraphs, sentences, and a variety of words.\n",
      "...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming all necessary imports are done and DocumentSplitter class is defined\n",
    "\n",
    "# Create a long text document\n",
    "text_content = \"\"\"\n",
    "Your long text content goes here. It should include multiple paragraphs, sentences, and a variety of words.\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "# Create a Document object\n",
    "long_document = Document(content=text_content, meta={\"name\": \"long_text_doc\"})\n",
    "\n",
    "# Initialize DocumentSplitters with different configurations\n",
    "word_splitter = DocumentSplitter(split_by=\"word\", split_length=50, split_overlap=10)\n",
    "sentence_splitter = DocumentSplitter(split_by=\"sentence\", split_length=5, split_overlap=1)\n",
    "passage_splitter = DocumentSplitter(split_by=\"passage\", split_length=2, split_overlap=0)\n",
    "\n",
    "# Function to print split documents\n",
    "def print_splits(documents, title):\n",
    "    print(f\"--- {title} ---\")\n",
    "    for i, doc in enumerate(documents['documents'], 1):\n",
    "        print(f\"Segment {i}:\\n{doc.content}\\n\")\n",
    "\n",
    "# Split the document in different ways\n",
    "word_splits = word_splitter.run([long_document])\n",
    "sentence_splits = sentence_splitter.run([long_document])\n",
    "passage_splits = passage_splitter.run([long_document])\n",
    "\n",
    "# Print the results\n",
    "print_splits(word_splits, \"Word Splits\")\n",
    "print_splits(sentence_splits, \"Sentence Splits\")\n",
    "print_splits(passage_splits, \"Passage Splits\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-pipelines",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
