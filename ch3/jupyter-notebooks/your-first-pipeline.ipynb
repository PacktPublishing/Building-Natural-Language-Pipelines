{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fa4f5b0",
   "metadata": {},
   "source": [
    "ðŸ”§ **Setup Required**: Before running this notebook, please follow the [setup instructions](../README.md#setup-instructions) to configure your environment and API keys."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af50d406",
   "metadata": {},
   "source": [
    "# Building Your First Haystack Pipeline\n",
    "This notebook walks you through the process of building and running a simple Haystack pipeline using prompt building and LLM generation components. Each step is explained to help you understand the pipeline structure and execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07310d06",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "We begin by loading environment variables (such as API keys) from a `.env` file. This keeps sensitive information secure and out of the codebase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7cd63bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from.env file\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "# You can now access the API key using os.getenv\n",
    "# openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af763efc",
   "metadata": {},
   "source": [
    "## 2. Preparing Data and Components\n",
    "We create a list of `Document` objects, each representing a piece of text with optional metadata. We also define a prompt template and instantiate the prompt builder and LLM generator components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdd45053",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.utils import Secret\n",
    "from haystack.dataclasses import Document\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# --- Prepare Data and Components ---\n",
    "\n",
    "# We'll use some simple documents for this example\n",
    "documents_for_pipeline = [\n",
    "    Document(\n",
    "        content=\"Rose Island was a micronation located in the Adriatic Sea. It declared independence in 1968 and had Italian as its official language.\",\n",
    "        meta={\"source\": \"history_docs\", \"author\": \"historian1\"},\n",
    "    ),\n",
    "    Document(\n",
    "        content=\"The capital of Rose Island was called 'Isola delle Rose'. It was known for its unique architecture and vibrant culture.\",\n",
    "        meta={\"source\": \"geography_docs\", \"author\": \"geographer1\"},\n",
    "    ),\n",
    "    Document(\n",
    "        content=\"Rose Island's economy was primarily based on tourism and the sale of souvenirs to visitors.\",\n",
    "        meta={\"source\": \"economy_docs\", \"author\": \"economist1\"},\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Define the prompt template again\n",
    "prompt_template_for_pipeline = \"\"\"\n",
    "Answer the question based on this context.\n",
    "\n",
    "Context:\n",
    "{% for doc in documents %}\n",
    "- {{ doc.content }}\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{ query }}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Instantiate the components we will use\n",
    "prompt_builder_inst = PromptBuilder(template=prompt_template_for_pipeline,\n",
    "                                    required_variables=\"*\")\n",
    "llm_generator_inst = OpenAIGenerator(api_key=Secret.from_env_var(\"OPENAI_API_KEY\"), model=\"gpt-4o-mini\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb522788",
   "metadata": {},
   "source": [
    "## 3. Building the Pipeline\n",
    "We instantiate a `Pipeline` object, add our components to it, and connect them. This defines the data flow: the prompt builder creates a prompt, which is then sent to the LLM generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e585d48c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x10f9c89b0>\n",
       "ðŸš… Components\n",
       "  - prompter: PromptBuilder\n",
       "  - llm: OpenAIGenerator\n",
       "ðŸ›¤ï¸ Connections\n",
       "  - prompter.prompt -> llm.prompt (str)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Build the Pipeline ---\n",
    "\n",
    "# 1. Instantiate the Pipeline\n",
    "basic_pipeline = Pipeline()\n",
    "\n",
    "# 2. Add Component Instances\n",
    "# We give each component a unique name: \"prompter\" and \"llm\".\n",
    "basic_pipeline.add_component(name=\"prompter\", instance=prompt_builder_inst)\n",
    "basic_pipeline.add_component(name=\"llm\", instance=llm_generator_inst)\n",
    "\n",
    "# 3. Connect the Components\n",
    "# This is the crucial step that defines the data flow.\n",
    "# We connect the 'prompt' output of the 'prompter' component\n",
    "# to the 'prompt' input of the 'llm' component.\n",
    "basic_pipeline.connect(\"prompter.prompt\", \"llm.prompt\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b627c59",
   "metadata": {},
   "source": [
    "## 5. Visualizing the Pipeline\n",
    "Haystack can visualize your pipeline as a graph, making it easier to understand the flow of data and components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82d7ee10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pipeline visualization saved to './images/prompt_pipeline.png'\n"
     ]
    }
   ],
   "source": [
    "basic_pipeline.draw(path=\"./images/prompt_pipeline.png\")\n",
    "\n",
    "print(\"\\nPipeline visualization saved to './images/prompt_pipeline.png'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a1ff88",
   "metadata": {},
   "source": [
    "![](./images/prompt_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca4e999",
   "metadata": {},
   "source": [
    "## 4. Running the Pipeline\n",
    "We provide input data (a query and documents) and execute the pipeline. The output is the LLM's answer to the question, based on the provided context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e950284f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Result:\n",
      "{'llm': {'replies': ['The official language of Rose Island was Italian.'], 'meta': [{'model': 'gpt-4o-mini-2024-07-18', 'index': 0, 'finish_reason': 'stop', 'usage': {'completion_tokens': 9, 'prompt_tokens': 103, 'total_tokens': 112, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}}]}}\n"
     ]
    }
   ],
   "source": [
    "# --- Run the Pipeline ---\n",
    "\n",
    "# The.run() method takes a dictionary as input.\n",
    "# The keys of the dictionary correspond to the names of the components in the pipeline.\n",
    "# The values are dictionaries of the inputs for that component.\n",
    "query_text = \"What was the official language of Rose Island?\"\n",
    "\n",
    "run_data = {\n",
    "    \"prompter\": {\n",
    "        \"query\": query_text,\n",
    "        \"documents\": documents_for_pipeline\n",
    "    }\n",
    "}\n",
    "\n",
    "# Execute the pipeline\n",
    "pipeline_result = basic_pipeline.run(run_data)\n",
    "\n",
    "# The output is a dictionary with the final results from the terminal component(s).\n",
    "print(\"Pipeline Result:\")\n",
    "print(pipeline_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d8c7f7",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary and Next Steps\n",
    "You have now built and run a simple Haystack pipeline! Try modifying the documents, prompt template, or components to experiment with different pipeline behaviors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-with-haystack-ch3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
