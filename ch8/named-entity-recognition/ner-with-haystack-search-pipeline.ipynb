{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.components.preprocessors import DocumentCleaner\n",
    "from haystack.components.websearch import SerperDevWebSearch\n",
    "from haystack.components.fetchers import LinkContentFetcher\n",
    "from haystack.components.converters import HTMLToDocument\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack import Pipeline\n",
    "from haystack.components.extractors import NamedEntityExtractor\n",
    "from haystack import component, Document\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "open_ai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "serper_api_key = os.getenv(\"SERPERDEV_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define custom component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component\n",
    "class NERPopulator():\n",
    "    \"\"\"This function extracts named entities from a list of\n",
    "    documents and returns the result in a structured format.\n",
    "\n",
    "    Args:\n",
    "        documents (list): List of Haystack Document objects\n",
    "\n",
    "    Returns:\n",
    "        extracted_data (list): A list of dictionaries containing the extracted entities, \n",
    "        to make it Haystack-compatible we will return this list as a dictionary with the key 'documents'\n",
    "    \"\"\"\n",
    "    \n",
    "    @component.output_types(documents=List[Document])\n",
    "    def run(self, sources: List[Document]) -> None:\n",
    "        extracted_data = []\n",
    "\n",
    "        for document in sources:\n",
    "            content = document.content\n",
    "            doc_id = document.id\n",
    "            named_entities = document.meta.get('named_entities', [])\n",
    "            url = document.meta.get('url', 'N/A')  # Default to 'N/A' if URL is not available\n",
    "\n",
    "            # Sets to store unique entities by type\n",
    "            entities_by_type = {\n",
    "                \"LOC\": set(),\n",
    "                \"PER\": set(),\n",
    "                \"ORG\": set(),\n",
    "                \"MISC\": set()\n",
    "            }\n",
    "            \n",
    "            # Loop through the entities and filter by score and type\n",
    "            for entity in named_entities:\n",
    "                if float(entity.score) < 0.8:\n",
    "                    continue\n",
    "                \n",
    "                word = content[entity.start:entity.end]\n",
    "                if entity.entity in entities_by_type:\n",
    "                    entities_by_type[entity.entity].add(word)  # Use set to ensure uniqueness\n",
    "            \n",
    "            # Prepare the meta field with comma-separated values\n",
    "            meta = {\n",
    "                \"LOC\": \",\".join(entities_by_type[\"LOC\"]),\n",
    "                \"PER\": \",\".join(entities_by_type[\"PER\"]),\n",
    "                \"ORG\": \",\".join(entities_by_type[\"ORG\"]),\n",
    "                \"MISC\": \",\".join(entities_by_type[\"MISC\"]),\n",
    "                \"url\": url\n",
    "            }\n",
    "            \n",
    "            # Append the result for this document\n",
    "            extracted_data.append({\n",
    "                'document_id': doc_id,\n",
    "                'content': content,\n",
    "                'meta': meta\n",
    "            })\n",
    "        \n",
    "\n",
    "        return {\"documents\": extracted_data}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Haystack pipeline with custom component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x33a2bb110>\n",
       "🚅 Components\n",
       "  - search: SerperDevWebSearch\n",
       "  - fetcher: LinkContentFetcher\n",
       "  - htmldocument: HTMLToDocument\n",
       "  - cleaner: DocumentCleaner\n",
       "  - extractor: NamedEntityExtractor\n",
       "  - ner: NERPopulator\n",
       "🛤️ Connections\n",
       "  - search.links -> fetcher.urls (List[str])\n",
       "  - fetcher.streams -> htmldocument.sources (List[ByteStream])\n",
       "  - htmldocument.documents -> cleaner.documents (List[Document])\n",
       "  - cleaner.documents -> extractor.documents (List[Document])\n",
       "  - extractor.documents -> ner.sources (List[Document])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Initialize pipeline\n",
    "pipeline = Pipeline()\n",
    "web_search = SerperDevWebSearch(top_k=5,\n",
    "                                allowed_domains=[\"https://www.britannica.com/\"])\n",
    "link_content = LinkContentFetcher(retry_attempts=3,\n",
    "                                  timeout=10)\n",
    "html_to_doc = HTMLToDocument()\n",
    "document_cleaner = DocumentCleaner(\n",
    "                                remove_empty_lines=True,\n",
    "                                remove_extra_whitespaces=True,\n",
    "                                remove_repeated_substrings=False,\n",
    "                                remove_substrings=['\\n-']\n",
    "                            )\n",
    "extractor = NamedEntityExtractor(backend=\"hugging_face\", model=\"dslim/bert-base-NER\")\n",
    "extractor.warm_up()\n",
    "\n",
    "ner_component = NERPopulator()\n",
    "\n",
    "# Add components\n",
    "pipeline.add_component(name='search', instance=web_search)\n",
    "pipeline.add_component(name ='fetcher' , instance= link_content)\n",
    "pipeline.add_component(name='htmldocument', instance=html_to_doc)\n",
    "pipeline.add_component(name='cleaner', instance=document_cleaner)\n",
    "pipeline.add_component(name='extractor', instance=extractor)\n",
    "pipeline.add_component(name='ner', instance=ner_component)\n",
    "\n",
    "# Connect components to one another\n",
    "pipeline.connect(\"search.links\", \"fetcher.urls\")\n",
    "pipeline.connect(\"fetcher\", \"htmldocument\")\n",
    "pipeline.connect(\"htmldocument\", \"cleaner\")\n",
    "pipeline.connect(\"cleaner\", \"extractor\")\n",
    "pipeline.connect(\"extractor\", \"ner\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use pipeline to search Encyclopedia Britannica for all articles related to Elon Musk and extract entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Elon Musk\"\n",
    "output = pipeline.run(data={\"search\":{\"query\":query}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_documents = output['ner']['documents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save document as DataFrame object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>content</th>\n",
       "      <th>meta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>98b3d89f61ff370a43e5ba5c19c1de3522851ded6feed6...</td>\n",
       "      <td>Elon Musk cofounded the electronic payment fir...</td>\n",
       "      <td>{'LOC': '', 'PER': '', 'ORG': 'X,Neuralink,Pay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1db89fda1c1c46e3c66e85aefd8e4d29c8c10d3e3c9e11...</td>\n",
       "      <td>SpaceX In full: Space Exploration Technologies...</td>\n",
       "      <td>{'LOC': 'Hawthorne,Earth,U.S.,California', 'PE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bf226b982f01f238d32d66213fb8628104153b23b832bf...</td>\n",
       "      <td>SpaceX, in full Space Exploration Technologies...</td>\n",
       "      <td>{'LOC': 'Hawthorne,Earth,U.S.,California', 'PE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ae773eb9d70c2c5a7a244895bec15745cefdeef14ef3ff...</td>\n",
       "      <td>Falcon Heavy\\nLearn about this topic in these ...</td>\n",
       "      <td>{'LOC': 'U.S,United States', 'PER': '', 'ORG':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8532677167dc67f0e2c6ba7ed206c89cda5c6b3dde3ea1...</td>\n",
       "      <td>Falcon Related Topics: launch vehicle Falcon H...</td>\n",
       "      <td>{'LOC': 'Florida,Kwajalein Atoll,S,Cape Canave...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         document_id  \\\n",
       "0  98b3d89f61ff370a43e5ba5c19c1de3522851ded6feed6...   \n",
       "1  1db89fda1c1c46e3c66e85aefd8e4d29c8c10d3e3c9e11...   \n",
       "2  bf226b982f01f238d32d66213fb8628104153b23b832bf...   \n",
       "3  ae773eb9d70c2c5a7a244895bec15745cefdeef14ef3ff...   \n",
       "4  8532677167dc67f0e2c6ba7ed206c89cda5c6b3dde3ea1...   \n",
       "\n",
       "                                             content  \\\n",
       "0  Elon Musk cofounded the electronic payment fir...   \n",
       "1  SpaceX In full: Space Exploration Technologies...   \n",
       "2  SpaceX, in full Space Exploration Technologies...   \n",
       "3  Falcon Heavy\\nLearn about this topic in these ...   \n",
       "4  Falcon Related Topics: launch vehicle Falcon H...   \n",
       "\n",
       "                                                meta  \n",
       "0  {'LOC': '', 'PER': '', 'ORG': 'X,Neuralink,Pay...  \n",
       "1  {'LOC': 'Hawthorne,Earth,U.S.,California', 'PE...  \n",
       "2  {'LOC': 'Hawthorne,Earth,U.S.,California', 'PE...  \n",
       "3  {'LOC': 'U.S,United States', 'PER': '', 'ORG':...  \n",
       "4  {'LOC': 'Florida,Kwajalein Atoll,S,Cape Canave...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(extracted_documents)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP pipelines",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
