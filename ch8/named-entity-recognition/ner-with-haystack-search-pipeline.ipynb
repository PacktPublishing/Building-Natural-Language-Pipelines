{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ”§ **Setup Required**: Before running this notebook, please follow the [setup instructions](../README.md#setup-instructions) to configure your environment and API keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition with Haystack Pipelines\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates how to build an end-to-end **Named Entity Recognition (NER)** pipeline using Haystack 2.0. We'll create a system that searches the web for articles, extracts content, and automatically identifies and categorizes named entities (people, organizations, locations, and miscellaneous entities).\n",
    "\n",
    "## What You'll Learn\n",
    "By the end of this notebook, you will understand:\n",
    "1. How to build modular Haystack pipelines with multiple components\n",
    "2. How to integrate web search capabilities into NLP workflows\n",
    "3. How to extract and process HTML content from web pages\n",
    "4. How to use pre-trained NER models with Haystack\n",
    "5. How to create custom Haystack components for specialized data processing\n",
    "6. How to structure extracted entities for downstream analysis\n",
    "\n",
    "## Use Cases\n",
    "This pipeline pattern is useful for:\n",
    "- **Media Monitoring**: Track mentions of people, organizations, and places in news articles\n",
    "- **Research**: Gather and categorize information about specific topics or entities\n",
    "- **Content Analysis**: Automatically tag and organize web content by entities\n",
    "- **Knowledge Base Creation**: Extract structured information from unstructured web data\n",
    "\n",
    "## Pipeline Architecture\n",
    "Our pipeline follows this flow:\n",
    "```\n",
    "Web Search â†’ Fetch URLs â†’ Convert HTML â†’ Clean Text â†’ Extract Entities â†’ Structure Results\n",
    "```\n",
    "\n",
    "Each component performs a specific task, and they're connected to create a complete workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.components.preprocessors import DocumentCleaner\n",
    "from haystack.components.websearch import SearchApiWebSearch\n",
    "from haystack.components.fetchers import LinkContentFetcher\n",
    "from haystack.components.converters import HTMLToDocument\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.utils import Secret\n",
    "from haystack import Pipeline\n",
    "from haystack.components.extractors import NamedEntityExtractor\n",
    "from haystack import component, Document\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Load .env from the root of ch8 directory\n",
    "root_dir = Path(__file__).parent.parent if \"__file__\" in globals() else Path.cwd().parent\n",
    "load_dotenv(root_dir / \".env\")\n",
    "\n",
    "open_ai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "serper_api_key = os.getenv(\"SERPERDEV_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries\n",
    "\n",
    "We'll import all necessary components from Haystack and supporting libraries:\n",
    "\n",
    "**Haystack Components:**\n",
    "- **`Pipeline`**: The core container that orchestrates all components\n",
    "- **`SearchApiWebSearch`**: Performs web searches using SearchAPI\n",
    "- **`LinkContentFetcher`**: Downloads content from URLs\n",
    "- **`HTMLToDocument`**: Converts HTML to Haystack Document objects\n",
    "- **`DocumentCleaner`**: Preprocesses text by removing unwanted content\n",
    "- **`NamedEntityExtractor`**: Identifies named entities using ML models\n",
    "- **`Secret`**: Securely manages API keys from environment variables\n",
    "\n",
    "**Supporting Libraries:**\n",
    "- **`dotenv`**: Loads environment variables from .env file\n",
    "- **`typing`**: Provides type hints for better code documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define a Custom Haystack Component\n",
    "\n",
    "### What is a Custom Component?\n",
    "Haystack allows you to create custom components to handle specialized processing tasks. A custom component must:\n",
    "1. Be decorated with `@component`\n",
    "2. Define input/output types using `@component.output_types()`\n",
    "3. Implement a `run()` method that processes data\n",
    "\n",
    "### Why Create NERPopulator?\n",
    "The built-in `NamedEntityExtractor` identifies entities, but we need to:\n",
    "- Filter entities by confidence score (only keep high-quality predictions)\n",
    "- Remove duplicates within each entity type\n",
    "- Organize entities into a structured format (LOC, PER, ORG, MISC)\n",
    "- Preserve source URLs for traceability\n",
    "\n",
    "### Component Architecture\n",
    "Our `NERPopulator` component:\n",
    "- **Input**: List of documents with `named_entities` in metadata\n",
    "- **Processing**: Filters, deduplicates, and categorizes entities\n",
    "- **Output**: Structured documents ready for analysis or storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component\n",
    "class NERPopulator():\n",
    "    \"\"\"This function extracts named entities from a list of\n",
    "    documents and returns the result in a structured format, and saves to CSV.\n",
    "\n",
    "    Args:\n",
    "        documents (list): List of Haystack Document objects\n",
    "        output_file (str): Optional path for the CSV output file. If not provided, auto-generates filename.\n",
    "\n",
    "    Returns:\n",
    "        extracted_data (list): A list of dictionaries containing the extracted entities, \n",
    "        csv_path (str): Path to the saved CSV file\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir: str = \"./ner_results\"):\n",
    "        \"\"\"\n",
    "        Initialize NERPopulator with output directory.\n",
    "        \n",
    "        Args:\n",
    "            output_dir: Directory where CSV files will be saved\n",
    "        \"\"\"\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    @component.output_types(documents=List[Document], csv_path=str, summary=str)\n",
    "    def run(self, sources: List[Document], output_file: str = None) -> Dict[str, Any]:\n",
    "        import pandas as pd\n",
    "        from datetime import datetime\n",
    "        \n",
    "        extracted_data = []\n",
    "\n",
    "        for document in sources:\n",
    "            content = document.content\n",
    "            doc_id = document.id\n",
    "            named_entities = document.meta.get('named_entities', [])\n",
    "            url = document.meta.get('url', 'N/A')  # Default to 'N/A' if URL is not available\n",
    "\n",
    "            # Sets to store unique entities by type\n",
    "            entities_by_type = {\n",
    "                \"LOC\": set(),\n",
    "                \"PER\": set(),\n",
    "                \"ORG\": set(),\n",
    "                \"MISC\": set()\n",
    "            }\n",
    "            \n",
    "            # Loop through the entities and filter by score and type\n",
    "            for entity in named_entities:\n",
    "                if float(entity.score) < 0.8:\n",
    "                    continue\n",
    "                \n",
    "                word = content[entity.start:entity.end]\n",
    "                if entity.entity in entities_by_type:\n",
    "                    entities_by_type[entity.entity].add(word)  # Use set to ensure uniqueness\n",
    "            \n",
    "            # Prepare the meta field with comma-separated values\n",
    "            meta = {\n",
    "                \"LOC\": \",\".join(entities_by_type[\"LOC\"]),\n",
    "                \"PER\": \",\".join(entities_by_type[\"PER\"]),\n",
    "                \"ORG\": \",\".join(entities_by_type[\"ORG\"]),\n",
    "                \"MISC\": \",\".join(entities_by_type[\"MISC\"]),\n",
    "                \"url\": url\n",
    "            }\n",
    "            \n",
    "            # Append the result for this document\n",
    "            extracted_data.append({\n",
    "                'document_id': doc_id,\n",
    "                'content': content,\n",
    "                'meta': meta\n",
    "            })\n",
    "        \n",
    "        # Generate CSV filename if not provided\n",
    "        if output_file is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            output_file = f\"ner_results_{timestamp}.csv\"\n",
    "        \n",
    "        csv_path = os.path.join(self.output_dir, output_file)\n",
    "        \n",
    "        # Create DataFrame and expand meta column\n",
    "        df = pd.DataFrame(extracted_data)\n",
    "        meta_df = pd.json_normalize(df['meta'])\n",
    "        result_df = pd.concat([df.drop('meta', axis=1), meta_df], axis=1)\n",
    "        \n",
    "        # Save to CSV\n",
    "        result_df.to_csv(csv_path, index=False)\n",
    "        \n",
    "        # Create summary\n",
    "        total_docs = len(extracted_data)\n",
    "        all_entities = {\n",
    "            \"LOC\": set(),\n",
    "            \"PER\": set(),\n",
    "            \"ORG\": set(),\n",
    "            \"MISC\": set()\n",
    "        }\n",
    "        \n",
    "        for doc in extracted_data:\n",
    "            for entity_type in all_entities:\n",
    "                entities = doc['meta'][entity_type]\n",
    "                if entities:\n",
    "                    all_entities[entity_type].update(entities.split(','))\n",
    "        \n",
    "        summary = f\"Processed {total_docs} documents.\\n\"\n",
    "        summary += f\"Found {len(all_entities['PER'])} unique people, \"\n",
    "        summary += f\"{len(all_entities['ORG'])} organizations, \"\n",
    "        summary += f\"{len(all_entities['LOC'])} locations, \"\n",
    "        summary += f\"and {len(all_entities['MISC'])} miscellaneous entities.\\n\"\n",
    "        summary += f\"Results saved to: {csv_path}\"\n",
    "\n",
    "        return {\n",
    "            \"documents\": extracted_data,\n",
    "            \"csv_path\": csv_path,\n",
    "            \"summary\": summary\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the NERPopulator Implementation\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "1. **Confidence Filtering**: `if float(entity.score) < 0.8: continue`\n",
    "   - Only entities with >80% confidence are kept\n",
    "   - Reduces false positives and improves data quality\n",
    "\n",
    "2. **Entity Categories (Standard NER labels)**:\n",
    "   - **LOC**: Locations (cities, countries, landmarks)\n",
    "   - **PER**: Persons (names of people)\n",
    "   - **ORG**: Organizations (companies, institutions)\n",
    "   - **MISC**: Miscellaneous entities (events, products, etc.)\n",
    "\n",
    "3. **Deduplication with Sets**: `entities_by_type[entity.entity].add(word)`\n",
    "   - Using Python sets automatically removes duplicate entities\n",
    "   - Important when the same entity appears multiple times in text\n",
    "\n",
    "4. **Metadata Preservation**:\n",
    "   - Original URL is maintained for source tracking\n",
    "   - Entities are stored as comma-separated strings for easy export\n",
    "\n",
    "**Output Format:**\n",
    "Each processed document contains:\n",
    "- `document_id`: Unique identifier\n",
    "- `content`: Full text content\n",
    "- `meta`: Dictionary with entity categories and source URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build the Complete Haystack Pipeline\n",
    "\n",
    "### Pipeline Components Explained\n",
    "\n",
    "In this step, we'll initialize each component and connect them into a working pipeline.\n",
    "\n",
    "**Component Configuration:**\n",
    "\n",
    "1. **SearchApiWebSearch**\n",
    "   - `top_k=5`: Returns the top 5 search results\n",
    "   - `api_key`: Loaded securely from environment variables\n",
    "   - `allowed_domains`: Restricts results to Britannica.com for quality content\n",
    "\n",
    "2. **LinkContentFetcher**\n",
    "   - `retry_attempts=3`: Retries failed downloads up to 3 times\n",
    "   - `timeout=10`: Waits maximum 10 seconds per request\n",
    "\n",
    "3. **HTMLToDocument**\n",
    "   - Converts raw HTML into structured Haystack Document objects\n",
    "\n",
    "4. **DocumentCleaner**\n",
    "   - `remove_empty_lines=True`: Removes blank lines\n",
    "   - `remove_extra_whitespaces=True`: Normalizes spacing\n",
    "   - `remove_substrings=['\\n-']`: Removes specific unwanted patterns\n",
    "\n",
    "5. **NamedEntityExtractor**\n",
    "   - `backend=\"hugging_face\"`: Uses HuggingFace models\n",
    "   - `model=\"dslim/bert-base-NER\"`: Pre-trained BERT model fine-tuned for NER\n",
    "   - `warm_up()`: Loads the model into memory for faster inference\n",
    "\n",
    "6. **NERPopulator**\n",
    "   - Our custom component for structuring the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x144b71c40>\n",
       "ðŸš… Components\n",
       "  - search: SearchApiWebSearch\n",
       "  - fetcher: LinkContentFetcher\n",
       "  - htmldocument: HTMLToDocument\n",
       "  - cleaner: DocumentCleaner\n",
       "  - extractor: NamedEntityExtractor\n",
       "  - ner: NERPopulator\n",
       "ðŸ›¤ï¸ Connections\n",
       "  - search.links -> fetcher.urls (list[str])\n",
       "  - fetcher.streams -> htmldocument.sources (list[ByteStream])\n",
       "  - htmldocument.documents -> cleaner.documents (list[Document])\n",
       "  - cleaner.documents -> extractor.documents (list[Document])\n",
       "  - extractor.documents -> ner.sources (list[Document])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Initialize pipeline\n",
    "pipeline = Pipeline()\n",
    "web_search = SearchApiWebSearch(top_k=5,\n",
    "                                api_key=Secret.from_env_var(\"SEARCH_API_KEY\"),\n",
    "                                allowed_domains=[\"https://www.britannica.com/\"])\n",
    "link_content = LinkContentFetcher(retry_attempts=3,\n",
    "                                  timeout=10)\n",
    "html_to_doc = HTMLToDocument()\n",
    "document_cleaner = DocumentCleaner(\n",
    "                                remove_empty_lines=True,\n",
    "                                remove_extra_whitespaces=True,\n",
    "                                remove_repeated_substrings=False,\n",
    "                                remove_substrings=['\\n-']\n",
    "                            )\n",
    "extractor = NamedEntityExtractor(backend=\"hugging_face\", model=\"dslim/bert-base-NER\")\n",
    "extractor.warm_up()\n",
    "\n",
    "ner_component = NERPopulator(output_dir=\"./ner_results\")\n",
    "\n",
    "# Add components\n",
    "pipeline.add_component(name='search', instance=web_search)\n",
    "pipeline.add_component(name ='fetcher' , instance= link_content)\n",
    "pipeline.add_component(name='htmldocument', instance=html_to_doc)\n",
    "pipeline.add_component(name='cleaner', instance=document_cleaner)\n",
    "pipeline.add_component(name='extractor', instance=extractor)\n",
    "pipeline.add_component(name='ner', instance=ner_component)\n",
    "\n",
    "# Connect components to one another\n",
    "pipeline.connect(\"search.links\", \"fetcher.urls\")\n",
    "pipeline.connect(\"fetcher\", \"htmldocument\")\n",
    "pipeline.connect(\"htmldocument\", \"cleaner\")\n",
    "pipeline.connect(\"cleaner\", \"extractor\")\n",
    "pipeline.connect(\"extractor\", \"ner\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Pipeline Connections\n",
    "\n",
    "**The Data Flow:**\n",
    "\n",
    "```\n",
    "1. search (SearchApiWebSearch)\n",
    "   â†“ outputs: links\n",
    "2. fetcher (LinkContentFetcher)\n",
    "   â†“ outputs: streams (raw HTML)\n",
    "3. htmldocument (HTMLToDocument)\n",
    "   â†“ outputs: documents\n",
    "4. cleaner (DocumentCleaner)\n",
    "   â†“ outputs: documents (cleaned)\n",
    "5. extractor (NamedEntityExtractor)\n",
    "   â†“ outputs: documents (with named_entities in meta)\n",
    "6. ner (NERPopulator)\n",
    "   â†“ outputs: documents (structured)\n",
    "```\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "- **Component Naming**: Each component gets a unique name (e.g., 'search', 'fetcher')\n",
    "- **Output Routing**: We specify which output connects to which input\n",
    "  - Example: `\"search.links\"` â†’ `\"fetcher.urls\"`\n",
    "- **Automatic Type Matching**: Haystack validates that outputs match expected inputs\n",
    "- **Sequential Processing**: Each component processes data from the previous one\n",
    "\n",
    "**Why This Order Matters:**\n",
    "1. Can't extract entities before fetching content\n",
    "2. Can't clean text before converting HTML\n",
    "3. Can't structure results before extracting entities\n",
    "\n",
    "This is a **linear pipeline** - each step depends on the previous one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run the Pipeline\n",
    "\n",
    "### How Pipeline Execution Works\n",
    "\n",
    "When you call `pipeline.run()`, Haystack:\n",
    "1. Passes your input to the first component ('search')\n",
    "2. Each component processes data and passes results to the next\n",
    "3. Returns the final output from the last component\n",
    "\n",
    "**Input Format:**\n",
    "```python\n",
    "data={\"search\": {\"query\": query}}\n",
    "```\n",
    "- Dictionary keys match component names\n",
    "- Nested dictionary contains the component's input parameters\n",
    "- For 'search', we only need to provide the 'query' parameter\n",
    "\n",
    "**What Happens During Execution:**\n",
    "1. SearchAPI finds 5 relevant articles about \"Elon Musk\"\n",
    "2. LinkContentFetcher downloads HTML from each URL\n",
    "3. HTMLToDocument converts HTML to text\n",
    "4. DocumentCleaner removes noise and formatting\n",
    "5. NamedEntityExtractor identifies people, places, organizations\n",
    "6. NERPopulator structures the entities into our custom format\n",
    "\n",
    "**Expected Processing Time:**\n",
    "- Web search: ~1-2 seconds\n",
    "- Content fetching: ~3-10 seconds (depends on website response)\n",
    "- NER extraction: ~5-10 seconds (model inference)\n",
    "- Total: ~10-20 seconds for 5 articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Elon Musk\"\n",
    "output = pipeline.run(data={\"search\":{\"query\":query}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Output Structure\n",
    "\n",
    "The `output` variable contains results from all pipeline components. Structure:\n",
    "```python\n",
    "{\n",
    "    'search': {...},      # Search results\n",
    "    'fetcher': {...},     # Downloaded content\n",
    "    'htmldocument': {...},# Converted documents\n",
    "    'cleaner': {...},     # Cleaned documents\n",
    "    'extractor': {...},   # Documents with raw entities\n",
    "    'ner': {              # Our final structured output\n",
    "        'documents': [...]\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "We're interested in the final output from our custom 'ner' component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_documents = output['ner']['documents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Extract and Examine Results\n",
    "\n",
    "Now we'll extract just the final processed documents from our pipeline output.\n",
    "\n",
    "**What's in extracted_documents?**\n",
    "Each document is a dictionary containing:\n",
    "- `document_id`: Unique identifier\n",
    "- `content`: Full article text\n",
    "- `meta`: Dictionary with:\n",
    "  - `LOC`: Comma-separated locations (e.g., \"California,Texas,Mars\")\n",
    "  - `PER`: Comma-separated person names (e.g., \"Elon Musk,Jeff Bezos\")\n",
    "  - `ORG`: Comma-separated organizations (e.g., \"Tesla,SpaceX,NASA\")\n",
    "  - `MISC`: Miscellaneous entities (e.g., \"Cybertruck,Model 3\")\n",
    "  - `url`: Source URL for the article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Convert to DataFrame for Analysis\n",
    "\n",
    "### Why Use a DataFrame?\n",
    "\n",
    "Converting our results to a pandas DataFrame provides:\n",
    "- **Tabular View**: Easy-to-read table format\n",
    "- **Data Analysis**: Use pandas methods for filtering, grouping, aggregating\n",
    "- **Export Options**: Save to CSV, Excel, or databases\n",
    "- **Visualization**: Create charts and graphs\n",
    "\n",
    "### What You'll See\n",
    "\n",
    "The DataFrame will have columns:\n",
    "- `document_id`: Unique ID for each article\n",
    "- `content`: Full article text (may be long)\n",
    "- `meta`: Nested dictionary with entity categories and URL\n",
    "\n",
    "**Tip**: You can expand the meta column to see all extracted entities for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>content</th>\n",
       "      <th>meta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b673a37853a56857b7ca61fa0597deb527c8b1aff16e1f...</td>\n",
       "      <td>Elon Musk\\nWhen was Elon Musk born?\\nElon Musk...</td>\n",
       "      <td>{'LOC': 'South,Pretoria,Philadelphia,States,Ki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>029037a6d9de96ba5d6150dfd119c79ef8afb66895aef8...</td>\n",
       "      <td>Elon Musk went to Queenâ€™s University in Kingst...</td>\n",
       "      <td>{'LOC': 'Kingston,Canada,Ontario,Philadelphia'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f85e239cb34a29728ea742ababe1795dec329132e6bf2b...</td>\n",
       "      <td>Tesla, Inc. formerly (2003â€“17): Tesla Motors D...</td>\n",
       "      <td>{'LOC': 'Austin,Texas', 'PER': 'Nikola Tesla,M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3808990b1c9929bc0c40cca51a77685593e5354bbd2bd4...</td>\n",
       "      <td>Tesla under Musk: New models, battery technolo...</td>\n",
       "      <td>{'LOC': 'Mexico,Buffalo,Europe,United States,S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c232b1eab7bc6b14cddbd7fc5c1f636c91a92e92064731...</td>\n",
       "      <td>Zip2 Date: 1995 - 1999 Ticker: HPQ Share price...</td>\n",
       "      <td>{'LOC': '', 'PER': 'Enrique J,Kim,Gregory Ko,o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         document_id  \\\n",
       "0  b673a37853a56857b7ca61fa0597deb527c8b1aff16e1f...   \n",
       "1  029037a6d9de96ba5d6150dfd119c79ef8afb66895aef8...   \n",
       "2  f85e239cb34a29728ea742ababe1795dec329132e6bf2b...   \n",
       "3  3808990b1c9929bc0c40cca51a77685593e5354bbd2bd4...   \n",
       "4  c232b1eab7bc6b14cddbd7fc5c1f636c91a92e92064731...   \n",
       "\n",
       "                                             content  \\\n",
       "0  Elon Musk\\nWhen was Elon Musk born?\\nElon Musk...   \n",
       "1  Elon Musk went to Queenâ€™s University in Kingst...   \n",
       "2  Tesla, Inc. formerly (2003â€“17): Tesla Motors D...   \n",
       "3  Tesla under Musk: New models, battery technolo...   \n",
       "4  Zip2 Date: 1995 - 1999 Ticker: HPQ Share price...   \n",
       "\n",
       "                                                meta  \n",
       "0  {'LOC': 'South,Pretoria,Philadelphia,States,Ki...  \n",
       "1  {'LOC': 'Kingston,Canada,Ontario,Philadelphia'...  \n",
       "2  {'LOC': 'Austin,Texas', 'PER': 'Nikola Tesla,M...  \n",
       "3  {'LOC': 'Mexico,Buffalo,Europe,United States,S...  \n",
       "4  {'LOC': '', 'PER': 'Enrique J,Kim,Gregory Ko,o...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(extracted_documents)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the DataFrame\n",
    "\n",
    "**Useful operations you can try:**\n",
    "\n",
    "```python\n",
    "# See all columns\n",
    "df.columns\n",
    "\n",
    "# View just the metadata\n",
    "df['meta']\n",
    "\n",
    "# Access specific entity types for first document\n",
    "df['meta'][0]['PER']  # People mentioned\n",
    "df['meta'][0]['ORG']  # Organizations mentioned\n",
    "df['meta'][0]['LOC']  # Locations mentioned\n",
    "\n",
    "# Count documents\n",
    "len(df)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('ner_results.csv', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Creating a SuperComponent\n",
    "\n",
    "### What is a SuperComponent?\n",
    "\n",
    "A SuperComponent wraps an entire pipeline into a single reusable component with a simplified interface. Benefits:\n",
    "\n",
    "1. **Simplified Interface**: Map multiple pipeline inputs to a single input parameter\n",
    "2. **Encapsulation**: Hide complex internal pipeline structure\n",
    "3. **Reusability**: Use the pipeline as a component in larger systems\n",
    "4. **Tool Creation**: Easily convert to a tool for agent-based systems\n",
    "\n",
    "### Our NER SuperComponent\n",
    "\n",
    "We'll create a SuperComponent that:\n",
    "- Takes a natural language query and optional website domain\n",
    "- Internally distributes the query to the search component\n",
    "- Returns structured entity data and CSV file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Pipeline has been successfully wrapped into a SuperComponent.\n"
     ]
    }
   ],
   "source": [
    "from haystack import SuperComponent\n",
    "\n",
    "# Create a SuperComponent with simplified input/output mapping\n",
    "ner_super_component = SuperComponent(\n",
    "    pipeline=pipeline,\n",
    "    input_mapping={\n",
    "        \"query\": [\"search.query\"],\n",
    "        \"allowed_domains\": [\"search.query\"]\n",
    "    },\n",
    "    output_mapping={\n",
    "        \"ner.documents\": \"documents\",\n",
    "        \"ner.csv_path\": \"csv_path\",\n",
    "        \"ner.summary\": \"summary\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"NER Pipeline has been successfully wrapped into a SuperComponent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Test the SuperComponent\n",
    "\n",
    "Let's test our SuperComponent with a simple query to ensure it works correctly before creating the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5 documents.\n",
      "Found 25 unique people, 17 organizations, 18 locations, and 13 miscellaneous entities.\n",
      "Results saved to: ./ner_results/ner_results_20251124_222007.csv\n",
      "\n",
      "CSV file saved at: ./ner_results/ner_results_20251124_222007.csv\n"
     ]
    }
   ],
   "source": [
    "# Test the SuperComponent directly\n",
    "test_result = ner_super_component.run(\n",
    "    query=\"Marie Curie\"\n",
    ")\n",
    "\n",
    "# Display the summary\n",
    "print(test_result['summary'])\n",
    "print(f\"\\nCSV file saved at: {test_result['csv_path']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Creating a Tool from the SuperComponent\n",
    "\n",
    "Now we'll wrap our SuperComponent into a ComponentTool that can be used by an AI agent. This allows the agent to:\n",
    "- Extract entities from any website based on natural language queries\n",
    "- Process complex user requests like \"Find all people mentioned on Wikipedia articles about Einstein\"\n",
    "- Automatically save results to CSV files\n",
    "\n",
    "### Tool Configuration\n",
    "\n",
    "The tool needs:\n",
    "1. **Name**: A simple identifier for the agent to reference\n",
    "2. **Description**: Clear explanation of what the tool does (crucial for agent decision-making)\n",
    "3. **Component**: Our NER SuperComponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER SuperComponent has been successfully wrapped into a Tool for agent use.\n"
     ]
    }
   ],
   "source": [
    "from haystack.tools.component_tool import ComponentTool\n",
    "\n",
    "# Create a tool from the SuperComponent\n",
    "ner_tool_name = \"entity_extraction_tool\"\n",
    "\n",
    "ner_tool_description = (\n",
    "    \"Use this tool to extract named entities (people, organizations, locations, and miscellaneous entities) \"\n",
    "    \"from web articles. Provide a query string describing what to search for (e.g., 'Albert Einstein', 'Tesla company') \"\n",
    "    \"and optionally specify allowed_domains as a list of website URLs to restrict the search. \"\n",
    "    \"The tool will search the web, fetch articles, and extract all named entities, saving results to a CSV file. \"\n",
    "    \"Returns a summary of extracted entities and the path to the CSV file.\"\n",
    ")\n",
    "\n",
    "ner_tool = ComponentTool(\n",
    "    name=ner_tool_name,\n",
    "    component=ner_super_component,\n",
    "    description=ner_tool_description,\n",
    ")\n",
    "\n",
    "print(\"NER SuperComponent has been successfully wrapped into a Tool for agent use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Building an Agent with the NER Tool\n",
    "\n",
    "### Agent Architecture\n",
    "\n",
    "An agent is an intelligent system that can:\n",
    "1. Understand natural language requests\n",
    "2. Decide which tools to use\n",
    "3. Execute tools with appropriate parameters\n",
    "4. Synthesize results into coherent responses\n",
    "\n",
    "### Our NER Agent Configuration\n",
    "\n",
    "We'll create an agent that:\n",
    "- Uses GPT-4 for reasoning and decision-making\n",
    "- Has access to our entity extraction tool\n",
    "- Can parse user requests like \"Extract entities about Marie Curie from Britannica\"\n",
    "- Handles the extraction and returns both the summary and CSV file location\n",
    "\n",
    "### System Prompt\n",
    "\n",
    "The system prompt guides the agent's behavior and teaches it how to use the tools effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Agent successfully created and ready to process queries!\n"
     ]
    }
   ],
   "source": [
    "from haystack.components.agents import Agent\n",
    "from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "from haystack.dataclasses import ChatMessage\n",
    "\n",
    "\n",
    "# # Initialize the agent's LLM (must support tool calling)\n",
    "# agent_llm = OpenAIChatGenerator(\n",
    "#     api_key=Secret.from_env_var(\"OPENAI_API_KEY\"),\n",
    "#     model=\"gpt-4o-mini\"\n",
    "# )\n",
    "\n",
    "# Or use Ollama models (uncomment to use):\n",
    "from haystack_integrations.components.generators.ollama import OllamaChatGenerator\n",
    "\n",
    "agent_llm = OllamaChatGenerator(\n",
    "    model=\"mistral-nemo:12b\",\n",
    "    generation_kwargs={\n",
    "        \"num_predict\": 100,\n",
    "        \"temperature\": 0.9,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Define the system prompt to guide the agent\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful Named Entity Recognition assistant. Your goal is to help users extract \n",
    "named entities (people, organizations, locations, and miscellaneous entities) from web articles.\n",
    "\n",
    "You have access to the entity_extraction_tool which can:\n",
    "- Search the web for articles based on a query\n",
    "- Extract named entities from the articles found\n",
    "- Save results to a CSV file\n",
    "\n",
    "When a user asks you to extract entities:\n",
    "1. Identify the search query from their request\n",
    "2. Call the entity_extraction_tool with the appropriate parameters\n",
    "3. Present the results including:\n",
    "   - Summary of entities found\n",
    "   - Path to the CSV file\n",
    "   - Key entities discovered (people, organizations, locations)\n",
    "\n",
    "Be conversational and helpful. Parse natural language requests carefully.\n",
    "\"\"\"\n",
    "\n",
    "# Create the agent with the NER tool\n",
    "tools = [ner_tool]\n",
    "\n",
    "ner_agent = Agent(\n",
    "    chat_generator=agent_llm,\n",
    "    tools=tools,\n",
    "    system_prompt=system_prompt\n",
    ")\n",
    "\n",
    "print(\"NER Agent successfully created and ready to process queries!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Using the Agent with Natural Language Queries\n",
    "\n",
    "Now let's put our agent to work! We'll send natural language queries that specify:\n",
    "1. What topic to search for\n",
    "2. Optionally, which website to search\n",
    "\n",
    "The agent will:\n",
    "- Parse the request\n",
    "- Call the entity extraction tool with appropriate parameters\n",
    "- Return a summary and CSV file path\n",
    "\n",
    "### Example Query Format\n",
    "\n",
    "Natural language examples:\n",
    "- \"Extract entities about Albert Einstein from Britannica\"\n",
    "- \"Find all people and organizations mentioned in articles about Marie Curie\"\n",
    "- \"Search for entities related to Tesla on Britannica\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query: Extract all named entities about Albert Einstein and Elon Musk\n",
      "\n",
      "Agent is processing your request...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Simple query with default website\n",
    "query1 = \"Extract all named entities about Albert Einstein and Elon Musk\"\n",
    "\n",
    "print(f\"User Query: {query1}\\n\")\n",
    "print(\"Agent is processing your request...\\n\")\n",
    "\n",
    "agent_result1 = ner_agent.run(messages=[ChatMessage.from_user(query1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Agent Response\n",
    "\n",
    "Let's create a helper function to nicely display the agent's response and show the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### User Query\n",
       "*Extract all named entities about Albert Einstein and Elon Musk*\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Agent Actions"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Tool Used:** `entity_extraction_tool`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Parameters:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- allowed_domains: `https://www.britannica.com https://www.wikipedia.org`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- query: `'Albert Einstein' AND 'Elon Musk'`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Agent Response"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Here's a summary of the extracted information from both texts:\n",
       "\n",
       "**Text 1:**\n",
       "- **People:** None mentioned\n",
       "- **Organizations:** Britannica\n",
       "- **Locations:** United States, Atlantic Ocean, Gulf of Mexico, Pacific Ocean\n",
       "- **Miscellaneous:** Progressive era, American Revolution\n",
       "\n",
       "**Text 2:**\n",
       "- **People:** Donald Trump\n",
       "- **Organizations:** U.S. (mentioned twice)\n",
       "- **Locations:** United States, Washington D.C., China, Russia"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "def display_agent_result(agent_result):\n",
    "    \"\"\"\n",
    "    Display the agent's reasoning process and final response in a formatted way.\n",
    "    \n",
    "    Args:\n",
    "        agent_result: The result dictionary from the agent's run\n",
    "    \"\"\"\n",
    "    # Display user query\n",
    "    user_query = agent_result['messages'][1]._content[0].text\n",
    "    display(Markdown(f\"### User Query\\n*{user_query}*\\n\"))\n",
    "    \n",
    "    # Display tool calls\n",
    "    display(Markdown(\"### Agent Actions\"))\n",
    "    for msg in agent_result['messages']:\n",
    "        if msg._role.value == 'assistant' and hasattr(msg._content[0], 'tool_name'):\n",
    "            for tool_call in msg._content:\n",
    "                display(Markdown(f\"**Tool Used:** `{tool_call.tool_name}`\"))\n",
    "                display(Markdown(f\"**Parameters:**\"))\n",
    "                for key, value in tool_call.arguments.items():\n",
    "                    display(Markdown(f\"- {key}: `{value}`\"))\n",
    "                display(Markdown(\"\"))\n",
    "    \n",
    "    # Display final response\n",
    "    display(Markdown(\"### Agent Response\"))\n",
    "    final_response = agent_result['last_message']._content[0].text\n",
    "    display(Markdown(final_response))\n",
    "\n",
    "# Display the result\n",
    "display_agent_result(agent_result1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Query with Custom Website\n",
    "\n",
    "Let's try another query where we specify a different website domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Query specifying a different topic\n",
    "query2 = \"Find entities about Nikola Tesla from Britannica and save them to a CSV file\"\n",
    "\n",
    "print(f\"User Query: {query2}\\n\")\n",
    "print(\"Agent is processing your request...\\n\")\n",
    "\n",
    "agent_result2 = ner_agent.run(messages=[ChatMessage.from_user(query2)])\n",
    "display_agent_result(agent_result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing the Saved CSV Files\n",
    "\n",
    "You can load and inspect the CSV files that were generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all CSV files in the results directory\n",
    "import glob\n",
    "\n",
    "csv_files = glob.glob(\"./ner_results/*.csv\")\n",
    "print(f\"Found {len(csv_files)} CSV files:\\n\")\n",
    "for file in csv_files:\n",
    "    print(f\"- {file}\")\n",
    "\n",
    "# Load and display the most recent CSV\n",
    "if csv_files:\n",
    "    latest_csv = max(csv_files, key=os.path.getctime)\n",
    "    print(f\"\\nLoading most recent file: {latest_csv}\\n\")\n",
    "    \n",
    "    df_result = pd.read_csv(latest_csv)\n",
    "    display(Markdown(f\"### Results from {latest_csv}\"))\n",
    "    display(df_result.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### What We've Accomplished\n",
    "\n",
    "1. **Enhanced NER Pipeline**: Modified the NERPopulator component to automatically save results to CSV files\n",
    "2. **Created a SuperComponent**: Wrapped the entire pipeline with a simplified interface\n",
    "3. **Built a Tool**: Converted the SuperComponent into a tool that agents can use\n",
    "4. **Implemented an Agent**: Created an intelligent agent that can process natural language queries\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- âœ… **Natural Language Interface**: Users can ask questions in plain English\n",
    "- âœ… **Automatic CSV Generation**: Results are automatically saved with timestamps\n",
    "- âœ… **Entity Categorization**: People, organizations, locations, and miscellaneous entities are separated\n",
    "- âœ… **Website Flexibility**: Can search specific domains or use defaults\n",
    "- âœ… **Summary Reports**: Get quick insights before diving into the CSV\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "This system is ideal for:\n",
    "- **Research**: Quickly extract entities from multiple articles about a topic\n",
    "- **Content Analysis**: Identify key people, places, and organizations in web content\n",
    "- **Knowledge Base Building**: Systematically extract structured data from unstructured text\n",
    "- **Media Monitoring**: Track entity mentions across different sources\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "You can extend this system by:\n",
    "- Adding more tools (e.g., sentiment analysis, summarization)\n",
    "- Implementing custom entity types\n",
    "- Integrating with databases for persistent storage\n",
    "- Creating visualizations of entity relationships\n",
    "- Adding entity disambiguation and linking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition to SuperComponent and Agent Architecture\n",
    "\n",
    "In the following sections, we'll take our NER pipeline to the next level by:\n",
    "\n",
    "1. **Creating a SuperComponent**: Wrapping our pipeline for easier reuse\n",
    "2. **Building a Tool**: Making the pipeline accessible to AI agents\n",
    "3. **Implementing an Agent**: Creating an intelligent assistant that can process natural language queries\n",
    "\n",
    "This architecture allows users to simply ask questions like \"Extract entities about Einstein from Britannica\" instead of manually configuring pipeline parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-with-haystack-ch8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
