{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.extractors import NamedEntityExtractor\n",
    "import pandas as pd\n",
    "from haystack.dataclasses import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_named_entities_with_idsl(documents):\n",
    "    \"\"\"This function extracts named entities from a list of\n",
    "    documents and returns the result in a structured format.\n",
    "\n",
    "    Args:\n",
    "        documents (list): List of Haystack Document objects\n",
    "\n",
    "    Returns:\n",
    "        extracted_data (list): A list of dictionaries containing the extracted entities\n",
    "    \"\"\"\n",
    "    extracted_data = []\n",
    "\n",
    "    for document in documents:\n",
    "        content = document.content\n",
    "        doc_id = document.id\n",
    "        named_entities = document.meta.get('named_entities', [])\n",
    "        \n",
    "        # Sets to store unique entities by type\n",
    "        entities_by_type = {\n",
    "            \"LOC\": set(),\n",
    "            \"PER\": set(),\n",
    "            \"ORG\": set()\n",
    "        }\n",
    "        \n",
    "        # Loop through the entities and filter by score and type\n",
    "        for entity in named_entities:\n",
    "            if float(entity.score) < 0.8 or entity.entity == \"MISC\":\n",
    "                continue\n",
    "            \n",
    "            word = content[entity.start:entity.end]\n",
    "            if entity.entity in entities_by_type:\n",
    "                entities_by_type[entity.entity].add(word)  # Use set to ensure uniqueness\n",
    "        \n",
    "        # Prepare the meta field with comma-separated values\n",
    "        meta = {\n",
    "            \"LOC\": \",\".join(entities_by_type[\"LOC\"]),\n",
    "            \"PER\": \",\".join(entities_by_type[\"PER\"]),\n",
    "            \"ORG\": \",\".join(entities_by_type[\"ORG\"])\n",
    "        }\n",
    "        \n",
    "        # Append the result for this document\n",
    "        extracted_data.append({\n",
    "            'document_id': doc_id,\n",
    "            'content': content,\n",
    "            'meta': meta\n",
    "        })\n",
    "    \n",
    "\n",
    "    return extracted_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Named Entity Extractor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "extractor = NamedEntityExtractor(backend=\"hugging_face\", model=\"dslim/bert-base-NER\")\n",
    "extractor.warm_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"df_file.csv\")\n",
    "sample_docs = df['Text'].to_list()\n",
    "documents = [Document(id=str(i), content=sample_docs[i]) for i in range(len(sample_docs))]\n",
    "\n",
    "# Apply extractor to the documents\n",
    "extractor.run(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract named entities from the documents\n",
    "extracted_documents = extract_named_entities_with_idsl(documents)\n",
    "df = pd.DataFrame(extracted_documents)\n",
    "df.to_csv(\"ner_output.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP pipelines",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
