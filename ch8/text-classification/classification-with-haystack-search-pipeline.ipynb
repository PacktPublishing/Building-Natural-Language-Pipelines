{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification with Haystack: A Complete Tutorial\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates how to build an end-to-end text classification pipeline using Haystack. We'll create a system that:\n",
    "1. Searches the web for articles about a specific topic\n",
    "2. Fetches and processes the content\n",
    "3. Automatically classifies articles into categories (Politics, Sport, Technology, Entertainment, Business)\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this tutorial, you will understand:\n",
    "- How to build multi-component Haystack pipelines\n",
    "- Web search and content fetching integration\n",
    "- Zero-shot text classification using transformer models\n",
    "- Creating custom Haystack components\n",
    "- Chaining components together for complex workflows\n",
    "\n",
    "## Prerequisites\n",
    "- Basic Python knowledge\n",
    "- Familiarity with NLP concepts\n",
    "- Haystack framework installed\n",
    "- SearchAPI key (stored in `.env` file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.components.preprocessors import DocumentCleaner\n",
    "from haystack.components.websearch import SearchApiWebSearch\n",
    "from haystack.components.fetchers import LinkContentFetcher\n",
    "from haystack.components.converters import HTMLToDocument\n",
    "from haystack.components.routers import TransformersZeroShotTextRouter\n",
    "from haystack import Pipeline\n",
    "from haystack import component, Document\n",
    "from haystack.utils import Secret\n",
    "from typing import List\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Load .env from the root of ch8 directory\n",
    "root_dir = Path(__file__).parent.parent if \"__file__\" in globals() else Path.cwd().parent\n",
    "load_dotenv(root_dir / \".env\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries\n",
    "\n",
    "In this section, we import all the necessary components from Haystack:\n",
    "\n",
    "- **Pipeline**: The core container that orchestrates component execution\n",
    "- **DocumentCleaner**: Preprocesses text by removing extra whitespace and formatting\n",
    "- **SearchApiWebSearch**: Performs web searches using the SearchAPI service\n",
    "- **LinkContentFetcher**: Downloads content from URLs\n",
    "- **HTMLToDocument**: Converts HTML content into Haystack Document objects\n",
    "- **TransformersZeroShotTextRouter**: Classifies text without prior training on specific categories\n",
    "- **component & Document**: Core building blocks for creating custom components\n",
    "- **Secret**: Securely handles API keys from environment variables\n",
    "\n",
    "We also use `python-dotenv` to load environment variables from a `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "text_router = TransformersZeroShotTextRouter(\n",
    "    model=\"MoritzLaurer/deberta-v3-large-zeroshot-v2.0\",\n",
    "    labels=[\"Politics\", \"Sport\",\"Technology\",\"Entertainment\", \"Business\"],)\n",
    "text_router.warm_up()\n",
    "\n",
    "@component\n",
    "class NewsClassifier:\n",
    "    @component.output_types(documents=List[Document])\n",
    "    def run(self, documents: List[Document]) -> List[Document]:\n",
    "        for document in documents:\n",
    "            text = document.content\n",
    "            meta = document.meta\n",
    "            labels = text_router.run(text)\n",
    "            meta['labels'] = list(labels.keys())[0]\n",
    "            \n",
    "        return {\"documents\": documents}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create a Custom News Classifier Component\n",
    "\n",
    "### Understanding Zero-Shot Classification\n",
    "Zero-shot classification allows us to classify text into categories without training a model specifically on those categories. The model (`MoritzLaurer/deberta-v3-large-zeroshot-v2.0`) uses its general language understanding to determine which label best fits the text.\n",
    "\n",
    "### The TransformersZeroShotTextRouter\n",
    "We initialize the router with 5 predefined categories:\n",
    "- **Politics**: Government, elections, policy news\n",
    "- **Sport**: Athletics, competitions, teams\n",
    "- **Technology**: Tech innovations, gadgets, software\n",
    "- **Entertainment**: Movies, music, celebrities\n",
    "- **Business**: Companies, markets, economy\n",
    "\n",
    "The `warm_up()` method loads the model into memory for faster inference.\n",
    "\n",
    "### Custom NewsClassifier Component\n",
    "This component:\n",
    "1. Takes a list of Document objects as input\n",
    "2. Extracts the text content from each document\n",
    "3. Runs classification on the text\n",
    "4. Stores the predicted label in the document's metadata\n",
    "5. Returns all documents with their classifications\n",
    "\n",
    "**Key Design Pattern**: By storing classifications in metadata, we maintain the original document structure while enriching it with additional information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build the Haystack Pipeline\n",
    "\n",
    "### Pipeline Architecture\n",
    "Our pipeline consists of 5 connected components that process data sequentially:\n",
    "\n",
    "```\n",
    "SearchApiWebSearch â†’ LinkContentFetcher â†’ HTMLToDocument â†’ DocumentCleaner â†’ NewsClassifier\n",
    "```\n",
    "\n",
    "### Component Configuration\n",
    "\n",
    "1. **SearchApiWebSearch**: \n",
    "   - Searches for articles (top_k=5 results)\n",
    "   - Restricted to Britannica for quality content\n",
    "   - Uses API key from environment variables\n",
    "\n",
    "2. **LinkContentFetcher**: \n",
    "   - Downloads web pages from search results\n",
    "   - 3 retry attempts for resilience\n",
    "   - 10-second timeout to prevent hanging\n",
    "\n",
    "3. **HTMLToDocument**: \n",
    "   - Converts raw HTML into structured Document objects\n",
    "   - Extracts main content, ignoring navigation and ads\n",
    "\n",
    "4. **DocumentCleaner**: \n",
    "   - Removes empty lines for cleaner text\n",
    "   - Eliminates extra whitespaces\n",
    "   - Removes specific substrings like '\\n-'\n",
    "   - Keeps text formatting consistent\n",
    "\n",
    "5. **NewsClassifier**: \n",
    "   - Our custom component that classifies each document\n",
    "   - Adds classification labels to metadata\n",
    "\n",
    "### Pipeline Connections\n",
    "Each `connect()` call defines data flow:\n",
    "- `search.links` â†’ `fetcher.urls`: URLs from search go to fetcher\n",
    "- `fetcher` â†’ `htmldocument`: Fetched HTML goes to converter\n",
    "- `htmldocument` â†’ `cleaner`: Documents go to cleaner\n",
    "- `cleaner` â†’ `classifier`: Clean documents get classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x150c47d40>\n",
       "ðŸš… Components\n",
       "  - search: SearchApiWebSearch\n",
       "  - fetcher: LinkContentFetcher\n",
       "  - htmldocument: HTMLToDocument\n",
       "  - cleaner: DocumentCleaner\n",
       "  - classifier: NewsClassifier\n",
       "ðŸ›¤ï¸ Connections\n",
       "  - search.links -> fetcher.urls (list[str])\n",
       "  - fetcher.streams -> htmldocument.sources (list[ByteStream])\n",
       "  - htmldocument.documents -> cleaner.documents (list[Document])\n",
       "  - cleaner.documents -> classifier.documents (list[Document])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Initialize pipeline\n",
    "pipeline = Pipeline()\n",
    "web_search = SearchApiWebSearch(top_k=5,\n",
    "                                api_key=Secret.from_env_var(\"SEARCH_API_KEY\"),\n",
    "                                allowed_domains=[\"https://www.britannica.com/\"])\n",
    "link_content = LinkContentFetcher(retry_attempts=3,\n",
    "                                  timeout=10)\n",
    "html_to_doc = HTMLToDocument()\n",
    "document_cleaner = DocumentCleaner(\n",
    "                                remove_empty_lines=True,\n",
    "                                remove_extra_whitespaces=True,\n",
    "                                remove_repeated_substrings=False,\n",
    "                                remove_substrings=['\\n-']\n",
    "                            )\n",
    "\n",
    "\n",
    "# Add components\n",
    "pipeline.add_component(name='search', instance=web_search)\n",
    "pipeline.add_component(name ='fetcher' , instance= link_content)\n",
    "pipeline.add_component(name='htmldocument', instance=html_to_doc)\n",
    "pipeline.add_component(name='cleaner', instance=document_cleaner)\n",
    "pipeline.add_component(name='classifier', instance=NewsClassifier())\n",
    "\n",
    "# Connect components to one another\n",
    "pipeline.connect(\"search.links\", \"fetcher.urls\")\n",
    "pipeline.connect(\"fetcher\", \"htmldocument\")\n",
    "pipeline.connect(\"htmldocument\", \"cleaner\")\n",
    "pipeline.connect(\"cleaner\", \"classifier\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run the Pipeline\n",
    "\n",
    "### Executing the Search and Classification\n",
    "Now we'll search for articles about \"Elon Musk\" and automatically classify them.\n",
    "\n",
    "**How it works:**\n",
    "1. The query is passed to the search component\n",
    "2. Search finds relevant URLs from Britannica\n",
    "3. Content is fetched, converted, and cleaned\n",
    "4. Each article is classified into one of our 5 categories\n",
    "5. Results are returned with classifications in metadata\n",
    "\n",
    "**Expected Output:**\n",
    "Articles about Elon Musk might be classified as:\n",
    "- **Technology**: Articles about Tesla, SpaceX innovations\n",
    "- **Business**: Articles about his companies and ventures\n",
    "- **Politics**: Articles about policy positions or government interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Elon Musk\"\n",
    "output = pipeline.run(data={\"search\":{\"query\":query}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_documents = output['classifier']['documents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Extract and Analyze Results\n",
    "\n",
    "### Accessing Classified Documents\n",
    "The pipeline output is a dictionary with nested results. We extract the documents from:\n",
    "- `output['classifier']['documents']` - documents processed by our classifier component\n",
    "\n",
    "Each document contains:\n",
    "- **content**: The full text of the article\n",
    "- **meta**: Metadata including:\n",
    "  - Original URL\n",
    "  - Title\n",
    "  - **labels**: Our classification result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content_type': 'text/html', 'url': 'https://www.britannica.com/money/Elon-Musk', 'labels': 'Technology'}\n",
      "----\n",
      "{'content_type': 'text/html', 'url': 'https://www.britannica.com/money/Zip2', 'labels': 'Technology'}\n",
      "----\n",
      "{'content_type': 'text/html', 'url': 'https://www.britannica.com/topic/Department-of-Government-Efficiency-United-States', 'labels': 'Politics'}\n",
      "----\n",
      "{'content_type': 'text/html', 'url': 'https://www.britannica.com/money/Tesla-Motors', 'labels': 'Business'}\n",
      "----\n",
      "{'content_type': 'text/html', 'url': 'https://www.britannica.com/money/xAI', 'labels': 'Technology'}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(extracted_documents)):\n",
    "    print(extracted_documents[i].meta)\n",
    "    # to get content of the document use extracted_documents[i].content\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Display Classification Results\n",
    "\n",
    "### Inspecting Document Metadata\n",
    "This loop displays the metadata for each classified document, including:\n",
    "- Source URL\n",
    "- Classification label\n",
    "- Any other metadata collected during processing\n",
    "\n",
    "**Try This:**\n",
    "- Uncomment `extracted_documents[i].content` to see the full article text\n",
    "- Notice how articles about the same person can be classified differently based on their focus\n",
    "- Try different queries to see how the classifier performs on various topics\n",
    "\n",
    "### Understanding the Results\n",
    "The classification is based on the overall content and context of each article. The zero-shot model uses its understanding of language patterns to determine which category best fits, even though it was never explicitly trained on these specific news categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises and Extensions\n",
    "\n",
    "### Try These Experiments:\n",
    "\n",
    "1. **Different Queries**: \n",
    "   - Search for \"climate change\", \"Olympics\", or \"cryptocurrency\"\n",
    "   - Observe how classification changes with topic\n",
    "\n",
    "2. **Modified Categories**:\n",
    "   - Change the labels in `TransformersZeroShotTextRouter` to different categories\n",
    "   - Example: [\"Science\", \"Health\", \"Education\", \"Environment\"]\n",
    "\n",
    "3. **Enhanced Classification**:\n",
    "   - Add confidence scores by accessing `labels` dictionary values\n",
    "   - Filter documents by classification (e.g., only show Technology articles)\n",
    "\n",
    "4. **Data Analysis**:\n",
    "   ```python\n",
    "   # Count articles per category\n",
    "   from collections import Counter\n",
    "   categories = [doc.meta['labels'] for doc in extracted_documents]\n",
    "   print(Counter(categories))\n",
    "   ```\n",
    "\n",
    "5. **Multiple Queries**:\n",
    "   - Run the pipeline with multiple queries in a loop\n",
    "   - Compare classification distributions across different topics\n",
    "\n",
    "### Key Takeaways:\n",
    "- âœ… Haystack pipelines enable modular, reusable NLP workflows\n",
    "- âœ… Zero-shot classification works without domain-specific training\n",
    "- âœ… Custom components extend Haystack's functionality\n",
    "- âœ… Component chaining creates powerful end-to-end systems\n",
    "- âœ… Metadata enrichment preserves document structure while adding intelligence\n",
    "\n",
    "### Next Steps:\n",
    "- Explore other Haystack components (embedders, retrievers, generators)\n",
    "- Build classification systems for your own domains\n",
    "- Combine classification with other NLP tasks (summarization, Q&A)\n",
    "- Deploy pipelines as REST APIs using Hayhooks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-with-haystack-ch8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
