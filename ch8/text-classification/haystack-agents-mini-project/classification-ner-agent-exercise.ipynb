{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d46e130e",
   "metadata": {},
   "source": [
    "# Reader Exercise: Building a Multi-Task NLP Agent with Classification and NER\n",
    "\n",
    "## üéØ Exercise Objective\n",
    "\n",
    "In this exercise, you will:\n",
    "\n",
    "1. **Build a Custom NER Component**: Create a component that extracts named entities from web articles and stores them in document metadata\n",
    "2. **Combine Two Pipelines**: Integrate text classification and named entity recognition into unified workflows\n",
    "3. **Create SuperComponents**: Wrap pipelines with simplified interfaces\n",
    "4. **Build an AI Agent**: Create an agent that can handle natural language queries for both classification and entity extraction\n",
    "5. **Deploy with Hayhooks**: Serialize and deploy your pipelines as REST APIs\n",
    "\n",
    "## üìö What You'll Learn\n",
    "\n",
    "- How to create custom Haystack components that combine multiple NLP tasks\n",
    "- Techniques for enriching document metadata with extracted information\n",
    "- Best practices for building SuperComponents from complex pipelines\n",
    "- How to create tools that agents can use to process natural language requests\n",
    "- Pipeline serialization and deployment strategies with Hayhooks\n",
    "\n",
    "## üîß Prerequisites\n",
    "\n",
    "Before starting, ensure you have:\n",
    "- Completed the NER and text classification tutorial notebooks\n",
    "- Understanding of Haystack pipeline architecture\n",
    "- API keys configured in your `.env` file:\n",
    "  - `OPENAI_API_KEY`\n",
    "  - `SEARCH_API_KEY` or `SERPERDEV_API_KEY`\n",
    "\n",
    "## üìã Exercise Structure\n",
    "\n",
    "This exercise is divided into the following sections:\n",
    "\n",
    "1. **Setup and Imports**\n",
    "2. **Part 1: Build Custom NER Component**\n",
    "3. **Part 2: Create Classification Pipeline**\n",
    "4. **Part 3: Build Combined NER + Classification Pipeline**\n",
    "5. **Part 4: Create SuperComponents**\n",
    "6. **Part 5: Build Component Tools for Agent**\n",
    "7. **Part 6: Implement the Multi-Task Agent**\n",
    "8. **Part 7: Serialize Pipelines**\n",
    "9. **Part 8: Deploy with Hayhooks**\n",
    "10. **Testing and Validation**\n",
    "\n",
    "## üí° Tips for Success\n",
    "\n",
    "- Review the reference notebooks before starting\n",
    "- Test each component individually before combining them\n",
    "- Pay attention to input/output types when connecting components\n",
    "- Use clear descriptions for tools so the agent understands when to use them\n",
    "- Test your agent with various natural language queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb051b6d",
   "metadata": {},
   "source": [
    "## Section 1: Setup and Imports\n",
    "\n",
    "### üìù Task\n",
    "Import all necessary libraries and load environment variables.\n",
    "\n",
    "### üí° Hint\n",
    "You'll need imports from both the NER and classification notebooks, plus additional imports for agents and Hayhooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5497d01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import Haystack core components\n",
    "from haystack import Pipeline, Document, component\n",
    "from haystack.utils import Secret\n",
    "\n",
    "# TODO: Import web search and content fetching components\n",
    "# Hint: SearchApiWebSearch, LinkContentFetcher, HTMLToDocument\n",
    "\n",
    "# TODO: Import preprocessing components\n",
    "# Hint: DocumentCleaner\n",
    "\n",
    "# TODO: Import NER and classification components\n",
    "# Hint: NamedEntityExtractor, TransformersZeroShotTextRouter\n",
    "\n",
    "# TODO: Import agent-related components\n",
    "# Hint: Agent, OpenAIChatGenerator, ChatMessage\n",
    "\n",
    "# TODO: Import SuperComponent and ComponentTool\n",
    "# Hint: SuperComponent, ComponentTool\n",
    "\n",
    "# TODO: Import supporting libraries\n",
    "from typing import List, Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "# Verify API keys are loaded\n",
    "# TODO: Add code to verify your API keys are present"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7933cec9",
   "metadata": {},
   "source": [
    "## Section 2: Part 1 - Build Custom NER Component\n",
    "\n",
    "### üìù Task\n",
    "Create a custom Haystack component called `EntityExtractor` that:\n",
    "1. Takes a list of documents as input\n",
    "2. Extracts named entities from each document using the NER model\n",
    "3. Stores entities in the document metadata organized by type (PER, ORG, LOC, MISC)\n",
    "4. Filters entities by confidence score (>0.8)\n",
    "5. Removes duplicate entities within each category\n",
    "6. Returns the enriched documents\n",
    "\n",
    "### üí° Hints\n",
    "- Use the `@component` decorator\n",
    "- Define output types with `@component.output_types(documents=List[Document])`\n",
    "- Implement a `run()` method that processes the documents\n",
    "- Store entities in `document.meta['entities']` as a dictionary with keys: LOC, PER, ORG, MISC\n",
    "- Use sets to automatically remove duplicates\n",
    "\n",
    "### üìñ Reference\n",
    "Review the `NERPopulator` component in the NER tutorial notebook for inspiration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cf5149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Initialize the NER extractor model\n",
    "# Hint: Use NamedEntityExtractor with \"dslim/bert-base-NER\" model\n",
    "\n",
    "ner_extractor = None  # Replace with actual initialization\n",
    "\n",
    "@component\n",
    "class EntityExtractor:\n",
    "    \"\"\"\n",
    "    Custom component that extracts named entities from documents\n",
    "    and stores them in document metadata.\n",
    "    \n",
    "    This component:\n",
    "    - Processes a list of documents\n",
    "    - Extracts named entities using a pre-trained NER model\n",
    "    - Filters entities by confidence score\n",
    "    - Organizes entities by type (PER, ORG, LOC, MISC)\n",
    "    - Removes duplicates within each category\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, confidence_threshold: float = 0.8):\n",
    "        \"\"\"\n",
    "        Initialize the EntityExtractor.\n",
    "        \n",
    "        Args:\n",
    "            confidence_threshold: Minimum confidence score for entity extraction (default: 0.8)\n",
    "        \"\"\"\n",
    "        # TODO: Store the confidence threshold\n",
    "        pass\n",
    "    \n",
    "    @component.output_types(documents=List[Document])\n",
    "    def run(self, documents: List[Document]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Extract entities from documents and store in metadata.\n",
    "        \n",
    "        Args:\n",
    "            documents: List of Haystack Document objects\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with 'documents' key containing enriched documents\n",
    "        \"\"\"\n",
    "        # TODO: Implement the entity extraction logic\n",
    "        # Steps:\n",
    "        # 1. Loop through each document\n",
    "        # 2. Get the document content\n",
    "        # 3. Run NER extraction on the content\n",
    "        # 4. Filter entities by confidence threshold\n",
    "        # 5. Organize entities by type using sets for deduplication\n",
    "        # 6. Store in document.meta['entities'] as a dictionary\n",
    "        # 7. Return the enriched documents\n",
    "        \n",
    "        enriched_documents = []\n",
    "        \n",
    "        for document in documents:\n",
    "            # TODO: Extract content from document\n",
    "            content = document.content\n",
    "            \n",
    "            # TODO: Run NER extraction\n",
    "            # Hint: Use the ner_extractor you initialized above\n",
    "            \n",
    "            # TODO: Initialize entity storage by type\n",
    "            entities_by_type = {\n",
    "                \"LOC\": set(),\n",
    "                \"PER\": set(),\n",
    "                \"ORG\": set(),\n",
    "                \"MISC\": set()\n",
    "            }\n",
    "            \n",
    "            # TODO: Loop through extracted entities\n",
    "            # Filter by confidence threshold\n",
    "            # Add to appropriate category\n",
    "            \n",
    "            # TODO: Convert sets to comma-separated strings or lists\n",
    "            # Store in document.meta['entities']\n",
    "            \n",
    "            enriched_documents.append(document)\n",
    "        \n",
    "        return {\"documents\": enriched_documents}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8195cacf",
   "metadata": {},
   "source": [
    "### Test Your EntityExtractor Component\n",
    "\n",
    "Before integrating into a pipeline, test your component with sample documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b56b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create test documents\n",
    "# test_docs = [Document(content=\"Your test text here\")]\n",
    "\n",
    "# TODO: Initialize your EntityExtractor\n",
    "# entity_extractor = EntityExtractor()\n",
    "\n",
    "# TODO: Run the component\n",
    "# result = entity_extractor.run(documents=test_docs)\n",
    "\n",
    "# TODO: Print the results to verify it works\n",
    "# print(result['documents'][0].meta.get('entities'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70243dc6",
   "metadata": {},
   "source": [
    "## Section 3: Part 2 - Create Classification Pipeline\n",
    "\n",
    "### üìù Task\n",
    "Build a pipeline that:\n",
    "1. Searches the web for articles\n",
    "2. Fetches and converts HTML content\n",
    "3. Cleans the text\n",
    "4. Classifies articles into categories\n",
    "\n",
    "### üí° Hints\n",
    "- Reuse the classification components from the tutorial\n",
    "- Use the `NewsClassifier` component pattern\n",
    "- Categories: Politics, Sport, Technology, Entertainment, Business\n",
    "\n",
    "### üìñ Reference\n",
    "Review the classification tutorial notebook for the complete pipeline structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1346720d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Initialize the zero-shot text router for classification\n",
    "# Hint: Use TransformersZeroShotTextRouter with appropriate labels\n",
    "\n",
    "# TODO: Create the NewsClassifier component (refer to classification notebook)\n",
    "\n",
    "# TODO: Initialize the classification pipeline\n",
    "classification_pipeline = Pipeline()\n",
    "\n",
    "# TODO: Add components to the pipeline\n",
    "# - SearchApiWebSearch\n",
    "# - LinkContentFetcher\n",
    "# - HTMLToDocument\n",
    "# - DocumentCleaner\n",
    "# - NewsClassifier\n",
    "\n",
    "# TODO: Connect the components\n",
    "# Follow the pattern: search -> fetcher -> html -> cleaner -> classifier\n",
    "\n",
    "print(\"Classification pipeline created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390c60cb",
   "metadata": {},
   "source": [
    "### Test the Classification Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64671e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test the classification pipeline with a query\n",
    "# test_query = \"artificial intelligence\"\n",
    "# classification_result = classification_pipeline.run(data={\"search\": {\"query\": test_query}})\n",
    "\n",
    "# TODO: Display the results\n",
    "# for doc in classification_result['classifier']['documents']:\n",
    "#     print(f\"Classification: {doc.meta.get('labels')}\")\n",
    "#     print(f\"URL: {doc.meta.get('url')}\")\n",
    "#     print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e923c994",
   "metadata": {},
   "source": [
    "## Section 4: Part 3 - Build Combined NER + Classification Pipeline\n",
    "\n",
    "### üìù Task\n",
    "Create a unified pipeline that performs BOTH classification and entity extraction:\n",
    "1. Searches and fetches articles\n",
    "2. Cleans the content\n",
    "3. Classifies articles into categories\n",
    "4. Extracts named entities\n",
    "\n",
    "### üí° Hints\n",
    "- You can chain the classifier and entity extractor in sequence\n",
    "- Both components should add metadata without overwriting each other\n",
    "- Test that both `labels` and `entities` appear in the final document metadata\n",
    "\n",
    "### ü§î Design Decision\n",
    "Should classification happen before or after entity extraction? Consider:\n",
    "- Do entities affect classification?\n",
    "- Does classification affect entity extraction?\n",
    "- Which order makes more logical sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455c4a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a combined pipeline\n",
    "combined_pipeline = Pipeline()\n",
    "\n",
    "# TODO: Initialize all necessary components\n",
    "# - Web search\n",
    "# - Link fetcher\n",
    "# - HTML converter\n",
    "# - Document cleaner\n",
    "# - Classification component\n",
    "# - Entity extraction component\n",
    "\n",
    "# TODO: Add components to the pipeline\n",
    "\n",
    "# TODO: Connect components in logical order\n",
    "# Recommended flow: search -> fetch -> convert -> clean -> classify -> extract entities\n",
    "\n",
    "print(\"Combined NER + Classification pipeline created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8524d8",
   "metadata": {},
   "source": [
    "### Test the Combined Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ab96da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test the combined pipeline\n",
    "# test_query = \"Elon Musk\"\n",
    "# combined_result = combined_pipeline.run(data={\"search\": {\"query\": test_query}})\n",
    "\n",
    "# TODO: Verify both classification and entities are present\n",
    "# for doc in combined_result['entity_extractor']['documents']:\n",
    "#     print(f\"Classification: {doc.meta.get('labels')}\")\n",
    "#     print(f\"Entities: {doc.meta.get('entities')}\")\n",
    "#     print(f\"URL: {doc.meta.get('url')}\")\n",
    "#     print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ca3326",
   "metadata": {},
   "source": [
    "## Section 5: Part 4 - Create SuperComponents\n",
    "\n",
    "### üìù Task\n",
    "Wrap your pipelines as SuperComponents with simplified interfaces.\n",
    "\n",
    "Create three SuperComponents:\n",
    "1. **Classification SuperComponent**: Takes query, returns classified documents\n",
    "2. **NER SuperComponent**: Takes query, returns documents with entities\n",
    "3. **Combined SuperComponent**: Takes query, returns documents with both classification and entities\n",
    "\n",
    "### üí° Hints\n",
    "- Use `input_mapping` to map external parameters to internal component inputs\n",
    "- Use `output_mapping` to expose specific outputs from the pipeline\n",
    "- Review the NER tutorial notebook for SuperComponent examples\n",
    "\n",
    "### üìñ Reference\n",
    "```python\n",
    "SuperComponent(\n",
    "    pipeline=your_pipeline,\n",
    "    input_mapping={\"query\": [\"search.query\"]},\n",
    "    output_mapping={\"component_name.output\": \"new_name\"}\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8604c1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import SuperComponent\n",
    "\n",
    "# TODO: Create Classification SuperComponent\n",
    "classification_super = None  # Replace with SuperComponent initialization\n",
    "\n",
    "# TODO: Create NER SuperComponent\n",
    "# ner_super = SuperComponent(\n",
    "#     pipeline=...,\n",
    "#     input_mapping={...},\n",
    "#     output_mapping={...}\n",
    "# )\n",
    "\n",
    "# TODO: Create Combined SuperComponent\n",
    "# combined_super = SuperComponent(\n",
    "#     pipeline=...,\n",
    "#     input_mapping={...},\n",
    "#     output_mapping={...}\n",
    "# )\n",
    "\n",
    "print(\"SuperComponents created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2184da3a",
   "metadata": {},
   "source": [
    "### Test SuperComponents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aadba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test each SuperComponent independently\n",
    "# test_query = \"machine learning\"\n",
    "\n",
    "# Test classification\n",
    "# result_classification = classification_super.run(query=test_query)\n",
    "\n",
    "# Test NER\n",
    "# result_ner = ner_super.run(query=test_query)\n",
    "\n",
    "# Test combined\n",
    "# result_combined = combined_super.run(query=test_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a56c793",
   "metadata": {},
   "source": [
    "## Section 6: Part 5 - Build Component Tools for Agent\n",
    "\n",
    "### üìù Task\n",
    "Convert your SuperComponents into tools that an AI agent can use.\n",
    "\n",
    "Create three tools:\n",
    "1. **Classification Tool**: For classifying articles\n",
    "2. **NER Tool**: For extracting entities\n",
    "3. **Combined Tool**: For both tasks simultaneously\n",
    "\n",
    "### üí° Critical: Tool Descriptions\n",
    "The tool description is crucial! The agent uses it to decide when to use each tool.\n",
    "\n",
    "Good description format:\n",
    "- Clear purpose statement\n",
    "- Input parameters explained\n",
    "- Output format described\n",
    "- Use cases or examples\n",
    "\n",
    "### üìñ Reference\n",
    "```python\n",
    "ComponentTool(\n",
    "    name=\"tool_name\",\n",
    "    component=your_super_component,\n",
    "    description=\"Clear description of what this tool does...\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13857346",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.tools.component_tool import ComponentTool\n",
    "\n",
    "# TODO: Create Classification Tool\n",
    "classification_tool = None  # Replace with ComponentTool initialization\n",
    "# Suggested name: \"article_classifier\"\n",
    "# Description should explain: what it classifies, categories used, input/output\n",
    "\n",
    "# TODO: Create NER Tool\n",
    "# ner_tool = ComponentTool(\n",
    "#     name=\"entity_extractor\",\n",
    "#     component=ner_super,\n",
    "#     description=\"...\"\n",
    "# )\n",
    "\n",
    "# TODO: Create Combined Tool\n",
    "# combined_tool = ComponentTool(\n",
    "#     name=\"classify_and_extract\",\n",
    "#     component=combined_super,\n",
    "#     description=\"...\"\n",
    "# )\n",
    "\n",
    "print(\"Component tools created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8672027a",
   "metadata": {},
   "source": [
    "## Section 7: Part 6 - Implement the Multi-Task Agent\n",
    "\n",
    "### üìù Task\n",
    "Create an AI agent that can:\n",
    "1. Understand natural language requests\n",
    "2. Decide which tool to use (classification, NER, or both)\n",
    "3. Execute the appropriate tool\n",
    "4. Summarize results for the user\n",
    "\n",
    "### üí° Hints\n",
    "- Use GPT-4 or GPT-4-mini for better tool selection\n",
    "- Write a comprehensive system prompt that explains each tool\n",
    "- Include example queries in the system prompt\n",
    "- Test with various natural language requests\n",
    "\n",
    "### üéØ Example Queries to Handle\n",
    "- \"Classify articles about climate change\"\n",
    "- \"Extract entities from articles about Tesla\"\n",
    "- \"Classify and extract entities from news about artificial intelligence\"\n",
    "- \"What people and organizations are mentioned in articles about SpaceX?\"\n",
    "- \"What category do articles about quantum computing fall into?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedb64a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.agents import Agent\n",
    "from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "from haystack.dataclasses import ChatMessage\n",
    "\n",
    "# TODO: Initialize the LLM for the agent\n",
    "# agent_llm = OpenAIChatGenerator(\n",
    "#     api_key=Secret.from_env_var(\"OPENAI_API_KEY\"),\n",
    "#     model=\"gpt-4o-mini\"  # or \"gpt-4\"\n",
    "# )\n",
    "\n",
    "# TODO: Write a comprehensive system prompt\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful NLP assistant with access to tools for analyzing web articles.\n",
    "\n",
    "Available tools:\n",
    "1. article_classifier: Classifies articles into categories (Politics, Sport, Technology, Entertainment, Business)\n",
    "2. entity_extractor: Extracts named entities (people, organizations, locations, misc) from articles\n",
    "3. classify_and_extract: Performs both classification and entity extraction simultaneously\n",
    "\n",
    "When users ask you to:\n",
    "- Only classify articles ‚Üí Use article_classifier\n",
    "- Only extract entities ‚Üí Use entity_extractor\n",
    "- Do both tasks ‚Üí Use classify_and_extract\n",
    "\n",
    "Always:\n",
    "- Parse the user's query to identify the search topic\n",
    "- Choose the appropriate tool based on the request\n",
    "- Summarize the results clearly\n",
    "- List key findings (classifications, entities, or both)\n",
    "\n",
    "Be conversational and helpful!\n",
    "\"\"\"\n",
    "\n",
    "# TODO: Create the agent with all three tools\n",
    "# tools = [classification_tool, ner_tool, combined_tool]\n",
    "\n",
    "# nlp_agent = Agent(\n",
    "#     chat_generator=agent_llm,\n",
    "#     tools=tools,\n",
    "#     system_prompt=system_prompt\n",
    "# )\n",
    "\n",
    "print(\"Multi-task NLP agent created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5218fad",
   "metadata": {},
   "source": [
    "### Test the Agent with Various Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce653741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test Query 1 - Classification only\n",
    "# query1 = \"Classify articles about renewable energy\"\n",
    "# result1 = nlp_agent.run(messages=[ChatMessage.from_user(query1)])\n",
    "# print(result1['last_message']._content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9e58d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test Query 2 - Entity extraction only\n",
    "# query2 = \"Extract all people and organizations from articles about Apple Inc\"\n",
    "# result2 = nlp_agent.run(messages=[ChatMessage.from_user(query2)])\n",
    "# print(result2['last_message']._content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46108422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test Query 3 - Both classification and entity extraction\n",
    "# query3 = \"Classify and extract entities from articles about artificial intelligence\"\n",
    "# result3 = nlp_agent.run(messages=[ChatMessage.from_user(query3)])\n",
    "# print(result3['last_message']._content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1e6f40",
   "metadata": {},
   "source": [
    "## Section 8: Part 7 - Serialize Pipelines\n",
    "\n",
    "### üìù Task\n",
    "Serialize your pipelines to YAML format for deployment with Hayhooks.\n",
    "\n",
    "Save three pipeline files:\n",
    "1. `classification_pipeline.yaml`\n",
    "2. `ner_pipeline.yaml`\n",
    "3. `combined_pipeline.yaml`\n",
    "\n",
    "### üí° Hints\n",
    "- Use `pipeline.dumps()` to serialize to YAML string\n",
    "- Save to the `pipelines/` directory\n",
    "- Verify the YAML files are valid\n",
    "- Check that API keys are properly referenced (should use env variables)\n",
    "\n",
    "### ‚ö†Ô∏è Security Note\n",
    "Never hardcode API keys in YAML files! Always use `Secret.from_env_var()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fce5a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Serialize classification pipeline\n",
    "# classification_yaml = classification_pipeline.dumps()\n",
    "# with open(\"pipelines/classification_pipeline.yaml\", \"w\") as f:\n",
    "#     f.write(classification_yaml)\n",
    "\n",
    "# TODO: Serialize NER pipeline\n",
    "# Create a standalone NER pipeline if needed (without classification)\n",
    "\n",
    "# TODO: Serialize combined pipeline\n",
    "# combined_yaml = combined_pipeline.dumps()\n",
    "# with open(\"pipelines/combined_pipeline.yaml\", \"w\") as f:\n",
    "#     f.write(combined_yaml)\n",
    "\n",
    "print(\"Pipelines serialized successfully!\")\n",
    "print(\"Files saved in pipelines/ directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d89cb59",
   "metadata": {},
   "source": [
    "### Verify Serialized Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf64afc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load and verify a serialized pipeline\n",
    "# test_pipeline = Pipeline.loads(classification_yaml)\n",
    "# test_result = test_pipeline.run(data={\"search\": {\"query\": \"test\"}})\n",
    "# print(\"Pipeline loaded and tested successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400b78a0",
   "metadata": {},
   "source": [
    "## Section 9: Part 8 - Deploy with Hayhooks\n",
    "\n",
    "### üìù Task\n",
    "Deploy your pipelines as REST APIs using Hayhooks.\n",
    "\n",
    "### üöÄ Deployment Steps\n",
    "\n",
    "1. **Install Hayhooks** (if not already installed):\n",
    "   ```bash\n",
    "   pip install hayhooks\n",
    "   ```\n",
    "\n",
    "2. **Start Hayhooks Server**:\n",
    "   ```bash\n",
    "   hayhooks run --pipelines-dir pipelines/\n",
    "   ```\n",
    "\n",
    "3. **Test Endpoints**:\n",
    "   - List pipelines: `GET http://localhost:1416/pipelines`\n",
    "   - Run pipeline: `POST http://localhost:1416/pipelines/classification_pipeline`\n",
    "\n",
    "### üí° Hints\n",
    "- Hayhooks automatically creates REST endpoints from YAML files\n",
    "- Each pipeline gets its own endpoint\n",
    "- Use tools like `curl`, Postman, or Python `requests` library to test\n",
    "- Check Hayhooks documentation for advanced configuration\n",
    "\n",
    "### üìñ References\n",
    "- Review the Yelp Navigator Hayhooks guide in your workspace\n",
    "- Check `yelp-navigator/yelp-navigator-hayhooks-guide.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2fea4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell provides Python code to test your deployed Hayhooks endpoints\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# TODO: Define Hayhooks server URL\n",
    "HAYHOOKS_URL = \"http://localhost:1416\"\n",
    "\n",
    "def list_pipelines():\n",
    "    \"\"\"List all deployed pipelines.\"\"\"\n",
    "    # TODO: Implement function to list pipelines\n",
    "    # Hint: GET request to /pipelines endpoint\n",
    "    pass\n",
    "\n",
    "def run_classification(query: str):\n",
    "    \"\"\"Run classification pipeline via Hayhooks.\"\"\"\n",
    "    # TODO: Implement function to call classification endpoint\n",
    "    # Hint: POST request to /pipelines/classification_pipeline\n",
    "    # Body should contain the search query\n",
    "    pass\n",
    "\n",
    "def run_ner(query: str):\n",
    "    \"\"\"Run NER pipeline via Hayhooks.\"\"\"\n",
    "    # TODO: Implement function to call NER endpoint\n",
    "    pass\n",
    "\n",
    "def run_combined(query: str):\n",
    "    \"\"\"Run combined pipeline via Hayhooks.\"\"\"\n",
    "    # TODO: Implement function to call combined endpoint\n",
    "    pass\n",
    "\n",
    "# TODO: Test the functions\n",
    "# print(list_pipelines())\n",
    "# print(run_classification(\"machine learning\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbba0eec",
   "metadata": {},
   "source": [
    "### Terminal Commands for Hayhooks\n",
    "\n",
    "Run these commands in your terminal:\n",
    "\n",
    "```bash\n",
    "# Start Hayhooks server\n",
    "hayhooks run --pipelines-dir pipelines/\n",
    "\n",
    "# In another terminal, test with curl:\n",
    "curl http://localhost:1416/pipelines\n",
    "\n",
    "# Test classification endpoint\n",
    "curl -X POST http://localhost:1416/pipelines/classification_pipeline \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"search\": {\"query\": \"artificial intelligence\"}}'\n",
    "\n",
    "# Test combined endpoint\n",
    "curl -X POST http://localhost:1416/pipelines/combined_pipeline \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"search\": {\"query\": \"Elon Musk\"}}'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6848de4",
   "metadata": {},
   "source": [
    "## Section 10: Testing and Validation\n",
    "\n",
    "### üìù Comprehensive Testing Checklist\n",
    "\n",
    "Test your complete system:\n",
    "\n",
    "#### ‚úÖ Component Testing\n",
    "- [ ] EntityExtractor extracts entities correctly\n",
    "- [ ] NewsClassifier classifies articles correctly\n",
    "- [ ] Both components preserve document metadata\n",
    "\n",
    "#### ‚úÖ Pipeline Testing\n",
    "- [ ] Classification pipeline works end-to-end\n",
    "- [ ] NER pipeline works end-to-end\n",
    "- [ ] Combined pipeline works end-to-end\n",
    "- [ ] Pipelines handle various query topics\n",
    "\n",
    "#### ‚úÖ Agent Testing\n",
    "- [ ] Agent correctly interprets classification requests\n",
    "- [ ] Agent correctly interprets NER requests\n",
    "- [ ] Agent correctly interprets combined requests\n",
    "- [ ] Agent provides clear, helpful responses\n",
    "- [ ] Agent handles ambiguous queries appropriately\n",
    "\n",
    "#### ‚úÖ Deployment Testing\n",
    "- [ ] Pipelines serialize to valid YAML\n",
    "- [ ] YAML files load without errors\n",
    "- [ ] Hayhooks server starts successfully\n",
    "- [ ] All endpoints are accessible\n",
    "- [ ] Endpoints return expected results\n",
    "\n",
    "### üß™ Test Cases\n",
    "\n",
    "Run these test cases and verify results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14efdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Case 1: Simple Classification\n",
    "# TODO: Test with a Technology topic\n",
    "# Expected: Articles should be classified as \"Technology\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14484aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Case 2: Simple NER\n",
    "# TODO: Test with a person's name (e.g., \"Marie Curie\")\n",
    "# Expected: Should extract PER, ORG, LOC entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5d14ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Case 3: Combined Task\n",
    "# TODO: Test with a company name (e.g., \"Tesla\")\n",
    "# Expected: Should have both classification and entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8ae567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Case 4: Agent Intelligence\n",
    "# TODO: Test with an ambiguous query\n",
    "# Example: \"Tell me about articles on quantum computing\"\n",
    "# Expected: Agent should decide which tool(s) to use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476ee41d",
   "metadata": {},
   "source": [
    "## üéì Summary and Key Learnings\n",
    "\n",
    "### What You've Accomplished\n",
    "\n",
    "Congratulations! You've built a sophisticated multi-task NLP system that:\n",
    "\n",
    "1. ‚úÖ **Combines Multiple NLP Tasks**: Classification and entity extraction in one system\n",
    "2. ‚úÖ **Uses Custom Components**: Extended Haystack with specialized functionality\n",
    "3. ‚úÖ **Implements SuperComponents**: Created reusable pipeline wrappers\n",
    "4. ‚úÖ **Built an AI Agent**: Enabled natural language interaction with NLP pipelines\n",
    "5. ‚úÖ **Deployed as APIs**: Made pipelines accessible via REST endpoints\n",
    "\n",
    "### Key Concepts Mastered\n",
    "\n",
    "- **Component Design**: Creating custom Haystack components with proper input/output types\n",
    "- **Pipeline Composition**: Connecting multiple components into coherent workflows\n",
    "- **Metadata Management**: Enriching documents without losing information\n",
    "- **Agent Architecture**: Building intelligent systems that choose appropriate tools\n",
    "- **API Deployment**: Serializing and deploying ML pipelines as web services\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "Your system can be used for:\n",
    "- **Content Analysis Platforms**: Automatically categorize and extract key information from articles\n",
    "- **Media Monitoring**: Track mentions of entities across different content categories\n",
    "- **Research Tools**: Systematically analyze and categorize large document collections\n",
    "- **News Aggregation**: Classify articles and identify key people, organizations, and locations\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Extend your system further:\n",
    "\n",
    "1. **Add More NLP Tasks**:\n",
    "   - Sentiment analysis\n",
    "   - Summarization\n",
    "   - Question answering\n",
    "   - Relation extraction\n",
    "\n",
    "2. **Enhance Entity Processing**:\n",
    "   - Entity linking (connect to knowledge bases)\n",
    "   - Coreference resolution\n",
    "   - Entity disambiguation\n",
    "\n",
    "3. **Improve Classification**:\n",
    "   - Add hierarchical categories\n",
    "   - Support multi-label classification\n",
    "   - Fine-tune models on domain-specific data\n",
    "\n",
    "4. **Scale the System**:\n",
    "   - Add caching for repeated queries\n",
    "   - Implement batch processing\n",
    "   - Deploy on cloud infrastructure\n",
    "   - Add database storage for results\n",
    "\n",
    "5. **Enhance the Agent**:\n",
    "   - Add conversation memory\n",
    "   - Support follow-up questions\n",
    "   - Implement result filtering and refinement\n",
    "   - Add visualization tools\n",
    "\n",
    "### Resources for Further Learning\n",
    "\n",
    "- **Haystack Documentation**: https://docs.haystack.deepset.ai/\n",
    "- **Hayhooks Documentation**: https://docs.haystack.deepset.ai/docs/hayhooks\n",
    "- **HuggingFace Models**: https://huggingface.co/models\n",
    "- **NER Research**: Explore different NER models and techniques\n",
    "- **Zero-Shot Learning**: Deep dive into zero-shot classification methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113fc985",
   "metadata": {},
   "source": [
    "## üìù Reflection Questions\n",
    "\n",
    "Take a moment to reflect on your learning:\n",
    "\n",
    "1. **What was the most challenging part of this exercise?**\n",
    "   - Building custom components?\n",
    "   - Connecting pipeline components?\n",
    "   - Designing the agent system?\n",
    "   - Deploying with Hayhooks?\n",
    "\n",
    "2. **How would you modify this system for your own use case?**\n",
    "   - Different NLP tasks?\n",
    "   - Different data sources?\n",
    "   - Different classification categories?\n",
    "\n",
    "3. **What performance optimizations would you consider?**\n",
    "   - Caching strategies?\n",
    "   - Parallel processing?\n",
    "   - Model selection?\n",
    "\n",
    "4. **How would you evaluate the quality of results?**\n",
    "   - Classification accuracy?\n",
    "   - Entity extraction precision/recall?\n",
    "   - Agent response quality?\n",
    "\n",
    "Write your reflections in the markdown cell below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2896f2",
   "metadata": {},
   "source": [
    "### Your Reflections\n",
    "\n",
    "*(Add your thoughts here)*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
