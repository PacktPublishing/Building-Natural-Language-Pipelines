{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cd2911f",
   "metadata": {},
   "source": [
    "üîß **Setup Required**: Before running this notebook, please follow the [setup instructions](../README.md#setup-instructions) to configure your environment and API keys."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bbb399",
   "metadata": {},
   "source": [
    "# Building a Tool-Calling AI Agent with Haystack\n",
    "\n",
    "This tutorial shows you how to build an AI agent that can autonomously use external tools (like web search) to answer questions.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- What tool-calling agents are and why they're useful\n",
    "- How to wrap Haystack components as tools for LLMs\n",
    "- How to build a decision loop for tool selection\n",
    "- Best practices for agentic workflows\n",
    "\n",
    "## What is a Tool-Calling Agent?\n",
    "\n",
    "A tool-calling agent can:\n",
    "1. Recognize when it needs external information\n",
    "2. Request specific tools to get that information\n",
    "3. Process the tool's output\n",
    "4. Generate a final answer using the results\n",
    "\n",
    "Think of it like a researcher who knows when to search the web or use a calculator.\n",
    "\n",
    "## Pipeline Flow\n",
    "\n",
    "```\n",
    "User Question ‚Üí LLM ‚Üí Router\n",
    "                       ‚îú‚îÄ‚Üí Tool Needed? ‚Üí Execute Tool ‚Üí Back to LLM\n",
    "                       ‚îî‚îÄ‚Üí No Tool? ‚Üí Return Answer\n",
    "```\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. **Tool Definition** - Wrap functionality (web search) as a tool\n",
    "2. **Generator (LLM)** - Decides when to use tools\n",
    "3. **Router** - Routes based on whether tool calls are present\n",
    "4. **Tool Invoker** - Executes the requested tool\n",
    "5. **Message Collector** - Maintains conversation history\n",
    "\n",
    "Let's build it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfe868d",
   "metadata": {},
   "source": [
    "## Step 1: Import Components\n",
    "\n",
    "We need these Haystack components:\n",
    "\n",
    "- **Pipeline** - Container for connecting components\n",
    "- **ToolInvoker** - Executes tools requested by the LLM\n",
    "- **OpenAIChatGenerator** - LLM that decides when to use tools\n",
    "- **ConditionalRouter** - Routes messages based on tool calls\n",
    "- **SearchApiWebSearch** - Web search tool\n",
    "- **ComponentTool** - Converts components into LLM-callable tools\n",
    "- **ChatMessage** - Message structure\n",
    "\n",
    "We'll also create a **MessageCollector** to manage conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b1f7a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import component, Pipeline\n",
    "from haystack.components.tools import ToolInvoker\n",
    "from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "from haystack.components.routers import ConditionalRouter\n",
    "from haystack.components.websearch import SearchApiWebSearch\n",
    "from haystack.core.component.types import Variadic\n",
    "from haystack.dataclasses import ChatMessage\n",
    "from haystack.tools import ComponentTool\n",
    "from dotenv import load_dotenv\n",
    "from haystack.utils import Secret\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Load .env from the root of ch8 directory\n",
    "root_dir = Path(__file__).parent.parent if \"__file__\" in globals() else Path.cwd().parent\n",
    "load_dotenv(root_dir / \".env\")\n",
    "\n",
    "from typing import Any, Dict, List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a2a40e",
   "metadata": {},
   "source": [
    "## Step 2: The MessageCollector Component\n",
    "\n",
    "**Why we need it:** When the LLM requests a tool, we need to:\n",
    "1. Remember the original question\n",
    "2. Collect the tool's response\n",
    "3. Send both back to the LLM for the final answer\n",
    "\n",
    "**What it does:**\n",
    "- Stores all messages (queries, tool calls, results)\n",
    "- Combines message lists using `Variadic[List[ChatMessage]]`\n",
    "- Returns complete conversation history to the LLM\n",
    "- Can be cleared with `clear()` for fresh conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d7866e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper component to temporarily store last user query before the tool call \n",
    "@component()\n",
    "class MessageCollector:\n",
    "    def __init__(self):\n",
    "        self._messages = []\n",
    "\n",
    "    @component.output_types(messages=List[ChatMessage])\n",
    "    def run(self, messages: Variadic[List[ChatMessage]]) -> Dict[str, Any]:\n",
    "\n",
    "        self._messages.extend([msg for inner in messages for msg in inner])\n",
    "        return {\"messages\": self._messages}\n",
    "\n",
    "    def clear(self):\n",
    "        self._messages = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d121c421",
   "metadata": {},
   "source": [
    "## Step 3: Create a Web Search Tool\n",
    "\n",
    "We wrap a web search component so the LLM can use it:\n",
    "\n",
    "**ComponentTool** converts any Haystack component into a tool that:\n",
    "- The LLM can \"see\" and understand\n",
    "- The LLM can request by name when needed\n",
    "- The ToolInvoker can execute automatically\n",
    "\n",
    "**Configuration:**\n",
    "- `top_k=5` - Returns top 5 search results\n",
    "- `api_key` - Authenticates with SearchAPI\n",
    "- `allowed_domains` - Optional: restrict to specific domains\n",
    "\n",
    "The LLM will automatically call this tool when it needs current web information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94f1ad0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tool from a component\n",
    "web_tool = ComponentTool(\n",
    "    component=SearchApiWebSearch(top_k=5,\n",
    "                                api_key=Secret.from_env_var(\"SEARCH_API_KEY\"),\n",
    "                                allowed_domains=[\"https://www.britannica.com/\"])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3370ee53",
   "metadata": {},
   "source": [
    "## Step 4: Define Routing Logic\n",
    "\n",
    "The **ConditionalRouter** checks if the LLM's response contains tool calls:\n",
    "\n",
    "**Route 1: Tool Call Detected**\n",
    "- Condition: `{{replies[0].tool_calls | length > 0}}`\n",
    "- Action: Execute the tool via ToolInvoker\n",
    "- Output: `there_are_tool_calls`\n",
    "\n",
    "**Route 2: No Tool Call**\n",
    "- Condition: `{{replies[0].tool_calls | length == 0}}`\n",
    "- Action: Return the final answer\n",
    "- Output: `final_replies`\n",
    "\n",
    "**How It Works:**\n",
    "1. LLM generates a response (with or without tool calls)\n",
    "2. Router checks for tool calls\n",
    "3. If tool calls exist ‚Üí execute and loop back to LLM\n",
    "4. If no tool calls ‚Üí return the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42741162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define routing conditions\n",
    "routes = [\n",
    "    {\n",
    "        \"condition\": \"{{replies[0].tool_calls | length > 0}}\",\n",
    "        \"output\": \"{{replies}}\",\n",
    "        \"output_name\": \"there_are_tool_calls\",\n",
    "        \"output_type\": List[ChatMessage],\n",
    "    },\n",
    "    {\n",
    "        \"condition\": \"{{replies[0].tool_calls | length == 0}}\",\n",
    "        \"output\": \"{{replies}}\",\n",
    "        \"output_name\": \"final_replies\",\n",
    "        \"output_type\": List[ChatMessage], \n",
    "    },\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7098f1e",
   "metadata": {},
   "source": [
    "## Step 5: Build and Connect the Pipeline\n",
    "\n",
    "**Components:**\n",
    "1. **message_collector** - Stores conversation history\n",
    "2. **generator** - OpenAI LLM with tool access\n",
    "3. **router** - Decides to invoke tools or return answer\n",
    "4. **tool_invoker** - Executes tool calls\n",
    "\n",
    "**Connection Flow:**\n",
    "\n",
    "```\n",
    "generator ‚Üí router\n",
    "    ‚îú‚îÄ‚Üí tool_invoker (execute)\n",
    "    ‚îú‚îÄ‚Üí message_collector (store)\n",
    "    ‚Üì\n",
    "tool results ‚Üí message_collector ‚Üí generator (feedback loop)\n",
    "```\n",
    "\n",
    "**The Feedback Loop:**\n",
    "1. Generator creates tool call\n",
    "2. Tool executes and stores results\n",
    "3. Complete history goes back to Generator\n",
    "4. Generator uses results for final answer\n",
    "\n",
    "This cycle allows the agent to use tools iteratively until it can answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72fcab3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-11-17T04:57:02.739211Z\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mUnsafe mode is enabled. This allows execution of arbitrary code in the Jinja template. Use this only if you trust the source of the template.\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m199\u001b[0m \u001b[36mmodule\u001b[0m=\u001b[35mhaystack.components.routers.conditional_router\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x120696a80>\n",
       "üöÖ Components\n",
       "  - message_collector: MessageCollector\n",
       "  - generator: OpenAIChatGenerator\n",
       "  - router: ConditionalRouter\n",
       "  - tool_invoker: ToolInvoker\n",
       "üõ§Ô∏è Connections\n",
       "  - message_collector.messages -> generator.messages (List[ChatMessage])\n",
       "  - generator.replies -> router.replies (list[ChatMessage])\n",
       "  - router.there_are_tool_calls -> tool_invoker.messages (List[ChatMessage])\n",
       "  - router.there_are_tool_calls -> message_collector.messages (List[ChatMessage])\n",
       "  - tool_invoker.tool_messages -> message_collector.messages (list[ChatMessage])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the pipeline\n",
    "tool_agent = Pipeline()\n",
    "tool_agent.add_component(\"message_collector\", MessageCollector())\n",
    "tool_agent.add_component(\"generator\", OpenAIChatGenerator(model=\"gpt-4o-mini\", tools=[web_tool]))\n",
    "tool_agent.add_component(\"router\", ConditionalRouter(routes, unsafe=True))\n",
    "tool_agent.add_component(\"tool_invoker\", ToolInvoker(tools=[web_tool]))\n",
    "\n",
    "tool_agent.connect(\"generator.replies\", \"router\")\n",
    "tool_agent.connect(\"router.there_are_tool_calls\", \"tool_invoker\")\n",
    "tool_agent.connect(\"router.there_are_tool_calls\", \"message_collector\")\n",
    "tool_agent.connect(\"tool_invoker.tool_messages\", \"message_collector\")\n",
    "tool_agent.connect(\"message_collector\", \"generator.messages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dde536",
   "metadata": {},
   "source": [
    "## Step 6: Visualize the Pipeline\n",
    "\n",
    "Let's draw the pipeline to see how components connect and data flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ddc4f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_agent.draw(path=\"./images/tool_agent_pipeline.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff69bdd",
   "metadata": {},
   "source": [
    "### Pipeline Diagram\n",
    "\n",
    "![](./images/tool_agent_pipeline.png)\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "1. **Entry** - Messages enter through the generator (LLM)\n",
    "2. **Decision** - Router checks for tool calls\n",
    "3. **Tool Path** - If detected ‚Üí tool_invoker executes ‚Üí results to message_collector\n",
    "4. **Feedback Loop** - message_collector sends history back to generator\n",
    "5. **Exit** - No tool call ‚Üí final answer via router.final_replies\n",
    "\n",
    "**Notice:**\n",
    "- Circular connection enables iterative tool use\n",
    "- Router splits into two paths (tool vs. answer)\n",
    "- message_collector accumulates the full conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae688f7e",
   "metadata": {},
   "source": [
    "## Step 7: Run the Agent\n",
    "\n",
    "Let's test with a question requiring current information.\n",
    "\n",
    "**Messages:**\n",
    "1. System: \"You're a helpful agent choosing the right tool when necessary\"\n",
    "2. User: \"How is the weather in Berlin?\"\n",
    "\n",
    "**Expected Flow:**\n",
    "1. LLM recognizes need for current data\n",
    "2. Generates web search tool call\n",
    "3. Router detects tool call ‚Üí routes to tool_invoker\n",
    "4. Web search executes and returns results\n",
    "5. MessageCollector combines everything\n",
    "6. LLM generates final answer using search results\n",
    "7. Router returns the answer\n",
    "\n",
    "Let's see it work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b818d429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The search did not provide specific current weather information for Berlin. However, you can check reliable weather websites or apps for the most accurate and updated weather conditions. Would you like me to search again or provide guidance on where to look?\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    ChatMessage.from_system(\"You're a helpful agent choosing the right tool when necessary\"), \n",
    "    ChatMessage.from_user(\"How is the weather in Berlin?\")]\n",
    "result = tool_agent.run({\"messages\": messages})\n",
    "\n",
    "print(result[\"router\"][\"final_replies\"][0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149aa7e4",
   "metadata": {},
   "source": [
    "## Understanding the Output\n",
    "\n",
    "The agent successfully:\n",
    "1. ‚úÖ Recognized it needed current information\n",
    "2. ‚úÖ Called the web search tool automatically\n",
    "3. ‚úÖ Retrieved search results\n",
    "4. ‚úÖ Created a natural language answer\n",
    "\n",
    "**Behind the Scenes:**\n",
    "```\n",
    "User asks question\n",
    "‚Üí LLM: \"I need current data, searching web\"\n",
    "‚Üí Router detects tool call\n",
    "‚Üí Tool executes web search\n",
    "‚Üí Results collected\n",
    "‚Üí LLM: \"Based on search, here's the weather...\"\n",
    "‚Üí Router returns final answer\n",
    "```\n",
    "\n",
    "**Key Points:**\n",
    "- **Autonomous** - Agent decided to use the tool\n",
    "- **Iterative** - Looped through components\n",
    "- **Context-Aware** - Combined search results with understanding\n",
    "- **Transparent** - User just got an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcba248d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Additional Examples and Experiments\n",
    "# ============================================================================\n",
    "\n",
    "# Example 1: Question that doesn't need a tool\n",
    "print(\"=\"*80)\n",
    "print(\"Example 1: Simple factual question\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "messages_simple = [\n",
    "    ChatMessage.from_system(\"You're a helpful agent choosing the right tool when necessary\"),\n",
    "    ChatMessage.from_user(\"What is 25 + 37?\")\n",
    "]\n",
    "result_simple = tool_agent.run({\"messages\": messages_simple})\n",
    "print(f\"Question: What is 25 + 37?\")\n",
    "print(f\"Answer: {result_simple['router']['final_replies'][0].text}\")\n",
    "print()\n",
    "\n",
    "# Example 2: Question that needs current information\n",
    "print(\"=\"*80)\n",
    "print(\"Example 2: Current events question\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "messages_current = [\n",
    "    ChatMessage.from_system(\"You're a helpful agent choosing the right tool when necessary\"),\n",
    "    ChatMessage.from_user(\"What are the latest developments in AI technology?\")\n",
    "]\n",
    "result_current = tool_agent.run({\"messages\": messages_current})\n",
    "print(f\"Question: What are the latest developments in AI technology?\")\n",
    "print(f\"Answer: {result_current['router']['final_replies'][0].text}\")\n",
    "print()\n",
    "\n",
    "# Note: Clear message collector between runs if needed for fresh context\n",
    "# tool_agent.get_component(\"message_collector\").clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0c5419",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### What We Built\n",
    "\n",
    "A **tool-calling agent** with:\n",
    "1. Autonomous decision-making\n",
    "2. Tool integration (web search)\n",
    "3. Iterative processing\n",
    "4. Conversation memory\n",
    "\n",
    "### When to Use Tool-Calling Agents\n",
    "\n",
    "**Good for:**\n",
    "- Questions requiring current/external information\n",
    "- Tasks needing calculations or specialized processing\n",
    "- Multi-step reasoning with data retrieval\n",
    "\n",
    "**Not ideal for:**\n",
    "- Simple Q&A (LLM already knows)\n",
    "- High-speed requirements (tool calls add latency)\n",
    "- Fully deterministic workflows\n",
    "\n",
    "### Try These Extensions\n",
    "\n",
    "1. Add more tools (calculator, database, code execution)\n",
    "2. Multi-turn conversations with persistent history\n",
    "3. Tool selection logic and conditions\n",
    "4. Fallback handling for failed tools\n",
    "5. Cost optimization (track LLM calls)\n",
    "\n",
    "### Manual vs Agent Component\n",
    "\n",
    "**Manual (this notebook):**\n",
    "- ‚úÖ Full control over routing\n",
    "- ‚úÖ Understand every step\n",
    "- ‚ùå More code to maintain\n",
    "\n",
    "**Agent Component:**\n",
    "- ‚úÖ Less boilerplate\n",
    "- ‚úÖ Built-in error handling\n",
    "- ‚ùå Less control\n",
    "\n",
    "Choose based on your needs!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-with-haystack-ch8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
